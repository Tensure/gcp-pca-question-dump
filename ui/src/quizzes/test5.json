{
    "count": 70,
    "next": null,
    "previous": null,
    "results": [
        {
            "_class": "assessment",
            "id": 81297176,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you want to control expenses and you want to be automatically informed about project expenses so that you can take action when you get close to your limit. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Create a budget alert for the appropriate levels for your total monthly budget (for example: 50%, 90%, 100%). -&gt; Correct. This option is correct because creating a budget alert can help control expenses by monitoring project spending and alerting when certain thresholds are reached. Budget alerts can be set up for total monthly spending or for specific services and can be set up to trigger at different levels to allow for proactive action to be taken to avoid overspending.</p><p><br></p><p>Set up a credit card with a monthly limit equal to your budget. -&gt; Incorrect. While this option can help control expenses, it doesn't provide automatic notification when project expenses are nearing the limit. This option also doesn't provide the same level of granularity as budget alerts and may require manual monitoring to ensure expenses are within the monthly limit.</p><p><br></p><p>Set up a PayPal account with a monthly limit equal to your budget. -&gt; Incorrect. This option doesn't provide automatic notification when project expenses are nearing the limit and doesn't offer the same level of granularity as budget alerts.</p><p><br></p><p>You can't automatically control your Google Cloud expenses. -&gt; Incorrect. This option is incorrect because Google Cloud provides several tools, such as budget alerts and quotas, to help control expenses and be automatically informed about project expenses.</p><p><br></p><p>https://cloud.google.com/billing/docs/how-to/budgets</p>",
                "answers": [
                    "<p>Create a budget alert for the appropriate levels for your total monthly budget (for example: 50%, 90%, 100%).</p>",
                    "<p>Set up a credit card with a monthly limit equal to your budget.</p>",
                    "<p>Set up a PayPal account with a monthly limit equal to your budget.</p>",
                    "<p>You can't automatically control your Google Cloud expenses.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you want to control expenses and you want to be automatically informed about project expenses so that you can take action when you get close to your limit. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297178,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you advise the development team. They have a new application and want to deploy it to a production environment. You need to estimate the costs of running this application in App Engine. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use the Google Cloud Price Calculator to accurately estimate expected expenses. -&gt; Correct. The Google Cloud Pricing Calculator provides an accurate estimate of the expected expenses of running an application in App Engine. The calculator takes into account factors such as the type and number of instances, storage, network usage, and other usage metrics that can impact cost.</p><p><br></p><p>Create a ticket for Cloud Billing Support to get an estimate. -&gt;&nbsp;Incorrect. Creating a ticket for Cloud Billing Support to get an estimate may not be the most efficient method for estimating costs. While Cloud Billing Support can provide assistance with billing-related inquiries, it may take time to receive a response, which could delay the estimation process.</p><p><br></p><p>Calculate costs based on the current price list. -&gt;&nbsp;Incorrect. Calculating costs based on the current price list can give you a general idea of the pricing structure for App Engine, but it may not provide an accurate estimate tailored to the specific requirements of the application. The price list typically provides the cost of each resource individually, and calculating the total costs based on these rates may not consider the application's specific usage patterns and requirements.</p><p><br></p><p>Calculate costs based on the expenses incurred during the development stage. -&gt;&nbsp;Incorrect. It may not provide an accurate estimate for the production environment. The development stage expenses may not reflect the actual resource usage, traffic, and other factors that could influence the costs in a production environment. It's important to consider the differences between development and production environments when estimating costs.</p><p><br></p><p>https://cloud.google.com/products/calculator</p>",
                "answers": [
                    "<p>Use the Google Cloud Price Calculator to accurately estimate expected expenses.</p>",
                    "<p>Create a ticket for Cloud Billing Support to get an estimate.</p>",
                    "<p>Calculate costs based on the current price list.</p>",
                    "<p>Calculate costs based on the expenses incurred during the development stage.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you advise the development team. They have a new application and want to deploy it to a production environment. You need to estimate the costs of running this application in App Engine. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297180,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>In a weather forecasting project where a company needs to process large amounts of time-stamped IoT data at high speed, which Google Cloud product should you recommend for efficient data write and change operations? Choose the correct option from the five given answers.</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Bigtable -&gt; Correct. Cloud Bigtable is the recommended product for efficiently handling high-speed data write and change operations, especially when dealing with large amounts of time-stamped IoT data. It is a NoSQL wide-column database that provides high scalability, low-latency reads and writes, and automatic scaling to handle petabytes of data.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. Cloud Datastore is a NoSQL document database and may not provide the same level of performance and scalability as Cloud Bigtable for high-speed data write and change operations.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. Cloud Storage is a scalable object storage solution but may not be optimized for high-speed data write and change operations, especially for time-stamped IoT data.</p><p><br></p><p>Cloud Pub/Sub -&gt; Incorrect. Cloud Pub/Sub is a messaging service for building event-driven systems and is not specifically designed for high-speed data write and change operations.</p>",
                "answers": [
                    "<p>Cloud Bigtable</p>",
                    "<p>Cloud Datastore</p>",
                    "<p>Cloud Storage</p>",
                    "<p>Cloud Pub/Sub</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "In a weather forecasting project where a company needs to process large amounts of time-stamped IoT data at high speed, which Google Cloud product should you recommend for efficient data write and change operations? Choose the correct option from the five given answers.",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297182,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>In a scenario where an application has a large international user group and runs stateless virtual machines in a Managed Instance Group across multiple Google Cloud locations, which storage solution is recommended for storing and analyzing large volumes of log data generated by the application? Choose the correct option from the five given answers.</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Logging with Cloud Storage export -&gt; Correct. Cloud Logging with Cloud Storage export is the recommended storage solution for storing and analyzing large volumes of log data. Cloud Logging allows you to centralize and aggregate logs generated by the application, while the export feature enables you to export logs to Cloud Storage for long-term storage or further analysis using other tools.</p><p><br></p><p>Persistent SSD on virtual machine instances -&gt; Incorrect. Persistent SSD on virtual machine instances is not suitable for storing and analyzing large volumes of log data. It is more suitable for fast and reliable storage of application-specific data within the virtual machine instances themselves.</p><p><br></p><p>Cloud Memorystore for Redis -&gt; Incorrect. Cloud Memorystore for Redis is an in-memory caching service and is not specifically designed for storing and analyzing log data. It is better suited for caching frequently accessed data to improve application performance.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. Cloud Datastore is a NoSQL document database and is not the ideal solution for storing and analyzing log data at scale.</p>",
                "answers": [
                    "<p>Cloud Logging with Cloud Storage export</p>",
                    "<p>Persistent SSD on virtual machine instances</p>",
                    "<p>Cloud Memorystore for Redis</p>",
                    "<p>Cloud Datastore</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "In a scenario where an application has a large international user group and runs stateless virtual machines in a Managed Instance Group across multiple Google Cloud locations, which storage solution is recommended for storing and analyzing large volumes of log data generated by the application? Choose the correct option from the five given answers.",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297184,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A development team needs to deploy a web application that will scale based on HTTP traffic. They have an instance template that contains this web application. What should you advise them?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>They should create a Managed Instance Group based on the instance template. Then configure autoscaling based on HTTP traffic and configure the instance group as the backend service of an HTTP load balancer. -&gt; Correct. This option meets the requirements by automatically scaling the number of instances in response to HTTP traffic, and the HTTP load balancer distributes traffic evenly among the instances.</p><p><br></p><p>They should create a virtual machine from the instance template. Then create an App Engine application in Automatic Scaling mode that forwards all traffic to this virtual machine. -&gt; Incorrect. This option is not ideal as it adds unnecessary complexity by involving App Engine, which is designed to work with its own scalable environment.</p><p><br></p><p>They should create the necessary number of instances required for peak traffic based on the instance template. -&gt; Incorrect. This option doesn't meet the requirement for automatic scaling, and the development team would need to manually manage the number of instances, which is not an efficient solution.</p><p><br></p><p>They should create a Managed Instance Group based on the instance template. Then configure autoscaling based on CPU utilization. -&gt; Incorrect. This option doesn't meet the requirement for scaling based on HTTP traffic, which is what the development team needs.</p><p><br></p><p>https://cloud.google.com/compute/docs/autoscaler</p>",
                "answers": [
                    "<p>They should create a Managed Instance Group based on the instance template. Then configure autoscaling based on HTTP traffic and configure the instance group as the backend service of an HTTP load balancer.</p>",
                    "<p>They should create a virtual machine from the instance template. Then create an App Engine application in Automatic Scaling mode that forwards all traffic to this virtual machine.</p>",
                    "<p>They should create the necessary number of instances required for peak traffic based on the instance template.</p>",
                    "<p>They should create a Managed Instance Group based on the instance template. Then configure autoscaling based on CPU utilization.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A development team needs to deploy a web application that will scale based on HTTP traffic. They have an instance template that contains this web application. What should you advise them?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297186,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A development team needs to create a Kubernetes Engine cluster to deploy multiple pods and use BigQuery to store all container logs for later analysis. What solution can you advise to follow Google's best practices?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>They should enable Cloud Logging when creating a Kubernetes Engine cluster. -&gt; Correct. This is the recommended approach as Cloud Logging provides a centralized location for logging across multiple Google Cloud Platform services, including Kubernetes Engine. Cloud Logging automatically collects logs generated by the Kubernetes Engine cluster and stores them in a single location. This makes it easy to search, analyze, and troubleshoot logs from multiple sources.</p><p><br></p><p>They should enable Cloud Monitoring when creating a Kubernetes Engine cluster. -&gt; Incorrect. While Cloud Monitoring can be used to monitor Kubernetes Engine clusters, it doesn't provide a solution for storing all container logs in BigQuery for later analysis.</p><p><br></p><p>They should use the Cloud Logging export feature to create a sink to Cloud Storage, than create a Cloud Dataflow job that imports log files from Cloud Storage to BigQuery. -&gt; Incorrect. This option is not necessary as Cloud Logging can already export logs to BigQuery.</p><p><br></p><p>The only solution is to develop a custom add-on that uses the Cloud Logging API and BigQuery API. -&gt; Incorrect. While this option is technically feasible, it requires additional development effort and maintenance, which is not necessary as there is an existing, recommended solution provided in the correct option.</p><p><br></p><p>https://cloud.google.com/logging/docs/quickstart</p>",
                "answers": [
                    "<p>They should enable Cloud Logging when creating a Kubernetes Engine cluster.</p>",
                    "<p>They should enable Cloud Monitoring when creating a Kubernetes Engine cluster.</p>",
                    "<p>They should use the Cloud Logging export feature to create a sink to Cloud Storage, than create a Cloud Dataflow job that imports log files from Cloud Storage to BigQuery.</p>",
                    "<p>The only solution is to develop a custom add-on that uses the Cloud Logging API and BigQuery API.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A development team needs to create a Kubernetes Engine cluster to deploy multiple pods and use BigQuery to store all container logs for later analysis. What solution can you advise to follow Google's best practices?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297188,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect responsible for a microservices-based application deployed in Google Kubernetes Engine (GKE). The application consists of several interacting containers. To optimize performance, you want to ensure that these containers are located as close to each other as possible. Which of the following would be the most effective way to achieve this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use the <code>PodAffinity</code> and <code>PodAntiAffinity</code> rules in your Kubernetes pod specification. -&gt; Correct. The <code>PodAffinity</code> and <code>PodAntiAffinity</code> rules are designed to control how Kubernetes schedules pods relative to each other. By properly configuring these rules, you can make sure that certain pods (and therefore the containers inside them) are co-located on the same node or within the same zone.</p><p><br></p><p>Use node taints and tolerations to force certain containers to run on specific nodes. -&gt; Incorrect. Node taints and tolerations are used to restrict the kinds of pods that can be scheduled on specific nodes, but they are not the best way to ensure that specific containers are co-located.</p><p><br></p><p>Use GKE Regional Clusters to deploy the containers across multiple zones in the same region. -&gt; Incorrect. GKE Regional Clusters are used to increase the availability of your application by deploying across multiple zones in the same region. This strategy might spread your containers further apart, not bring them closer together.</p><p><br></p><p>Increase the number of replicas for each container to ensure they are distributed across all nodes in the cluster. -&gt;&nbsp;Incorrect. Increasing the number of replicas can improve the availability and performance of your application by distributing the workload across more containers. However, it does not ensure that specific containers are co-located.</p>",
                "answers": [
                    "<p>Use the <code>PodAffinity</code> and <code>PodAntiAffinity</code> rules in your Kubernetes pod specification.</p>",
                    "<p>Use node taints and tolerations to force certain containers to run on specific nodes.</p>",
                    "<p>Use GKE Regional Clusters to deploy the containers across multiple zones in the same region.</p>",
                    "<p>Increase the number of replicas for each container to ensure they are distributed across all nodes in the cluster.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect responsible for a microservices-based application deployed in Google Kubernetes Engine (GKE). The application consists of several interacting containers. To optimize performance, you want to ensure that these containers are located as close to each other as possible. Which of the following would be the most effective way to achieve this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297190,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>In an e-commerce project where a customer wants to balance traffic between backend virtual machines in a multi-tier application, which load balancing option should they choose? Choose the correct option from the five given answers.</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>The regional internal load balancer -&gt; Correct. The regional internal load balancer is the recommended choice for balancing traffic between backend virtual machines in a multi-tier application within a specific region. It ensures that traffic is distributed evenly across the virtual machines to optimize performance and avoid overloading any specific instance.</p><p><br></p><p>The global TCP proxy -&gt; Incorrect. The global TCP proxy load balancer is designed for distributing TCP traffic globally and is not specifically focused on load balancing traffic between backend virtual machines in a single region.</p><p><br></p><p>The global SSL proxy -&gt; Incorrect. The global SSL proxy load balancer is designed for distributing SSL traffic globally and is not specifically focused on load balancing traffic between backend virtual machines in a single region.</p><p><br></p><p>The network load balancer -&gt; Incorrect. The network load balancer is designed for load balancing traffic at the network level and is not specifically focused on load balancing traffic between backend virtual machines in a multi-tier application.</p>",
                "answers": [
                    "<p>The regional internal load balancer</p>",
                    "<p>The global TCP proxy</p>",
                    "<p>The global SSL proxy</p>",
                    "<p>The network load balancer</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "In an e-commerce project where a customer wants to balance traffic between backend virtual machines in a multi-tier application, which load balancing option should they choose? Choose the correct option from the five given answers.",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297192,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You don't know how long your data will be stored in the Google Cloud bucket and you are currently using Standard storage class. Which bucket feature you can use to switch storage class for objects when they reach or pass a certain age (for example 30 days)?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Object lifecycle management rules. -&gt; Correct. Object lifecycle management rules can be used to switch the storage class for objects in a Google Cloud bucket based on certain conditions, such as their age. With lifecycle management rules, you can define actions to be taken on objects after a specific duration, like transitioning them to a different storage class or deleting them. In this case, you can set a rule to switch the storage class for objects when they reach or pass a certain age, such as 30 days.</p><p><br></p><p>Object versioning setting. -&gt;&nbsp;Incorrect. Object versioning setting in Google Cloud Storage enables the storage and retrieval of multiple versions of an object. However, it does not directly address the need to switch the storage class based on the age of the objects.</p><p><br></p><p>Object permissions. -&gt;&nbsp;Incorrect. Object permissions determine who has access to the objects and what actions they can perform on them. While object permissions are essential for controlling access to the objects, they do not provide a feature for automatically switching the storage class based on the age of the objects.</p><p><br></p><p>Object protection. -&gt;&nbsp;Incorrect. Object protection typically refers to mechanisms for securing objects from unauthorized access or accidental deletion. While object protection is important for maintaining data integrity and security, it does not provide the functionality to switch the storage class based on the age of the objects.</p><p><br></p><p>https://cloud.google.com/storage/docs/lifecycle</p>",
                "answers": [
                    "<p>Object lifecycle management rules.</p>",
                    "<p>Object versioning setting.</p>",
                    "<p>Object permissions.</p>",
                    "<p>Object protection.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You don't know how long your data will be stored in the Google Cloud bucket and you are currently using Standard storage class. Which bucket feature you can use to switch storage class for objects when they reach or pass a certain age (for example 30 days)?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297194,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is developing a mobile game application that is expected to have millions of users worldwide. The application needs to read and write user data quickly with low latency, and the data model is fairly simple. As the cloud architect, you have been tasked with choosing the most suitable NoSQL database for this requirement. Which of the following would be the most appropriate choice?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Firestore in Datastore mode. -&gt; Correct. Firestore in Datastore mode is optimized for server-side applications that require massive scale and have a fairly simple data model, making it the best fit for this use case.</p><p><br></p><p>Use Cloud Bigtable. -&gt; Incorrect. Cloud Bigtable is designed for large analytical and operational workloads. However, for mobile and web applications where the data model is fairly simple, Firestore in Datastore mode is typically a more suitable choice.</p><p><br></p><p>Use Cloud SQL. -&gt; Incorrect. Cloud SQL is a fully managed relational database service, while the requirement here is for a NoSQL database. Thus, it doesn't fit the requirement.</p><p><br></p><p>Use Cloud Spanner. -&gt; Incorrect. Cloud Spanner is a fully managed relational database that provides strong consistency at a global scale. It is overkill for this scenario where the requirement is a simple NoSQL database.</p>",
                "answers": [
                    "<p>Use Cloud Firestore in Datastore mode.</p>",
                    "<p>Use Cloud Bigtable.</p>",
                    "<p>Use Cloud SQL.</p>",
                    "<p>Use Cloud Spanner.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company is developing a mobile game application that is expected to have millions of users worldwide. The application needs to read and write user data quickly with low latency, and the data model is fairly simple. As the cloud architect, you have been tasked with choosing the most suitable NoSQL database for this requirement. Which of the following would be the most appropriate choice?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297196,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company has implemented a microservices architecture on Google Cloud and needs to streamline their deployment processes. They want to implement a Continuous Integration/Continuous Deployment (CI/CD) pipeline. What is the most appropriate action to take?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Utilize Google Cloud Source Repositories and set up triggers to automatically deploy code to App Engine. -&gt;&nbsp;Correct. This approach makes use of Cloud Source Repositories for version control and deploys the applications to App Engine when changes are made. This is the most suitable option for setting up a CI/CD pipeline.</p><p><br></p><p>Use Cloud Functions to automate the deployment of services. -&gt;&nbsp;Incorrect. While Cloud Functions can be used to automate certain processes, they aren't designed for managing an entire CI/CD pipeline.</p><p><br></p><p>Use Cloud Scheduler to schedule deployments. -&gt;&nbsp;Incorrect. Cloud Scheduler is more for scheduling tasks based on a timer, rather than reacting to events, such as a code push. It wouldn't be the best choice for a CI/CD pipeline.</p><p><br></p><p>Manually deploy code to Compute Engine instances. -&gt;&nbsp;Incorrect. Manual deployments would not allow for continuous integration and continuous deployment, defeating the purpose of a CI/CD pipeline.</p>",
                "answers": [
                    "<p>Utilize Google Cloud Source Repositories and set up triggers to automatically deploy code to App Engine.</p>",
                    "<p>Use Cloud Functions to automate the deployment of services.</p>",
                    "<p>Use Cloud Scheduler to schedule deployments.</p>",
                    "<p>Manually deploy code to Compute Engine instances.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company has implemented a microservices architecture on Google Cloud and needs to streamline their deployment processes. They want to implement a Continuous Integration/Continuous Deployment (CI/CD) pipeline. What is the most appropriate action to take?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297198,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You're a cloud architect of a technology company that is transitioning its existing monolithic application to a microservices architecture and intends to deploy these services as containerized applications on Google Cloud. The application must be highly available, scalable, and capable of rolling updates without downtime. Which of the following solutions should you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Google Kubernetes Engine (GKE) -&gt; Correct. GKE is a managed, production-ready environment for deploying containerized applications. It offers high availability, scalability, and supports rolling updates without downtime. This makes it a perfect choice for a complex microservices-based application.</p><p><br></p><p>Compute Engine instances with Docker installed -&gt; Incorrect. Deploying containers on Compute Engine instances would not provide the orchestration capabilities such as automatic scaling, self-healing, and rolling updates that Kubernetes provides. Hence, this would not be the best solution in this scenario.</p><p><br></p><p>Cloud Functions -&gt; Incorrect. Cloud Functions is a serverless execution environment for building and connecting cloud services. While it does offer scalability, it does not support containerized applications and is more suitable for smaller, discrete tasks triggered by events.</p><p><br></p><p>Cloud Run -&gt; Incorrect. While Cloud Run is a great option for running containerized applications and does provide automatic scaling and high availability, it doesn't offer the same level of control over the infrastructure and configuration settings that GKE does. For more complex microservice architectures, GKE would be a better fit.</p>",
                "answers": [
                    "<p>Google Kubernetes Engine (GKE)</p>",
                    "<p>Compute Engine instances with Docker installed</p>",
                    "<p>Cloud Functions</p>",
                    "<p>Cloud Run</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You're a cloud architect of a technology company that is transitioning its existing monolithic application to a microservices architecture and intends to deploy these services as containerized applications on Google Cloud. The application must be highly available, scalable, and capable of rolling updates without downtime. Which of the following solutions should you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297200,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect of a firm, you need to implement a solution that regularly runs a batch job to transfer data from your company's CRM system to a BigQuery dataset. The transfer involves substantial data transformation. Considering cost optimization and the non-urgent, fault-tolerant nature of the task, you have opted to use preemptible VMs for this task. Which of the following would be the best approach?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Composer to orchestrate the data transfer and transformation job using a preemptible Compute Engine instance. -&gt; Correct. Cloud Composer, which is a managed Apache Airflow service, is designed to orchestrate complex workflows, making it a good fit for this scenario. It can leverage preemptible VMs to control costs.</p><p><br></p><p>Create a Compute Engine instance with the necessary transformation scripts, schedule it to run at regular intervals using Cloud Scheduler, and use a regular (non-preemptible) VM instance. -&gt; Incorrect. This option does not make use of preemptible VMs and may result in higher costs, which contradicts the requirements.</p><p><br></p><p>Create a Kubernetes Engine cluster with preemptible VMs and schedule the batch jobs using Kubernetes cron jobs. -&gt; Incorrect. While GKE with preemptible VMs could technically accomplish the task, the added complexity of managing a Kubernetes cluster might not be necessary if you're just running batch jobs. Cloud Composer would be a simpler and more manageable option.</p><p><br></p><p>Use Dataproc to run the transformation job on a preemptible VM cluster. -&gt; Incorrect. Dataproc is used for running big data tasks like Hadoop and Spark. It would be an overkill for this scenario where we are just transferring and transforming data to BigQuery.</p>",
                "answers": [
                    "<p>Use Cloud Composer to orchestrate the data transfer and transformation job using a preemptible Compute Engine instance.</p>",
                    "<p>Create a Compute Engine instance with the necessary transformation scripts, schedule it to run at regular intervals using Cloud Scheduler, and use a regular (non-preemptible) VM instance.</p>",
                    "<p>Create a Kubernetes Engine cluster with preemptible VMs and schedule the batch jobs using Kubernetes cron jobs.</p>",
                    "<p>Use Dataproc to run the transformation job on a preemptible VM cluster.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect of a firm, you need to implement a solution that regularly runs a batch job to transfer data from your company's CRM system to a BigQuery dataset. The transfer involves substantial data transformation. Considering cost optimization and the non-urgent, fault-tolerant nature of the task, you have opted to use preemptible VMs for this task. Which of the following would be the best approach?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297202,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company's development team is using a monorepo for their codebase. They need to set up a CI/CD pipeline that is capable of triggering a build only when changes are made to a certain directory in the repo. Which option below will achieve this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Utilize Cloud Build and create a trigger that filters on file changes within the desired directory. -&gt;&nbsp;Correct. Cloud Build supports using a regular expression to match file paths within your source repository when configuring a build trigger. This way, you can specify which directory changes will trigger the build.</p><p><br></p><p>Use Google Cloud Functions to monitor the Git repo and manually trigger a build process when changes are detected. -&gt;&nbsp;Incorrect. This approach is less efficient and more complex than simply using Cloud Build, and it involves maintaining an extra piece of infrastructure.</p><p><br></p><p>Use Cloud Scheduler to execute the build process at a specific time regardless of the changes. -&gt;&nbsp;Incorrect. While Cloud Scheduler can help execute scheduled tasks, it does not provide a solution for triggering builds based on changes to a specific directory.</p><p><br></p><p>Use Google Cloud Storage to store the repo and set up object change notifications. -&gt;&nbsp;Incorrect. This approach is not practical. Git repositories are better managed using dedicated source control tools, not object storage services.</p>",
                "answers": [
                    "<p>Utilize Cloud Build and create a trigger that filters on file changes within the desired directory.</p>",
                    "<p>Use Google Cloud Functions to monitor the Git repo and manually trigger a build process when changes are detected.</p>",
                    "<p>Use Cloud Scheduler to execute the build process at a specific time regardless of the changes.</p>",
                    "<p>Use Google Cloud Storage to store the repo and set up object change notifications.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company's development team is using a monorepo for their codebase. They need to set up a CI/CD pipeline that is capable of triggering a build only when changes are made to a certain directory in the repo. Which option below will achieve this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297204,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A mission-critical application is migrated to Google Kubernetes Engine from your on-premises data center and uses <code>e2-standard-4</code> machine types. How can your development team deploy additional pods on <code>e2-standard-32</code> machine types without causing application downtime?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>They should create a new cluster with two node pools - one with <code>e2-standard-4</code> machine types and other with <code>e2-standard-32</code> machine types. Then deploy the application on this new cluster and remove the older one. -&gt; Correct. Creating a new cluster with two node pools, one with <code>e2-standard-4</code> machine types and the other with <code>e2-standard-32</code> machine types, allows the development team to deploy additional pods on the new node pool without causing application downtime. They can deploy the application on the new cluster and then remove the older one.</p><p><br></p><p>They should update the existing cluster to add a new node pool with <code>e2-standard-32</code> machine types and deploy the pods. -&gt; Incorrect. Updating the existing cluster to add a new node pool with <code>e2-standard-32</code> machine types could cause application downtime if the pods need to be rescheduled to the new node pool.</p><p><br></p><p>Your development team cannot deploy additional pods on <code>e2-standard-32</code> machine types, as this will cause application downtime. -&gt; incorrect. It suggests that it is not possible to deploy additional pods on <code>e2-standard-32</code> machine types without causing application downtime, which is not true.</p><p><br></p><p>They should create a new cluster with node pool instances with <code>e2-standard-32</code> machine types. Then deploy the application on the new cluster and remove the older one. -&gt; Incorrect. Creating a new cluster with node pool instances with <code>e2-standard-32</code> machine types requires more effort and would not be the most efficient solution.</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools</p>",
                "answers": [
                    "<p>They should create a new cluster with two node pools - one with <code>e2-standard-4</code> machine types and other with <code>e2-standard-32</code> machine types. Then deploy the application on this new cluster and remove the older one.</p>",
                    "<p>They should update the existing cluster to add a new node pool with <code>e2-standard-32</code> machine types and deploy the pods.</p>",
                    "<p>They should create a new cluster with node pool instances with <code>e2-standard-32</code> machine types. Then deploy the application on the new cluster and remove the older one.</p>",
                    "<p>Your development team cannot deploy additional pods on <code>e2-standard-32</code> machine types, as this will cause application downtime.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A mission-critical application is migrated to Google Kubernetes Engine from your on-premises data center and uses e2-standard-4 machine types. How can your development team deploy additional pods on e2-standard-32 machine types without causing application downtime?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297206,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>An internal company application is deployed with Compute Engine VMs. This application is used only during regular business hours. Your development team needs to backup the VMs outside the business hours and remove images older than 30 days to reduce expenses. As a cloud architect, what should you advise them?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>They should enable a snapshot schedule for automated creation of daily snapshots and set snapshot retention policy to 30 days. -&gt; Correct. Enabling a snapshot schedule for automated creation of daily snapshots and setting the snapshot retention policy to 30 days is the recommended approach. Compute Engine provides built-in snapshot functionality that allows for automated, regular backups of VM disks. By enabling the snapshot schedule and setting the retention policy to 30 days, the development team can ensure that backups are taken outside business hours and older images are automatically removed after the specified period, reducing expenses.</p><p><br></p><p>They should add three metadata tags on the Compute Engine instance&nbsp;(enabling snapshot creation, specifying the snapshot schedule, specifying the retention period = 30 days). -&gt;&nbsp;Incorrect. Compute Engine metadata tags are used for attaching additional information to instances but do not directly provide the backup and retention functionality required in this scenario.</p><p><br></p><p>They should use Cloud Scheduler to trigger a Cloud Function that creates snapshots of the disk on a daily basis. Also they should use Cloud Scheduler to trigger another Cloud Function that iterates over the snapshots and removes older than 30 days. -&gt;&nbsp;Incorrect. It introduces additional complexity compared to the built-in snapshot functionality provided by Compute Engine. It may not be the most straightforward and efficient solution for achieving the desired backup and retention tasks.</p><p><br></p><p>They should use AppEngine Cron service to trigger a custom script that creates snapshots of the disk on a daily basis. Also they should use AppEngine Cron service to trigger another custom script that iterates over the snapshots and removes snapshots older than 30 days. -&gt;&nbsp;Incorrect. App Engine is designed for building and running web applications and may not provide the same level of integration and functionality as Compute Engine for VM backups and retention.</p><p><br></p><p>https://cloud.google.com/compute/docs/disks/scheduled-snapshots</p>",
                "answers": [
                    "<p>They should enable a snapshot schedule for automated creation of daily snapshots and set snapshot retention policy to 30 days.</p>",
                    "<p>They should add three metadata tags on the Compute Engine instance&nbsp;(enabling snapshot creation, specifying the snapshot schedule, specifying the retention period = 30 days).</p>",
                    "<p>They should use Cloud Scheduler to trigger a Cloud Function that creates snapshots of the disk on a daily basis. Also they should use Cloud Scheduler to trigger another Cloud Function that iterates over the snapshots and removes older than 30 days.</p>",
                    "<p>They should use AppEngine Cron service to trigger a custom script that creates snapshots of the disk on a daily basis. Also they should use AppEngine Cron service to trigger another custom script that iterates over the snapshots and removes snapshots older than 30 days.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "An internal company application is deployed with Compute Engine VMs. This application is used only during regular business hours. Your development team needs to backup the VMs outside the business hours and remove images older than 30 days to reduce expenses. As a cloud architect, what should you advise them?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297208,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Within your company, each developer possesses an individual development Google Cloud Platform (GCP) project associated with a central billing account. You have recommended that they establish alerts for any situations where a developer exceeds a monthly expenditure of $500. What actions should they take to implement these alerts?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>They should set up a budget for each development projects. Then, set an alert for each budget when expenses exceed $ 500. -&gt; Correct. Setting up a budget for each development project and then setting an alert for each budget when expenses exceed $500 is the recommended approach. By creating separate budgets for each project, developers can track their individual expenses and receive alerts when they exceed the defined threshold. This ensures that each developer is notified when their expenditures reach or surpass the specified limit.</p><p><br></p><p>Export billing data from all development projects to a single BigQuery dataset. Use a Data Studio dashboard to plot expenses. -&gt;&nbsp;Incorrect. It does not provide a straightforward way to set up individual alerts for each developer's project when expenses exceed $500. It focuses more on analyzing expenses collectively rather than monitoring individual project budgets.</p><p><br></p><p>They should set up a single budget for all development projects. Then, set an alert for budget when expenses exceed $ 500. -&gt;&nbsp;Incorrect. It is not the most suitable solution for this scenario. Since each developer has their own individual project associated with a central billing account, it is more appropriate to have separate budgets and alerts for each project to monitor their specific expenditures.</p><p><br></p><p>They should set up a single budget for all development projects. Then, set an alert for budget when expenses exceed $ 500 multiplied by the number of developers. -&gt;&nbsp;Incorrect. It is not necessary or efficient. Each developer's project should be monitored individually, and setting up a separate budget and alert for each project is more appropriate than multiplying the budget threshold by the number of developers.</p><p><br></p><p>https://cloud.google.com/billing/docs/how-to/budgets</p>",
                "answers": [
                    "<p>They should set up a budget for each development projects. Then, set an alert for each budget when expenses exceed $ 500.</p>",
                    "<p>Export billing data from all development projects to a single BigQuery dataset. Use a Data Studio dashboard to plot expenses.</p>",
                    "<p>They should set up a single budget for all development projects. Then, set an alert for budget when expenses exceed $ 500.</p>",
                    "<p>They should set up a single budget for all development projects. Then, set an alert for budget when expenses exceed $ 500 multiplied by the number of developers.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Within your company, each developer possesses an individual development Google Cloud Platform (GCP) project associated with a central billing account. You have recommended that they establish alerts for any situations where a developer exceeds a monthly expenditure of $500. What actions should they take to implement these alerts?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297210,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>There are three projects in your organization, for development, testing and production. Your manager wants to monitor resource utilization (RAM, disk, network, CPU) for all applications in these three projects. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should create a Cloud Monitoring workspace in the production project and add development and testing projects to it. -&gt; Correct. It is the recommended approach. By creating a centralized workspace in the production project, you can monitor resource utilization for all applications across the three projects. This allows for a unified monitoring experience and provides a centralized view of resource metrics for all projects.</p><p><br></p><p>In Cloud Monitoring, share charts from development, testing and production projects. -&gt;&nbsp;Incorrect. It may not provide a comprehensive and centralized monitoring solution for monitoring resource utilization across all projects.</p><p><br></p><p>You should use the default Cloud Monitoring dashboards in all the projects. -&gt;&nbsp;Incorrect. It may not allow for a consolidated view or unified monitoring across all projects. The default dashboards are specific to individual projects and may not provide the desired visibility across all applications.</p><p><br></p><p>You cannot combine metrics from different projects. -&gt;&nbsp;Incorrect. With the appropriate setup, you can aggregate and combine metrics from different projects in Cloud Monitoring by creating a centralized workspace. This allows for monitoring resource utilization across multiple projects.</p><p><br></p><p>https://cloud.google.com/monitoring/docs</p>",
                "answers": [
                    "<p>You should create a Cloud Monitoring workspace in the production project and add development and testing projects to it.</p>",
                    "<p>In Cloud Monitoring, share charts from development, testing and production projects.</p>",
                    "<p>You should use the default Cloud Monitoring dashboards in all the projects.</p>",
                    "<p>You cannot combine metrics from different projects.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "There are three projects in your organization, for development, testing and production. Your manager wants to monitor resource utilization (RAM, disk, network, CPU) for all applications in these three projects. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297212,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>In a multi-regional Cloud Storage bucket, your company stores Personally Identifiable Information (PII) of customers. Your compliance department has asked you to record all operations/requests on this bucket. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should turn on data access audit logging in Cloud Storage to record this information. -&gt;&nbsp;Correct. To record all operations/requests on a multi-regional Cloud Storage bucket that stores Personally Identifiable Information (PII) of customers, data access audit logging in Cloud Storage should be turned on. Data access audit logs can provide information about who accessed the data, what actions were taken on the data, and when those actions were taken. This information can be useful for audit, compliance, and forensic purposes.</p><p><br></p><p>You should use the Identity-Aware Proxy API to record this information. -&gt; Incorrect. It is incorrect because the Identity-Aware Proxy API is used to authenticate and authorize users before they access an application or resource, not to record audit logs.</p><p><br></p><p>You should use the Data Loss Prevention API to record this information. -&gt; Incorrect. It is also incorrect because the Data Loss Prevention API is used to scan and analyze data to identify and prevent the accidental or intentional disclosure of sensitive data, not to record audit logs.</p><p><br></p><p>You should enable the default Cloud Storage service account exclusive access to read all operations and record them. -&gt; Incorrect. It is also incorrect because the default Cloud Storage service account is used to perform operations on objects in a bucket and does not have the capability to record audit logs.</p><p><br></p><p>https://cloud.google.com/logging/docs/audit</p>",
                "answers": [
                    "<p>You should turn on data access audit logging in Cloud Storage to record this information.</p>",
                    "<p>You should use the Identity-Aware Proxy API to record this information.</p>",
                    "<p>You should use the Data Loss Prevention API to record this information.</p>",
                    "<p>You should enable the default Cloud Storage service account exclusive access to read all operations and record them.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "In a multi-regional Cloud Storage bucket, your company stores Personally Identifiable Information (PII) of customers. Your compliance department has asked you to record all operations/requests on this bucket. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297214,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company has a service that needs to run on a fleet of identical instances and scales according to traffic patterns. You are tasked with setting up a Managed Instance Group (MIG) on Google Cloud Platform. The instances are created from an instance template and a startup script, which fetches the latest version of the application code from a Cloud Storage bucket every time an instance starts. However, your company wants to avoid any potential downtime when deploying updates to the application. Which strategy should you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use a rolling update to gradually replace instances in the MIG. -&gt; Correct. Rolling updates gradually replace instances in the MIG, which helps to maintain service availability during updates. When an instance is replaced, the startup script will fetch the latest version of the application code from the Cloud Storage bucket.</p><p><br></p><p>Increase the number of instances in the MIG before deploying an update to the application. -&gt; Incorrect. Increasing the number of instances before an update would not ensure that the new instances fetch the updated application code. It would also increase costs.</p><p><br></p><p>Manually replace instances in the MIG after updating the application. -&gt; Incorrect. Manually replacing instances would not be an efficient use of resources, especially in a large MIG. It could also cause service interruptions.</p><p><br></p><p>Modify the application code to poll the Cloud Storage bucket for updates periodically. -&gt; Incorrect. Modifying the application to poll for updates would complicate the application logic and might not work if instances need to be replaced (e.g., for maintenance or if an instance becomes unhealthy). It also doesn't take advantage of the built-in capabilities of MIGs to handle updates.</p>",
                "answers": [
                    "<p>Use a rolling update to gradually replace instances in the MIG.</p>",
                    "<p>Increase the number of instances in the MIG before deploying an update to the application.</p>",
                    "<p>Manually replace instances in the MIG after updating the application.</p>",
                    "<p>Modify the application code to poll the Cloud Storage bucket for updates periodically.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company has a service that needs to run on a fleet of identical instances and scales according to traffic patterns. You are tasked with setting up a Managed Instance Group (MIG) on Google Cloud Platform. The instances are created from an instance template and a startup script, which fetches the latest version of the application code from a Cloud Storage bucket every time an instance starts. However, your company wants to avoid any potential downtime when deploying updates to the application. Which strategy should you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297216,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A web application is running on App Engine. You created an update for this application and want to deploy this update without impacting users. If this update fails, you want to be able to roll back as quickly as possible. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should deploy the update as a new version, then migrate traffic from the current version to the new version. If it fails, migrate the traffic back to your older version. -&gt; Correct. Deploying the update as a new version and then migrating traffic from the current version to the new version is the recommended approach for deploying updates without impacting users and allowing for a quick rollback. By creating a new version, you ensure that the existing version remains active and accessible to users. If the update fails, you can easily roll back by migrating traffic back to the older version, minimizing the impact on users.</p><p><br></p><p>You should deploy the update as the same version that is currently running. If the update fails, redeploy your older version using the same version identifier. -&gt;&nbsp;Incorrect. It is not an effective approach for a quick rollback. Redeploying the older version using the same version identifier may cause confusion and potential issues with caching and consistency. It may not provide a straightforward way to roll back to the previous version.</p><p><br></p><p>You should deploy the update as the same version that is currently running because you are sure it won't fail. -&gt;&nbsp;Incorrect. Deploying the update as the same version that is currently running assumes that the update won't fail. However, software updates can have unexpected issues or bugs that may cause failures. Not having a separate version for the update can lead to difficulties in rolling back if the update does fail, as there won't be a previous version to switch back to.</p><p><br></p><p>You should notify your users of an upcoming maintenance window and ask them not to use your application during that window. Then, deploy the update in that maintenance window. -&gt;&nbsp;Incorrect. Notifying users of an upcoming maintenance window and asking them not to use the application during that window is not the most user-friendly approach and may result in a negative user experience. It is also not necessary if you can deploy the update without impacting users and provide a quick rollback option if needed.</p><p><br></p><p>https://cloud.google.com/appengine/docs</p>",
                "answers": [
                    "<p>You should deploy the update as a new version, then migrate traffic from the current version to the new version. If it fails, migrate the traffic back to your older version.</p>",
                    "<p>You should deploy the update as the same version that is currently running. If the update fails, redeploy your older version using the same version identifier.</p>",
                    "<p>You should deploy the update as the same version that is currently running because you are sure it won't fail.</p>",
                    "<p>You should notify your users of an upcoming maintenance window and ask them not to use your application during that window. Then, deploy the update in that maintenance window.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A web application is running on App Engine. You created an update for this application and want to deploy this update without impacting users. If this update fails, you want to be able to roll back as quickly as possible. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297218,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a new cloud architect, you need to manage your first GCP project. The project will involve product owners, developers and testers. You need to make sure that only specific members of the development team have access to sensitive information (PII data). To do this, you want to assign the appropriate IAM roles. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should create groups. Assign an IAM Predefined role to each group as required, including those who should have access to sensitive data. Than, assign users to groups. -&gt; Correct. In this scenario, the cloud architect needs to manage a GCP project that involves product owners, developers, and testers. The goal is to restrict access to sensitive information (PII data) to specific members of the development team. To accomplish this, the cloud architect should first create groups and assign IAM predefined roles to each group as required, including those who should have access to sensitive data. The cloud architect can then assign users to the groups. Using groups is important because it simplifies the management of permissions. If the cloud architect assigned roles to individual users instead of groups, the process would be more difficult to manage and would require updating each user's permissions individually.</p><p><br></p><p>You should create groups. Assign a Custom role to each group, including those who should have access to sensitive data. Then, assign users to groups. -&gt; Incorrect. Custom roles can be created, but it is not necessary in this scenario since IAM predefined roles already exist that can grant the necessary permissions.</p><p><br></p><p>You should assign a basic role to each user. -&gt; Incorrect. Assigning basic roles to each user is not sufficient to restrict access to sensitive information.</p><p><br></p><p>You should create groups. Assign a basic role to each group, and then assign users to groups. -&gt; Incorrect. Assigning basic roles to each group is not sufficient to restrict access to sensitive information.</p>",
                "answers": [
                    "<p>You should create groups. Assign an IAM Predefined role to each group as required, including those who should have access to sensitive data. Than, assign users to groups.</p>",
                    "<p>You should create groups. Assign a Custom role to each group, including those who should have access to sensitive data. Then, assign users to groups.</p>",
                    "<p>You should create groups. Assign a basic role to each group, and then assign users to groups.</p>",
                    "<p>You should assign a basic role to each user.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a new cloud architect, you need to manage your first GCP project. The project will involve product owners, developers and testers. You need to make sure that only specific members of the development team have access to sensitive information (PII data). To do this, you want to assign the appropriate IAM roles. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297220,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you need to design an IoT application that requires data storage up to 30 petabytes. Your application must support fast reads and writes. Your data schema is rather simple and you want to use the most economical solution for this. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should store the data in Cloud Bigtable. -&gt; Correct. Cloud Bigtable is a highly scalable NoSQL database service that is designed to handle massive amounts of data with low latency. It is ideal for IoT applications that require fast reads and writes and can store up to hundreds of petabytes of data. In addition, Cloud Bigtable is an economical solution for storing large amounts of data as it is charged based on usage and is highly optimized for cost-effective storage. </p><p><br></p><p>You should use BigQuery, and implement the business logic in SQL. -&gt; Incorrect. BigQuery is a data warehousing solution and may not be the most efficient or economical solution for storing data with simple schema. </p><p><br></p><p>You should store the data in Cloud Storage. -&gt; Incorrect. Cloud Storage is a good option for storing unstructured data, but may not be ideal for IoT data that requires fast reads and writes. </p><p><br></p><p>You should store the data in Cloud Spanner, and add an in-memory cache for speed. -&gt; Incorrect. Cloud Spanner is a fully-managed, scalable, relational database service, which is a good option for transactions across large datasets, but may not be the most economical solution for storing 30 petabytes of data.</p><p><br></p><p>https://cloud.google.com/bigtable/docs</p>",
                "answers": [
                    "<p>You should store the data in Cloud Bigtable.</p>",
                    "<p>You should use BigQuery, and implement the business logic in SQL.</p>",
                    "<p>You should store the data in Cloud Storage.</p>",
                    "<p>You should store the data in Cloud Spanner, and add an in-memory cache for speed.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you need to design an IoT application that requires data storage up to 30 petabytes. Your application must support fast reads and writes. Your data schema is rather simple and you want to use the most economical solution for this. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297222,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect for a large-scale company that is transitioning its monolithic applications to a microservices-based architecture on Google Cloud. You have been tasked with recommending an optimal strategy for deploying, managing, and scaling these microservices. Which of the following approaches would be most effective?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Deploy the microservices on Google Kubernetes Engine (GKE), utilizing the orchestration capabilities of Kubernetes to manage and scale services. -&gt;&nbsp;Correct. GKE provides a powerful platform for managing microservices through Kubernetes, which provides features like service discovery, load balancing, and automated scaling that are ideal for a microservices architecture.</p><p><br></p><p>Use Compute Engine to individually manage each microservice, utilizing autoscaling groups to handle scaling. -&gt; Incorrect. Compute Engine is more suitable for traditional and monolithic applications. It can be used for microservices, but it would involve significant manual effort in managing and coordinating between services, which is not ideal.</p><p><br></p><p>Deploy each microservice as a separate App Engine application, utilizing App Engine's automatic scaling and load balancing capabilities. -&gt; Incorrect. App Engine is a platform as a service (PaaS) and is designed for web applications rather than microservices. While it can support microservices, it would not be as effective for managing a large-scale microservices architecture as other options.</p><p><br></p><p>Use Cloud Functions for each microservice, utilizing the event-driven nature of Cloud Functions to manage and scale services. -&gt; Incorrect. Cloud Functions are designed for event-driven, single-purpose applications and would not be as effective for large-scale, multi-functional microservices. Also, they can become expensive and complex to manage at scale.</p>",
                "answers": [
                    "<p>Deploy the microservices on Google Kubernetes Engine (GKE), utilizing the orchestration capabilities of Kubernetes to manage and scale services.</p>",
                    "<p>Use Compute Engine to individually manage each microservice, utilizing autoscaling groups to handle scaling.</p>",
                    "<p>Deploy each microservice as a separate App Engine application, utilizing App Engine's automatic scaling and load balancing capabilities.</p>",
                    "<p>Use Cloud Functions for each microservice, utilizing the event-driven nature of Cloud Functions to manage and scale services.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect for a large-scale company that is transitioning its monolithic applications to a microservices-based architecture on Google Cloud. You have been tasked with recommending an optimal strategy for deploying, managing, and scaling these microservices. Which of the following approaches would be most effective?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297224,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization is preparing to build a complex data science solution on Google Cloud Platform. The solution involves various stages, including data collection, cleaning, analysis, machine learning model training, and deploying models for real-time predictions. The data volume is significant, and the solution will require multiple services on GCP for various stages. As the cloud architect, which of the following architectures will you recommend for managing this data science solution effectively?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Pub/Sub for data collection, Dataflow for cleaning and analysis, AutoML for machine learning model training, and Cloud Run for deploying models. -&gt; Correct. Cloud Pub/Sub is designed for reliable messaging in applications. Dataflow can effectively handle the cleaning and analysis stages. AutoML is ideal for training machine learning models without requiring machine learning expertise. Cloud Run is designed to run containers and would be an excellent choice for deploying machine learning models packaged as containers.</p><p><br></p><p>Use Cloud Storage for data collection, Dataflow for cleaning and analysis, Cloud Dataprep for machine learning model training, and App Engine for deploying models. -&gt; Incorrect. Cloud Dataprep is a data service for visually exploring, cleaning, and preparing data for analysis. However, it is not designed for machine learning model training.</p><p><br></p><p>Use Pub/Sub for data collection, Dataflow for cleaning and analysis, BigQuery ML for machine learning model training, and Cloud Functions for deploying models. -&gt; Incorrect. BigQuery ML enables users to create and execute machine learning models in BigQuery using SQL queries. However, it may not handle the complex machine learning requirements of large data science solutions as effectively as AutoML or Cloud ML Engine.</p><p><br></p><p>Use Cloud Storage for data collection, Cloud Dataproc for cleaning and analysis, Cloud ML Engine for machine learning model training, and Cloud Endpoints for deploying models. -&gt; Incorrect. This approach is viable for managing a data science solution, but Cloud Endpoints is generally used for deploying APIs, not machine learning models.</p>",
                "answers": [
                    "<p>Use Cloud Pub/Sub for data collection, Dataflow for cleaning and analysis, AutoML for machine learning model training, and Cloud Run for deploying models.</p>",
                    "<p>Use Cloud Storage for data collection, Dataflow for cleaning and analysis, Cloud Dataprep for machine learning model training, and App Engine for deploying models.</p>",
                    "<p>Use Pub/Sub for data collection, Dataflow for cleaning and analysis, BigQuery ML for machine learning model training, and Cloud Functions for deploying models.</p>",
                    "<p>Use Cloud Storage for data collection, Cloud Dataproc for cleaning and analysis, Cloud ML Engine for machine learning model training, and Cloud Endpoints for deploying models.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization is preparing to build a complex data science solution on Google Cloud Platform. The solution involves various stages, including data collection, cleaning, analysis, machine learning model training, and deploying models for real-time predictions. The data volume is significant, and the solution will require multiple services on GCP for various stages. As the cloud architect, which of the following architectures will you recommend for managing this data science solution effectively?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297226,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>An insurance company uses several third-party enterprise applications that require special licenses. These licenses are not transferrable to the cloud. The third-party software vendor offers an option to pay a licensing fee based on how long you use the application in the cloud. What is this approach called?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Pay-as-you-go license -&gt; Correct. The approach of paying a licensing fee based on how long you use the application in the cloud is commonly known as a \"pay-as-you-go\" license. This approach allows organizations to pay for software licenses only when they are needed, rather than paying for a fixed number of licenses upfront, regardless of how much they are used. This can help reduce costs and increase flexibility in the cloud.</p><p><br></p><p>Bringing your own licenses -&gt;&nbsp;Incorrect. Bringing your own licenses (BYOL) refers to a licensing model where the company brings their existing licenses from on-premises or other environments to use in the cloud. This option is not applicable in this scenario as the third-party licenses are not transferrable to the cloud.</p><p><br></p><p>On-demand pricing -&gt;&nbsp;Incorrect. On-demand pricing typically refers to a pricing model where services are billed based on usage, usually with per-hour or per-minute rates. While it is a common pricing model for cloud services, it does not specifically address the licensing aspect mentioned in the question.</p><p><br></p><p>Flat-rate pricing -&gt;&nbsp;Incorrect. Flat-rate pricing refers to a fixed pricing model where a set fee is charged regardless of usage. This option does not align with the scenario described, as the licensing fee in this case is based on how long the application is used in the cloud, rather than a fixed flat rate.</p><p><br></p><p>https://cloud.google.com/blog/products/compute/compute-engine-licensing-explained</p>",
                "answers": [
                    "<p>Pay-as-you-go license</p>",
                    "<p>Bringing your own licenses</p>",
                    "<p>On-demand pricing</p>",
                    "<p>Flat-rate pricing</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "An insurance company uses several third-party enterprise applications that require special licenses. These licenses are not transferrable to the cloud. The third-party software vendor offers an option to pay a licensing fee based on how long you use the application in the cloud. What is this approach called?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297228,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you need to establish connection between your on-premises network and Google Cloud. Your company will need 2 Gbps of bandwidth in total between the on-premises data center and Google Cloud. How many VPN endpoints will you need?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>1 -&gt; Correct. A VPN endpoint is a virtual device that is used to establish a secure connection between your on-premises network and Google Cloud. A single VPN gateway can support up to 3 Gbps of throughput, so in this scenario, a single VPN endpoint is sufficient to support the required bandwidth of 2 Gbps.</p><p><br></p><p>https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview#network-bandwidth</p>",
                "answers": [
                    "<p>1</p>",
                    "<p>2</p>",
                    "<p>3</p>",
                    "<p>6</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you need to establish connection between your on-premises network and Google Cloud. Your company will need 2 Gbps of bandwidth in total between the on-premises data center and Google Cloud. How many VPN endpoints will you need?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297232,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As you plan your migration, you find out that traffic to the subnet containing databases must be restricted. As a cloud architect, what mechanism should you use to control this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Firewall rules -&gt; Correct. Firewall rules can be used to restrict traffic to a specific subnet in Google Cloud. Firewall rules allow you to define what traffic is allowed to enter or leave a particular subnet or VM instance based on its protocol, port, and IP address. By creating firewall rules that only allow traffic from authorized sources, you can effectively restrict traffic to a subnet containing databases. </p><p><br></p><p>Virtual Private Networks -&gt; Incorrect. Virtual Private Networks is not used to restrict traffic to a subnet, but rather to provide secure connectivity between networks or to isolate resources within a network. </p><p><br></p><p>Virtual Private Clouds -&gt; Incorrect. Virtual Private Clouds is not used to restrict traffic to a subnet, but rather to provide secure connectivity between networks or to isolate resources within a network. </p><p><br></p><p>IAM&nbsp;roles -&gt; Incorrect. IAM roles control who can access resources in a project, but do not restrict traffic to a specific subnet.</p><p><br></p><p>https://cloud.google.com/vpc/docs/firewalls</p>",
                "answers": [
                    "<p>Firewall rules</p>",
                    "<p>Virtual Private Networks</p>",
                    "<p>Virtual Private Clouds</p>",
                    "<p>IAM&nbsp;roles</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As you plan your migration, you find out that traffic to the subnet containing databases must be restricted. As a cloud architect, what mechanism should you use to control this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297234,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A manufacturing company uses IoT devices and collects data from millions of devices around the world. The IoT data is streamed from each device every 5 seconds - 5 KB per message. This company wants to use a managed service from Google Cloud. What would you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Bigtable -&gt; Correct. Cloud Bigtable is a highly scalable NoSQL database that can handle high volume and high-speed data ingestion, storage, and retrieval. It is designed for applications that require very high throughput and low-latency data access, making it a good choice for storing and analyzing large amounts of IoT data that are generated frequently. </p><p><br></p><p>BigQuery -&gt;&nbsp;Incorrect. BigQuery is a fully managed, serverless data warehouse that excels at analyzing large datasets using SQL queries. While it is a powerful tool for analytics and querying, it may not be the best fit for real-time ingestion and processing of streaming data from IoT devices.</p><p><br></p><p>Cloud SQL -&gt;&nbsp;Incorrect. Cloud SQL is a managed relational database service that provides a traditional SQL database environment. While it can handle structured data, it may not be the most efficient or scalable solution for managing and analyzing high volumes of IoT data.</p><p><br></p><p>Cloud Spanner -&gt;&nbsp;Incorrect. Cloud Spanner is a globally distributed, horizontally scalable relational database service. While it can handle high-volume transactions and provides strong consistency, it may not be the most cost-effective solution for storing and analyzing IoT data that is constantly streamed from millions of devices.</p><p><br></p><p>Dataproc -&gt;&nbsp;Incorrect. Dataproc is a managed Apache Hadoop and Spark service. While it can be used for processing and analyzing data, it may not be the most suitable choice for real-time analytics and querying of streaming IoT data.</p><p><br></p><p>https://cloud.google.com/bigtable/docs/overview</p>",
                "answers": [
                    "<p>Cloud Bigtable</p>",
                    "<p>BigQuery</p>",
                    "<p>Cloud SQL</p>",
                    "<p>Cloud Spanner</p>",
                    "<p>Dataproc</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A manufacturing company uses IoT devices and collects data from millions of devices around the world. The IoT data is streamed from each device every 5 seconds - 5 KB per message. This company wants to use a managed service from Google Cloud. What would you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297236,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization is operating several Compute Engine instances on Google Cloud Platform. You've been asked to ensure that logs from these instances are centralized and accessible from the Cloud Logging. Given the different operational needs and data sensitivities across instances, which strategy would ensure appropriate setup of the Cloud Logging agent?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Install the Cloud Logging agent on each instance and have it log data to individual Cloud Logging projects based on each instance's operational needs and data sensitivity. -&gt; Correct. By installing the Cloud Logging agent on each instance and configuring each agent to log data to the appropriate Cloud Logging project, you can separate data according to its sensitivity and operational needs.</p><p><br></p><p>Install the Cloud Logging agent on each instance and log data to a single Cloud Logging project to consolidate all logs. -&gt; Incorrect. This strategy may create problems in separating concerns between different types of data or different sensitivity levels, even though it centralizes all the logs.</p><p><br></p><p>Install the Cloud Logging agent on a single, dedicated instance and use that to log data from all other instances. -&gt; Incorrect. Having a single instance log data for all other instances would not be efficient or scalable and could lead to a single point of failure.</p><p><br></p><p>Install the Cloud Logging agent on each instance and configure the agent to log data to Cloud Storage instead of Cloud Logging to save costs. -&gt; Incorrect. While storing logs in Cloud Storage may have cost advantages in some cases, it doesn't provide the real-time analysis, metrics, and log-based alerting that Cloud Logging offers. This strategy might not meet all operational needs.</p>",
                "answers": [
                    "<p>Install the Cloud Logging agent on each instance and have it log data to individual Cloud Logging projects based on each instance's operational needs and data sensitivity.</p>",
                    "<p>Install the Cloud Logging agent on each instance and log data to a single Cloud Logging project to consolidate all logs.</p>",
                    "<p>Install the Cloud Logging agent on a single, dedicated instance and use that to log data from all other instances.</p>",
                    "<p>Install the Cloud Logging agent on each instance and configure the agent to log data to Cloud Storage instead of Cloud Logging to save costs.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization is operating several Compute Engine instances on Google Cloud Platform. You've been asked to ensure that logs from these instances are centralized and accessible from the Cloud Logging. Given the different operational needs and data sensitivities across instances, which strategy would ensure appropriate setup of the Cloud Logging agent?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297238,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you work for a courier company that delivers packages all over the world. You are responsible for preparing a system that will track the location of packages. This solution must be scalable and ensure high consistency. Your data storage solution must also support SQL queries. Which GCP&nbsp;service should you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Spanner -&gt;&nbsp;Correct. It is a fully managed, horizontally scalable, and strongly consistent relational database service. It is designed to scale globally and provide strong consistency across regions. It also supports SQL queries, making it suitable for the scenario described in the question, where a scalable and highly consistent solution is required to track package locations. </p><p><br></p><p>Cloud SQL -&gt; Incorrect. It is also a relational database service, but it does not provide the same level of scalability and consistency as Cloud Spanner. </p><p><br></p><p>BigQuery -&gt;&nbsp;Incorrect. It is a data warehouse service that supports SQL queries, but it is not a relational database and does not provide the same level of consistency. </p><p><br></p><p>Dataproc -&gt; Incorrect. It is a managed service for running Apache Hadoop and Apache Spark, which is not a suitable choice for this scenario.</p><p><br></p><p>https://cloud.google.com/spanner/docs/quickstart-console</p>",
                "answers": [
                    "<p>Cloud Spanner</p>",
                    "<p>Cloud SQL</p>",
                    "<p>BigQuery</p>",
                    "<p>Dataproc</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you work for a courier company that delivers packages all over the world. You are responsible for preparing a system that will track the location of packages. This solution must be scalable and ensure high consistency. Your data storage solution must also support SQL queries. Which GCP&nbsp;service should you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297240,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As part of the software development life cycle, you are designing a testing strategy for a new application being developed on Google Cloud. The application is expected to have high demand and needs to be highly reliable. Which approach would be the most appropriate?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use blue-green deployments to perform end-to-end testing. -&gt;&nbsp;Correct. Blue-green deployments involve running two environments, \"blue\" and \"green,\" where one serves live traffic (blue) while the other (green) is used for testing. Once testing is done, traffic is switched to the green environment. This ensures high availability and allows for comprehensive testing.</p><p><br></p><p>Implement unit tests only to speed up the development process. -&gt;&nbsp;Incorrect. While unit tests are important, they only cover individual components of your application. To ensure reliability, more comprehensive testing strategies are needed.</p><p><br></p><p>Use a canary release strategy for testing. -&gt;&nbsp;Incorrect. A canary release is a technique to reduce the risk of introducing a new software version in production by gradually rolling out the change to a small subset of users. While it can be useful, it may not provide the comprehensive testing required for high reliability.</p><p><br></p><p>Test in the production environment to ensure accurate results. -&gt;&nbsp;Incorrect. Testing in production might expose users to bugs and can lead to a poor user experience. It's better to use strategies like blue-green deployments where testing is done in a separate environment.</p>",
                "answers": [
                    "<p>Use blue-green deployments to perform end-to-end testing.</p>",
                    "<p>Implement unit tests only to speed up the development process.</p>",
                    "<p>Use a canary release strategy for testing.</p>",
                    "<p>Test in the production environment to ensure accurate results.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As part of the software development life cycle, you are designing a testing strategy for a new application being developed on Google Cloud. The application is expected to have high demand and needs to be highly reliable. Which approach would be the most appropriate?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297242,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>As a cloud architect, you plan to migrate your on-premises data warehouse to Google Cloud using BigQuery. You need to make a presentation to management of what the costs look like in BigQuery. Select all true statements about the BigQuery pricing model. (select 2)</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>BigQuery offers a choice of two pricing models for running queries: On-demand pricing and Flat-rate pricing. -&gt; Correct. BigQuery offers a choice of two pricing models for running queries: On-demand pricing and Flat-rate pricing. On-demand pricing charges users based on the amount of data processed by the query, while flat-rate pricing provides a predictable monthly cost for a set amount of query processing capacity.</p><p><br></p><p>BigQuery charges for certain operations, such as streaming inserts and using the BigQuery Storage API. -&gt; Correct. BigQuery charges for certain operations, such as streaming inserts and using the BigQuery Storage API. These operations are charged separately from query processing.</p><p><br></p><p>BigQuery has no free usage tier. -&gt; Incorrect. BigQuery actually does offer a free tier, which allows for 1 TB of data processed per month at no cost.</p><p><br></p><p>By default, queries are billed using the Flat-rate pricing model. -&gt; Incorrect. The default pricing model for queries in BigQuery is actually On-demand pricing, not Flat-rate pricing. Flat-rate pricing must be explicitly enabled and configured.</p><p><br></p><p>https://cloud.google.com/bigquery/pricing</p>",
                "answers": [
                    "<p>BigQuery offers a choice of two pricing models for running queries: On-demand pricing and Flat-rate pricing.</p>",
                    "<p>BigQuery charges for certain operations, such as streaming inserts and using the BigQuery Storage API.</p>",
                    "<p>BigQuery has no free usage tier.</p>",
                    "<p>By default, queries are billed using the Flat-rate pricing model.</p>"
                ]
            },
            "correct_response": [
                "a",
                "b"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you plan to migrate your on-premises data warehouse to Google Cloud using BigQuery. You need to make a presentation to management of what the costs look like in BigQuery. Select all true statements about the BigQuery pricing model. (select 2)",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297244,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>In GCP, your ingestion services cannot keep up with the rate that new data is received. What can you do to ensure that data is not lost if ingestion services cannot keep up with the rate at which new data is received?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You can use Cloud&nbsp;Pub/Sub queue. Add data to a queue. Than, you can use Cloud Function to remove the data from the queue, transform and write it to another storage system. -&gt; Correct. Using Cloud Pub/Sub queue allows you to decouple the ingestion services from the processing services. In this approach, the ingestion services add data to a Pub/Sub queue, which acts as a buffer. Cloud Functions can then be used to consume messages from the queue, perform any necessary transformations, and write the data to another storage system. This ensures that data is not lost even if the ingestion services cannot keep up with the incoming data rate.</p><p><br></p><p>You should change the capacity of your storage system. -&gt;&nbsp;Incorrect. Changing the capacity of your storage system may help in some cases, but it does not directly address the issue of data loss when the ingestion services cannot keep up with the data rate. Simply increasing the storage capacity will not guarantee that the incoming data is processed and stored in a timely manner.</p><p><br></p><p>You can use BigQuery dataset. Add data to a temporary table. Than, use Dataflow to process the data and writes it to another storage system. -&gt;&nbsp;Incorrect. Using a BigQuery dataset and a temporary table can be a valid approach, but it does not directly address the issue of data loss when the ingestion services are overwhelmed. While Dataflow can be used to process the data and write it to another storage system, the temporary table in BigQuery may not be sufficient to handle the high influx of incoming data if the ingestion services are unable to keep up.</p><p><br></p><p>You cannot prevent data loss. -&gt;&nbsp;Incorrect. Saying that you cannot prevent data loss is not entirely accurate. While it may not be possible to completely eliminate the risk of data loss, using appropriate buffering mechanisms like Cloud Pub/Sub queues can help mitigate the risk and ensure that data is not lost if the ingestion services are unable to keep up.</p><p><br></p><p>https://cloud.google.com/pubsub/docs/overview</p>",
                "answers": [
                    "<p>You can use Cloud&nbsp;Pub/Sub queue. Add data to a queue. Than, you can use Cloud Function to remove the data from the queue, transform and write it to another storage system.</p>",
                    "<p>You should change the capacity of your storage system.</p>",
                    "<p>You can use BigQuery dataset. Add data to a temporary table. Than, use Dataflow to process the data and writes it to another storage system.</p>",
                    "<p>You cannot prevent data loss.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "In GCP, your ingestion services cannot keep up with the rate that new data is received. What can you do to ensure that data is not lost if ingestion services cannot keep up with the rate at which new data is received?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297246,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>As a cloud architect, you need to deploy MySQL database using Google Cloud Platform. Select all possible options for deploying MySQL database to Google Cloud.</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You can use Cloud SQL to host MySQL database. This option reduces administrative duties. -&gt;&nbsp;Correct. Cloud SQL is a fully managed relational database service that supports MySQL. It provides automated backups, replication, scaling, and patching, reducing administrative duties for managing MySQL databases.</p><p><br></p><p>You can use Cloud Marketplace with click-to-deploy interface to install MySQL onto a Compute Engine instance. -&gt; Correct. Cloud Marketplace offers click-to-deploy interfaces for deploying various applications, including MySQL, on Compute Engine instances. This option provides easy and fast deployment of MySQL databases.</p><p><br></p><p>You can manually install and customize MySQL on your Compute Engine instance. -&gt; Correct. You can manually install and customize MySQL on your Compute Engine instance. This option provides full control and customization over the MySQL installation.</p><p><br></p><p>You can use Cloud Marketplace with click-to-deploy interface to install MySQL using Google Kubernetes Engine. -&gt; Incorrect. It is not a valid answer since it mentions using Google Kubernetes Engine (GKE) to deploy MySQL. GKE is a container orchestration service and is not recommended for hosting databases directly.</p><p><br></p><p>https://cloud.google.com/architecture/setup-mysql?hl=en</p>",
                "answers": [
                    "<p>You can use Cloud SQL to host MySQL database. This option reduces administrative duties.</p>",
                    "<p>You can use Cloud Marketplace with click-to-deploy interface to install MySQL onto a Compute Engine instance.</p>",
                    "<p>You can manually install and customize MySQL on your Compute Engine instance.</p>",
                    "<p>You can use Cloud Marketplace with click-to-deploy interface to install MySQL using Google Kubernetes Engine.</p>"
                ]
            },
            "correct_response": [
                "a",
                "b",
                "c"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you need to deploy MySQL database using Google Cloud Platform. Select all possible options for deploying MySQL database to Google Cloud.",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297248,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company has two Google Cloud projects - Project A and Project B. The goal is to move data from a Cloud Storage bucket in Project A to another bucket in Project B on a regular schedule. Which of the following is the best way to achieve this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use the Storage Transfer Service to schedule and manage the data transfer between the source and destination buckets. -&gt; Correct. Storage Transfer Service is designed specifically for this use case. It provides a simple, robust solution for transferring large amounts of data between Cloud Storage buckets, even across different projects or regions. It also supports scheduling, so it is the correct answer.</p><p><br></p><p>Use the <code>gsutil cp</code> command to manually copy the objects from the source bucket to the destination bucket. -&gt; Incorrect. While <code>gsutil cp</code> could be used to copy objects between buckets, it's a manual process and is not suitable for regular, automated data transfers.</p><p><br></p><p>Use Cloud Functions to trigger a Cloud Storage event whenever data is added to the source bucket, which then copies the data to the destination bucket. -&gt; Incorrect. Cloud Functions can react to changes in a Cloud Storage bucket, but they are more suited to processing individual objects rather than transferring large amounts of data on a regular schedule.</p><p><br></p><p>Use Cloud Dataflow to create a pipeline that reads from the source bucket and writes to the destination bucket. -&gt; Incorrect. Cloud Dataflow is a powerful service for processing and transforming large data streams, but it's overkill for this scenario and it doesn't support scheduling natively. It would also require more management and configuration than the Storage Transfer Service.</p>",
                "answers": [
                    "<p>Use the Storage Transfer Service to schedule and manage the data transfer between the source and destination buckets.</p>",
                    "<p>Use the <code>gsutil cp</code> command to manually copy the objects from the source bucket to the destination bucket.</p>",
                    "<p>Use Cloud Functions to trigger a Cloud Storage event whenever data is added to the source bucket, which then copies the data to the destination bucket.</p>",
                    "<p>Use Cloud Dataflow to create a pipeline that reads from the source bucket and writes to the destination bucket.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company has two Google Cloud projects - Project A and Project B. The goal is to move data from a Cloud Storage bucket in Project A to another bucket in Project B on a regular schedule. Which of the following is the best way to achieve this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297250,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdf</p><p><br></p><p>As a cloud architect for Mountkirk Games, your responsibility is to ensure that their new gaming platform adheres to Google's best practices. Your objective is to validate the implementation of Google's recommended security practices while also providing the necessary metrics to support your operations teams. What steps should you take to achieve this goal? (select 2)</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Ensure that you are not running privileged containers. -&gt;&nbsp;Correct. This is High Priority according to Google best practices.</p><p><br></p><p>Ensure that you are using the native logging mechanisms. -&gt; Correct. This is High Priority according to Google best practices.</p><p><br></p><p>Ensure that you are using obfuscated Tags on workloads. -&gt;&nbsp;Incorrect. Tags should be readable and useful to the operations teams when they are working on the clusters.</p><p><br></p><p>Ensure that workloads are not using securityContext to run as a group. -&gt;&nbsp;Incorrect. This may be required for some workloads.</p><p><br></p><p>Ensure that each cluster is running GKE metering so each team can be charged for their usage. -&gt; Incorrect. Although from a business process this may be useful it wont impact the operations or security of the cluster.</p><p><br></p><p>https://cloud.google.com/architecture/best-practices-for-operating-containers</p>",
                "answers": [
                    "<p>Ensure that you are not running privileged containers.</p>",
                    "<p>Ensure that you are using the native logging mechanisms.</p>",
                    "<p>Ensure that you are using obfuscated Tags on workloads.</p>",
                    "<p>Ensure that workloads are not using securityContext to run as a group.</p>",
                    "<p>Ensure that each cluster is running GKE metering so each team can be charged for their usage.</p>"
                ]
            },
            "correct_response": [
                "a",
                "b"
            ],
            "section": "",
            "question_plain": "Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdfAs a cloud architect for Mountkirk Games, your responsibility is to ensure that their new gaming platform adheres to Google's best practices. Your objective is to validate the implementation of Google's recommended security practices while also providing the necessary metrics to support your operations teams. What steps should you take to achieve this goal? (select 2)",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297252,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Refer to the TerramEarth case study for this question: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdf</p><p><br></p><p>As the Data Compliance Officer for TerramEarth, your primary responsibility is to safeguard customers' personally identifiable information (PII), including sensitive data like credit card information. TerramEarth aims to offer personalized product recommendations to its extensive base of industrial customers. It is crucial to prioritize data privacy while designing an appropriate solution. What steps would you propose to address this challenge?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should use the Cloud Data Loss Prevention (DLP) API to provide data to the recommendation service. -&gt; Correct. Cloud DLP was specifically designed for this use case.</p><p><br></p><p>You should use AutoML to provide data to the recommendation service. -&gt; Incorrect. AutoML does not inherently provide data de-identification.</p><p><br></p><p>You should process PII data on-premises to keep the private information more secure. -&gt; Incorrect. TerramEarth's requirements are to go into the cloud, not stay on-premises.</p><p><br></p><p>You should manually build, train, and test machine learning models to provide product recommendations anonymously. -&gt;&nbsp;Incorrect. Developing machine learning models is an excessive way to de-identify data.</p><p><br></p><p>https://cloud.google.com/dlp/docs/deidentify-sensitive-data</p>",
                "answers": [
                    "<p>You should use the Cloud Data Loss Prevention (DLP) API to provide data to the recommendation service.</p>",
                    "<p>You should use AutoML to provide data to the recommendation service.</p>",
                    "<p>You should process PII data on-premises to keep the private information more secure.</p>",
                    "<p>You should manually build, train, and test machine learning models to provide product recommendations anonymously.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Refer to the TerramEarth case study for this question: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdfAs the Data Compliance Officer for TerramEarth, your primary responsibility is to safeguard customers' personally identifiable information (PII), including sensitive data like credit card information. TerramEarth aims to offer personalized product recommendations to its extensive base of industrial customers. It is crucial to prioritize data privacy while designing an appropriate solution. What steps would you propose to address this challenge?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297254,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are responsible for architecting a three-tier web application on Google Cloud Platform. This application includes a web server tier, an application server tier, and a database tier, each tier hosted on separate Compute Engine instances. You need to control network traffic between these tiers and from the public internet. To achieve this, you decide to use tags and firewall rules. Which of the following options would be the most effective way to apply these tags and set up firewall rules?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Assign each tier a unique tag and create individual firewall rules to control traffic between tags and from the public internet. -&gt; Correct. By tagging each tier separately and creating individual firewall rules for each, you can finely control the allowed traffic between tiers and from the public internet, enhancing security.</p><p><br></p><p>Add a single tag to all instances irrespective of the tier and create one firewall rule to allow all inbound traffic. -&gt; Incorrect. Using a single tag for all instances would not allow for differentiated access control between tiers. One firewall rule for all inbound traffic would not provide the necessary security controls.</p><p><br></p><p>Assign the same tag to the web server and application server tier, and a different tag to the database tier. Then, create a firewall rule to allow all traffic within the same tag and limited traffic to the database tag. -&gt; Incorrect. This approach does not provide a differentiated control between the web server and application server tiers. Also, while the database tier should indeed have restricted access, this option doesn't provide sufficient granularity in its controls.</p><p><br></p><p>Add a tag only to the database tier and create a firewall rule that only allows traffic from the IP addresses of the other two tiers. -&gt; Incorrect. Adding a tag only to the database tier would not allow for differentiated access control for the other two tiers.</p>",
                "answers": [
                    "<p>Assign each tier a unique tag and create individual firewall rules to control traffic between tags and from the public internet.</p>",
                    "<p>Add a single tag to all instances irrespective of the tier and create one firewall rule to allow all inbound traffic.</p>",
                    "<p>Assign the same tag to the web server and application server tier, and a different tag to the database tier. Then, create a firewall rule to allow all traffic within the same tag and limited traffic to the database tag.</p>",
                    "<p>Add a tag only to the database tier and create a firewall rule that only allows traffic from the IP addresses of the other two tiers.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are responsible for architecting a three-tier web application on Google Cloud Platform. This application includes a web server tier, an application server tier, and a database tier, each tier hosted on separate Compute Engine instances. You need to control network traffic between these tiers and from the public internet. To achieve this, you decide to use tags and firewall rules. Which of the following options would be the most effective way to apply these tags and set up firewall rules?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297256,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization has a suite of applications that have been containerized. You've been tasked to design a deployment strategy that leverages the power of Google Cloud's managed services, allows for auto-scaling based on demand, provides an ability to deploy updates with zero-downtime, and supports granular IAM roles and policies for your DevOps team. What would be your recommended approach?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Utilize Google Kubernetes Engine (GKE) for deploying and managing the containerized applications, utilizing Kubernetes' native support for autoscaling and rolling updates. -&gt; Correct. GKE is a managed Kubernetes service that provides a powerful platform for managing containerized applications. It allows for automatic scaling, zero-downtime deployments, and supports granular IAM roles and policies.</p><p><br></p><p>Deploy the containers directly onto Compute Engine VM instances and manually manage scaling and updates. -&gt; Incorrect. Compute Engine provides the necessary infrastructure to run the containers, but it would require manual management for scaling and updates, which is not efficient or recommended for containerized applications.</p><p><br></p><p>Use Cloud Run to deploy the containerized applications and benefit from its automatic scaling and deployment features. -&gt; Incorrect. Cloud Run can be a great option for stateless containerized applications as it provides automatic scaling and easy deployments. However, it lacks the granular control over the environment that Kubernetes offers.</p><p><br></p><p>Deploy the containers on App Engine standard environment and let App Engine handle scaling and updates. -&gt; Incorrect. App Engine standard environment does not support containers. App Engine flexible environment does, but it does not offer the same level of control and orchestration that GKE provides.</p>",
                "answers": [
                    "<p>Utilize Google Kubernetes Engine (GKE) for deploying and managing the containerized applications, utilizing Kubernetes' native support for autoscaling and rolling updates.</p>",
                    "<p>Deploy the containers directly onto Compute Engine VM instances and manually manage scaling and updates.</p>",
                    "<p>Use Cloud Run to deploy the containerized applications and benefit from its automatic scaling and deployment features.</p>",
                    "<p>Deploy the containers on App Engine standard environment and let App Engine handle scaling and updates.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization has a suite of applications that have been containerized. You've been tasked to design a deployment strategy that leverages the power of Google Cloud's managed services, allows for auto-scaling based on demand, provides an ability to deploy updates with zero-downtime, and supports granular IAM roles and policies for your DevOps team. What would be your recommended approach?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297258,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect working on a web application that requires horizontal scaling based on traffic load. The application runs on stateless virtual machines, and you plan to use Managed Instance Groups (MIGs) for this purpose. You also want to ensure that any updates to the instance templates do not disrupt the service. Which of the following deployment strategies would you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Rolling update strategy with Proactive update mode and Automatic scaling. -&gt; Correct. The Rolling update strategy gradually replaces instances in the group with instances based on the new template, which ensures minimal disruption. The Proactive update mode starts the update as soon as the group is stable, and Automatic scaling adjusts the number of instances based on the load, which is ideal for a web application that requires horizontal scaling based on traffic.</p><p><br></p><p>Use Canary update strategy with Proactive update mode and Manual scaling. -&gt; Incorrect. While the Canary update strategy allows you to roll out updates to a subset of instances before updating the rest, Manual scaling would not provide the necessary flexibility for a load-based scaling scenario.</p><p><br></p><p>Use Rolling update strategy with Opportunistic update mode and Manual scaling. -&gt; Incorrect. The Opportunistic update mode updates instances when they are recreated for other reasons, such as auto-healing, which does not guarantee timely updates. Manual scaling would not provide the necessary flexibility for a load-based scaling scenario.</p><p><br></p><p>Use Opportunistic update mode with Maximum surge policy and Automatic scaling. -&gt;&nbsp;Incorrect. The Opportunistic update mode and Maximum surge policy are not the ideal combination for this scenario. The Opportunistic update mode may delay updates, and the Maximum surge policy, which defines the number of additional instances that can be created, does not have any direct impact on the update strategy.</p>",
                "answers": [
                    "<p>Use Rolling update strategy with Proactive update mode and Automatic scaling.</p>",
                    "<p>Use Canary update strategy with Proactive update mode and Manual scaling.</p>",
                    "<p>Use Rolling update strategy with Opportunistic update mode and Manual scaling.</p>",
                    "<p>Use Opportunistic update mode with Maximum surge policy and Automatic scaling.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect working on a web application that requires horizontal scaling based on traffic load. The application runs on stateless virtual machines, and you plan to use Managed Instance Groups (MIGs) for this purpose. You also want to ensure that any updates to the instance templates do not disrupt the service. Which of the following deployment strategies would you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297260,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are designing a hybrid cloud setup where you need to connect on-premises infrastructure with Google Cloud. The on-premises network uses the IP range 192.168.0.0/16. You need to ensure that the IP range used on Google Cloud does not overlap with the on-premises range to avoid IP conflicts. Which of the following strategies should you adopt?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Choose an IP range of 10.0.0.0/16 for the Google Cloud network. -&gt; Correct. The IP range 10.0.0.0/16 is within the private IP range defined by RFC1918 and does not overlap with the on-premises IP range.</p><p><br></p><p>Choose an IP range of 192.168.0.0/24 for the Google Cloud network. -&gt; Incorrect. This IP range overlaps with the on-premises IP range because it is a subset of 192.168.0.0/16.</p><p><br></p><p>Choose an IP range of 192.168.1.0/24 for the Google Cloud network. -&gt; Incorrect. This IP range overlaps with the on-premises IP range because it is a subset of 192.168.0.0/16.</p><p><br></p><p>Choose an IP range of 192.168.0.0/16 for the Google Cloud network. -&gt; Incorrect. This IP range is exactly the same as the on-premises IP range, which would lead to IP address conflicts.</p>",
                "answers": [
                    "<p>Choose an IP range of 10.0.0.0/16 for the Google Cloud network.</p>",
                    "<p>Choose an IP range of 192.168.0.0/24 for the Google Cloud network.</p>",
                    "<p>Choose an IP range of 192.168.1.0/24 for the Google Cloud network.</p>",
                    "<p>Choose an IP range of 192.168.0.0/16 for the Google Cloud network.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are designing a hybrid cloud setup where you need to connect on-premises infrastructure with Google Cloud. The on-premises network uses the IP range 192.168.0.0/16. You need to ensure that the IP range used on Google Cloud does not overlap with the on-premises range to avoid IP conflicts. Which of the following strategies should you adopt?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297262,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You're architecting a web application on Google Cloud that is expected to store and process personal data from users in the European Union (EU), thus it must meet the General Data Protection Regulation (GDPR) requirements. Which of the following strategies would you adopt?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Store all personal data in a regional storage bucket in an EU country, leverage Google Cloud Armor for data protection, and use Cloud Audit Logs for auditing. -&gt; Correct. This strategy covers data storage in the EU, data protection, and auditing, all of which are crucial for GDPR compliance. Google Cloud Armor helps protect against Distributed Denial of Service (DDoS) attacks, while Cloud Audit Logs provides visibility into how your cloud resources are being used.</p><p><br></p><p>Store all personal data in a regional storage bucket in the United States, and leverage Google's built-in data protection features. -&gt; Incorrect. GDPR has strict requirements about transferring personal data outside the EU. Therefore, storing personal data in a regional storage bucket in the United States could potentially violate GDPR, even if Google's built-in data protection features are used.</p><p><br></p><p>Store all personal data in a multi-regional storage bucket, and leverage Google Cloud Data Loss Prevention (DLP) to discover, classify, and redact sensitive data. -&gt; Incorrect. Google Cloud DLP is a powerful tool to discover, classify, and redact sensitive data. However, using a multi-regional storage bucket could potentially violate GDPR if personal data of EU users is stored outside the EU.</p><p><br></p><p>Implement user authentication and authorization using Firebase Authentication, and store all personal data in a multi-regional storage bucket. -&gt; Incorrect. Even though Firebase Authentication can help implement user authentication and authorization, storing personal data in a multi-regional storage bucket could potentially violate GDPR as personal data of EU users may be stored outside the EU.</p>",
                "answers": [
                    "<p>Store all personal data in a regional storage bucket in an EU country, leverage Google Cloud Armor for data protection, and use Cloud Audit Logs for auditing.</p>",
                    "<p>Store all personal data in a regional storage bucket in the United States, and leverage Google's built-in data protection features.</p>",
                    "<p>Store all personal data in a multi-regional storage bucket, and leverage Google Cloud Data Loss Prevention (DLP) to discover, classify, and redact sensitive data.</p>",
                    "<p>Implement user authentication and authorization using Firebase Authentication, and store all personal data in a multi-regional storage bucket.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You're architecting a web application on Google Cloud that is expected to store and process personal data from users in the European Union (EU), thus it must meet the General Data Protection Regulation (GDPR) requirements. Which of the following strategies would you adopt?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297264,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are designing a solution to deploy a stateful workload on Google Cloud, and one of your requirements is to provide the same POSIX filesystem to all the nodes. Which of the following strategies should you adopt?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Deploy the application on a Google Kubernetes Engine cluster and use Cloud Filestore as the storage backend. -&gt; Correct. Cloud Filestore provides fully managed NFS file servers on Google Cloud for applications that require a filesystem interface and a shared filesystem for data. It can be used as a storage backend for a stateful application running on a GKE cluster, meeting the requirement.</p><p><br></p><p>Deploy the application on Compute Engine instances and use local SSDs for storage. -&gt; Incorrect. Local SSDs provide high-performance storage but do not provide a shared POSIX filesystem. Each Compute Engine instance will have its own separate filesystem.</p><p><br></p><p>Deploy the application on a Google Kubernetes Engine cluster and use Persistent Disk for storage. -&gt; Incorrect. While Persistent Disk provides durable and high-performance block storage for Google Cloud instances, it does not provide a shared filesystem that can be accessed simultaneously by multiple nodes of the application.</p><p><br></p><p>Deploy the application on Compute Engine instances and use Cloud Storage for shared filesystem. -&gt; Incorrect. Cloud Storage provides object storage and does not provide a shared POSIX filesystem. Therefore, it cannot be used as a shared filesystem for Compute Engine instances.</p>",
                "answers": [
                    "<p>Deploy the application on a Google Kubernetes Engine cluster and use Cloud Filestore as the storage backend.</p>",
                    "<p>Deploy the application on Compute Engine instances and use local SSDs for storage.</p>",
                    "<p>Deploy the application on a Google Kubernetes Engine cluster and use Persistent Disk for storage.</p>",
                    "<p>Deploy the application on Compute Engine instances and use Cloud Storage for shared filesystem.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are designing a solution to deploy a stateful workload on Google Cloud, and one of your requirements is to provide the same POSIX filesystem to all the nodes. Which of the following strategies should you adopt?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297266,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You're a cloud architect who is developing a system that needs to trigger a Cloud Function on a regular basis. You've decided to use Cloud Scheduler to achieve this task. The Cloud Function you've developed is designed to automatically update the inventory in a Cloud Firestore database every day at midnight based on information pulled from an external API. Given this scenario, which of the following approaches is the most suitable way to accomplish this task?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Create a Pub/Sub topic and use Cloud Scheduler to publish a message to that topic at midnight every day. Set up the Cloud Function to trigger on this topic. -&gt; Correct. Cloud Scheduler can trigger a Cloud Function indirectly by publishing a message to a Pub/Sub topic. This is a common and recommended way to schedule function execution in Google Cloud.</p><p><br></p><p>Set up the Cloud Scheduler to trigger a Compute Engine instance that runs a script to call the Cloud Function at midnight every day. -&gt; Incorrect. This method introduces unnecessary complexity and potential failure points by incorporating Compute Engine, and it's not the recommended way to schedule a Cloud Function.</p><p><br></p><p>Set up the Cloud Function to trigger at midnight every day using its built-in scheduling functionality. -&gt; Incorrect. Cloud Functions do not have built-in scheduling functionality. You would need to use an external service like Cloud Scheduler to trigger the function.</p><p><br></p><p>Use Cloud Scheduler to create a cron job that runs on a Kubernetes Engine cluster to call the Cloud Function at midnight every day. -&gt; Incorrect. This method introduces unnecessary complexity by incorporating a Kubernetes Engine cluster, and it's not the recommended way to schedule a Cloud Function. It would be more efficient and cost-effective to use Pub/Sub with Cloud Scheduler to trigger the function.</p>",
                "answers": [
                    "<p>Create a Pub/Sub topic and use Cloud Scheduler to publish a message to that topic at midnight every day. Set up the Cloud Function to trigger on this topic.</p>",
                    "<p>Set up the Cloud Scheduler to trigger a Compute Engine instance that runs a script to call the Cloud Function at midnight every day.</p>",
                    "<p>Set up the Cloud Function to trigger at midnight every day using its built-in scheduling functionality.</p>",
                    "<p>Use Cloud Scheduler to create a cron job that runs on a Kubernetes Engine cluster to call the Cloud Function at midnight every day.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You're a cloud architect who is developing a system that needs to trigger a Cloud Function on a regular basis. You've decided to use Cloud Scheduler to achieve this task. The Cloud Function you've developed is designed to automatically update the inventory in a Cloud Firestore database every day at midnight based on information pulled from an external API. Given this scenario, which of the following approaches is the most suitable way to accomplish this task?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297268,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect working on an application that makes HTTP requests to a third-party API. To make your application more resilient, you decide to implement retry logic using a truncated exponential backoff strategy. Which of the following approaches would be the most effective way to implement this in your application?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use the <code>Retry</code> class from the Google API client library, and configure it to use exponential backoff. -&gt; Correct. The <code><strong>Retry</strong></code> class from the Google API client library provides functionality for retrying requests with exponential backoff, which fits the requirement of implementing a truncated exponential backoff strategy.</p><p><br></p><p>Implement a static wait time between retries, irrespective of the number of attempts made. -&gt; Incorrect. Implementing a static wait time between retries does not follow the exponential backoff strategy. The idea behind exponential backoff is to gradually increase the wait time after each failed attempt, thus reducing the potential for contention.</p><p><br></p><p>Implement a linear backoff strategy, increasing the wait time by a fixed amount after each failed attempt. -&gt; Incorrect. A linear backoff strategy increases the wait time by a fixed amount after each failed attempt, which is not as efficient as the exponential backoff strategy in terms of reducing contention and system load.</p><p><br></p><p>Use the <code>Retry</code> class from the Google API client library, and configure it to use a constant backoff strategy. -&gt; Incorrect. A constant backoff strategy, where the wait time between retries remains constant, is not as effective as the exponential backoff strategy in terms of reducing contention and system load.</p><p><br></p><p>Immediately retry the request upon each failure without any wait time. -&gt; Incorrect. Immediately retrying the request upon each failure without any wait time can increase contention and system load, and can potentially lead to a failure spiral if the external system is already under high load or experiencing temporary issues. It is not a recommended approach to handle failures.</p>",
                "answers": [
                    "<p>Use the <code>Retry</code> class from the Google API client library, and configure it to use exponential backoff.</p>",
                    "<p>Implement a static wait time between retries, irrespective of the number of attempts made.</p>",
                    "<p>Implement a linear backoff strategy, increasing the wait time by a fixed amount after each failed attempt.</p>",
                    "<p>Use the <code>Retry</code> class from the Google API client library, and configure it to use a constant backoff strategy.</p>",
                    "<p>Immediately retry the request upon each failure without any wait time.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect working on an application that makes HTTP requests to a third-party API. To make your application more resilient, you decide to implement retry logic using a truncated exponential backoff strategy. Which of the following approaches would be the most effective way to implement this in your application?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297270,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you have been tasked with setting up a robust and flexible content delivery system. The requirement is to have different Compute Engine instances serve content based on the URL path. For example, requests to <code><strong>www.example.com/audio/*</strong></code> should be served by one set of instances, and requests to <code><strong>www.example.com/video/*</strong></code> should be served by another set of instances. Which of the following would be the most appropriate approach to achieve this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Create an HTTPS load balancer and define URL maps to route traffic to the appropriate set of instances based on the path. -&gt; Correct. With Google Cloud HTTPS load balancer, you can create URL maps to route requests to specified backend services based on the URL paths.</p><p><br></p><p>Create two separate HTTPS load balancers, one for each path (<code><strong>/audio</strong></code> and <code><strong>/video</strong></code>), each pointing to a different set of instances. -&gt; Incorrect. Creating two separate HTTPS load balancers would not be cost-effective or efficient. A single load balancer with URL maps can handle different URL paths appropriately.</p><p><br></p><p>Create an HTTPS load balancer with a single backend service. Use instance tagging to route traffic to the correct set of instances. -&gt; Incorrect. Instance tagging is primarily used for applying metadata to instances and for network firewall rules. It cannot route traffic based on the URL path.</p><p><br></p><p>Create a single instance group and use instance templates to define the content to be served based on the URL path. -&gt;&nbsp;Incorrect. Instance templates define the machine type, boot disk image, and other instance properties for instances in a managed instance group. They cannot route traffic based on the URL path.</p>",
                "answers": [
                    "<p>Create an HTTPS load balancer and define URL maps to route traffic to the appropriate set of instances based on the path.</p>",
                    "<p>Create two separate HTTPS load balancers, one for each path (<code><strong>/audio</strong></code> and <code><strong>/video</strong></code>), each pointing to a different set of instances.</p>",
                    "<p>Create an HTTPS load balancer with a single backend service. Use instance tagging to route traffic to the correct set of instances.</p>",
                    "<p>Create a single instance group and use instance templates to define the content to be served based on the URL path.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you have been tasked with setting up a robust and flexible content delivery system. The requirement is to have different Compute Engine instances serve content based on the URL path. For example, requests to www.example.com/audio/* should be served by one set of instances, and requests to www.example.com/video/* should be served by another set of instances. Which of the following would be the most appropriate approach to achieve this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297272,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization has deployed a series of containerized applications on Google Kubernetes Engine (GKE). Given the unpredictable demand for these applications, you've been asked to ensure they scale efficiently to handle increased load without over-provisioning resources. Specifically, you've been asked to configure the system so that it automatically adds or removes pods based on the CPU utilization of existing ones. Which approach should you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Configure the Horizontal Pod Autoscaler for your deployments, setting an appropriate target CPU utilization. -&gt; Correct. The Horizontal Pod Autoscaler automatically scales the number of pods in a replication controller, deployment, replica set, or stateful set based on observed CPU utilization.</p><p><br></p><p>Implement a custom solution using Compute Engine instances that manually scales the number of pods based on CPU utilization. -&gt; Incorrect. While it's possible to implement a custom solution using Compute Engine, it would be labor-intensive and would not take full advantage of the automated scaling capabilities available within the Kubernetes platform.</p><p><br></p><p>Use the Vertical Pod Autoscaler to automatically adjust the CPU requests for pods, effectively scaling the pod's resources up and down based on utilization. -&gt; Incorrect. The Vertical Pod Autoscaler adjusts the CPU requests and limits of the pods, but does not adjust the number of pods. This approach would help optimize resource use per pod but would not help directly with scaling the number of pods to meet demand.</p><p><br></p><p>Implement a custom scaling solution using Cloud Functions to monitor CPU utilization and add or remove pods as needed. -&gt; Incorrect. While technically possible, implementing a custom scaling solution with Cloud Functions would likely be more complex, less efficient, and less reliable than utilizing the built-in Horizontal Pod Autoscaler.</p>",
                "answers": [
                    "<p>Configure the Horizontal Pod Autoscaler for your deployments, setting an appropriate target CPU utilization.</p>",
                    "<p>Implement a custom solution using Compute Engine instances that manually scales the number of pods based on CPU utilization.</p>",
                    "<p>Use the Vertical Pod Autoscaler to automatically adjust the CPU requests for pods, effectively scaling the pod's resources up and down based on utilization.</p>",
                    "<p>Implement a custom scaling solution using Cloud Functions to monitor CPU utilization and add or remove pods as needed.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization has deployed a series of containerized applications on Google Kubernetes Engine (GKE). Given the unpredictable demand for these applications, you've been asked to ensure they scale efficiently to handle increased load without over-provisioning resources. Specifically, you've been asked to configure the system so that it automatically adds or removes pods based on the CPU utilization of existing ones. Which approach should you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297274,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company wants to implement a scalable and highly available solution for a new web-based service. The service needs to be able to handle a large number of concurrent users, and must have the ability to automatically recover from individual machine failures. Which Google Cloud Platform service would you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Compute Engine instances in an auto-scaling group behind a load balancer. -&gt; Correct. It is the best option because it provides scalability, high availability, and automatic recovery from machine failures. Compute Engine instances can be set up in an auto-scaling group to automatically add or remove instances based on the current demand. By using a load balancer, traffic can be distributed across the instances, ensuring that the service remains available even if some of the instances fail.</p><p><br></p><p>App Engine Standard environment. -&gt; Incorrect. It is also a good option for scalability and automatic recovery, but it has some limitations in terms of customizability and control. It may not be suitable for all types of applications.</p><p><br></p><p>Google Kubernetes Engine cluster. -&gt; Incorrect. It requires more setup and management than Compute Engine instances in an auto-scaling group.</p><p><br></p><p>Cloud Functions with a managed instance group. -&gt; Incorrect. It is not a suitable option for this requirement, as Cloud Functions are event-driven and not designed for handling a large number of concurrent users.</p><p><br></p><p>https://cloud.google.com/compute/docs/autoscaler</p>",
                "answers": [
                    "<p>Compute Engine instances in an auto-scaling group behind a load balancer.</p>",
                    "<p>App Engine Standard environment.</p>",
                    "<p>Google Kubernetes Engine cluster.</p>",
                    "<p>Cloud Functions with a managed instance group.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company wants to implement a scalable and highly available solution for a new web-based service. The service needs to be able to handle a large number of concurrent users, and must have the ability to automatically recover from individual machine failures. Which Google Cloud Platform service would you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297276,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is rapidly expanding its operations globally and the amount of data you need to store and analyze is increasing at an exponential rate. The data is highly variable, comprising structured and unstructured data, and comes from various sources such as logs, user-generated content, and IoT devices. As a cloud architect, you are tasked with devising a strategy to store and analyze this data in a cost-effective manner, while ensuring performance, scalability, and data accessibility. Which of the following options would be most suitable?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Store all data in Google Cloud Storage (GCS) and use Google BigQuery for analysis. -&gt;&nbsp;Correct. Cloud Storage is cost-effective for storing large amounts of data, and it can handle structured and unstructured data. BigQuery is a serverless, highly scalable, and cost-effective multi-cloud data warehouse designed for business agility, which can be used to analyze the data stored in Cloud Storage.</p><p><br></p><p>Use Cloud Bigtable for both storing and analyzing the data. -&gt; Incorrect. Cloud Bigtable is designed for storing very large amounts of single-keyed data with low latency. It's excellent for time-series data like IoT, but it might not be as cost-effective for diverse data types or for complex analytical queries, compared to BigQuery.</p><p><br></p><p>Store structured data in Cloud SQL and unstructured data in Cloud Storage, using BigQuery for analysis. -&gt; Incorrect. While this approach might work for some use-cases, it requires managing two different storage services and may not be as cost-effective as option A, considering the overhead of managing Cloud SQL instances.</p><p><br></p><p>Store all data in Cloud Spanner and use BigQuery for analysis. -&gt; Incorrect. loud Spanner is a fully managed, mission-critical, relational database service that offers transactional consistency at global scale, schemas, SQL querying, and automatic, synchronous replication for high availability but it could be an overkill and not cost-effective for this scenario. It's better suited for application data which requires strong transactional consistency. Using BigQuery for analysis is right but storing data in Cloud Spanner might not be cost-effective.</p>",
                "answers": [
                    "<p>Store all data in Google Cloud Storage (GCS) and use Google BigQuery for analysis.</p>",
                    "<p>Use Cloud Bigtable for both storing and analyzing the data.</p>",
                    "<p>Store structured data in Cloud SQL and unstructured data in Cloud Storage, using BigQuery for analysis.</p>",
                    "<p>Store all data in Cloud Spanner and use BigQuery for analysis.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company is rapidly expanding its operations globally and the amount of data you need to store and analyze is increasing at an exponential rate. The data is highly variable, comprising structured and unstructured data, and comes from various sources such as logs, user-generated content, and IoT devices. As a cloud architect, you are tasked with devising a strategy to store and analyze this data in a cost-effective manner, while ensuring performance, scalability, and data accessibility. Which of the following options would be most suitable?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297278,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>When designing a disaster recovery solution for a multi-tier application on Google Cloud Platform (GCP), which of the following options should be considered in order to ensure a recovery time objective (RTO) of less than 1 hour?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Spanner for database management with multi-region replication. -&gt;&nbsp;Correct. Cloud Spanner is a horizontally scalable, globally-distributed database service that features multi-region replication. It can automatically handle replication and failover without manual intervention, making it ideal to meet a short RTO like 1 hour. It ensures that your data is available and consistent even in the event of a disaster.</p><p><br></p><p>Use Cloud Storage for data backup and replication to a secondary region. -&gt;&nbsp;Incorrect. While Cloud Storage is suitable for data backup and replication, this strategy alone may not guarantee a recovery time objective (RTO) of less than 1 hour due to potential time required for restoring the system state and data from the backup.</p><p><br></p><p>Use Cloud Load Balancer with instance groups in multiple regions for high availability. -&gt;&nbsp;Incorrect. Cloud Load Balancer with instance groups in multiple regions indeed improves availability, but in the context of a disaster recovery, it doesn't cover database or storage failover. Hence, it may not be able to ensure an RTO of less than 1 hour.</p><p><br></p><p>Use Cloud SQL with automatic failover to a secondary zone in the same region. -&gt;&nbsp;Incorrect. Cloud SQL with automatic failover to a secondary zone improves high availability but it's limited to the same region. In a disaster that affects an entire region, this solution may not meet the RTO requirement of less than 1 hour.</p>",
                "answers": [
                    "<p>Use Cloud Storage for data backup and replication to a secondary region.</p>",
                    "<p>Use Cloud Load Balancer with instance groups in multiple regions for high availability.</p>",
                    "<p>Use Cloud SQL with automatic failover to a secondary zone in the same region.</p>",
                    "<p>Use&nbsp; Cloud Spanner for database management with multi-region replication.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "When designing a disaster recovery solution for a multi-tier application on Google Cloud Platform (GCP), which of the following options should be considered in order to ensure a recovery time objective (RTO) of less than 1 hour?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297280,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>In order to achieve compliance with data privacy regulations, a company must encrypt sensitive data in transit and at rest. In Google Cloud Platform (GCP), which of the following options would meet this requirement for data stored in Google Cloud Storage?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use SSL/TLS encryption to secure data in transit and enable Google Cloud Key Management Service (KMS) encryption for data at rest. -&gt;&nbsp;Correct. In Google Cloud Platform,&nbsp; SSL/TLS encryption should be used to secure data in transit, and Google Cloud Key Management Service (KMS) encryption should be enabled for data at rest. This meets the requirement for data privacy regulations to encrypt sensitive data in transit and at rest. </p><p><br></p><p>Use Google Cloud Storage Transfer Service to encrypt data in transit and enable server-side encryption in Google Cloud Storage. -&gt; Incorrect. It is incorrect because it only provides server-side encryption for data at rest but does not cover data in transit. </p><p><br></p><p>Use customer-managed encryption keys for data in transit and at rest using Google Cloud Storage Bucket Policy. -&gt; Incorrect. It is incorrect because customer-managed encryption keys only cover data at rest and not data in transit. </p><p><br></p><p>Enable Google-managed encryption for data in transit and at rest using Google Cloud Storage Bucket Policy. -&gt; Incorrect. It is incorrect because Google-managed encryption keys also only cover data at rest and not data in transit.</p>",
                "answers": [
                    "<p>Use Google Cloud Storage Transfer Service to encrypt data in transit and enable server-side encryption in Google Cloud Storage.</p>",
                    "<p>Use SSL/TLS encryption to secure data in transit and enable Google Cloud Key Management Service (KMS) encryption for data at rest.</p>",
                    "<p>Use customer-managed encryption keys for data in transit and at rest using Google Cloud Storage Bucket Policy.</p>",
                    "<p>Enable Google-managed encryption for data in transit and at rest using Google Cloud Storage Bucket Policy.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "In order to achieve compliance with data privacy regulations, a company must encrypt sensitive data in transit and at rest. In Google Cloud Platform (GCP), which of the following options would meet this requirement for data stored in Google Cloud Storage?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297282,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A company is planning to deploy a new web application on Google Cloud Platform (GCP) that will handle sensitive customer data. In order to meet security and compliance requirements, which of the following strategies should be considered when designing the application's architecture?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Store sensitive data in Cloud Datastore and use Cloud Data Loss Prevention (DLP) to classify and redact sensitive data. -&gt;&nbsp;Correct. Storing sensitive data in Cloud Datastore and using Cloud Data Loss Prevention (DLP) provides a comprehensive solution for securing and managing sensitive data. It includes features for data classification and redaction, ensuring compliance with security and privacy requirements.</p><p><br></p><p>Store sensitive data in Cloud SQL and implement role-based access controls to restrict access. -&gt; Incorrect. It may not cover all aspects of security and compliance requirements. Additional measures, such as encryption of data at rest and in transit, may be necessary to ensure the confidentiality and integrity of sensitive data.</p><p><br></p><p>Store sensitive data in Cloud Bigtable and use Identity and Access Management (IAM) to manage access. -&gt; Incorrect. It may not provide the same level of data security and compliance as the correct choice. Bigtable is a NoSQL database and may require additional security measures to ensure data confidentiality and integrity.</p><p><br></p><p>Store sensitive data in Cloud Storage and use customer-managed encryption keys to secure data at rest. -&gt; Incorrect. It may not directly address the need for sensitive data handling and compliance requirements, such as data classification and redaction.</p>",
                "answers": [
                    "<p>Store sensitive data in Cloud SQL and implement role-based access controls to restrict access.</p>",
                    "<p>Store sensitive data in Cloud Bigtable and use Identity and Access Management (IAM) to manage access.</p>",
                    "<p>Store sensitive data in Cloud Storage and use customer-managed encryption keys to secure data at rest.</p>",
                    "<p>Store sensitive data in Cloud Datastore and use Cloud Data Loss Prevention (DLP) to classify and redact sensitive data.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "A company is planning to deploy a new web application on Google Cloud Platform (GCP) that will handle sensitive customer data. In order to meet security and compliance requirements, which of the following strategies should be considered when designing the application's architecture?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297284,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A company is planning to migrate a large number of virtual machines (VMs) from an on-premises data center to Google Cloud Platform (GCP). The VMs are running a mix of Linux and Windows operating systems and have varying CPU, memory, and storage requirements. Which of the following options would be the most effective approach to automate the migration process and minimize downtime?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use the Google Cloud Migrate for Compute Engine to automate the discovery, assessment, and migration of the VMs to Google Compute Engine. -&gt; Correct. Google Cloud Migrate for Compute Engine is a service designed to simplify the migration of virtual machines from on-premises data centers or other cloud platforms to Google Compute Engine. It automates the discovery, assessment, and migration of virtual machines, making the migration process more efficient and reducing the risk of downtime.</p><p><br></p><p>Use Google Cloud Storage Transfer Service to transfer data to GCP and then manually create and configure new instances in Google Compute Engine. -&gt; Incorrect. This option requires manual creation and configuration of new instances, which can be&nbsp; time-consuming and increase the risk of errors.</p><p><br></p><p>Use the Google Cloud Deployment Manager to automate the creation and configuration of new instances in Google Compute Engine, and then manually transfer data to the new instances. -&gt; Incorrect. This option requires manual creation and configuration of new instances, which can be&nbsp; time-consuming and increase the risk of errors.</p><p><br></p><p>Use the Google Cloud Dataproc to automate the creation and configuration of new instances in Google Compute Engine, and then use the Hadoop Distributed File System (HDFS) to transfer data to the new instances. -&gt; Incorrect. It is designed for running big data workloads, and is not the best option for VM migration.</p>",
                "answers": [
                    "<p>Use Google Cloud Storage Transfer Service to transfer data to GCP and then manually create and configure new instances in Google Compute Engine.</p>",
                    "<p>Use the Google Cloud Deployment Manager to automate the creation and configuration of new instances in Google Compute Engine, and then manually transfer data to the new instances.</p>",
                    "<p>Use the Google Cloud Migrate for Compute Engine to automate the discovery, assessment, and migration of the VMs to Google Compute Engine.</p>",
                    "<p>Use the Google Cloud Dataproc to automate the creation and configuration of new instances in Google Compute Engine, and then use the Hadoop Distributed File System (HDFS) to transfer data to the new instances.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "A company is planning to migrate a large number of virtual machines (VMs) from an on-premises data center to Google Cloud Platform (GCP). The VMs are running a mix of Linux and Windows operating systems and have varying CPU, memory, and storage requirements. Which of the following options would be the most effective approach to automate the migration process and minimize downtime?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297286,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A company is planning to deploy a new e-commerce application on Google Cloud Platform (GCP) that will handle high volumes of traffic during peak periods. The application will be deployed in multiple regions to provide low latency and high availability to customers globally. Which of the following options would be the most effective approach to handle traffic spikes and ensure that the application can scale dynamically to meet demand?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Kubernetes Engine to deploy and manage containers for the application and handle spikes in traffic. -&gt;&nbsp;Correct. Using Cloud Kubernetes Engine to deploy and manage containers provides a scalable and flexible solution for handling traffic spikes and ensuring that the application can dynamically scale to meet demand. It offers efficient container management, auto-scaling capabilities, and high availability, making it suitable for an e-commerce application that needs to handle high volumes of traffic during peak periods.</p><p><br></p><p>Use Cloud Load Balancer with instance groups in each region to distribute traffic and handle spikes. -&gt;&nbsp;Incorrect. It may not provide the same level of scalability and flexibility as container-based solutions. Load balancers can distribute traffic, but managing and scaling individual instances may be more cumbersome compared to container-based deployments.</p><p><br></p><p>Use Cloud CDN to cache content closer to the end-user and handle spikes in traffic. -&gt;&nbsp;Incorrect. While it can help with offloading static content and reducing the load on the application servers, it may not be the most effective approach for handling dynamic scaling of the application itself.</p><p><br></p><p>Use Cloud AutoML to build and deploy custom machine learning models that can automatically adjust the number of instances in each region based on traffic patterns. -&gt;&nbsp;Incorrect. It may introduce unnecessary complexity for handling traffic spikes. AutoML is typically used for building machine learning models and making predictions rather than directly managing application scalability.</p>",
                "answers": [
                    "<p>Use Cloud Load Balancer with instance groups in each region to distribute traffic and handle spikes.</p>",
                    "<p>Use Cloud CDN to cache content closer to the end-user and handle spikes in traffic.</p>",
                    "<p>Use Cloud AutoML to build and deploy custom machine learning models that can automatically adjust the number of instances in each region based on traffic patterns.</p>",
                    "<p>Use Google Cloud Kubernetes Engine to deploy and manage containers for the application and handle spikes in traffic.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "A company is planning to deploy a new e-commerce application on Google Cloud Platform (GCP) that will handle high volumes of traffic during peak periods. The application will be deployed in multiple regions to provide low latency and high availability to customers globally. Which of the following options would be the most effective approach to handle traffic spikes and ensure that the application can scale dynamically to meet demand?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297288,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A multinational corporation with offices in multiple regions is looking to deploy a disaster recovery solution to ensure business continuity in the event of a regional disaster. They have the following requirements:</p><ul><li><p>minimize downtime in the event of a disaster</p></li><li><p>automatically failover to a secondary site in the event of a disaster</p></li><li><p>ensure data consistency between the primary and secondary sites</p></li><li><p>cost-effective solution</p></li></ul><p><br></p><p>Which of the following Google Cloud solutions would best meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Storage with object versioning and multi-region bucket replication. -&gt;&nbsp;Correct. It is the recommended solution for disaster recovery in this scenario. Object versioning allows for the preservation of previous versions of objects, ensuring data consistency and recoverability. Multi-region bucket replication automatically replicates data to a secondary location, providing automatic failover capabilities and minimizing downtime in the event of a disaster. This solution is cost-effective and meets all the specified requirements.</p><p><br></p><p>Cloud Load Balancer with auto-scaling groups. -&gt;&nbsp;Incorrect. While it can help with minimizing downtime, it does not directly address the requirement of automatically failing over to a secondary site in the event of a disaster or ensuring data consistency.</p><p><br></p><p>Cloud Datastore with multi-region replication. -&gt;&nbsp;Incorrect. It may not be the most suitable solution for disaster recovery as it does not offer automatic failover capabilities or provide the same level of data consistency between primary and secondary sites as other options.</p><p><br></p><p>Cloud SQL with read replicas in multiple regions. -&gt;&nbsp;Incorrect. While it can help with data consistency and scalability, it does not provide automatic failover capabilities or the same level of disaster recovery features as other options.</p>",
                "answers": [
                    "<p>Cloud Load Balancer with auto-scaling groups.</p>",
                    "<p>Cloud Datastore with multi-region replication.</p>",
                    "<p>Cloud SQL with read replicas in multiple regions.</p>",
                    "<p>Cloud Storage with object versioning and multi-region bucket replication.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "A multinational corporation with offices in multiple regions is looking to deploy a disaster recovery solution to ensure business continuity in the event of a regional disaster. They have the following requirements:minimize downtime in the event of a disasterautomatically failover to a secondary site in the event of a disasterensure data consistency between the primary and secondary sitescost-effective solutionWhich of the following Google Cloud solutions would best meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297290,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A large financial services company is looking to migrate its legacy data warehousing solution to the cloud to reduce costs and improve performance. The data warehousing solution must handle the following requirements:</p><ul><li><p>store and process petabytes of financial data</p></li><li><p>support real-time data ingestion and analysis</p></li><li><p>ensure data security and compliance with industry regulations</p></li><li><p>provide a flexible and scalable architecture for future growth</p></li></ul><p><br></p><p>Which of the following Google Cloud solutions would best meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>BigQuery with Cloud Dataflow and Cloud Pub/Sub -&gt;&nbsp;Correct. BigQuery is designed to handle petabyte-scale data and supports real-time data analysis, meeting the primary requirements. It also provides robust security measures and supports compliance with regulations. Cloud Dataflow can manage real-time data ingestion and processing, and Cloud Pub/Sub can handle real-time messaging, which all together provide a flexible and scalable solution for the company's needs.</p><p><br></p><p>Cloud SQL with Cloud Storage and Cloud Data Fusion -&gt;&nbsp;Incorrect. Cloud SQL is a relational database service and might not be the best fit for handling petabytes of data or real-time analysis. Cloud Storage, although durable and scalable, is not ideal for real-time data ingestion and processing. Cloud Data Fusion is an integrated data pipeline solution, but does not fully support real-time operations.</p><p><br></p><p>Bigtable with Cloud Storage and Cloud Functions -&gt;&nbsp;Incorrect. While Bigtable is designed for large operational and analytical workloads, it might not be cost-effective for petabytes of financial data. Cloud Functions is a serverless execution environment and may not handle real-time data ingestion and processing at the required scale.</p><p><br></p><p>Cloud Dataproc with Cloud Storage and Cloud Datastore -&gt;&nbsp;Incorrect. Cloud Dataproc is a managed Hadoop and Spark service, which is not designed for real-time data ingestion and analysis. Cloud Datastore is a NoSQL document database which is not ideal for large-scale, complex analytical workloads.</p>",
                "answers": [
                    "<p>BigQuery with Cloud Dataflow and Cloud Pub/Sub</p>",
                    "<p>Cloud SQL with Cloud Storage and Cloud Data Fusion</p>",
                    "<p>Bigtable with Cloud Storage and Cloud Functions</p>",
                    "<p>Cloud Dataproc with Cloud Storage and Cloud Datastore</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A large financial services company is looking to migrate its legacy data warehousing solution to the cloud to reduce costs and improve performance. The data warehousing solution must handle the following requirements:store and process petabytes of financial datasupport real-time data ingestion and analysisensure data security and compliance with industry regulationsprovide a flexible and scalable architecture for future growthWhich of the following Google Cloud solutions would best meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297292,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are designing a public-facing web application on Google Cloud. The application is expected to handle sensitive user data. Which of the following should you consider implementing to enhance security?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use IAM roles to restrict access to resources. -&gt; Correct. IAM roles are critical for controlling who has what kind of access to your resources. By giving the least privileges required to perform a task, you can greatly enhance security.</p><p><br></p><p>Enable HTTP(S) Load Balancing to distribute traffic. -&gt;&nbsp;Incorrect. While HTTP(S) Load Balancing can distribute traffic, it doesn't directly enhance security.</p><p><br></p><p>Use Cloud CDN to cache and deliver content. -&gt;&nbsp;Incorrect. While Cloud CDN can improve the speed of content delivery, it doesn't directly enhance security.</p><p><br></p><p>Enable Cloud Trace to monitor application performance. -&gt;&nbsp;Incorrect. Cloud Trace helps you analyze the latency of your application, but it doesn't directly enhance security.</p>",
                "answers": [
                    "<p>Use IAM roles to restrict access to resources.</p>",
                    "<p>Enable HTTP(S) Load Balancing to distribute traffic.</p>",
                    "<p>Use Cloud CDN to cache and deliver content.</p>",
                    "<p>Enable Cloud Trace to monitor application performance.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are designing a public-facing web application on Google Cloud. The application is expected to handle sensitive user data. Which of the following should you consider implementing to enhance security?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297294,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A company has recently adopted Google Cloud Platform (GCP) for its infrastructure and wants to ensure that its virtual machine (VM) instances are automatically restarted if they fail. Which of the following options should be used to meet this requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement a managed instance group and enable automatic restart. -&gt; Correct. A managed instance group (MIG) is a collection of homogeneous VM instances that are created from a common instance template. By using a MIG, you can configure automatic restart for failed instances. The MIG will automatically recreate any instance that has failed due to a software or hardware failure. You can also specify a health check for the instances in the MIG, so that the MIG can determine when an instance is unhealthy and replace it with a new instance.</p><p><br></p><p>Enable automatic restart for individual instances in the Cloud Console. -&gt; Incorrect. It is not scalable and requires manual intervention. </p><p><br></p><p>Use a startup script to automatically restart instances. -&gt; Incorrect. It can be used to automate the restart process, but does not provide the benefits of a MIG, such as scalability and automatic replacement of unhealthy instances. </p><p><br></p><p>Use a custom health check to determine instance failure and trigger an automatic restart. -&gt; Incorrect. It is not as efficient as using a MIG because it requires manual intervention to trigger the restart process.</p>",
                "answers": [
                    "<p>Enable automatic restart for individual instances in the Cloud Console.</p>",
                    "<p>Use a startup script to automatically restart instances.</p>",
                    "<p>Implement a managed instance group and enable automatic restart.</p>",
                    "<p>Use a custom health check to determine instance failure and trigger an automatic restart.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "A company has recently adopted Google Cloud Platform (GCP) for its infrastructure and wants to ensure that its virtual machine (VM) instances are automatically restarted if they fail. Which of the following options should be used to meet this requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297296,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A company is planning to migrate its on-premises data center to Google Cloud Platform (GCP). The company has large amounts of data and wants to minimize downtime during the migration process and ensure that its data is secure during the transfer. Which of the following options should be used to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Google Transfer Appliance to physically transfer data from on-premises to GCP and encrypt data at rest using Google-managed encryption keys. -&gt;&nbsp;Correct. It is the recommended approach. Google Transfer Appliance allows for offline, high-speed data transfer by physically shipping the appliance to the company's premises. It provides a secure and efficient method to migrate large amounts of data while ensuring data security at rest through encryption with Google-managed encryption keys. This option helps minimize downtime during the migration process.</p><p><br></p><p>Use <code>gsutil</code> to transfer data from on-premises to GCP and encrypt data in transit using SSL. -&gt;&nbsp;Incorrect. It does not specifically address the requirement of minimizing downtime during the migration process. Additionally, encrypting data in transit ensures data security during transfer but does not address data security at rest.</p><p><br></p><p>Use <code>rsync</code> to transfer data from on-premises to GCP and encrypt data at rest using customer-supplied encryption keys. -&gt;&nbsp;Incorrect. It does not address the need to minimize downtime during the migration process. <code>rsync</code> is a file synchronization tool and may not be the most efficient option for transferring large amounts of data during migration.</p><p><br></p><p>Use <code>gcloud compute scp</code> to transfer data from on-premises to GCP and encrypt data in transit using SSH. -&gt;&nbsp;Incorrect. It does not specifically address the requirement of minimizing downtime during the migration process. While encrypting data in transit provides security during transfer, it does not cover data security at rest.</p>",
                "answers": [
                    "<p>Use <code>gsutil</code> to transfer data from on-premises to GCP and encrypt data in transit using SSL.</p>",
                    "<p>Use <code>rsync</code> to transfer data from on-premises to GCP and encrypt data at rest using customer-supplied encryption keys.</p>",
                    "<p>Use <code>gcloud compute scp</code> to transfer data from on-premises to GCP and encrypt data in transit using SSH.</p>",
                    "<p>Use Google Transfer Appliance to physically transfer data from on-premises to GCP and encrypt data at rest using Google-managed encryption keys.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "A company is planning to migrate its on-premises data center to Google Cloud Platform (GCP). The company has large amounts of data and wants to minimize downtime during the migration process and ensure that its data is secure during the transfer. Which of the following options should be used to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297298,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A company is using Cloud Storage as its primary object storage solution and wants to ensure that all data stored in the storage is available even in the event of a regional outage. Which of the following options should be used to meet this requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Enable multi-region bucket replication for the storage. -&gt;&nbsp;Correct. Enabling multi-region bucket replication for the storage is the recommended approach to ensure data availability even in the event of a regional outage. With multi-region bucket replication, the data stored in Cloud Storage is automatically replicated to multiple regions, providing redundancy and allowing for continued access to the data even if one region becomes unavailable.</p><p><br></p><p>Enable cross-region bucket transfer for the storage. -&gt; Incorrect. Enabling cross-region bucket transfer for the storage is not the most suitable option for ensuring data availability in the event of a regional outage. Cross-region bucket transfer is typically used for copying or migrating data between different regions, but it does not provide the same level of redundancy and continuous availability as multi-region bucket replication.</p><p><br></p><p>Enable cross-project bucket transfer for the storage. -&gt; Incorrect. Enabling cross-project bucket transfer for the storage allows for transferring data between different projects but does not directly address the requirement of ensuring data availability in the event of a regional outage. Cross-project bucket transfer focuses on sharing or moving data between projects rather than providing redundancy for high availability.</p><p><br></p><p>Enable multi-project bucket replication for the storage. -&gt; Incorrect. Enabling multi-project bucket replication for the storage is not a valid option as there is no built-in feature in Cloud Storage specifically called \"multi-project bucket replication.\"</p>",
                "answers": [
                    "<p>Enable multi-region bucket replication for the storage.</p>",
                    "<p>Enable cross-region bucket transfer for the storage.</p>",
                    "<p>Enable cross-project bucket transfer for the storage.</p>",
                    "<p>Enable multi-project bucket replication for the storage.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A company is using Cloud Storage as its primary object storage solution and wants to ensure that all data stored in the storage is available even in the event of a regional outage. Which of the following options should be used to meet this requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297300,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your client is planning to deploy a multi-tier application in Google Cloud that involves storing and processing highly sensitive user data. What is the most secure method for managing secrets such as database credentials and API keys within this environment?</p>",
                "relatedLectureIds": [],
                "links": [],
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement secrets using Google Cloud Secret Manager and enforce access control with IAM policies. -&gt; Correct. Google Cloud Secret Manager provides a centralized and secure service for managing, accessing, and auditing secrets across Google Cloud, with IAM providing fine-grained access control.</p><p><br></p><p>Hard-code secrets into the application source code and rely on source code management for security. -&gt; Incorrect. Hard-coding secrets in source code exposes sensitive information to unnecessary risks and is not recommended as it violates basic security best practices.</p><p><br></p><p>Store secrets in environment variables of the cloud VM instances. -&gt; Incorrect. Although environment variables are a common method to handle secrets, they can be accessed by any process in the VM and are not secure enough for highly sensitive data.</p><p><br></p><p>Utilize Cloud Security Command Center to automatically manage and rotate secrets. -&gt; Incorrect. Cloud Security Command Center is primarily used for security monitoring and compliance and does not manage secrets.</p>",
                "answers": [
                    "<p>Implement secrets using Google Cloud Secret Manager and enforce access control with IAM policies.</p>",
                    "<p>Hard-code secrets into the application source code and rely on source code management for security.</p>",
                    "<p>Store secrets in environment variables of the cloud VM instances.</p>",
                    "<p>Utilize Cloud Security Command Center to automatically manage and rotate secrets.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your client is planning to deploy a multi-tier application in Google Cloud that involves storing and processing highly sensitive user data. What is the most secure method for managing secrets such as database credentials and API keys within this environment?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297302,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A company is using Google Cloud Platform (GCP) for hosting its mission-critical applications and wants to ensure that the data stored in Google Cloud Storage is accessible from its on-premises data center. Which of the following options should be used to meet this requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Interconnect to connect the company's on-premises data center to GCP. -&gt;&nbsp;Correct. If a company wants to ensure that the data stored in Google Cloud Storage is accessible from its on-premises data center, Cloud Interconnect should be used to connect the two. Cloud Interconnect is a service that allows customers to connect their on-premises data centers to Google Cloud Platform (GCP) over a dedicated, highly available, low-latency connection.</p><p><br></p><p>Enable Direct Peering between the company's on-premises data center and GCP. -&gt; Incorrect. It is incorrect because Direct Peering is used to connect a company's network to Google's network at one of Google's edge locations, not to connect on-premises data centers to GCP.</p><p><br></p><p>Mount Cloud Storage as a network file system using GCSFuse. -&gt; Incorrect. It is also incorrect because GCSFuse is a tool that allows Cloud Storage buckets to be mounted as file systems on virtual machines and does not provide connectivity between on-premises data centers and GCP.</p><p><br></p><p>Use Transfer Appliance to physically transfer data from GCP to the company's on-premises data center. -&gt; Incorrect. It is also incorrect because Transfer Appliance is used to securely transfer large amounts of data to Google Cloud, but it is not a solution for ongoing data access between on-premises data centers and GCP.</p><p><br></p><p>https://cloud.google.com/network-connectivity/docs/interconnect</p>",
                "answers": [
                    "<p>Enable Direct Peering between the company's on-premises data center and GCP.</p>",
                    "<p>Use Cloud Interconnect to connect the company's on-premises data center to GCP.</p>",
                    "<p>Mount Cloud Storage as a network file system using GCSFuse.</p>",
                    "<p>Use Transfer Appliance to physically transfer data from GCP to the company's on-premises data center.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "A company is using Google Cloud Platform (GCP) for hosting its mission-critical applications and wants to ensure that the data stored in Google Cloud Storage is accessible from its on-premises data center. Which of the following options should be used to meet this requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297304,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>In a gaming project where a company is planning to deploy a high-performance transactional database on Google Cloud Platform (GCP) and prioritize high availability and scalability, which GCP service should be used to meet these requirements? Choose the correct option from the five given answers.</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Spanner -&gt; Correct. Cloud Spanner is a globally distributed and highly available relational database service on GCP. It provides strong consistency guarantees, automatic scaling, and built-in high availability features. It is an ideal choice for a high-performance database in a gaming project that requires both scalability and high availability.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. Cloud Datastore is a NoSQL document database and may not provide the same level of performance and scalability as Cloud Spanner in this gaming project scenario.</p><p><br></p><p>Cloud Pub/Sub -&gt; Incorrect. Cloud Pub/Sub is a messaging service for building event-driven systems and does not focus on high-performance databases or addressing high availability and sudden traffic spikes.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. Cloud Storage is a scalable object storage solution, but it is not specifically designed for hosting high-performance databases or addressing high availability and sudden traffic spikes in a gaming project.</p>",
                "answers": [
                    "<p>Cloud Spanner</p>",
                    "<p>Cloud Datastore</p>",
                    "<p>Cloud Pub/Sub</p>",
                    "<p>Cloud Storage</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "In a gaming project where a company is planning to deploy a high-performance transactional database on Google Cloud Platform (GCP) and prioritize high availability and scalability, which GCP service should be used to meet these requirements? Choose the correct option from the five given answers.",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297306,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization has a web application running on a single virtual machine (VM) in Google Compute Engine. The application is experiencing high traffic and the VM is struggling to handle the load. You want to scale the application to multiple VMs to increase performance and availability. However, the application requires access to a shared file system for storing user data. What is the most efficient and cost-effective way to scale the application while still maintaining access to the shared file system?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use a managed file storage service, such as Google Cloud Filestore, and configure each VM to mount the shared file system. -&gt; Correct. This is the correct answer because it is the most efficient and cost-effective solution. Google Cloud Filestore is a fully managed service that provides a high-performance file system with low latency access to data. It can be easily mounted by each VM in the auto-scaling group, allowing the application to scale seamlessly while still maintaining access to the shared file system.</p><p><br></p><p>Use Google Cloud Storage to store user data and modify the application to access the data through the Cloud Storage API. -&gt; Incorrect. While this is a viable solution, it would require significant changes to the application code and could potentially result in performance issues due to the latency involved in accessing data over the network.</p><p><br></p><p>Create a dedicated VM to host the shared file system and configure each VM in the auto-scaling group to mount the file system. -&gt; Incorrect. While this would work, it would be less efficient and more expensive than using a managed file storage service like Google Cloud Filestore.</p><p><br></p><p>Modify the application to store user data in a distributed database, such as Cloud Spanner, and configure each VM to access the database. -&gt; Incorrect. While this is a valid solution, it would require significant changes to the application code and would be more expensive than using a managed file storage service like Google Cloud Filestore.</p>",
                "answers": [
                    "<p>Use a managed file storage service, such as Google Cloud Filestore, and configure each VM to mount the shared file system.</p>",
                    "<p>Use Google Cloud Storage to store user data and modify the application to access the data through the Cloud Storage API.</p>",
                    "<p>Create a dedicated VM to host the shared file system and configure each VM in the auto-scaling group to mount the file system.</p>",
                    "<p>Modify the application to store user data in a distributed database, such as Cloud Spanner, and configure each VM to access the database.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization has a web application running on a single virtual machine (VM) in Google Compute Engine. The application is experiencing high traffic and the VM is struggling to handle the load. You want to scale the application to multiple VMs to increase performance and availability. However, the application requires access to a shared file system for storing user data. What is the most efficient and cost-effective way to scale the application while still maintaining access to the shared file system?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297308,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You're a cloud architect assigned to manage access to resources in a Google Cloud project for a team. The team consists of a Data Scientist who needs to run BigQuery jobs, a Cloud Engineer who deploys applications on App Engine, a Network Engineer who manages VPCs, and an Auditor who checks IAM permissions and resource usage. Which of the following sets of predefined roles should you assign to each team member to grant them the least privilege they need to do their job effectively?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>BigQuery User to the Data Scientist, App Engine Admin to the Cloud Engineer, Compute Network Admin to the Network Engineer, and Security Reviewer to the Auditor. -&gt; Correct. The BigQuery User role allows the Data Scientist to run BigQuery jobs. The App Engine Admin role allows the Cloud Engineer to deploy applications. The Compute Network Admin role allows the Network Engineer to manage VPCs. The Security Reviewer role allows the Auditor to view IAM permissions and resource usage.</p><p><br></p><p>BigQuery User to the Data Scientist, App Engine Viewer to the Cloud Engineer, Compute Network Admin to the Network Engineer, and Viewer to the Auditor. -&gt; Incorrect. In this set, the Cloud Engineer and Network Engineer don't have the necessary roles to carry out their duties effectively. App Engine Viewer and Compute Network Admin do not provide enough permissions for application deployment and network management, respectively.</p><p><br></p><p>BigQuery User to the Data Scientist, App Engine Admin to the Cloud Engineer, Compute Network User to the Network Engineer, and Security Reviewer to the Auditor. -&gt;&nbsp;Incorrect. In this set, the Network Engineer has the Compute Network User role, which doesn't allow managing VPCs.</p><p><br></p><p>BigQuery Data Editor to the Data Scientist, App Engine Service Admin to the Cloud Engineer, Compute Network Viewer to the Network Engineer, and Security Reviewer to the Auditor. -&gt; Incorrect. In this set, the Data Scientist and Network Engineer roles are inadequate. BigQuery Data Editor doesn't provide permission for job execution, and Compute Network Viewer doesn't provide enough permissions for network management.</p>",
                "answers": [
                    "<p>BigQuery User to the Data Scientist, App Engine Admin to the Cloud Engineer, Compute Network Admin to the Network Engineer, and Security Reviewer to the Auditor.</p>",
                    "<p>BigQuery User to the Data Scientist, App Engine Viewer to the Cloud Engineer, Compute Network Admin to the Network Engineer, and Viewer to the Auditor.</p>",
                    "<p>BigQuery User to the Data Scientist, App Engine Admin to the Cloud Engineer, Compute Network User to the Network Engineer, and Security Reviewer to the Auditor.</p>",
                    "<p>BigQuery Data Editor to the Data Scientist, App Engine Service Admin to the Cloud Engineer, Compute Network Viewer to the Network Engineer, and Security Reviewer to the Auditor.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You're a cloud architect assigned to manage access to resources in a Google Cloud project for a team. The team consists of a Data Scientist who needs to run BigQuery jobs, a Cloud Engineer who deploys applications on App Engine, a Network Engineer who manages VPCs, and an Auditor who checks IAM permissions and resource usage. Which of the following sets of predefined roles should you assign to each team member to grant them the least privilege they need to do their job effectively?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297310,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You're a cloud architect working for a company that heavily uses BigQuery for data analysis. Recently, the company has experienced a surge in costs from BigQuery operations, and you've been tasked with finding a solution to reduce these costs without affecting the data analysis process significantly. Which of the following strategies will effectively achieve this goal?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement query caching to avoid running redundant queries and paying for them. -&gt; Correct. BigQuery's built-in query caching can store results of a query for up to 24 hours, and re-running the same query within that timeframe can retrieve results from cache instead of re-running the entire query, thus reducing costs.</p><p><br></p><p>Migrate all data from BigQuery to Cloud SQL to reduce storage costs. -&gt; Incorrect. Migrating all data from BigQuery to Cloud SQL could increase costs due to the overhead of maintaining a fully managed relational database, and it would not be the right solution for big data analysis that BigQuery is designed for.</p><p><br></p><p>Schedule queries during off-peak hours to benefit from lower demand pricing. -&gt; Incorrect. BigQuery does not have demand-based pricing, so running queries during off-peak hours would not reduce costs.</p><p><br></p><p>Increase the number of slots in your BigQuery reservations to improve query performance and reduce cost. -&gt; Incorrect. Increasing the number of slots in your BigQuery reservations would increase costs since you pay for slots on an hourly basis whether they are used or not.</p>",
                "answers": [
                    "<p>Implement query caching to avoid running redundant queries and paying for them.</p>",
                    "<p>Migrate all data from BigQuery to Cloud SQL to reduce storage costs.</p>",
                    "<p>Schedule queries during off-peak hours to benefit from lower demand pricing.</p>",
                    "<p>Increase the number of slots in your BigQuery reservations to improve query performance and reduce cost.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You're a cloud architect working for a company that heavily uses BigQuery for data analysis. Recently, the company has experienced a surge in costs from BigQuery operations, and you've been tasked with finding a solution to reduce these costs without affecting the data analysis process significantly. Which of the following strategies will effectively achieve this goal?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297312,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company has made the decision to adopt Google Cloud Platform to host its sensitive application data. As a security measure, it is imperative that the Virtual Machines (VMs) hosting this application are safeguarded from threats like rootkits and boot malware. Which of the following would be the most suitable approach?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Leverage the use of Shielded VMs. -&gt; Correct. Shielded VMs are designed to offer superior security for running applications on Google Cloud Platform. They come with features like secure boot, vTPM enabled, and integrity monitoring, making them the best fit for hosting sensitive applications.</p><p><br></p><p>Deploy the application on Preemptible VMs with an additional layer of network security controls. -&gt; Incorrect. Preemptible VMs are not suitable for sensitive applications as they are short-lived and can be terminated by Google Cloud at any time.</p><p><br></p><p>Use Compute Engine VMs with encrypted disk storage. -&gt; Incorrect. While using encrypted disk storage with Compute Engine VMs adds an extra layer of security, it does not protect against boot malware or rootkits as Shielded VMs do.</p><p><br></p><p>Run the application on Google Kubernetes Engine with Workload Identity enabled. -&gt; Incorrect. Google Kubernetes Engine (GKE) with Workload Identity does provide an identity for each service in the cluster, but it doesn't provide protection against boot malware or rootkits like Shielded VMs do.</p>",
                "answers": [
                    "<p>Leverage the use of Shielded VMs.</p>",
                    "<p>Deploy the application on Preemptible VMs with an additional layer of network security controls.</p>",
                    "<p>Use Compute Engine VMs with encrypted disk storage.</p>",
                    "<p>Run the application on Google Kubernetes Engine with Workload Identity enabled.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company has made the decision to adopt Google Cloud Platform to host its sensitive application data. As a security measure, it is imperative that the Virtual Machines (VMs) hosting this application are safeguarded from threats like rootkits and boot malware. Which of the following would be the most suitable approach?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297314,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization is moving towards a DevOps culture and wants to leverage Google Cloud for managing the software development life cycle. They are looking to standardize the development and deployment process to minimize errors and improve efficiency. Which approach should you suggest?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Source Repositories for version control, Cloud Build for building and testing, and Google Kubernetes Engine (GKE) for deployment. -&gt;&nbsp;Correct. This answer integrates several Google Cloud services to manage the SDLC in a streamlined, efficient manner, embodying the principles of a DevOps culture.</p><p><br></p><p>Use Compute Engine instances for deployment and ask developers to manually upload their code. -&gt;&nbsp;Incorrect. This approach lacks the automated, streamlined process that DevOps emphasizes. Manual operations can lead to human error and inconsistencies.</p><p><br></p><p>Develop on local machines and then use Cloud Storage for deployment. -&gt;&nbsp;Incorrect. While Cloud Storage is effective for storing static content, it is not designed to be a destination for application deployments.</p><p><br></p><p>Rely heavily on manual testing to catch all potential errors before deployment. -&gt;&nbsp;Incorrect. While testing is a crucial part of SDLC, solely relying on manual testing goes against the DevOps principle of automation and does not scale effectively.</p>",
                "answers": [
                    "<p>Use Cloud Source Repositories for version control, Cloud Build for building and testing, and Google Kubernetes Engine (GKE) for deployment.</p>",
                    "<p>Use Compute Engine instances for deployment and ask developers to manually upload their code.</p>",
                    "<p>Develop on local machines and then use Cloud Storage for deployment.</p>",
                    "<p>Rely heavily on manual testing to catch all potential errors before deployment.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization is moving towards a DevOps culture and wants to leverage Google Cloud for managing the software development life cycle. They are looking to standardize the development and deployment process to minimize errors and improve efficiency. Which approach should you suggest?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297316,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are working on an application deployed on Google Kubernetes Engine (GKE). The application has a large number of microservices and has been experiencing intermittent failures. You've been asked to establish a testing and validation process to identify the issues. What is the best approach to take?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Leverage Cloud Logging and Cloud Monitoring to identify issues, followed by integration tests for microservices communication. -&gt;&nbsp;Correct. Cloud Logging and Cloud Monitoring can help identify where failures are occurring. After identifying potential issues, performing integration tests ensures that the services can effectively communicate with each other, which is key in a microservices architecture.</p><p><br></p><p>Perform extensive manual testing for each microservice. -&gt; Incorrect. While manual testing might be helpful in certain contexts, it is not efficient or effective for a large number of microservices that could have interaction issues. Automation would be a more effective approach.</p><p><br></p><p>Use Cloud Scheduler to schedule regular restarts of the services to avoid failure. -&gt; Incorrect. Regularly restarting services doesn't address the root cause of the failures. It's more of a workaround than a solution, and it can lead to additional problems such as downtime during the restarts.</p><p><br></p><p>Implement unit tests for each service and ignore integration testing. -&gt; Incorrect. While unit tests are necessary to validate individual service functionality, ignoring integration testing in a microservices architecture can miss the issues occurring in the interaction between services.</p>",
                "answers": [
                    "<p>Leverage Cloud Logging and Cloud Monitoring to identify issues, followed by integration tests for microservices communication.</p>",
                    "<p>Perform extensive manual testing for each microservice.</p>",
                    "<p>Use Cloud Scheduler to schedule regular restarts of the services to avoid failure.</p>",
                    "<p>Implement unit tests for each service and ignore integration testing.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are working on an application deployed on Google Kubernetes Engine (GKE). The application has a large number of microservices and has been experiencing intermittent failures. You've been asked to establish a testing and validation process to identify the issues. What is the best approach to take?",
            "related_lectures": []
        }
    ]
}