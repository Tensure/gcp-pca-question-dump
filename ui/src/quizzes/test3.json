{
    "count": 70,
    "next": null,
    "previous": null,
    "results": [
        {
            "_class": "assessment",
            "id": 82146042,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>When handling Personally Identifiable Information (PII) in Google Cloud Platform (GCP), which of the following statements regarding Customer-managed encryption keys (CMEK) is correct?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>CMEK provides customers with full control and ownership of encryption keys, allowing them to manage the encryption of all types of data within GCP. -&gt;&nbsp;Correct. CMEK provides customers with full control and ownership of encryption keys, allowing them to manage the encryption of all types of data within GCP. This statement accurately represents the purpose and functionality of CMEK, empowering customers to maintain control over their encryption keys and data security.</p><p><br></p><p>CMEK is a feature exclusively available for Google-managed services and cannot be utilized for customer-owned resources. -&gt;&nbsp;Incorrect. It falsely claims that CMEK is exclusively available for Google-managed services. CMEK can be used to encrypt data for both Google-managed services and customer-owned resources within GCP.</p><p><br></p><p>CMEK ensures compliance with data protection regulations, eliminating the need for customers to implement additional security measures. -&gt; Incorrect. It wrongly suggests that CMEK alone ensures compliance with data protection regulations. While CMEK plays a crucial role in data security and encryption, additional security measures are often required to achieve full compliance with regulations.</p><p><br></p><p>CMEK is automatically enabled by default for all GCP services, providing enhanced encryption and protection for PII. -&gt; Incorrect. It incorrectly states that CMEK is automatically enabled by default for all GCP services. CMEK needs to be explicitly configured and enabled by the customer for each specific service and resource that requires customer-managed encryption keys.</p>",
                "answers": [
                    "<p>CMEK provides customers with full control and ownership of encryption keys, allowing them to manage the encryption of all types of data within GCP.</p>",
                    "<p>CMEK is a feature exclusively available for Google-managed services and cannot be utilized for customer-owned resources.</p>",
                    "<p>CMEK ensures compliance with data protection regulations, eliminating the need for customers to implement additional security measures.</p>",
                    "<p>CMEK is automatically enabled by default for all GCP services, providing enhanced encryption and protection for PII.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "When handling Personally Identifiable Information (PII) in Google Cloud Platform (GCP), which of the following statements regarding Customer-managed encryption keys (CMEK) is correct?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146044,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect tasked with designing a system to store petabytes of NoSQL data for a large-scale, global e-commerce company. The system needs to support high-speed writes and reads, provide seamless scalability, and ensure near real-time consistency. The data will be heavily used for both transactional and analytical workloads. Which Google Cloud service should you choose to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Bigtable -&gt;&nbsp;Correct. It's designed to handle massive workloads at consistent low latency, can scale to petabytes of data, and is suitable for both transactional and analytical workloads. It provides strong consistency within a single row, but eventual consistency for reading multiple rows (which can be mitigated by proper design).</p><p><br></p><p>BigQuery -&gt;&nbsp;Incorrect. While it can store petabytes of data and is excellent for analytical workloads, it isn't designed for high-speed transactional workloads or real-time consistency.</p><p><br></p><p>Cloud Spanner -&gt;&nbsp;Incorrect. While it provides global transactions, strong consistency, and can handle massive amounts of data, it is a relational database, not a NoSQL database.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. While it is a NoSQL document database built for automatic scaling, high performance, and ease of application development, it is not designed to handle petabyte-scale data and has limitations for certain transactional workloads.</p>",
                "answers": [
                    "<p>Cloud Bigtable</p>",
                    "<p>BigQuery</p>",
                    "<p>Cloud Spanner</p>",
                    "<p>Cloud Datastore</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect tasked with designing a system to store petabytes of NoSQL data for a large-scale, global e-commerce company. The system needs to support high-speed writes and reads, provide seamless scalability, and ensure near real-time consistency. The data will be heavily used for both transactional and analytical workloads. Which Google Cloud service should you choose to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146046,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization operates a large-scale web application on Google Cloud, and you need to design a solution to analyze logs from the application in near real-time to detect any potential issues or anomalies. You also want to do some transformation of log data before storing. Which of the following approaches should you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Pub/Sub to ingest logs, Cloud Dataflow to transform, and then BigQuery to analyze. -&gt;&nbsp;Correct. Using Cloud Pub/Sub for log ingestion, Cloud Dataflow for transformation, and then storing and analyzing in BigQuery is a common pattern for handling and analyzing large-scale, real-time data in Google Cloud.</p><p><br></p><p>Store logs in Cloud Storage, then use Cloud Dataflow to move them to BigQuery for analysis. -&gt; Incorrect. This approach doesn't support real-time or near real-time analysis as there would be a delay in moving data from Cloud Storage to BigQuery.</p><p><br></p><p>Use Cloud Functions to process each log entry and send it to BigQuery. -&gt; Incorrect. While Cloud Functions could be used for this purpose, it might not be the most cost-effective or efficient way to process large-scale log data in real-time.</p><p><br></p><p>Stream logs directly from the application to BigQuery. -&gt; Incorrect. Streaming logs directly to BigQuery may be expensive and does not support advanced parsing or transformation that may be needed before analysis.</p>",
                "answers": [
                    "<p>Use Cloud Pub/Sub to ingest logs, Cloud Dataflow to transform, and then BigQuery to analyze.</p>",
                    "<p>Store logs in Cloud Storage, then use Cloud Dataflow to move them to BigQuery for analysis.</p>",
                    "<p>Use Cloud Functions to process each log entry and send it to BigQuery.</p>",
                    "<p>Stream logs directly from the application to BigQuery.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization operates a large-scale web application on Google Cloud, and you need to design a solution to analyze logs from the application in near real-time to detect any potential issues or anomalies. You also want to do some transformation of log data before storing. Which of the following approaches should you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146048,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A social media startup deployed their image-sharing application using App Engine in the <code>us-west1</code> region. After a few months, they notice that most of their users are based in Brazil. They want to minimize latency for their users. As a cloud architect, what should you advise them?</p>",
                "relatedLectureIds": [],
                "links": [],
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>They should move this application deployment to <code>southamerica-east1</code> region and create a new GCP project. Then, they should create a new App Engine application in the new GCP project and set its region to <code>southamerica-east1</code>. Finally, they should remove the old App Engine application. -&gt;&nbsp;Correct. App Engine applications are region-specific and cannot be moved between regions. Creating a new project and deploying the application in the desired region is necessary to minimize latency for the users in Brazil.</p><p><br></p><p>They should use a global CDN to cache their content closer to their users in Brazil. -&gt; Incorrect. While using a CDN can help reduce latency for static content, it doesn't address the need for the application itself to be closer to users. Moving the App Engine deployment is still necessary for the best performance.</p><p><br></p><p>They should update the default region to <code>southamerica-east1</code> in the App Engine. -&gt; Incorrect. You cannot update the default region of an existing App Engine application. Regions are fixed once set during the creation of the App Engine application.</p><p><br></p><p>They should create a ticket to Google Support to change application deployment region in App Engine. -&gt; Incorrect. Google Support cannot change the region of an existing App Engine application. A new deployment in the desired region within a new project is necessary.</p>",
                "answers": [
                    "<p>They should move this application deployment to <code>southamerica-east1</code> region and create a new GCP project. Then, they should create a new App Engine application in the new GCP project and set its region to <code>southamerica-east1</code>. Finally, they should remove the old App Engine application.</p>",
                    "<p>They should use a global CDN to cache their content closer to their users in Brazil.</p>",
                    "<p>They should update the default region to <code>southamerica-east1</code> in the App Engine.</p>",
                    "<p>They should create a ticket to Google Support to change application deployment region in App Engine.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A social media startup deployed their image-sharing application using App Engine in the us-west1 region. After a few months, they notice that most of their users are based in Brazil. They want to minimize latency for their users. As a cloud architect, what should you advise them?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146050,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>One of your applications is deployed to the GKE&nbsp;cluster as a Kubernetes workload with DaemonSets and is gaining popularity. As a cloud architect, you want to add more pods to your workload and want to make sure the cluster scales up and down automatically based on the volume. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should enable autoscaling on Kubernetes Engine. -&gt; Correct. Enabling autoscaling on Kubernetes Engine allows the cluster to automatically adjust the number of nodes based on the demand. This means that as the number of pods in the workload increases, the cluster will automatically add more nodes to handle the load. Similarly, if the workload decreases, the cluster will scale down and remove unnecessary nodes, helping to save costs. This is a more efficient solution than manually modifying the machine type or creating another workload. </p><p><br></p><p>You should perform a rolling update to modify machine type to a higher one. -&gt;&nbsp;Incorrect. It is incorrect because modifying the machine type will not scale the cluster automatically.</p><p><br></p><p>You should enable Horizontal Pod Autoscaling for the Kubernetes deployment. -&gt;&nbsp;Incorrect. It is incorrect because Horizontal Pod Autoscaling scales the number of pods within a deployment, but not the number of nodes in the cluster. </p><p><br></p><p>You should create another identical Kubernetes workload and split traffic between the two workloads. -&gt; Incorrect. It is also incorrect because creating another workload is not necessary and may result in unnecessary resource usage.</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler</p>",
                "answers": [
                    "<p>You should enable autoscaling on Kubernetes Engine.</p>",
                    "<p>You should perform a rolling update to modify machine type to a higher one.</p>",
                    "<p>You should enable Horizontal Pod Autoscaling for the Kubernetes deployment.</p>",
                    "<p>You should create another identical Kubernetes workload and split traffic between the two workloads.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "One of your applications is deployed to the GKE&nbsp;cluster as a Kubernetes workload with DaemonSets and is gaining popularity. As a cloud architect, you want to add more pods to your workload and want to make sure the cluster scales up and down automatically based on the volume. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146052,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, it is your responsibility to monitor any modifications made to the Cloud Storage bucket. For each change, you are required to trigger an action that will promptly validate the compliance of the modification in near real-time. What action should you take?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should use Cloud Function events, and call your security script from the Cloud Function triggers. -&gt;&nbsp;Correct. Cloud Functions allow you to execute your code in response to an event in Cloud Storage. You can use Cloud Storage triggers to execute your Cloud Function whenever an object is created or updated in the Cloud Storage bucket. This will allow you to verify the compliance of the change in near real-time. The security script can be called from the Cloud Function triggers.</p><p><br></p><p>You should use the built-in triggering mechanism of Cloud Storage to run your security script. -&gt; Incorrect. It is incorrect because while Cloud Storage does have a built-in triggering mechanism, it does not provide the ability to verify the compliance of the change in near real-time.</p><p><br></p><p>You should use a Python script to get appropriate logs, analyze them, and run the security script. -&gt; Incorrect. It is incorrect because although you can use Python scripts to get logs and analyze them, this solution would not be real-time, and it would be less efficient and less scalable than using Cloud Functions.</p><p><br></p><p>You should use Crone Scheduler to schedule your security script. -&gt; Incorrect. It is incorrect because Cron Scheduler is a time-based job scheduler and is not suitable for monitoring changes in real-time.</p><p><br></p><p>https://cloud.google.com/functions/docs/how-to</p>",
                "answers": [
                    "<p>You should use Cloud Function events, and call your security script from the Cloud Function triggers.</p>",
                    "<p>You should use the built-in triggering mechanism of Cloud Storage to run your security script.</p>",
                    "<p>You should use a Python script to get appropriate logs, analyze them, and run the security script.</p>",
                    "<p>You should use Crone Scheduler to schedule your security script.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, it is your responsibility to monitor any modifications made to the Cloud Storage bucket. For each change, you are required to trigger an action that will promptly validate the compliance of the modification in near real-time. What action should you take?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146054,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Suppose you run a small business and need to grant a role for your accountant to view billing reports and approve invoices. With Google's best practices in mind, which Billing IAM role should you grant to your accountant?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Billing Account Viewer -. Correct. The Billing Account Viewer role allows the user to view billing reports, cost trends, and related information without being able to make any changes or perform any actions. This is the least privileged role that grants access to billing information.</p><p><br></p><p>Billing Account Administrator -&gt; Incorrect. The Billing Account Administrator role provides full control over billing information, including the ability to make changes, modify budgets, and manage payment methods. </p><p><br></p><p>Billing Account User Project Creator -&gt; Incorrect. The Billing Account User Project Creator role allows users to create projects within a billing account, but does not provide access to billing information.</p><p><br></p><p>Billing Account Creator -&gt; Incorrect. The Billing Account Creator role is a highly privileged role that allows the user to create new billing accounts, but it does not provide access to billing information or the ability to manage existing accounts.</p><p><br></p><p>https://cloud.google.com/billing/docs/how-to/billing-access</p><p>https://cloud.google.com/billing/docs/how-to/billing-access#overview-of-cloud-billing-roles-in-cloud-iam</p>",
                "answers": [
                    "<p>Billing Account Viewer</p>",
                    "<p>Billing Account Administrator</p>",
                    "<p>Billing Account User Project Creator</p>",
                    "<p>Billing Account Creator</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Suppose you run a small business and need to grant a role for your accountant to view billing reports and approve invoices. With Google's best practices in mind, which Billing IAM role should you grant to your accountant?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146056,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company has an existing monolithic application running on-premises and you've been tasked to migrate this application to Google Cloud Platform (GCP). The company wants to start taking advantage of microservices for scalability and maintainability. What strategy should you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Refactor the monolithic application into microservices and deploy using GKE. -&gt;&nbsp;Correct. This is a good strategy because Google Kubernetes Engine (GKE) is designed to manage, scale, and deploy containerized applications, which are well-suited to a microservices architecture.</p><p><br></p><p>Refactor the monolithic application into microservices and deploy each one as a Cloud Function. -&gt;&nbsp;Incorrect. While Cloud Functions can run individual pieces of code in response to events, they are typically used for lightweight, event-driven processing and may not be suitable for larger microservices or long-running processes.</p><p><br></p><p>Migrate the monolithic application to App Engine Standard, then refactor for microservices. -&gt;&nbsp;Incorrect. App Engine Standard is primarily designed for simple, stateless applications. While it could run a monolithic application, it may not be well-suited to a complex microservices architecture.</p><p><br></p><p>Migrate the application to Cloud Run without refactoring, then move individual services to GKE as they are broken out. -&gt;&nbsp;Incorrect. While Cloud Run can host containerized applications, it's better suited to stateless, request-driven workloads. It might not be the best fit for complex, stateful microservices.</p>",
                "answers": [
                    "<p>Refactor the monolithic application into microservices and deploy using GKE.</p>",
                    "<p>Refactor the monolithic application into microservices and deploy each one as a Cloud Function.</p>",
                    "<p>Migrate the monolithic application to App Engine Standard, then refactor for microservices.</p>",
                    "<p>Migrate the application to Cloud Run without refactoring, then move individual services to GKE as they are broken out.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company has an existing monolithic application running on-premises and you've been tasked to migrate this application to Google Cloud Platform (GCP). The company wants to start taking advantage of microservices for scalability and maintainability. What strategy should you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146058,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Suppose your company has two VPC networks and they want to communicate internally between these networks. So that traffic stays within Google's network and doesn't traverse the public internet. Which service should you advise?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>VPC Network Peering -&gt; Correct. It is the recommended option for establishing private communication between two VPC networks within Google Cloud. This service allows VPC networks to communicate with each other using private IP addresses, and the traffic stays within Google's network, which provides faster and more secure communication between the two networks. </p><p><br></p><p>Cloud VPN -&gt; Incorrect. It is used to establish secure connections between on-premises networks and Google Cloud VPC networks, and it uses the public internet to transmit traffic. </p><p><br></p><p>Cloud DNS -&gt;&nbsp;Incorrect. It is a service used for managing DNS records and resolving domain names to IP addresses. </p><p><br></p><p>Hybrid Connectivity -&gt;&nbsp;Incorrect. It refers to the ability to connect on-premises infrastructure to cloud-based infrastructure.</p><p><br></p><p>https://cloud.google.com/vpc/docs/vpc-peering</p>",
                "answers": [
                    "<p>VPC Network Peering</p>",
                    "<p>Cloud VPN</p>",
                    "<p>Cloud DNS</p>",
                    "<p> Hybrid Connectivity</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Suppose your company has two VPC networks and they want to communicate internally between these networks. So that traffic stays within Google's network and doesn't traverse the public internet. Which service should you advise?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146060,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are tasked with creating a signed URL for a Google Cloud Storage (GCS) bucket. The requirement is to allow access to a certain object in the bucket for a third-party service, but only for 30 minutes. The solution should ensure minimum privileges and security. Which of the following options should you choose to implement this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Create a signed URL using a user-managed service account with read access on the bucket and a 30-minute expiration. -&gt; Correct. Creating a signed URL using a user-managed service account that only has read access to the bucket follows the principle of least privilege. The 30-minute expiration limits the availability of the object, providing further security.</p><p><br></p><p>Create a signed URL using the bucket's default service account with a 30-minute expiration. -&gt; Incorrect. The bucket's default service account should not be used to create a signed URL because it typically has more permissions than necessary for this specific task. This could potentially expose the bucket to risks if the signed URL is compromised.</p><p><br></p><p>Create a signed URL using a user-managed service account with full access on the bucket and a 30-minute expiration. -&gt; Incorrect. Granting full access to a service account for the purpose of creating a signed URL to read an object violates the principle of least privilege, even if the URL is set to expire after 30 minutes.</p><p><br></p><p>Create a signed URL using the bucket's default service account with no expiration, and manually invalidate the URL after 30 minutes. -&gt;&nbsp;Incorrect. Signed URLs should always have an expiration time to minimize the potential impact of a security compromise. Manually invalidating the URL after 30 minutes is not reliable or efficient.</p>",
                "answers": [
                    "<p>Create a signed URL using a user-managed service account with read access on the bucket and a 30-minute expiration.</p>",
                    "<p>Create a signed URL using the bucket's default service account with a 30-minute expiration.</p>",
                    "<p>Create a signed URL using a user-managed service account with full access on the bucket and a 30-minute expiration.</p>",
                    "<p>Create a signed URL using the bucket's default service account with no expiration, and manually invalidate the URL after 30 minutes.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are tasked with creating a signed URL for a Google Cloud Storage (GCS) bucket. The requirement is to allow access to a certain object in the bucket for a third-party service, but only for 30 minutes. The solution should ensure minimum privileges and security. Which of the following options should you choose to implement this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146062,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working on a complex scenario where you have to deploy a monitoring pod in a DaemonSet object across a GKE cluster for a client's application. You want the monitoring pod to be deployed on every node of the GKE cluster. Which approach will allow you to efficiently achieve this objective?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Deploy the monitoring pod using a DaemonSet across the nodes in the GKE cluster. -&gt; Correct. A DaemonSet ensures that all (or some) nodes run a copy of a pod. This is suitable for deploying system daemons such as log collectors, monitoring services etc. When a node is added to the cluster, the pod gets added to the new node, making it ideal for our scenario.</p><p><br></p><p>Deploy the monitoring pod individually on each node in the GKE cluster. -&gt; Incorrect. Deploying the monitoring pod individually on each node would not be efficient, as you would have to manually handle scaling and the addition of new nodes.</p><p><br></p><p>Deploy the monitoring pod as a ReplicaSet across the nodes in the GKE cluster. -&gt;&nbsp;Incorrect. A ReplicaSet ensures that a specified number of pod \"replicas\" are running at any given time. However, it doesn't ensure that pods are run on every node in the cluster.</p><p><br></p><p>Deploy the monitoring pod using a StatefulSet across the nodes in the GKE cluster. -&gt; Incorrect. StatefulSet is used for workloads that require stable network identifiers, stable persistent storage, and orderly, graceful deployment and scaling. It's not specifically designed to ensure that a pod runs on every node in a cluster.</p>",
                "answers": [
                    "<p>Deploy the monitoring pod using a DaemonSet across the nodes in the GKE cluster.</p>",
                    "<p>Deploy the monitoring pod individually on each node in the GKE cluster.</p>",
                    "<p>Deploy the monitoring pod as a ReplicaSet across the nodes in the GKE cluster.</p>",
                    "<p>Deploy the monitoring pod using a StatefulSet across the nodes in the GKE cluster.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working on a complex scenario where you have to deploy a monitoring pod in a DaemonSet object across a GKE cluster for a client's application. You want the monitoring pod to be deployed on every node of the GKE cluster. Which approach will allow you to efficiently achieve this objective?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146064,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You're a cloud architect and have been assigned a task to develop a model that will predict the type of product a customer is most likely to purchase next, based on their past purchases and behavior. The client has a massive amount of historic data available but lacks machine learning expertise. Furthermore, the solution needs to be able to constantly learn from new data. Which of the following would be the best approach for this situation?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use AutoML Tables, retrain the model periodically with new data. -&gt; Correct. AutoML Tables is the right solution because it is specifically designed for tabular or structured data, like historic purchase data. AutoML allows for automatic model training, tuning, and deployment, which suits the lack of machine learning expertise. It also allows the model to be retrained periodically with new data, which meets the requirement of constantly learning from new data.</p><p><br></p><p>Use AutoML Vision, retrain the model periodically with new data. -&gt; Incorrect. AutoML Vision is incorrect because it is designed for image analysis tasks, not for analyzing structured tabular data.</p><p><br></p><p>Use BigQuery ML, retrain the model manually with new data. -&gt; Incorrect. BigQuery ML allows machine learning models to be built using SQL queries, but in this case, it might not be suitable as it requires manual retraining and might not be as flexible or automated as AutoML for complex predictive models.</p><p><br></p><p>Use Cloud Machine Learning Engine with TensorFlow and retrain the model manually with new data. -&gt;&nbsp;Incorrect. Cloud Machine Learning Engine with TensorFlow might be a viable solution for some scenarios, but in this case, it requires manual model training and deep machine learning expertise, which is not available according to the scenario.</p>",
                "answers": [
                    "<p>Use AutoML Tables, retrain the model periodically with new data.</p>",
                    "<p>Use AutoML Vision, retrain the model periodically with new data.</p>",
                    "<p>Use BigQuery ML, retrain the model manually with new data.</p>",
                    "<p>Use Cloud Machine Learning Engine with TensorFlow and retrain the model manually with new data.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You're a cloud architect and have been assigned a task to develop a model that will predict the type of product a customer is most likely to purchase next, based on their past purchases and behavior. The client has a massive amount of historic data available but lacks machine learning expertise. Furthermore, the solution needs to be able to constantly learn from new data. Which of the following would be the best approach for this situation?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146066,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization has an existing system on-premise and plans to migrate to Google Cloud. However, they plan to keep some parts of their system on-premises for the foreseeable future. The goal is to have the on-premise and cloud systems communicate securely and efficiently. What should be your strategy?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud VPN to establish a secure connection between the on-premise system and Google Cloud -&gt;&nbsp;Correct. Cloud VPN can create a secure connection between an on-premise system and Google Cloud over the internet, which is ideal for hybrid systems that span on-premise and cloud environments.</p><p><br></p><p>Use VPC Network Peering to connect the on-premise system to the VPC in Google Cloud -&gt;&nbsp;Incorrect. VPC Network Peering is used to connect two VPCs, even across projects or organizations. It's not meant for connecting on-premise systems directly to Google Cloud.</p><p><br></p><p>Use Cloud Interconnect to connect the on-premise system to Google Cloud -&gt;&nbsp;Incorrect. Cloud Interconnect provides a direct connection to Google's network but may not be necessary or cost-effective for all businesses, especially if the traffic volume does not warrant it.</p><p><br></p><p>Use Cloud Endpoints to create APIs for the on-premise system and access them from Google Cloud -&gt;&nbsp;Incorrect. Cloud Endpoints is a development tool used to develop, deploy, protect, and monitor APIs. It's not primarily used for connecting on-premise systems with Google Cloud.</p>",
                "answers": [
                    "<p>Use Cloud VPN to establish a secure connection between the on-premise system and Google Cloud</p>",
                    "<p>Use VPC Network Peering to connect the on-premise system to the VPC in Google Cloud</p>",
                    "<p>Use Cloud Interconnect to connect the on-premise system to Google Cloud</p>",
                    "<p>Use Cloud Endpoints to create APIs for the on-premise system and access them from Google Cloud</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization has an existing system on-premise and plans to migrate to Google Cloud. However, they plan to keep some parts of their system on-premises for the foreseeable future. The goal is to have the on-premise and cloud systems communicate securely and efficiently. What should be your strategy?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146068,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect working for a fintech company that wants to deploy a critical, containerized application in a microservices architecture. The application needs to support high levels of incoming traffic with low latency, ensure zero-downtime deployments, allow for automatic scaling based on CPU utilization, and maintain end-to-end encryption. Which of the following deployment methods should you choose for this scenario?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Deploy the application on Google Kubernetes Engine (GKE) with an HTTPS load balancer. -&gt; Correct. It's designed for deploying, scaling, and managing containerized applications. It supports zero-downtime deployments through rolling updates, autoscaling based on CPU utilization, and end-to-end encryption can be achieved through an HTTPS load balancer.</p><p><br></p><p>Deploy the application on Compute Engine instances behind a load balancer. -&gt; Incorrect. Compute Engine instances could be a potential choice for containerized applications, but managing zero-downtime deployments and automatic scaling can be challenging and complex without orchestration.</p><p><br></p><p>Deploy the application on App Engine Standard environment. -&gt; Incorrect. App Engine Standard environment does not natively support containerized applications. Although it supports automatic scaling and zero-downtime deployments, this option is not suitable for this scenario.</p><p><br></p><p>Deploy the application on App Engine Flexible environment. -&gt; Incorrect. While the App Engine Flexible environment does support containerized applications, it may not provide the same level of control over deployments, scaling, and management as GKE does, making it less suitable for this specific scenario. </p>",
                "answers": [
                    "<p>Deploy the application on Google Kubernetes Engine (GKE) with an HTTPS load balancer.</p>",
                    "<p>Deploy the application on Compute Engine instances behind a load balancer.</p>",
                    "<p>Deploy the application on App Engine Standard environment.</p>",
                    "<p>Deploy the application on App Engine Flexible environment.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect working for a fintech company that wants to deploy a critical, containerized application in a microservices architecture. The application needs to support high levels of incoming traffic with low latency, ensure zero-downtime deployments, allow for automatic scaling based on CPU utilization, and maintain end-to-end encryption. Which of the following deployment methods should you choose for this scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146070,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working for a company that needs to perform a large-scale data processing task that is highly parallelizable and not time-sensitive. This operation should be completed within a reasonable time frame, but it doesn't require an immediate result. The cost of operation is a major consideration. The company has asked you for the most cost-effective solution that matches these requirements. What would be your recommendation?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Google Cloud Dataproc with preemptible Compute Engine machines. -&gt;&nbsp;Correct. Dataproc is designed for fast, easy, and cost-efficient processing of big data workloads. Using preemptible Compute Engine machines, which are significantly cheaper than standard machines, can save costs. Since the task is not time-sensitive, the fact that preemptible instances can be terminated at any time (up to a maximum of 24 hours) is not a significant issue.</p><p><br></p><p>Use Google Cloud Dataflow with non-preemptible Compute Engine machines. -&gt;&nbsp;Incorrect. While Dataflow is a great option for large-scale data processing tasks, using non-preemptible Compute Engine machines may not be the most cost-effective solution for a non-time-sensitive task.</p><p><br></p><p>Use Google Kubernetes Engine with preemptible Compute Engine machines. -&gt;&nbsp;Incorrect. Kubernetes Engine can run large-scale data processing tasks, but it might be overkill for this specific scenario. Moreover, using Kubernetes Engine introduces additional overhead in managing and orchestrating containers.</p><p><br></p><p>Use Google Cloud Functions for the data processing task. -&gt; Incorrect. Cloud Functions is designed for executing event-driven functions and it's not suitable for large-scale data processing tasks.</p>",
                "answers": [
                    "<p>Use Google Cloud Dataproc with preemptible Compute Engine machines.</p>",
                    "<p>Use Google Cloud Dataflow with non-preemptible Compute Engine machines.</p>",
                    "<p>Use Google Kubernetes Engine with preemptible Compute Engine machines.</p>",
                    "<p>Use Google Cloud Functions for the data processing task.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working for a company that needs to perform a large-scale data processing task that is highly parallelizable and not time-sensitive. This operation should be completed within a reasonable time frame, but it doesn't require an immediate result. The cost of operation is a major consideration. The company has asked you for the most cost-effective solution that matches these requirements. What would be your recommendation?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146072,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>You have application data that you need to access on a monthly basis, with the requirement that data older than five years is no longer necessary. What steps can you take to minimize storage expenses? Select all that apply.</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should set an object lifecycle management policy to remove data older than 5 years. -&gt; Correct. By setting an object lifecycle management policy, you can define rules that automatically delete data that is older than a certain threshold, in this case, 5 years. This ensures that you only retain and pay for the necessary data, minimizing storage expenses.</p><p><br></p><p>You should store infrequently accessed data in a Nearline storage class. -&gt; Correct. The Nearline storage class is designed for data that is accessed less frequently but still requires relatively fast access times. By storing infrequently accessed data in the Nearline storage class, you can benefit from lower storage costs compared to the Standard storage class, while still maintaining reasonable access times for the data.</p><p><br></p><p>You should store infrequently accessed data in a Standard storage class. -&gt;&nbsp;Incorrect. In this scenario where the data is accessed on a monthly basis, storing it in the Standard storage class might result in higher storage costs, as you would be paying for a higher storage tier without utilizing the frequent access benefits.</p><p><br></p><p>You should store infrequently accessed data in a multi-regional bucket. -&gt;&nbsp;Incorrect. Storing infrequently accessed data in a multi-regional bucket might not be the most cost-effective option, as multi-regional storage typically comes with higher costs compared to regional or nearline storage options. It is more suitable for data that requires low-latency access from multiple regions.</p><p><br></p><p>You should set an object lifecycle management policy to change the storage class to Archive for data older than 5 years. -&gt;&nbsp;Incorrect. The requirement states that the data needs to be accessed on a monthly basis, and the Archive storage class is intended for long-term storage with infrequent access. Retrieving data from the Archive storage class incurs additional retrieval costs and longer access times, which may not align with the requirement of monthly access.</p><p><br></p><p>https://cloud.google.com/storage/docs/lifecycle</p><p>https://cloud.google.com/storage/docs/storage-classes#nearline</p>",
                "answers": [
                    "<p>You should set an object lifecycle management policy to remove data older than 5 years.</p>",
                    "<p>You should store infrequently accessed data in a Nearline storage class.</p>",
                    "<p>You should store infrequently accessed data in a Standard storage class.</p>",
                    "<p>You should store infrequently accessed data in a multi-regional bucket.</p>",
                    "<p>You should set an object lifecycle management policy to change the storage class to Archive for data older than 5 years.</p>"
                ]
            },
            "correct_response": [
                "a",
                "b"
            ],
            "section": "",
            "question_plain": "You have application data that you need to access on a monthly basis, with the requirement that data older than five years is no longer necessary. What steps can you take to minimize storage expenses? Select all that apply.",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146074,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>Every year, an audit takes place within your company, and it is necessary to grant external auditors access to the audit logs from the past five years. What should you do to minimize the cost and operational overhead? (select 3)</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Export audit logs to Cloud Storage. -&gt; Correct. Exporting audit logs to Cloud Storage is a common practice to store large amounts of data in a cost-effective manner. It allows external auditors to access the audit logs easily without having to access the underlying infrastructure directly.</p><p><br></p><p>Grant external auditors the role of Storage Object Viewer on the logs storage bucket. -&gt; Correct. Granting external auditors the role of Storage Object Viewer on the logs storage bucket ensures they can access the audit logs in the Cloud Storage bucket without granting them additional permissions or access to other resources.</p><p><br></p><p>Configure a lifecycle management policy on the logs bucket to delete objects older than 5 years. -&gt; Correct. Configuring a lifecycle management policy on the logs bucket to delete objects older than 5 years can help minimize storage costs and operational overhead. It ensures that the storage cost is optimized by deleting objects that are no longer required, and it also reduces the chances of security breaches by removing outdated logs that might be subject to data privacy and regulatory requirements.</p><p><br></p><p>Export audit logs to BigQuery. -&gt; Incorrect.</p><p>Export audit logs to Cloud Filestore. -&gt; Incorrect.</p><p><br></p><p>https://cloud.google.com/storage/docs/audit-logging</p><p>https://cloud.google.com/storage/docs/access-control/iam-roles#standard-roles</p><p>https://cloud.google.com/storage/docs/lifecycle</p>",
                "answers": [
                    "<p>Export audit logs to Cloud Storage.</p>",
                    "<p>Grant external auditors the role of Storage Object Viewer on the logs storage bucket.</p>",
                    "<p>Configure a lifecycle management policy on the logs bucket to delete objects older than 5 years.</p>",
                    "<p>Export audit logs to BigQuery.</p>",
                    "<p>Export audit logs to Cloud Filestore.</p>"
                ]
            },
            "correct_response": [
                "a",
                "b",
                "c"
            ],
            "section": "",
            "question_plain": "Every year, an audit takes place within your company, and it is necessary to grant external auditors access to the audit logs from the past five years. What should you do to minimize the cost and operational overhead? (select 3)",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146076,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A web application uses Cloud SQL to store user data which is very critical for the application flow. As a cloud architect, you want to protect your data from zone failures. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should configure High Availability for Cloud SQL. -&gt; Correct. High Availability (HA) is the recommended configuration for critical data in Cloud SQL. It provides a failover replica in another zone in the same region to ensure data protection in the event of a zone outage. With HA, Cloud SQL automatically replicates data to a standby replica in a different zone within the same region. If there is a zone failure, Cloud SQL automatically fails over to the standby replica, ensuring that your application remains available. Therefore, configuring High Availability for Cloud SQL is the best option for protecting critical data from zone failures.</p><p><br></p><p>You should create a Read replica in a different region. -&gt; Incorrect. It suggest creating Read replica, which can help improve read performance, but it does not provide protection against zone failures.</p><p><br></p><p>You should create a Read replica in the same region but in a different zone. -&gt; Incorrect. It suggest creating Read replica, which can help improve read performance, but it does not provide protection against zone failures.</p><p><br></p><p>You cannot increase availability of your application in this case. -&gt; Incorrect. It is incorrect because you can increase the availability of the application by configuring High Availability for Cloud SQL.</p><p><br></p><p>https://cloud.google.com/sql/docs/mysql/high-availability</p>",
                "answers": [
                    "<p>You should configure High Availability for Cloud SQL.</p>",
                    "<p>You should create a Read replica in a different region.</p>",
                    "<p>You should create a Read replica in the same region but in a different zone.</p>",
                    "<p>You cannot increase availability of your application in this case.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A web application uses Cloud SQL to store user data which is very critical for the application flow. As a cloud architect, you want to protect your data from zone failures. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146078,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are tasked with designing a solution for a social media application that stores user-uploaded images in a Cloud Storage bucket. For each uploaded image, a thumbnail version needs to be generated for displaying in the application. The solution should be highly scalable to handle sudden spikes in uploads, cost-effective, and able to process images as soon as they are uploaded. What architecture would you suggest for this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Functions triggered by Cloud Storage events to generate thumbnails. -&gt;&nbsp;Correct. Cloud Functions is a serverless execution environment that can automatically run your function in response to events from Cloud Storage, such as when a new image is uploaded. This makes it an ideal choice for creating a scalable, event-driven, and cost-effective solution to generate thumbnails.</p><p><br></p><p>Use App Engine to constantly check for new images and generate thumbnails. -&gt; Incorrect. App Engine constantly checking for new images could lead to inefficiencies and unnecessary costs, as it would need to run continuously, even when there are no new images to process.</p><p><br></p><p>Use Compute Engine with a startup script to generate thumbnails. -&gt; Incorrect. Compute Engine could technically handle the task, but it may not be as scalable, cost-effective, or event-driven as a serverless option like Cloud Functions. It would also require additional management and configuration.</p><p><br></p><p>Use Kubernetes Engine to constantly check for new images and generate thumbnails. -&gt;&nbsp;Incorrect. Like the App Engine option, Kubernetes Engine constantly checking for new images can lead to inefficiencies and unnecessary costs. It also introduces complexity that may not be needed for this particular task.</p>",
                "answers": [
                    "<p>Use Cloud Functions triggered by Cloud Storage events to generate thumbnails.</p>",
                    "<p>Use App Engine to constantly check for new images and generate thumbnails.</p>",
                    "<p>Use Compute Engine with a startup script to generate thumbnails.</p>",
                    "<p>Use Kubernetes Engine to constantly check for new images and generate thumbnails.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are tasked with designing a solution for a social media application that stores user-uploaded images in a Cloud Storage bucket. For each uploaded image, a thumbnail version needs to be generated for displaying in the application. The solution should be highly scalable to handle sudden spikes in uploads, cost-effective, and able to process images as soon as they are uploaded. What architecture would you suggest for this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146080,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>The raw format of CCTV footage videos is stored by a machine learning startup in a Cloud Storage bucket. During the initial two-week period, the footage undergoes consistent processing to identify and detect potential threats. What storage approach would you suggest to minimize costs when storing the videos?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should use Standard storage class for the first two weeks, and use lifecycle rules to transition to Coldline. -&gt; Correct. Using the Standard storage class for the initial two-week period allows for immediate access and processing of the CCTV footage videos when they are actively being analyzed for potential threats. After this initial processing period, you can leverage lifecycle rules to automatically transition the videos to the Coldline storage class. Coldline storage is designed for long-term storage of infrequently accessed data and offers lower storage costs compared to Standard storage, helping to minimize costs for storing the videos.</p><p><br></p><p>You should use Standard storage class for the first two weeks, and use lifecycle rules to transition to Nearline. -&gt;&nbsp;Incorrect. While using the Standard storage class for the initial two weeks is a good approach for immediate access and processing, transitioning the videos to the Nearline storage class may not be the most cost-effective option. Nearline storage is designed for data that is accessed less frequently than Standard storage but more frequently than Coldline storage. Since the scenario mentions that the footage undergoes consistent processing, it suggests that the videos might not be frequently accessed after the initial two-week period, making Coldline a more suitable and cost-effective option.</p><p><br></p><p>You should use Standard storage class for the first two weeks, and then move videos to Persistent Disk. -&gt;&nbsp;Incorrect. Using Persistent Disk for storing CCTV footage videos may not be the most cost-effective approach. Persistent Disk is primarily designed for block storage and is more suitable for hosting operating system files and application data within virtual machines. It may not offer the same cost optimization and data durability as storage classes specifically designed for long-term object storage like Coldline.</p><p><br></p><p>You should use Standard storage class for the first 30 days, and use lifecycle rules to transition to Coldline. -&gt;&nbsp;Incorrect. While transitioning to Coldline storage using lifecycle rules is a recommended approach, using the Standard storage class for the first 30 days may not be necessary if the processing period mentioned in the scenario is only two weeks. By using Standard storage for the entire 30 days, you would incur higher storage costs compared to using it for just the initial two weeks. Transitioning to Coldline storage after the two-week processing period would be more cost-effective.</p><p><br></p><p>https://cloud.google.com/storage/docs/storage-classes#standard</p><p>https://cloud.google.com/storage/docs/lifecycle</p>",
                "answers": [
                    "<p>You should use Standard storage class for the first two weeks, and use lifecycle rules to transition to Coldline.</p>",
                    "<p>You should use Standard storage class for the first two weeks, and use lifecycle rules to transition to Nearline.</p>",
                    "<p>You should use Standard storage class for the first two weeks, and then move videos to Persistent Disk.</p>",
                    "<p>You should use Standard storage class for the first 30 days, and use lifecycle rules to transition to Coldline.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "The raw format of CCTV footage videos is stored by a machine learning startup in a Cloud Storage bucket. During the initial two-week period, the footage undergoes consistent processing to identify and detect potential threats. What storage approach would you suggest to minimize costs when storing the videos?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146082,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are migrating an application to GCP&nbsp;using overnight batch jobs that take approximately one hour. Which service should you recommend to do this with minimal cost?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should run the batch jobs in a preemptible compute engine instance. -&gt; Correct. Preemptible Compute Engine instances are a cost-effective option for running batch jobs. Preemptible instances are regular instances that are priced lower than standard instances and can run for up to 24 hours. However, these instances can be terminated at any time if Google needs to reclaim the resources, and their availability is not guaranteed. Since the batch jobs are running overnight, and the workloads are not mission-critical, it is acceptable to use preemptible instances. Additionally, the batch jobs are not GPU-intensive, so there is no need to use GPU-enabled instances.</p><p><br></p><p>You should run the batch jobs in a GKE cluster. -&gt; Incorrect. It is not ideal since setting up a Kubernetes cluster incurs additional management overhead and cost, and is best suited for running long-running services. It is also not the most cost-effective solution for running batch jobs that only run for a short period.</p><p><br></p><p>You should run the batch jobs in a normal virtual machine instance. -&gt; Incorrect. It is a viable option, but using preemptible instances is a more cost-effective solution.</p><p><br></p><p>You should run the batch jobs in a virtual machine instance with GPUs. -&gt; Incorrect. It is not necessary since the batch jobs are not GPU-intensive. Using GPU-enabled instances would add unnecessary cost.</p><p><br></p><p>https://cloud.google.com/compute/docs/instances/preemptible#what_is_a_preemptible_instance</p>",
                "answers": [
                    "<p>You should run the batch jobs in a preemptible compute engine instance.</p>",
                    "<p>You should run the batch jobs in a GKE cluster.</p>",
                    "<p>You should run the batch jobs in a normal virtual machine instance.</p>",
                    "<p>You should run the batch jobs in a virtual machine instance with GPUs.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are migrating an application to GCP&nbsp;using overnight batch jobs that take approximately one hour. Which service should you recommend to do this with minimal cost?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146084,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A mobile gaming company decided to migrate its analytics to BigQuery. Their analytics team needs access to perform queries against the data in BigQuery to improve user acquisition expenses for marketing campaigns. This analytics team members may change frequently. With Google best practices in mind, how do you grant access?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should create a Cloud Identity account for each analyst and add them all to a group. Then, grant BigQuery Data Viewer role to this group. -&gt; Correct. Creating a Cloud Identity account for each analyst and adding them to a group provides a centralized approach for managing access to BigQuery. By granting the BigQuery Data Viewer role to the group, all analysts within the group will inherit the necessary permissions to perform queries against the data in BigQuery. This approach allows for easy management of access and ensures that new analysts can be added or removed from the group as needed, without individually granting or revoking access for each analyst.</p><p><br></p><p>You should create a Cloud Identity account for each analyst. Then, grant BigQuery Data Viewer role to each account. -&gt; Incorrect. While creating a Cloud Identity account for each analyst is a good step, granting the BigQuery Data Viewer role to each individual account can become cumbersome to manage as the number of analysts changes over time. It may require manual adjustments each time an analyst joins or leaves the team. This approach lacks the scalability and efficiency of managing access through groups.</p><p><br></p><p>You should create a Cloud Identity account for each analyst and add them all to a group. Then, grant BigQuery Data Owner role to this group. -&gt; Incorrect. Granting the BigQuery Data Owner role to a group may provide more permissions than necessary for the analytics team members. The Data Owner role includes privileges that go beyond performing queries, such as the ability to modify datasets and manage access controls. It is generally recommended to grant the least privilege required to perform the job, and in this case, the Data Viewer role is more appropriate.</p><p><br></p><p>You should create a Cloud Identity account for each analyst. Then, grant BigQuery Data Owner role to each account. -&gt; Incorrect. Granting the BigQuery Data Owner role to each individual account can give excessive privileges to analysts. This approach could lead to potential security risks and potential accidental modifications to datasets or access controls. It is preferable to assign more specific and limited roles that align with the actual requirements of the analytics team members.</p><p><br></p><p>https://cloud.google.com/identity/docs/set-up-cloud-identity-admin</p><p>https://cloud.google.com/bigquery/docs/access-control#bigquery</p>",
                "answers": [
                    "<p>You should create a Cloud Identity account for each analyst and add them all to a group. Then, grant BigQuery Data Viewer role to this group.</p>",
                    "<p>You should create a Cloud Identity account for each analyst. Then, grant BigQuery Data Viewer role to each account.</p>",
                    "<p>You should create a Cloud Identity account for each analyst and add them all to a group. Then, grant BigQuery Data Owner role to this group.</p>",
                    "<p>You should create a Cloud Identity account for each analyst. Then, grant BigQuery Data Owner role to each account.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A mobile gaming company decided to migrate its analytics to BigQuery. Their analytics team needs access to perform queries against the data in BigQuery to improve user acquisition expenses for marketing campaigns. This analytics team members may change frequently. With Google best practices in mind, how do you grant access?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146086,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>In your company, each employee has a credit card assigned to their account. As a cloud architect, you want to consolidate the billing of all GCP projects into a new billing account. With Google best practices in mind, how should you do it?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>In the GCP Console, move all projects to the root organization in the Resource Manager. -&gt;&nbsp;Correct. Moving all projects to the root organization in the Resource Manager allows you to create a new billing account at the organization level and link all the projects to it. This approach is recommended by Google as it helps to centralize billing and provides better visibility into the costs and usage of all GCP projects. Once the projects are linked to the new billing account, you can remove the credit cards associated with individual accounts and use a single payment method for all projects. </p><p><br></p><p>You should create a new Billing account and set up a payment method with company credit card. -&gt;&nbsp;Incorrect. It is incorrect because it only addresses the creation of a new billing account but doesn't mention the steps needed to link the existing projects to the new account. </p><p><br></p><p>You should send an email to Google Billing Support and request them to create a new billing account and link all the projects to the billing account. -&gt; Incorrect. It is incorrect because it's not necessary to contact Google Billing Support for this task, and you can do it using the GCP Console. </p><p><br></p><p>Once a credit card is assigned to the project, it cannot be changed. You have to create all the projects from scratch with new billing account. -&gt;&nbsp;Incorrect. It is incorrect because it's not true that you cannot change the payment method associated with a GCP project.</p><p><br></p><p>https://cloud.google.com/resource-manager/docs/quickstart-organizations</p>",
                "answers": [
                    "<p>In the GCP Console, move all projects to the root organization in the Resource Manager.</p>",
                    "<p>You should create a new Billing account and set up a payment method with company credit card.</p>",
                    "<p>You should send an email to Google Billing Support and request them to create a new billing account and link all the projects to the billing account.</p>",
                    "<p>Once a credit card is assigned to the project, it cannot be changed. You have to create all the projects from scratch with new billing account.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "In your company, each employee has a credit card assigned to their account. As a cloud architect, you want to consolidate the billing of all GCP projects into a new billing account. With Google best practices in mind, how should you do it?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146088,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You need to migrate your legacy on-premises applications to Google Cloud that are written in C++ and you want to use the serverless approach. What GCP compute services should you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should deploy the containerized version of the application in Cloud Run. -&gt; Correct. Cloud Run is a serverless compute platform that allows you to run stateless containers in a fully managed environment. It supports containerized applications built on any language or framework, including C++. By deploying the containerized version of the application to Cloud Run, you can take advantage of the serverless approach, where you only pay for the actual usage of resources, and the environment is fully managed by Google Cloud.</p><p><br></p><p>You should deploy the containerized version of the application in Google Kubernetes Engine. -&gt; Incorrect. It requires more management overhead and may not be the best fit for a serverless approach. Kubernetes Engine provides a managed environment for running containerized applications, but it requires more management overhead and may not be as cost-effective as a serverless solution.</p><p><br></p><p>You should deploy the containerized version of the application in App Engine Flexible. -&gt; Incorrect. It may not be the best fit for applications that require more customization and control over the infrastructure.</p><p><br></p><p>You should deploy this application using a Managed Instance Group. -&gt; Incorrect. It is not the best option for the given requirement because Managed Instance Groups are primarily used for managing groups of virtual machine instances, which require more management overhead and may not be as cost-effective as a serverless solution like Cloud Run.</p><p><br></p><p>https://cloud.google.com/run/docs/concepts</p>",
                "answers": [
                    "<p>You should deploy the containerized version of the application in Cloud Run.</p>",
                    "<p>You should deploy the containerized version of the application in Google Kubernetes Engine.</p>",
                    "<p>You should deploy the containerized version of the application in App Engine Flexible.</p>",
                    "<p>You should deploy this application using a Managed Instance Group.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You need to migrate your legacy on-premises applications to Google Cloud that are written in C++ and you want to use the serverless approach. What GCP compute services should you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146090,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>An e-commerce company has petabytes of customer behavior data stored in private data center. Due to storage limitations in private data center, this company decided to migrate this data to GCP. The data must be available for your analysts, who have strong SQL background. How should you store the data to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should import data into BigQuery. -&gt;&nbsp;Correct. BigQuery is a fully-managed, cloud-native data warehouse that enables super-fast SQL queries using the processing power of Google's infrastructure. It can handle petabytes of data and is designed for analyzing large datasets with ease. It is also highly scalable and supports real-time analysis of data. Analysts can use their existing SQL skills to query data stored in BigQuery without the need for any special software or hardware, making it a perfect fit for the requirements of the e-commerce company in this scenario.</p><p><br></p><p>You should import flat files into Cloud Storage. -&gt; Incorrect. It is not the best answer as flat files stored in Cloud Storage would require additional processing to make them queryable with SQL.</p><p>You should import data into Cloud Datastore. -&gt;&nbsp;Incorrect. It is not the best answer as Cloud Datastore is a NoSQL document database, which is not ideal for SQL queries.</p><p><br></p><p>You should import data into Cloud SQL. -&gt; Incorrect. It is not the best answer as Cloud SQL is a relational database service which requires data to be structured in a specific way and has limitations on data size, making it less suitable for the petabyte-scale data in this scenario.</p><p><br></p><p>https://cloud.google.com/bigquery/docs/introduction</p>",
                "answers": [
                    "<p>You should import data into BigQuery.</p>",
                    "<p>You should import flat files into Cloud Storage.</p>",
                    "<p>You should import data into Cloud Datastore.</p>",
                    "<p>You should import data into Cloud SQL.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "An e-commerce company has petabytes of customer behavior data stored in private data center. Due to storage limitations in private data center, this company decided to migrate this data to GCP. The data must be available for your analysts, who have strong SQL background. How should you store the data to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146092,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>An application processes a significant number of transactions that exceed the capabilities of a single virtual machine. As a cloud architect, you want to spread transactions across multiple servers in real time and in the most cost-effective way. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should send transactions to Pub/Sub and process them in virtual machines in a Managed Instance Group. -&gt;&nbsp;Correct. Pub/Sub is a messaging service that allows decoupling between the senders and receivers of messages, enabling the sender to send messages without worrying about who will receive them. A Managed Instance Group is a group of virtual machine instances that are managed together and can be scaled up or down automatically based on the workload. By using Pub/Sub and a Managed Instance Group, the transactions can be sent to Pub/Sub, and the virtual machines can subscribe to the relevant Pub/Sub topic to receive and process the transactions. This approach ensures that the transactions are spread across multiple servers, allowing for parallel processing and improved performance.</p><p><br></p><p>You should send transactions to Cloud Bigtable, and poll for new transactions from the VMs. -&gt; Incorrect. Cloud Bigtable is a NoSQL database for storing large amounts of data, and it may not be the most suitable solution for real-time transaction processing. </p><p><br></p><p>You should set up Cloud SQL with a memory cache for speed. On your multiple servers, poll for transactions that do not have the processed key, and mark them processed when done. -&gt; Incorrect. Cloud SQL is a managed relational database service that can be used for storing transaction data, but it does not provide the scalability or real-time processing capabilities required for this scenario. </p><p><br></p><p>You should send transactions to BigQuery. On the virtual machines, poll for transactions that don't have the processed key, and mark them processed when done. -&gt; Incorrect. BigQuery is a data warehouse that can be used for batch processing and analytics, but it may not be the best solution for real-time transaction processing.</p><p><br></p><p>https://cloud.google.com/pubsub/docs/quickstart-console</p>",
                "answers": [
                    "<p>You should send transactions to Pub/Sub and process them in virtual machines in a Managed Instance Group.</p>",
                    "<p>You should send transactions to Cloud Bigtable, and poll for new transactions from the VMs.</p>",
                    "<p>You should set up Cloud SQL with a memory cache for speed. On your multiple servers, poll for transactions that do not have the processed key, and mark them processed when done.</p>",
                    "<p>You should send transactions to BigQuery. On the virtual machines, poll for transactions that don't have the processed key, and mark them processed when done.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "An application processes a significant number of transactions that exceed the capabilities of a single virtual machine. As a cloud architect, you want to spread transactions across multiple servers in real time and in the most cost-effective way. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146094,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization provides a video streaming service to global users. Recently, users are complaining about the high latency when watching videos. As a cloud architect, how would you improve the streaming experience?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud CDN to cache and serve video content. -&gt;&nbsp;Correct. Cloud CDN leverages Google's global edge network to cache and serve content closer to users, which can significantly reduce latency and improve the streaming experience.</p><p><br></p><p>Store all video content in a single region in Cloud Storage. -&gt;&nbsp;Incorrect. Storing content in a single region can cause latency for users that are geographically distant. This solution would not address the latency issue.</p><p><br></p><p>Use Cloud Pub/Sub to push video content to users. -&gt;&nbsp;Incorrect. Cloud Pub/Sub is a messaging service and not suited for video streaming, and would likely introduce more latency than current setup.</p><p><br></p><p>Use Cloud Spanner to store and serve video content. -&gt;&nbsp;Incorrect. Cloud Spanner is a globally distributed, horizontally scalable, relational database service and is not the best choice for storing and serving video content, as it might not reduce latency.</p>",
                "answers": [
                    "<p>Use Cloud CDN to cache and serve video content.</p>",
                    "<p>Store all video content in a single region in Cloud Storage.</p>",
                    "<p>Use Cloud Pub/Sub to push video content to users.</p>",
                    "<p>Use Cloud Spanner to store and serve video content.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization provides a video streaming service to global users. Recently, users are complaining about the high latency when watching videos. As a cloud architect, how would you improve the streaming experience?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146096,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company runs several critical applications on Google Cloud. There has been a significant increase in the number of user-reported incidents recently, indicating performance issues. As a cloud architect, how would you improve the monitoring to proactively identify and address performance issues?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Set up custom metrics in Cloud Monitoring for all critical applications and configure alerting based on those metrics. -&gt;&nbsp;Correct. Cloud Monitoring allows you to set up custom metrics for monitoring specific aspects of your applications, and you can configure alerts to be notified when these metrics cross certain thresholds. This proactive approach can help you address performance issues before they impact users.</p><p><br></p><p>Use Cloud Profiler to continuously profile the applications. -&gt; Incorrect. Cloud Profiler is used to analyze the performance of your applications and understand where resources are being spent. However, it's primarily used for analyzing the performance of individual services or functions, rather than overall application monitoring.</p><p><br></p><p>Use Cloud Trace to trace all requests to the applications. -&gt; Incorrect. Cloud Trace allows you to analyze the latency of your application but it doesn't provide comprehensive application monitoring. While it's part of the overall solution, it should be used in conjunction with other tools.</p><p><br></p><p>Use Cloud Debugger to debug the applications in real-time. -&gt; Incorrect. Cloud Debugger allows you to inspect the state of your application in real time, but it's not a comprehensive monitoring solution and wouldn't be the most efficient tool for proactively identifying performance issues.</p>",
                "answers": [
                    "<p>Set up custom metrics in Cloud Monitoring for all critical applications and configure alerting based on those metrics.</p>",
                    "<p>Use Cloud Profiler to continuously profile the applications.</p>",
                    "<p>Use Cloud Trace to trace all requests to the applications.</p>",
                    "<p>Use Cloud Debugger to debug the applications in real-time.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company runs several critical applications on Google Cloud. There has been a significant increase in the number of user-reported incidents recently, indicating performance issues. As a cloud architect, how would you improve the monitoring to proactively identify and address performance issues?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146098,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You plan to migrate your on-premises MySQL and PostgreSQL databases to the Google Cloud using Lift and Shift approach. Which Google Cloud service should you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Database Migration Service -&gt; Correct. Google's Database Migration Service is specifically designed to migrate databases to Google Cloud, including MySQL and PostgreSQL. It supports various migration source types, including on-premises databases, and provides a simple and automated way to migrate databases with minimal downtime. </p><p><br></p><p>Migrate for Anthos -&gt; Incorrect. Migrate for Anthos is used to modernize the applications.</p><p><br></p><p>BigQuery Data Transfer Service -&gt;&nbsp;Incorrect. BigQuery Data Transfer Service is used to transfer data to BigQuery.</p><p><br></p><p>Storage Transfer Service -&gt; Incorrect. Storage Transfer Service is used to transfer data to Cloud Storage.</p><p><br></p><p>https://cloud.google.com/database-migration/docs</p>",
                "answers": [
                    "<p>Database Migration Service</p>",
                    "<p>Migrate for Anthos</p>",
                    "<p>BigQuery Data Transfer Service</p>",
                    "<p>Storage Transfer Service</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You plan to migrate your on-premises MySQL and PostgreSQL databases to the Google Cloud using Lift and Shift approach. Which Google Cloud service should you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146100,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization uses BigQuery extensively for data analysis and you are developing a new solution to automate some recurring tasks. As part of this solution, you need to create a new table in BigQuery and load data into it from a CSV file in Cloud Storage, all from a Compute Engine instance. What would be the correct approach?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use the <code>bq</code> command-line tool to both create the table and load the data from Cloud Storage. -&gt; Correct. The <code>bq</code> tool can be used to perform a variety of BigQuery operations, including creating tables and loading data from Cloud Storage.</p><p><br></p><p>Use the <code>bq</code> command-line tool to create the table and then the <code>gsutil cp</code> command to copy the data from Cloud Storage to BigQuery. -&gt; Incorrect. <code>gsutil cp</code> command is used to copy files to and from Cloud Storage, not for loading data into BigQuery tables.</p><p><br></p><p>Use the <code>gsutil</code> command to create the table in BigQuery and then the <code>bq</code> command-line tool to load the data. -&gt; Incorrect. The <code>gsutil</code> tool is not used to interact with BigQuery, it's designed for working with Cloud Storage.</p><p><br></p><p>Use the <code>gcloud</code> command-line tool to create the table and load the data into BigQuery. -&gt; Incorrect. While <code>gcloud</code> is a comprehensive command-line tool for Google Cloud, specific BigQuery operations like creating tables and loading data are typically performed with the <code>bq</code> tool.</p>",
                "answers": [
                    "<p>Use the <code>bq</code> command-line tool to both create the table and load the data from Cloud Storage.</p>",
                    "<p>Use the <code>bq</code> command-line tool to create the table and then the <code>gsutil cp</code> command to copy the data from Cloud Storage to BigQuery.</p>",
                    "<p>Use the <code>gsutil</code> command to create the table in BigQuery and then the <code>bq</code> command-line tool to load the data.</p>",
                    "<p>Use the <code>gcloud</code> command-line tool to create the table and load the data into BigQuery.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization uses BigQuery extensively for data analysis and you are developing a new solution to automate some recurring tasks. As part of this solution, you need to create a new table in BigQuery and load data into it from a CSV file in Cloud Storage, all from a Compute Engine instance. What would be the correct approach?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146102,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization operates a multi-tier web application on Google Cloud. You have been tasked with implementing a solution to ensure operational reliability and quickly identify any issues that might affect the application's performance. Which of the following would be the best approach?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Enable Cloud Logging and Monitoring for the entire project. -&gt; Correct. This option includes both Logging and Monitoring, which can be used to collect logs, metrics, events, and metadata from your Cloud project. This will allow you to monitor the performance of your application and quickly identify any issues.</p><p><br></p><p>Configure Cloud Pub/Sub to publish messages whenever there is a change in the application. -&gt;&nbsp;Incorrect. Cloud Pub/Sub is a messaging service that is not specifically designed for monitoring applications. While it could be used in conjunction with other services for monitoring, it is not a complete solution on its own.</p><p><br></p><p>Enable Cloud Debugger on all application instances. -&gt;&nbsp;Incorrect. Cloud Debugger allows you to inspect the state of an application at any code location without affecting the performance of the production application. However, it's not meant for general application performance monitoring, and using it for this purpose could lead to unnecessary complexity and overhead.</p><p><br></p><p>Use Cloud Dataflow to process logs in real-time. -&gt;&nbsp;Incorrect. While Cloud Dataflow can process and analyze real-time data, it's not specifically designed for monitoring applications. It would be overkill for this use case and may not provide the insights you need without additional tools.</p>",
                "answers": [
                    "<p>Enable Cloud Logging and Monitoring for the entire project.</p>",
                    "<p>Configure Cloud Pub/Sub to publish messages whenever there is a change in the application.</p>",
                    "<p>Enable Cloud Debugger on all application instances.</p>",
                    "<p>Use Cloud Dataflow to process logs in real-time.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization operates a multi-tier web application on Google Cloud. You have been tasked with implementing a solution to ensure operational reliability and quickly identify any issues that might affect the application's performance. Which of the following would be the best approach?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146104,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You're designing a CI/CD pipeline for an application hosted on GCP. As part of the pipeline, you need to programmatically deploy and manage several GCP resources. Which approach should you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Google Cloud SDK and REST APIs -&gt;&nbsp;Correct. The Google Cloud SDK and GCP REST APIs provide the programmatic interfaces needed to manage GCP resources within a CI/CD pipeline. They allow for the automation of resource management tasks.</p><p><br></p><p>Google Cloud Console -&gt;&nbsp;Incorrect. The Google Cloud Console is a web interface and not suitable for programmatically managing GCP resources within a CI/CD pipeline.</p><p><br></p><p>Cloud Shell -&gt;&nbsp;Incorrect. While Cloud Shell can be used to interact with GCP resources programmatically, it's an interactive, browser-based tool not suitable for use within a CI/CD pipeline.</p><p><br></p><p>Cloud Deployment Manager -&gt;&nbsp;Incorrect. Cloud Deployment Manager can be used to automate the creation and management of GCP resources, but it's more focused on infrastructure as code (IaC) and less suitable for integrating with a CI/CD pipeline compared to the Google Cloud SDK and GCP REST APIs.</p>",
                "answers": [
                    "<p>Google Cloud SDK and REST APIs</p>",
                    "<p>Google Cloud Console</p>",
                    "<p>Cloud Shell</p>",
                    "<p>Cloud Deployment Manager</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You're designing a CI/CD pipeline for an application hosted on GCP. As part of the pipeline, you need to programmatically deploy and manage several GCP resources. Which approach should you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146106,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are developing an application that needs to interact with Google Cloud Storage to store and retrieve data. You need to authenticate the application programmatically with GCP without user intervention. Which method would you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Service Account -&gt; Correct. Service accounts are used for server-to-server interactions and are not associated with individual end users. These are ideal for authenticating an application that needs to interact with Google Cloud Storage programmatically.</p><p><br></p><p>OAuth 2.0 Client ID -&gt; Incorrect. OAuth 2.0 Client IDs are used for web applications that need to interact with Google APIs on behalf of users. This does not suit an application that needs to authenticate with GCP without user intervention.</p><p><br></p><p>Cloud Identity -&gt; Incorrect. Cloud Identity is used for managing users, groups, and devices, not for programmatically authenticating applications with GCP.</p><p><br></p><p>Cloud Shell -&gt; Incorrect. Cloud Shell provides a shell environment for managing resources and applications hosted on Google Cloud. It does not provide a method to programmatically authenticate an application with GCP.</p>",
                "answers": [
                    "<p>Service Account</p>",
                    "<p>OAuth 2.0 Client ID</p>",
                    "<p>Cloud Identity</p>",
                    "<p>Cloud Shell</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are developing an application that needs to interact with Google Cloud Storage to store and retrieve data. You need to authenticate the application programmatically with GCP without user intervention. Which method would you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146108,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You have an application with static content that is deployed in US region. What can you do to bring your content closer to European users?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should distribute static content using Cloud CDN. -&gt;&nbsp;Correct. Using Cloud CDN will cache static content in edge locations worldwide, including in Europe, and deliver it from the nearest location to the user, reducing latency and improving performance. </p><p><br></p><p>You should scale up the size of the web server. -&gt; Incorrect. Scaling up the size of the web server will not have an impact on the user's latency.</p><p><br></p><p>You should move the server to Europe. -&gt; Incorrect. Moving the server to Europe would increase the latency for users in the US. Using </p><p><br></p><p>You should distribute static content using Cloud VPN. -&gt; Incorrect. Cloud VPN is a way to establish a secure connection between two networks over the public internet but not for content distribution.</p><p><br></p><p>https://cloud.google.com/cdn/docs/overview</p>",
                "answers": [
                    "<p>You should distribute static content using Cloud CDN.</p>",
                    "<p>You should scale up the size of the web server.</p>",
                    "<p>You should move the server to Europe.</p>",
                    "<p>You should distribute static content using Cloud VPN.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You have an application with static content that is deployed in US region. What can you do to bring your content closer to European users?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146110,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect for a company that has a multi-tier application running on GCP. The application includes a load balancer, several web servers, and a back-end database. As part of your security strategy, you want to allow HTTP(S) traffic only from the load balancer to the web servers. Which of the following would be the best approach?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Create a firewall rule with a high priority (low numeric value) to allow traffic from the load balancer, and a default rule to deny all other traffic. -&gt;&nbsp;Correct. Firewall rules with a lower numeric value have higher priority and are evaluated before rules with a higher numeric value. This setup ensures that the allowed traffic from the load balancer is not blocked by the default rule.</p><p><br></p><p>Create a firewall rule with a low priority (high numeric value) to allow traffic from the load balancer, and a default rule to deny all other traffic. -&gt; Incorrect. This approach would not work as expected because the default deny rule (which typically has a higher priority/lower numeric value) would block the traffic before the allow rule for the load balancer (with lower priority/higher numeric value) is evaluated.</p><p><br></p><p>Create a firewall rule with a high priority (low numeric value) to deny all traffic, and a rule with a low priority (high numeric value) to allow traffic from the load balancer. -&gt; Incorrect. This setup will not work as intended because the high priority rule denying all traffic would be evaluated before the rule allowing traffic from the load balancer, thus blocking all traffic.</p><p><br></p><p>Create two firewall rules with the same priority, one to allow traffic from the load balancer, and one to deny all other traffic. -&gt; Incorrect. If two rules have the same priority, GCP uses other criteria (like network ranges and protocols) to determine which rule applies, so this setup is not guaranteed to work as intended.</p>",
                "answers": [
                    "<p>Create a firewall rule with a high priority (low numeric value) to allow traffic from the load balancer, and a default rule to deny all other traffic.</p>",
                    "<p>Create a firewall rule with a low priority (high numeric value) to allow traffic from the load balancer, and a default rule to deny all other traffic.</p>",
                    "<p>Create a firewall rule with a high priority (low numeric value) to deny all traffic, and a rule with a low priority (high numeric value) to allow traffic from the load balancer.</p>",
                    "<p>Create two firewall rules with the same priority, one to allow traffic from the load balancer, and one to deny all other traffic.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect for a company that has a multi-tier application running on GCP. The application includes a load balancer, several web servers, and a back-end database. As part of your security strategy, you want to allow HTTP(S) traffic only from the load balancer to the web servers. Which of the following would be the best approach?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146112,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company has several applications running on GCP and on-premises. You need to create a hybrid cloud strategy to allow applications running on GCP to access data from your on-premises data center with low latency. What is the best strategy?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Interconnect to create a dedicated connection from your on-premises network to your VPC network on Google Cloud. -&gt;&nbsp;Correct. Cloud Interconnect provides a direct, enterprise-grade connection to Google's network, which can offer lower latency than VPN connections over the public internet.</p><p><br></p><p>Use Cloud VPN to create an IPsec VPN tunnel from the on-premises network to your VPC network on Google Cloud. -&gt;&nbsp;Incorrect. Cloud VPN allows secure communication between your on-premise network and GCP over the public internet. However, it may not provide the low latency required by the applications.</p><p><br></p><p>Use VPC Network Peering to connect the on-premises network to your VPC network on Google Cloud. -&gt;&nbsp;Incorrect. VPC Network Peering is used to connect two VPC networks within Google Cloud or with other cloud providers. It doesn't apply to on-premise networks.</p><p><br></p><p>Use Cloud Storage to store a copy of your on-premises data and access it from your applications on GCP. -&gt;&nbsp;Incorrect. While this approach might provide data to GCP applications, it doesn't maintain real-time consistency with on-premise data sources and may not meet low latency requirements.</p>",
                "answers": [
                    "<p>Use Cloud Interconnect to create a dedicated connection from your on-premises network to your VPC network on Google Cloud.</p>",
                    "<p>Use Cloud VPN to create an IPsec VPN tunnel from the on-premises network to your VPC network on Google Cloud.</p>",
                    "<p>Use VPC Network Peering to connect the on-premises network to your VPC network on Google Cloud.</p>",
                    "<p>Use Cloud Storage to store a copy of your on-premises data and access it from your applications on GCP.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company has several applications running on GCP and on-premises. You need to create a hybrid cloud strategy to allow applications running on GCP to access data from your on-premises data center with low latency. What is the best strategy?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146114,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are responsible for preparing a migration strategy. You need to deploy a disaster recovery infrastructure with the same design and configuration as your production environment using Google Cloud. Which topology would you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Mirrored topology -&gt; Correct. A mirrored topology is a disaster recovery strategy that involves creating a mirror of the production environment in a separate location. In this strategy, the disaster recovery environment is an exact replica of the production environment, including the same design and configuration. If a disaster occurs in the production environment, traffic can be quickly redirected to the disaster recovery environment.</p><p><br></p><p>Gated egress topology -&gt; Incorrect. A gated egress topology involves routing traffic through a set of security controls before it leaves the network. This topology is not related to disaster recovery.</p><p><br></p><p>Gated ingress topology -&gt; Incorrect. A gated ingress topology involves routing traffic through a set of security controls before it enters the network. This topology is not related to disaster recovery.</p><p><br></p><p>Handover topology -&gt; Incorrect. A handover topology is a strategy in which traffic is handed over from one network to another, such as from a public cloud to a private cloud. While this can be part of a disaster recovery strategy, it does not involve creating a mirror of the production environment.</p><p><br></p><p>https://cloud.google.com/architecture/hybrid-and-multi-cloud-network-topologies</p>",
                "answers": [
                    "<p>Mirrored topology</p>",
                    "<p>Gated egress topology</p>",
                    "<p>Handover topology</p>",
                    "<p>Gated ingress topology</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are responsible for preparing a migration strategy. You need to deploy a disaster recovery infrastructure with the same design and configuration as your production environment using Google Cloud. Which topology would you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146116,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is deploying a new application on GCP that requires global availability and has heavy content, such as video streaming. The application is designed to handle varying levels of traffic, and you expect users from around the world. What type of load balancer would be the best choice?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>HTTP(S) Load Balancer -&gt;&nbsp;Correct. HTTP(S) Load Balancer is a global, proxy-based load balancer that supports content-based routing. It integrates with Cloud CDN and Google's global edge network, making it suitable for applications with global reach and heavy content like video streaming.</p><p><br></p><p>Internal HTTP(S) Load Balancer -&gt; Incorrect. The Internal HTTP(S) Load Balancer is for private, internal load balancing and not suitable for applications that require global availability.</p><p><br></p><p>SSL Proxy Load Balancer -&gt; Incorrect. The SSL Proxy Load Balancer is used for SSL (TLS) traffic. It is not designed for global load balancing or heavy content like video streaming.</p><p><br></p><p>Network Load Balancer -&gt; Incorrect. The Network Load Balancer is a regional, non-proxy load balancer that distributes traffic based on IP protocol data, such as address, port, and protocol type. It is not optimized for global applications or heavy content.</p>",
                "answers": [
                    "<p>HTTP(S) Load Balancer</p>",
                    "<p>Internal HTTP(S) Load Balancer</p>",
                    "<p>SSL Proxy Load Balancer</p>",
                    "<p>Network Load Balancer</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company is deploying a new application on GCP that requires global availability and has heavy content, such as video streaming. The application is designed to handle varying levels of traffic, and you expect users from around the world. What type of load balancer would be the best choice?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146118,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You're designing a high-traffic web application on Google Cloud. The application involves dynamic content that changes frequently, but some parts of the data remain the same across different user sessions. You need to improve the application's performance and user experience by reducing the load on your databases. Which strategy should you adopt?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement Cloud MemoryStore with Redis as an in-memory data store. -&gt;&nbsp;Correct. Cloud MemoryStore provides a fully managed in-memory data store service built on scalable, secure, and highly available infrastructure managed by Google. Using Redis for caching can significantly improve the application's performance by providing fast access to frequently used data.</p><p><br></p><p>Implement a global Cloud Load Balancer. -&gt;&nbsp;Incorrect. Although a global Cloud Load Balancer can distribute incoming traffic, it won't serve the purpose of caching data to reduce database load.</p><p><br></p><p>Use Cloud CDN to cache dynamic content. -&gt;&nbsp;Incorrect. While Cloud CDN is an excellent tool for caching static content at the edge, it is not designed to cache frequently changing dynamic content.</p><p><br></p><p>Utilize Cloud Storage to store and retrieve commonly accessed data. -&gt;&nbsp;Incorrect. Cloud Storage is not suitable for caching as it is designed for long-term unstructured data storage. The latency for data access may not meet the performance needs of a caching solution.</p>",
                "answers": [
                    "<p>Implement Cloud MemoryStore with Redis as an in-memory data store.</p>",
                    "<p>Implement a global Cloud Load Balancer.</p>",
                    "<p>Use Cloud CDN to cache dynamic content.</p>",
                    "<p>Utilize Cloud Storage to store and retrieve commonly accessed data.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You're designing a high-traffic web application on Google Cloud. The application involves dynamic content that changes frequently, but some parts of the data remain the same across different user sessions. You need to improve the application's performance and user experience by reducing the load on your databases. Which strategy should you adopt?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146120,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are responsible for preparing a migration plan for your company. You want to migrate Apache Kafka (real-time streaming data pipelines) to the Google Cloud. Which Google Cloud service should you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Pub/Sub -&gt; Correct. It is a fully managed real-time messaging service that enables you to send and receive messages between independent applications. It provides a scalable, durable message queue that can handle millions of messages per second. Apache Kafka is a popular messaging system used for real-time streaming data pipelines. Cloud Pub/Sub provides similar functionality as Kafka, such as message ordering and at-least-once delivery, making it a good choice for migrating Kafka workloads to the cloud.</p><p><br></p><p>Cloud Bigtable -&gt;&nbsp;Incorrect. It is a fully managed NoSQL database service that is optimized for handling large amounts of data with low latency. While Cloud Bigtable can store real-time streaming data, it is not designed for real-time messaging or queuing.</p><p><br></p><p>Cloud Firestore -&gt; Incorrect. It is a fully managed NoSQL document database service that is optimized for mobile and web application development. While Cloud Firestore can store real-time streaming data, it is not designed for real-time messaging or queuing.</p><p><br></p><p>Cloud Datastore -&gt;&nbsp;Incorrect. It is a fully managed NoSQL document database service that is optimized for handling large amounts of data with high query throughput. While Cloud Datastore can store real-time streaming data, it is not designed for real-time messaging or queuing.</p><p><br></p><p>https://cloud.google.com/architecture/migrating-from-kafka-to-pubsub?hl=en</p>",
                "answers": [
                    "<p>Cloud Pub/Sub</p>",
                    "<p>Cloud Bigtable</p>",
                    "<p>Cloud Firestore</p>",
                    "<p>Cloud Datastore</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are responsible for preparing a migration plan for your company. You want to migrate Apache Kafka (real-time streaming data pipelines) to the Google Cloud. Which Google Cloud service should you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146122,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Refer to the EHR Healthcare case study for this question: https://services.google.com/fh/files/blogs/master_case_study_ehr_healthcare.pdf</p><p><br></p><p>The sales employees of EHR work remotely and travel to various locations for their job. These employees require access to web-based sales tools located in the EHR data center. EHR has made the decision to retire their existing Virtual Private Network (VPN) infrastructure, necessitating the migration of the web-based sales tools to a BeyondCorp access model. Each sales employee possesses a Google Workspace account, which they utilize for single sign-on (SSO) purposes. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should create an Identity-Aware Proxy (IAP) connector that points to the sales tool application. -&gt; Correct.</p><p><br></p><p>You should create a Google group for the sales tool application, and upgrade that group to a security group. -&gt; Incorrect. Google groups by themselves do not grant access to an application nor do they move an application to a BeyondCorp model.</p><p><br></p><p>You should deploy an external HTTP(S) load balancer and create a custom Cloud Armor policy for the sales tool application. -&gt;&nbsp;Incorrect. Cloud Armor does not authenticate or authorize application access.</p><p><br></p><p>For every sales employee who needs access to the sales tool application, you should give their Google Workspace user account the predefined AppEngine Viewer role. -&gt; Incorrect. The application is installed in the datacenter, not in the AppEngine environment.</p><p><br></p><p>https://cloud.google.com/iap/docs/cloud-iap-for-on-prem-apps-overview</p>",
                "answers": [
                    "<p>You should create an Identity-Aware Proxy (IAP) connector that points to the sales tool application.</p>",
                    "<p>You should create a Google group for the sales tool application, and upgrade that group to a security group.</p>",
                    "<p>You should deploy an external HTTP(S) load balancer and create a custom Cloud Armor policy for the sales tool application.</p>",
                    "<p>For every sales employee who needs access to the sales tool application, you should give their Google Workspace user account the predefined AppEngine Viewer role.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Refer to the EHR Healthcare case study for this question: https://services.google.com/fh/files/blogs/master_case_study_ehr_healthcare.pdfThe sales employees of EHR work remotely and travel to various locations for their job. These employees require access to web-based sales tools located in the EHR data center. EHR has made the decision to retire their existing Virtual Private Network (VPN) infrastructure, necessitating the migration of the web-based sales tools to a BeyondCorp access model. Each sales employee possesses a Google Workspace account, which they utilize for single sign-on (SSO) purposes. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146124,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdf</p><p><br></p><p>As your new game is currently in public beta on the Google Cloud platform, it is essential to establish significant service level objectives (SLOs) before its official release to the public. What steps should you take to accomplish this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should define one SLO as 99% HTTP requests return the 2xx status code. Define the other SLO as 99% requests return within 100 ms. -&gt; Correct. It clearly defines the service level indicators and how to measure them.</p><p><br></p><p>You should define one SLO as 99.9% game server availability. Define the other SLO as less than 100-ms latency. -&gt;&nbsp;Incorrect. It doesn't clearly define how to measure both the availability and latency.</p><p><br></p><p>You should define one SLO as service availability that is the same as Google Cloud's availability. Define the other SLO as 100-ms latency. -&gt;&nbsp;Incorrect. Google Cloud availability has an impact on customer availability but it is only one factor. Also, for different Google Cloud products, the availability could be different.</p><p><br></p><p>You should define one SLO as total uptime of the game server within a week. Define the other SLO as the mean response time of all HTTP requests that are less than 100 ms. -&gt;&nbsp;Incorrect. There is no objective for the server uptime.</p><p><br></p><p>https://sre.google/workbook/implementing-slos/</p>",
                "answers": [
                    "<p>You should define one SLO as 99% HTTP requests return the 2xx status code. Define the other SLO as 99% requests return within 100 ms.</p>",
                    "<p>You should define one SLO as 99.9% game server availability. Define the other SLO as less than 100-ms latency.</p>",
                    "<p>You should define one SLO as service availability that is the same as Google Cloud's availability. Define the other SLO as 100-ms latency.</p>",
                    "<p>You should define one SLO as total uptime of the game server within a week. Define the other SLO as the mean response time of all HTTP requests that are less than 100 ms.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdfAs your new game is currently in public beta on the Google Cloud platform, it is essential to establish significant service level objectives (SLOs) before its official release to the public. What steps should you take to accomplish this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146126,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>As a cloud architect, you work for a company that wants to try out the cloud with low risk. They want to archive approximately 500 TB of their log data to the cloud and test the serverless analytics features available to them there, while also retaining that data as a long-term disaster recovery backup. Which two steps should they take?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Load logs into BigQuery. -&gt;&nbsp;Correct. BigQuery is a serverless cloud data warehouse for analytics and supports the volume and analytics requirement.</p><p><br></p><p>Upload log files into Cloud Storage. -&gt; Correct. Cloud Storage provides the Coldline and Archive storage classes to support long-term storage with infrequent access, which would support the long-term disaster recovery backup requirement.</p><p><br></p><p>Load logs into Cloud SQL. -&gt; Incorrect. Cloud SQL does not support the expected 100 TB. Additionally, Cloud SQL is a relational database and not the best fit for time-series log data formats.</p><p><br></p><p>Import logs into Cloud Logging. -&gt; Incorrect. Cloud Logging is optimized for monitoring, error reporting, and debugging instead of analytics queries.</p><p><br></p><p>Insert logs into Cloud Bigtable. -&gt;&nbsp;Incorrect. Cloud Bigtable is optimized for read-write latency and analytics throughput, not analytics querying and reporting.</p><p><br></p><p>https://cloud.google.com/bigquery/</p>",
                "answers": [
                    "<p>Load logs into BigQuery.</p>",
                    "<p>Upload log files into Cloud Storage.</p>",
                    "<p>Load logs into Cloud SQL.</p>",
                    "<p>Import logs into Cloud Logging.</p>",
                    "<p>Insert logs into Cloud Bigtable.</p>"
                ]
            },
            "correct_response": [
                "a",
                "b"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you work for a company that wants to try out the cloud with low risk. They want to archive approximately 500 TB of their log data to the cloud and test the serverless analytics features available to them there, while also retaining that data as a long-term disaster recovery backup. Which two steps should they take?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146128,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>The database administration team has asked you to help them improve the performance of a new database server running on Compute Engine. The database is used to import and normalize company performance statistics. It is built with MySQL running on Debian Linux. They have an n1-standard-8 VM with 80 GB of SSD zonal persistent disk which they can't restart until the next maintenance event. What should they change to get better performance from this system as soon as possible and in a cost-effective manner?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Dynamically resize the SSD persistent disk to 500 GB. -&gt; Correct. Persistent disk performance is based on the total persistent disk capacity attached to an instance and the number of vCPUs that the instance has. Incrementing the persistent disk capacity will increment its throughput and IOPS, which in turn improve the performance of MySQL.</p><p><br></p><p>Increase the virtual machines memory to 64 GB. -&gt; Increasing the memory size requires a VM restart.</p><p><br></p><p>Create a new virtual machine running PostgreSQL. -&gt;&nbsp;Incorrect. The DB administration team is requesting help with their MySQL instance. Migration to a different product should not be the solution when other optimization techniques can still be applied first.</p><p><br></p><p>Migrate their performance metrics warehouse to BigQuery. -&gt; Incorrect. The DB administration team is requesting help with their MySQL instance. Migration to a different product should not be the solution when other optimization techniques can still be applied first.</p><p><br></p><p>https://cloud.google.com/compute/docs/disks/#pdspecs</p><p>https://cloud.google.com/compute/docs/disks/performance</p>",
                "answers": [
                    "<p>Dynamically resize the SSD persistent disk to 500 GB.</p>",
                    "<p>Increase the virtual machines memory to 64 GB.</p>",
                    "<p>Create a new virtual machine running PostgreSQL.</p>",
                    "<p>Migrate their performance metrics warehouse to BigQuery.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "The database administration team has asked you to help them improve the performance of a new database server running on Compute Engine. The database is used to import and normalize company performance statistics. It is built with MySQL running on Debian Linux. They have an n1-standard-8 VM with 80 GB of SSD zonal persistent disk which they can't restart until the next maintenance event. What should they change to get better performance from this system as soon as possible and in a cost-effective manner?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146130,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Refer to the TerramEarth case study for this question: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdf</p><p><br></p><p>As TerramEarth increases its adoption of the Google Cloud Platform, which specific legacy enterprise processes within the company will undergo substantial modifications?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Capacity planning, TCO calculations, OPEX/CAPEX allocation -&gt; Correct. All of these tasks are big changes when moving to the cloud. Capacity planning for cloud is different than for on-premises data centers; TCO calculations are adjusted because TerramEarth is using services, not leasing/buying servers; OPEX/CAPEX allocation is adjusted as services are consumed vs. using capital expenditures.</p><p><br></p><p>OPEX/CAPEX allocation, LAN change management, capacity planning -&gt; Incorrect. LAN change management processes don't need to change significantly. TerramEarth can easily peer their on-premises LAN with their Google Cloud Platform VPCs, and as devices and subnets move to the cloud, the LAN team's implementation will change, but the change management process doesn't have to.</p><p><br></p><p>Capacity planning, utilization measurement, data center expansion -&gt; Incorrect. Measuring utilization can be done in the same way, often with the same tools (along with some new ones). Data center expansion is not a concern for cloud customers; it is part of the undifferentiated heavy lifting that is taken care of by the cloud provider.</p><p><br></p><p>Data center expansion, TCO calculations, utilization measurement -&gt; Incorrect. Data center expansion is not a concern for cloud customers; it is part of the undifferentiated heavy lifting that is taken care of by the cloud provider. Measuring utilization can be done in the same way, often with the same tools (along with some new ones).</p>",
                "answers": [
                    "<p>Capacity planning, TCO calculations, OPEX/CAPEX allocation</p>",
                    "<p>OPEX/CAPEX allocation, LAN change management, capacity planning</p>",
                    "<p>Capacity planning, utilization measurement, data center expansion</p>",
                    "<p>Data center expansion, TCO calculations, utilization measurement</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Refer to the TerramEarth case study for this question: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdfAs TerramEarth increases its adoption of the Google Cloud Platform, which specific legacy enterprise processes within the company will undergo substantial modifications?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146132,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdf</p><p><br></p><p>Mountkirk Games aims to establish a continuous delivery pipeline for their architecture, which comprises numerous small services requiring quick updates and rollbacks. The following requirements are specified by Mountkirk Games:</p><ul><li><p>redundant deployment of services across multiple regions in the US and Europe.</p></li><li><p>only the frontend services are accessible via the public internet</p></li><li><p>allocation of a single frontend IP address for their entire service fleet</p></li><li><p>deployment artifacts are immutable</p></li></ul><p><br></p><p>Which product combination would be most suitable for their needs?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Container Registry, Google Kubernetes Engine, Cloud Load Balancing -&gt;&nbsp;Correct. Google Kubernetes Engine is ideal for deploying small services that can be updated and rolled back quickly. It is a best practice to manage services using immutable containers. Cloud Load Balancing supports globally distributed services across multiple regions. It provides a single global IP address that can be used in DNS records. Using URL Maps, the requests can be routed to only the services that Mountkirk wants to expose. Container Registry is a single place for a team to manage Docker images for the services.</p><p><br></p><p>Cloud Storage, Cloud Dataflow, Compute Engine -&gt; Incorrect. Mountkirk Games wants to set up a continuous delivery pipeline, not a data processing pipeline. Cloud Dataflow is a fully managed service for creating data processing pipelines.</p><p><br></p><p>Cloud Storage, App Engine, Cloud Load Balancing -&gt; Incorrect. Cloud Load Balancer distributes traffic to Compute Engine instances. App Engine and Cloud Load Balancer are parts of different solutions.</p><p><br></p><p>Cloud Functions, Cloud Pub/Sub, Cloud Deployment Manager -&gt; Incorrect. You cannot reserve a single frontend IP for cloud functions. When deployed, an HTTP-triggered cloud function creates an endpoint with an automatically assigned IP.</p>",
                "answers": [
                    "<p>Container Registry, Google Kubernetes Engine, Cloud Load Balancing</p>",
                    "<p>Cloud Storage, Cloud Dataflow, Compute Engine</p>",
                    "<p>Cloud Storage, App Engine, Cloud Load Balancing</p>",
                    "<p>Cloud Functions, Cloud Pub/Sub, Cloud Deployment Manager</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdfMountkirk Games aims to establish a continuous delivery pipeline for their architecture, which comprises numerous small services requiring quick updates and rollbacks. The following requirements are specified by Mountkirk Games:redundant deployment of services across multiple regions in the US and Europe.only the frontend services are accessible via the public internetallocation of a single frontend IP address for their entire service fleetdeployment artifacts are immutableWhich product combination would be most suitable for their needs?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146134,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>In your role as a cloud architect, you are using Cloud Shell and have the requirement to install a custom utility that will be utilized at a later stage. Where can you place the file to ensure it is in the default execution path and remains persistent across multiple sessions?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p><code>~/bin</code> -&gt; Correct. Cloud Shell is a web-based command-line tool provided by Google Cloud Platform (GCP) for managing resources and interacting with various services. When you work within Cloud Shell, you have a persistent home directory (represented by the tilde \"~\"), which is associated with your user account. By placing the file in the <code>~/bin</code> directory, you ensure that it is within the default execution path. This directory is typically included in the default execution path for user accounts in Linux-based systems. When you install a utility in this directory, it becomes accessible from anywhere within the command-line environment without specifying the full path to the utility. Additionally, the file will remain persistent across multiple sessions because the home directory is associated with your user account and persists between Cloud Shell sessions. This means that the file will be available even if you close and reopen Cloud Shell or switch to a different GCP project.</p><p><br></p><p>In a Cloud Storage bucket -&gt;&nbsp;Incorrect. Cloud Storage buckets are designed for storing objects (files) in a distributed and scalable manner. While you can certainly store files in a Cloud Storage bucket, they won't be in the default execution path within Cloud Shell. You would need to download the file from the bucket and specify its full path to execute it.</p><p><br></p><p><code>/usr/local/bin</code> -&gt; Incorrect. This is a system-level directory typically used for installing programs or utilities that are available to all users on the system. However, in Cloud Shell, you don't have administrative privileges to write directly to this directory. You would need root access to install files in this location, which is not available in Cloud Shell.</p><p><br></p><p><code>/google/scripts/bin</code> -&gt; Incorrect. This directory path does not exist by default in Cloud Shell.</p><p><br></p><p>https://cloud.google.com/shell/docs/how-cloud-shell-works</p>",
                "answers": [
                    "<p><code>~/bin</code> </p>",
                    "<p>In a Cloud Storage bucket</p>",
                    "<p><code>/google/scripts/bin</code> </p>",
                    "<p><code>/usr/local/bin</code> </p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "In your role as a cloud architect, you are using Cloud Shell and have the requirement to install a custom utility that will be utilized at a later stage. Where can you place the file to ensure it is in the default execution path and remains persistent across multiple sessions?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146136,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you're working for a large e-commerce company that wants to detect anomalies in their sales data in real-time. The data is stored in BigQuery and is updated every few minutes with new sales information from around the globe. The goal is to quickly identify unusual spikes or drops in sales, so the business can take immediate actions. What would be the most efficient way to architect this solution?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use BigQuery ML to create a machine learning model and use scheduled queries to periodically perform anomaly detection on the recent data. -&gt; Correct. BigQuery ML enables users to create and execute machine learning models in BigQuery using SQL queries, which is perfect for the given requirement. Scheduled queries can be used to perform anomaly detection on the latest data periodically.</p><p><br></p><p>Use Cloud DLP (Data Loss Prevention) to monitor the sales data in BigQuery for anomalies. -&gt; Incorrect. Cloud DLP (Data Loss Prevention) is designed for discovering, classifying, and protecting sensitive data. It is not meant for anomaly detection in the context of sales data.</p><p><br></p><p>Use Cloud Storage to store the sales data and periodically run batch processing jobs using Dataflow to detect anomalies. -&gt; Incorrect. It introduces delays, complexity, and additional costs associated with batch processing using Dataflow, while not leveraging the real-time analytics capabilities of BigQuery, which is already being used to store the sales data.</p><p><br></p><p>Use Cloud Monitoring to monitor the sales data in BigQuery for anomalies. -&gt; Incorrect. Cloud Monitoring is not the optimal tool for monitoring sales data and performing real-time anomaly detection in this specific use case. BigQuery provides more appropriate functionalities for analyzing the sales data efficiently.</p>",
                "answers": [
                    "<p>Use BigQuery ML to create a machine learning model and use scheduled queries to periodically perform anomaly detection on the recent data.</p>",
                    "<p>Use Cloud DLP (Data Loss Prevention) to monitor the sales data in BigQuery for anomalies.</p>",
                    "<p>Use Cloud Storage to store the sales data and periodically run batch processing jobs using Dataflow to detect anomalies.</p>",
                    "<p>Use Cloud Monitoring to monitor the sales data in BigQuery for anomalies.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you're working for a large e-commerce company that wants to detect anomalies in their sales data in real-time. The data is stored in BigQuery and is updated every few minutes with new sales information from around the globe. The goal is to quickly identify unusual spikes or drops in sales, so the business can take immediate actions. What would be the most efficient way to architect this solution?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146138,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are operating in a high-security environment where Compute Engine VMs are not permitted to access the public internet. Currently, you lack a VPN connection to access a local network file server. You have a necessity to deploy a certain software on a Compute Engine instance. What is the recommended method for installing the software?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Upload the necessary installation files to Cloud Storage. Configure the VM on a subnet with Private Google Access. Ensure the VM is assigned only an internal IP address. Use gsutil to download the installation files to the VM. -&gt; Correct. It provides a suitable solution for installing the software in a high-security environment where Compute Engine VMs are not allowed to access the public internet. In this scenario, uploading the necessary installation files to Cloud Storage allows you to store the files securely in Google Cloud. By configuring the VM on a subnet with Private Google Access, you ensure that the VM can access Google APIs and services like Cloud Storage without requiring access to the public internet. Assigning only an internal IP address to the VM further enforces the restriction on accessing the internet. Using gsutil, which is a command-line tool for interacting with Cloud Storage, you can download the installation files directly to the VM from Cloud Storage. This method allows you to retrieve the necessary files securely and efficiently within the restricted environment.</p><p><br></p><p>Upload the necessary installation files to Cloud Storage, and implement firewall rules to block all traffic except the IP address range specific to Cloud Storage. Utilize gsutil to download the files to the VM. -&gt; Incorrect. While this may provide some level of isolation, it may not be the most effective solution in a high-security environment where VMs are not permitted to access the public internet.</p><p><br></p><p>Upload the necessary installation files to Cloud Source Repositories. Set up the VM on a subnet with Private Google Access. Assign only an internal IP address to the VM. Use gcloud to download the installation files to the VM. -&gt; Incorrect. It suggests uploading the installation files to Cloud Source Repositories and using gcloud to download them to the VM. However, Cloud Source Repositories are primarily used for version control and collaborative development, not for hosting installation files. It is not the recommended approach for software installation.</p><p><br></p><p>Upload the necessary installation files to Cloud Source Repositories, and establish firewall rules to block all traffic except the IP address range for Cloud Source Repositories. Use gsutil to download the files to the VM. -&gt; Incorrect. It suggests blocking all traffic except the IP address range for Cloud Source Repositories and using gsutil to download the files. Using Cloud Source Repositories for software installation is not the appropriate use case, and the solution does not align with the given requirements.</p>",
                "answers": [
                    "<p>Upload the necessary installation files to Cloud Storage. Configure the VM on a subnet with Private Google Access. Ensure the VM is assigned only an internal IP address. Use gsutil to download the installation files to the VM.</p>",
                    "<p>Upload the necessary installation files to Cloud Storage, and implement firewall rules to block all traffic except the IP address range specific to Cloud Storage. Utilize gsutil to download the files to the VM.</p>",
                    "<p>Upload the necessary installation files to Cloud Source Repositories. Set up the VM on a subnet with Private Google Access. Assign only an internal IP address to the VM. Use gcloud to download the installation files to the VM.</p>",
                    "<p>Upload the necessary installation files to Cloud Source Repositories, and establish firewall rules to block all traffic except the IP address range for Cloud Source Repositories. Use gsutil to download the files to the VM.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are operating in a high-security environment where Compute Engine VMs are not permitted to access the public internet. Currently, you lack a VPN connection to access a local network file server. You have a necessity to deploy a certain software on a Compute Engine instance. What is the recommended method for installing the software?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146140,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect and have been tasked with creating a data retention policy on a Google Cloud Storage (GCS) bucket that holds sensitive client information. This policy should ensure that objects older than 90 days are not accessible, even if they haven't been deleted. Also, to ensure business continuity, deleted objects should be restorable within 5 days. Which of the following methods would be the most suitable for implementing this policy?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Apply a 90-day lifecycle rule to delete objects, enable versioning, and use a 5-day lifecycle rule to delete previous versions of objects. -&gt; Correct. A lifecycle rule with a 90-day condition will ensure objects older than 90 days are deleted. Bucket versioning will keep all versions of an object, including those that have been deleted. A second lifecycle rule with a 5-day condition on previous versions will ensure that deleted objects can be restored within 5 days.</p><p><br></p><p>Apply a 90-day lifecycle rule to delete objects and a 5-day retention policy to retain deleted objects. -&gt; Incorrect. A retention policy in Google Cloud Storage prevents an object's deletion, not its access. Additionally, retention policies do not retain deleted objects; they prevent an object's deletion.</p><p><br></p><p>Apply a 90-day lifecycle rule to archive objects and enable versioning with a 5-day lifecycle rule to delete previous versions of objects. -&gt; Incorrect. Lifecycle rules to archive objects in Google Cloud Storage move the objects to a colder storage class like \"Nearline\", \"Coldline\", or \"Archive\" to reduce costs. It does not prevent access to the objects. Also, lifecycle rules do not apply to previous versions of objects.</p><p><br></p><p>Apply a 90-day retention policy to the bucket and a 5-day lifecycle rule to delete older versions of objects. -&gt; Incorrect. A retention policy prevents an object's deletion for a specified period, it doesn't prevent access to it. Also, lifecycle rules on Google Cloud Storage do not apply to older versions of objects.</p>",
                "answers": [
                    "<p>Apply a 90-day lifecycle rule to delete objects, enable versioning, and use a 5-day lifecycle rule to delete previous versions of objects.</p>",
                    "<p>Apply a 90-day lifecycle rule to delete objects and a 5-day retention policy to retain deleted objects.</p>",
                    "<p>Apply a 90-day lifecycle rule to archive objects and enable versioning with a 5-day lifecycle rule to delete previous versions of objects.</p>",
                    "<p>Apply a 90-day retention policy to the bucket and a 5-day lifecycle rule to delete older versions of objects.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect and have been tasked with creating a data retention policy on a Google Cloud Storage (GCS) bucket that holds sensitive client information. This policy should ensure that objects older than 90 days are not accessible, even if they haven't been deleted. Also, to ensure business continuity, deleted objects should be restorable within 5 days. Which of the following methods would be the most suitable for implementing this policy?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146142,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, while utilizing the Google Network Intelligence Center's Firewall Insights feature, you observe that there are no log rows available for viewing when accessing the Firewall Insights page in the Google Cloud console. This prompts the need for assessing the effectiveness of the applied firewall ruleset, considering the existence of multiple firewall rules associated with the Compute Engine instance. To troubleshoot the issue, what steps should you take?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Enable Firewall Rules Logging for the firewall rules you want to monitor. -&gt;&nbsp;Correct. Enabling Firewall Rules Logging allows you to capture logs for the specified firewall rules. By enabling logging for the relevant firewall rules, you can troubleshoot the issue of missing log rows in Firewall Insights. This step ensures that the necessary logs are generated and available for analysis, providing visibility into the effectiveness of the firewall ruleset.</p><p><br></p><p>Enable Virtual Private Cloud (VPC) flow logging. -&gt;&nbsp;Incorrect. While VPC flow logging can be useful for overall network traffic analysis, it may not specifically address the issue of missing log rows in Firewall Insights. Enabling VPC flow logging alone does not guarantee the availability of firewall-specific log data required for assessing the firewall ruleset.</p><p><br></p><p>Verify that your user account is assigned the <code>compute.networkAdmin</code> Identity and Access Management (IAM) role. -&gt;&nbsp;Incorrect. While having the necessary IAM role is important for managing and configuring firewall rules, it does not directly address the issue of missing log rows in Firewall Insights. Verifying the IAM role is more relevant to ensuring the appropriate permissions for managing firewall rules, rather than troubleshooting log availability.</p><p><br></p><p>Install the Google Cloud SDK, and verify that there are no Firewall logs in the command line output. -&gt;&nbsp;Incorrect. Installing the Google Cloud SDK and checking for Firewall logs in the command-line output can help validate if Firewall logs are being generated. However, this step does not directly address the issue of missing log rows in Firewall Insights, nor does it provide a solution for troubleshooting or resolving the issue.</p><p><br></p><p>https://cloud.google.com/network-intelligence-center/docs/firewall-insights/how-to/view-understand-insights</p>",
                "answers": [
                    "<p>Enable Firewall Rules Logging for the firewall rules you want to monitor.</p>",
                    "<p>Enable Virtual Private Cloud (VPC) flow logging.</p>",
                    "<p>Verify that your user account is assigned the <code>compute.networkAdmin</code> Identity and Access Management (IAM) role.</p>",
                    "<p>Install the Google Cloud SDK, and verify that there are no Firewall logs in the command line output.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, while utilizing the Google Network Intelligence Center's Firewall Insights feature, you observe that there are no log rows available for viewing when accessing the Firewall Insights page in the Google Cloud console. This prompts the need for assessing the effectiveness of the applied firewall ruleset, considering the existence of multiple firewall rules associated with the Compute Engine instance. To troubleshoot the issue, what steps should you take?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146144,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are assisting a global organization in migrating its petabyte-scale on-premises data to Google Cloud. The organization has an extremely slow internet connection, strict compliance and security standards, and cannot tolerate any downtime during working hours. The migration should be completed as quickly as possible without interrupting business operations. Which of the following strategies would you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Transfer Appliance to ship data to Google Cloud. -&gt; Correct. This hardware appliance can be loaded with data and physically shipped to Google, where the data can be uploaded to the cloud. This approach is particularly useful in scenarios where an organization has a slow or unreliable internet connection, needs to meet strict compliance and security standards, and requires a quick data migration process.</p><p><br></p><p>Use Cloud Dataflow to process and migrate data. -&gt; Incorrect. Cloud Dataflow is primarily used for processing and transforming large datasets. It is not the most suitable tool for data migration, particularly when internet connectivity is slow.</p><p><br></p><p>Use <code>gsutil</code> command-line tool to upload data to Cloud Storage. -&gt;&nbsp;Incorrect. The <code>gsutil</code> command-line tool could be used to upload data to Cloud Storage, but this method would rely heavily on the organization's slow internet connection. It would not be an efficient solution for migrating petabyte-scale data.</p><p><br></p><p>Use Storage Transfer Service for online transfer. -&gt; Incorrect. Storage Transfer Service is a powerful tool for moving data between online storage services. However, the success of this method is also dependent on the organization's internet connection speed.</p>",
                "answers": [
                    "<p>Use Transfer Appliance to ship data to Google Cloud.</p>",
                    "<p>Use Cloud Dataflow to process and migrate data.</p>",
                    "<p>Use <code>gsutil</code> command-line tool to upload data to Cloud Storage.</p>",
                    "<p>Use Storage Transfer Service for online transfer.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are assisting a global organization in migrating its petabyte-scale on-premises data to Google Cloud. The organization has an extremely slow internet connection, strict compliance and security standards, and cannot tolerate any downtime during working hours. The migration should be completed as quickly as possible without interrupting business operations. Which of the following strategies would you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146146,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect tasked with designing an application to regularly fetch and process social media data every 15 minutes. The processed data is then stored in Cloud Storage and made available to an analytics team. This operation must be reliable and scalable to handle periods of high demand, while minimizing costs. Which of the following is the most suitable approach to handle this scenario?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use App Engine Standard with the Cron service to trigger a Cloud Run service every 15 minutes. The service retrieves and processes the data and then stores it in Cloud Storage. -&gt;&nbsp;Correct. App Engine's Cron service can reliably trigger tasks on a schedule. Cloud Run can handle processing tasks and automatically manages and scales the underlying infrastructure, allowing it to handle high-demand periods. Cloud Run services only run when they're needed, which can help minimize costs.</p><p><br></p><p>Use Compute Engine with a Cron job installed on the instance to fetch, process, and store the data. -&gt; Incorrect. Compute Engine could handle the task, but it might not scale as well during high demand periods. Also, managing Cron jobs directly on Compute Engine instances can add unnecessary complexity.</p><p><br></p><p>Use App Engine Flexible with the Cron service to run a dedicated service for fetching and processing the data every 15 minutes. -&gt; Incorrect. App Engine Flexible could handle the task, but it might not be the most cost-effective solution, as instances of App Engine Flexible are running continuously, even if the service only needs to operate every 15 minutes.</p><p><br></p><p>Use Cloud Scheduler to trigger a Pub/Sub topic every 15 minutes, which then triggers a Dataflow job to fetch, process, and store the data. -&gt; Incorrect. This approach might be overkill for the given task. Cloud Dataflow is more suited to handling complex real-time or batch data processing workloads, which might not be required in this case.</p>",
                "answers": [
                    "<p>Use App Engine Standard with the Cron service to trigger a Cloud Run service every 15 minutes. The service retrieves and processes the data and then stores it in Cloud Storage.</p>",
                    "<p>Use Compute Engine with a Cron job installed on the instance to fetch, process, and store the data.</p>",
                    "<p>Use App Engine Flexible with the Cron service to run a dedicated service for fetching and processing the data every 15 minutes.</p>",
                    "<p>Use Cloud Scheduler to trigger a Pub/Sub topic every 15 minutes, which then triggers a Dataflow job to fetch, process, and store the data.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect tasked with designing an application to regularly fetch and process social media data every 15 minutes. The processed data is then stored in Cloud Storage and made available to an analytics team. This operation must be reliable and scalable to handle periods of high demand, while minimizing costs. Which of the following is the most suitable approach to handle this scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146148,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>In your role as a cloud architect, your task involves deploying PHP App Engine Standard alongside Cloud SQL as the backend, with the goal of reducing the number of queries to the database. What actions should you take?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Set the memcache service level to dedicated. Create a key from the hash of the query, and return database values from memcache before issuing a query to Cloud SQL. -&gt;&nbsp;Correct. By setting the memcache service level to dedicated, you ensure that you have a dedicated cache for your application, which can provide better performance and reliability. Creating a key from the hash of the query allows you to uniquely identify the result of each query. By checking if the result exists in memcache before issuing a query to Cloud SQL, you can reduce the number of queries to the database and retrieve the data directly from the cache if it's available.</p><p><br></p><p>Set the memcache service level to dedicated. Create a cron task that runs every minute to populate the cache with keys containing query results. -&gt;&nbsp;Incorrect. While setting the memcache service level to dedicated is a good step, creating a cron task to populate the cache every minute may not be the most efficient approach. It may lead to outdated or stale data in the cache, as it populates the cache at fixed intervals rather than dynamically based on the actual queries made by the application.</p><p><br></p><p>Set the memcache service level to shared. Create a cron task that runs every minute to save all expected queries to a key called cached_queries. -&gt;&nbsp;Incorrect. Setting the memcache service level to shared may limit the performance and reliability of the cache, as it is shared among multiple applications. Additionally, saving all expected queries to a single key called cached_queries may not be an efficient approach, as it does not provide individual caching for each query. It also does not address the goal of reducing the number of queries to the database.</p><p><br></p><p>Set the memcache service level to shared. Create a key called cached_queries, and return database values from the key before using a query to Cloud SQL. -&gt;&nbsp;Incorrect. Setting the memcache service level to shared may not provide the best performance and reliability. Creating a single key called cached_queries is not an efficient approach for caching individual query results. It does not effectively reduce the number of queries to the database or retrieve specific query results from the cache.</p><p><br></p><p>https://cloud.google.com/appengine/docs/legacy/standard/python/memcache/using</p>",
                "answers": [
                    "<p>Set the memcache service level to dedicated. Create a key from the hash of the query, and return database values from memcache before issuing a query to Cloud SQL.</p>",
                    "<p>Set the memcache service level to dedicated. Create a cron task that runs every minute to populate the cache with keys containing query results.</p>",
                    "<p>Set the memcache service level to shared. Create a cron task that runs every minute to save all expected queries to a key called cached_queries.</p>",
                    "<p>Set the memcache service level to shared. Create a key called cached_queries, and return database values from the key before using a query to Cloud SQL.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "In your role as a cloud architect, your task involves deploying PHP App Engine Standard alongside Cloud SQL as the backend, with the goal of reducing the number of queries to the database. What actions should you take?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146150,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your customer is planning to run a machine learning workload on Google Cloud. The customer has the following requirements:</p><ul><li><p>the workload must be able to scale dynamically to handle large amounts of data</p></li><li><p>the workload must be able to process data in parallel</p></li><li><p>the workload must be able to handle failures and recover gracefully</p></li><li><p>the workload must be cost-effective</p></li></ul><p><br></p><p>Which Google Cloud service should you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud AI Platform -&gt;&nbsp;Correct. Cloud AI Platform is a comprehensive platform for building, training, and deploying machine learning models. It offers scalability and the ability to process data in parallel through distributed training. It also provides fault tolerance and automatic model versioning, which allows graceful recovery from failures. Additionally, Cloud AI Platform offers cost-effective options such as using preemptible virtual machines for training to reduce costs.</p><p><br></p><p>Cloud Machine Learning Engine -&gt;&nbsp;Incorrect. While it provides scalability and fault tolerance, it is not specifically designed to handle large amounts of data or process data in parallel. It is more focused on model training and deployment rather than handling the entire workload.</p><p><br></p><p>Cloud AutoML -&gt;&nbsp;Incorrect. While it may offer some scalability and fault tolerance features, it is primarily focused on simplifying the model-building process rather than handling the entire workload with large amounts of data.</p><p><br></p><p>TensorFlow&nbsp; -&gt;&nbsp;Incorrect. TensorFlow is an open-source machine learning framework developed by Google. It is designed to facilitate the development and deployment of machine learning models, particularly deep learning models.</p><p><br></p><p>https://cloud.google.com/ai-platform/docs</p>",
                "answers": [
                    "<p>Cloud Machine Learning Engine</p>",
                    "<p>Cloud AI Platform</p>",
                    "<p>Cloud AutoML</p>",
                    "<p>TensorFlow</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "Your customer is planning to run a machine learning workload on Google Cloud. The customer has the following requirements:the workload must be able to scale dynamically to handle large amounts of datathe workload must be able to process data in parallelthe workload must be able to handle failures and recover gracefullythe workload must be cost-effectiveWhich Google Cloud service should you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146152,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your customer is planning to store and analyze a large amount of time-series data in Google Cloud. The customer has the following requirements:</p><ul><li><p>the data must be stored in a highly available and scalable storage system</p></li><li><p>the data must be easily accessible for real-time analytics</p></li><li><p>single-digit millisecond latency</p></li></ul><p><br></p><p>Which Google Cloud service should you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Bigtable -&gt;&nbsp;It is a NoSQL database that is designed to handle large-scale workloads and is optimized for storing and querying large amounts of structured data. Cloud Bigtable supports the storage of time-series data and can be easily accessed and analyzed using time-series databases like InfluxDB, as well as tools like Apache Beam and Cloud Dataflow. Additionally, Cloud Bigtable provides a cost-effective pricing model based on usage and can scale to meet the storage and query requirements of time-series data.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. It can also provide highly available and scalable storage for time-series data, but it does not have built-in analytics features for time-series data analysis, which may make it less suitable for the given requirements.</p><p><br></p><p>Cloud SQL -&gt; Incorrect. It is a fully managed relational database service that offers high availability, scalability, and supports SQL-like syntax for querying. However, it may not be the most cost-effective solution for storing and analyzing time-series data, especially for large-scale workloads.</p><p><br></p><p>Cloud Firestore -&gt; Incorrect. It is a fully managed NoSQL document database that can store, sync, and query data for mobile and web applications. It is optimized for real-time updates and serverless access and may not be the most suitable option for storing and analyzing time-series data.</p><p><br></p><p>https://cloud.google.com/bigtable/docs/schema-design-time-series</p>",
                "answers": [
                    "<p>Cloud Storage</p>",
                    "<p>Cloud Bigtable</p>",
                    "<p>Cloud SQL</p>",
                    "<p>Cloud Firestore</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "Your customer is planning to store and analyze a large amount of time-series data in Google Cloud. The customer has the following requirements:the data must be stored in a highly available and scalable storage systemthe data must be easily accessible for real-time analyticssingle-digit millisecond latencyWhich Google Cloud service should you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146154,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your customer is planning to run a big data analytics workload in Google Cloud. The customer has the following requirements:</p><ul><li><p>the workload must be able to handle a large volume of data and scale dynamically</p></li><li><p>the workload must be able to process data in parallel</p></li><li><p>the workload must be able to handle failures and recover gracefully</p></li><li><p>the data must be easily queryable using SQL-like syntax</p></li></ul><p><br></p><p>Which Google Cloud service should you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud BigQuery -&gt;&nbsp;Correct. It is a fully-managed, serverless data warehouse that can handle large volumes of data and scale dynamically as required. It is designed for processing data in parallel, and can handle failures and recover gracefully. It supports querying data using SQL-like syntax, which makes it easy for users to interact with the data.</p><p><br></p><p>Cloud Dataflow -&gt; Incorrect. It is a fully-managed service for building and executing data processing pipelines, which can also handle large volumes of data and scale dynamically. However, it may not be as well-suited for SQL-like queries as Cloud BigQuery.</p><p><br></p><p>Cloud Dataproc -&gt; Incorrect. It is a fully-managed service for running Apache Hadoop and Apache Spark clusters in Google Cloud. While it can also handle large volumes of data and scale dynamically, it may not be as well-suited for SQL-like queries as Cloud BigQuery.</p><p><br></p><p>Cloud SQL -&gt; Incorrect. It is a fully-managed relational database service, which may not be the best fit for a big data analytics workload that requires handling large volumes of data and processing it in parallel.</p><p><br></p><p>https://cloud.google.com/bigquery/docs</p>",
                "answers": [
                    "<p>Cloud Dataflow</p>",
                    "<p>Cloud Dataproc</p>",
                    "<p>Cloud BigQuery</p>",
                    "<p>Cloud SQL</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "Your customer is planning to run a big data analytics workload in Google Cloud. The customer has the following requirements:the workload must be able to handle a large volume of data and scale dynamicallythe workload must be able to process data in parallelthe workload must be able to handle failures and recover gracefullythe data must be easily queryable using SQL-like syntaxWhich Google Cloud service should you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146156,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your customer is planning to store and analyze large amounts of structured and unstructured data in Google Cloud. The customer has the following requirements:</p><ul><li><p>the data must be stored in a highly available and scalable storage system</p></li><li><p>the data must be easily searchable and filterable</p></li><li><p>the data must be easily queryable using SQL-like syntax</p></li><li><p>the data must be easily integratable with other Google Cloud services</p></li></ul><p><br></p><p>Which Google Cloud service should you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Firestore -&gt; Correct. Cloud Firestore is a fully managed NoSQL document database that is designed to store and manage structured and semi-structured data. It provides a highly available and scalable storage system with automatic sharding and load balancing, making it a good choice for storing large amounts of data. Firestore also provides powerful querying capabilities, with support for SQL-like syntax and complex queries on both structured and unstructured data. It also provides a flexible data model, making it easy to store and retrieve data in a variety of formats. In addition, Firestore is easily integratable with other Google Cloud services such as App Engine, Cloud Functions, and BigQuery, making it a good choice for integrating with other services as required by the customer.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. It is primarily designed for object storage, and while it is highly available and scalable, it does not provide the query capabilities required by the customer.</p><p><br></p><p>Cloud Datastore -. Incorrect. It is a NoSQL database, but it is optimized for storing small entities and may not be suitable for storing large amounts of data or complex queries.</p><p><br></p><p>Cloud Bigtable -&gt; Incorrect. It is designed for handling large volumes of structured data with low latency, but it may not be the most suitable for unstructured data or complex queries. It also does not support SQL-like syntax.</p><p><br></p><p>https://firebase.google.com/docs/firestore</p>",
                "answers": [
                    "<p>Cloud Storage</p>",
                    "<p>Cloud Datastore</p>",
                    "<p>Cloud Bigtable</p>",
                    "<p>Cloud Firestore</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "Your customer is planning to store and analyze large amounts of structured and unstructured data in Google Cloud. The customer has the following requirements:the data must be stored in a highly available and scalable storage systemthe data must be easily searchable and filterablethe data must be easily queryable using SQL-like syntaxthe data must be easily integratable with other Google Cloud servicesWhich Google Cloud service should you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146158,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your customer is planning to deploy a web application that requires high performance and low latency. The customer has the following requirements:</p><ul><li><p>the application must be easily deployable and manageable</p></li><li><p>the application must be highly available and recover from failures automatically</p></li><li><p>the application must be able to handle incoming traffic spikes and scale dynamically</p></li><li><p>the application must be cost-effective</p></li></ul><p><br></p><p>Which Google Cloud service should you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>App Engine -&gt; Correct. The customer's requirements suggest that they need a serverless platform that can scale dynamically, be highly available, and cost-effective. App Engine meets all these requirements as it is a fully managed platform that can automatically scale up or down based on incoming traffic and provides high availability and fault tolerance. Additionally, it offers a simple and easy way to deploy and manage web applications. </p><p><br></p><p>Cloud Functions -&gt; Incorrect. Cloud Functions could be an alternative, but it has a cold start issue and may not be suitable for long-running services. </p><p><br></p><p>Google Kubernetes Engine -&gt; Incorrect. Google Kubernetes Engine would require more management and maintenance than App Engine. </p><p><br></p><p>Cloud Load Balancer -&gt; Incorrect. Cloud Load Balancer is not a service that can fulfill all the requirements mentioned.</p><p><br></p><p>https://cloud.google.com/appengine/docs</p>",
                "answers": [
                    "<p>App Engine</p>",
                    "<p>Google Kubernetes Engine</p>",
                    "<p>Cloud Functions</p>",
                    "<p>Cloud Load Balancer</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your customer is planning to deploy a web application that requires high performance and low latency. The customer has the following requirements:the application must be easily deployable and manageablethe application must be highly available and recover from failures automaticallythe application must be able to handle incoming traffic spikes and scale dynamicallythe application must be cost-effectiveWhich Google Cloud service should you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146160,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A financial services company is planning to implement a high-performance computing solution on GCP to support its algorithmic trading operations. The company wants to ensure that the solution is scalable, low-latency, and able to handle a large volume of data. The company also wants to ensure that the solution is secure and that sensitive financial data is protected. Which of the following options would be the most effective approach to meet these requirements?</p>",
                "relatedLectureIds": [],
                "links": [],
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Gogle Cloud Pub/Sub to stream the data and Cloud Dataflow to process the data. Use Cloud Bigtable to store the data and Cloud IAM to control access to the data. Implement security using Cloud KMS. -&gt;&nbsp;Correct. Google Cloud Pub/Sub is designed for scalable, low-latency data streaming, while Cloud Dataflow offers powerful data processing capabilities. Cloud Bigtable is a high-performance NoSQL database that can handle large volumes of data with low latency. Cloud IAM provides access control and authorization mechanisms, ensuring the security of the sensitive financial data. Cloud KMS (Key Management Service) can be used to secure data encryption keys and provide additional security measures.</p><p><br></p><p>Use Cloud Functions to stream the data and Cloud Dataflow to process the data. Use BigQuery to store the data and Cloud IAM to control access to the data. Implement security using Cloud KMS. -&gt; Incorrect. While Cloud Functions and Cloud Dataflow offer streaming and data processing capabilities, BigQuery may not be the most suitable choice for handling large volumes of data with low-latency requirements. </p><p><br></p><p>Use Cloud Pub/Sub to stream the data and Cloud Dataproc to process the data. Use BigQuery to store the data and Cloud IAM to control access to the data. Implement security using Cloud IAP. -&gt; Incorrect. While Cloud Pub/Sub and Cloud Dataproc can handle streaming and data processing, BigQuery may not provide the low-latency requirements needed for algorithmic trading operations. Cloud IAP (Identity-Aware Proxy) provides security measures, but it may not fully address the specific security requirements for protecting sensitive financial data.</p><p><br></p><p>Use Cloud Pub/Sub to stream the data and Cloud Dataproc to process the data. Use Cloud Bigtable to store the data and Cloud IAM to control access to the data. Implement security using Cloud IAP. -&gt; Incorrect. Pub/Sub and Dataproc are suitable, and Bigtable is good for low-latency storage. However, Cloud IAP is not the best option for data security compared to Cloud KMS.</p>",
                "answers": [
                    "<p>Use Cloud Functions to stream the data and Cloud Dataflow to process the data. Use BigQuery to store the data and Cloud IAM to control access to the data. Implement security using Cloud KMS.</p>",
                    "<p>Use Cloud Pub/Sub to stream the data and Cloud Dataproc to process the data. Use BigQuery to store the data and Cloud IAM to control access to the data. Implement security using Cloud IAP.</p>",
                    "<p>Use Gogle Cloud Pub/Sub to stream the data and Cloud Dataflow to process the data. Use Cloud Bigtable to store the data and Cloud IAM to control access to the data. Implement security using Cloud KMS.</p>",
                    "<p>Use Cloud Pub/Sub to stream the data and Cloud Dataproc to process the data. Use Cloud Bigtable to store the data and Cloud IAM to control access to the data. Implement security using Cloud IAP.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "A financial services company is planning to implement a high-performance computing solution on GCP to support its algorithmic trading operations. The company wants to ensure that the solution is scalable, low-latency, and able to handle a large volume of data. The company also wants to ensure that the solution is secure and that sensitive financial data is protected. Which of the following options would be the most effective approach to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146162,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A multinational retail company has stores in multiple countries and wants to leverage GCP to manage its inventory and ordering processes. The company wants to implement a mobile application that provides real-time visibility into inventory levels and enables employees to place orders for out-of-stock items from any location. The solution should also be scalable and be able to handle high volume transactions. Which of the following options would be the most effective approach to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud SQL to store the inventory data and App Engine to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data. -&gt; Correct. Cloud SQL is a fully-managed database service that provides a highly scalable and available relational database engine for storing and managing data. It is a good option for storing inventory data because it supports high transaction rates and can handle large volumes of data. App Engine, on the other hand, is a serverless platform for building and deploying web and mobile applications, which can be used to trigger the ordering process in response to requests from the mobile application. This makes App Engine a good choice for implementing the ordering process because it can handle high volumes of transactions and scale automatically.</p><p><br></p><p>Use Cloud Storage to store the inventory data and Cloud Functions to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data. -&gt; Incorrect. It may not be the best solution for this use case, as Cloud Functions is more suitable for processing small, event-driven tasks, rather than processing large amounts of data in real-time.</p><p><br></p><p>Use Cloud Bigtable to store the inventory data and Cloud Dataflow to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data. -&gt; Incorrect. It may not be the best fit for this scenario. Cloud Bigtable is designed for very large amounts of data that require high throughput and scalability, which might be more than necessary for typical inventory data. Cloud Dataflow is a stream and batch data processing service that's more aligned with data analytics than triggering specific transactional operations like ordering.</p><p><br></p><p>Use Cloud BigQuery to store the inventory data and Cloud Functions to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data. -&gt; Incorrect. It may not be the best solution for this use case, as BigQuery is more suited for querying and analyzing structured data rather than processing large volumes of data in real-time. Additionally, Cloud Functions may not be the best choice for handling high volumes of transactions, which is required for the ordering process in this scenario.</p>",
                "answers": [
                    "<p>Use Cloud Storage to store the inventory data and Cloud Functions to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data.</p>",
                    "<p>Use Cloud SQL to store the inventory data and App Engine to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data.</p>",
                    "<p>Use Cloud Bigtable to store the inventory data and Cloud Dataflow to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data.</p>",
                    "<p>Use Cloud BigQuery to store the inventory data and Cloud Functions to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "A multinational retail company has stores in multiple countries and wants to leverage GCP to manage its inventory and ordering processes. The company wants to implement a mobile application that provides real-time visibility into inventory levels and enables employees to place orders for out-of-stock items from any location. The solution should also be scalable and be able to handle high volume transactions. Which of the following options would be the most effective approach to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146164,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A financial services company is looking to implement a new fraud detection system for their credit card transactions. The system should be able to detect fraud in real-time and provide alerts to security personnel. The company wants to leverage the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use BigQuery to store transaction data and use Cloud Dataflow to process transactions in real-time and trigger fraud alerts.&nbsp; -&gt; Correct. BigQuery is a powerful and scalable data warehousing solution that can handle large volumes of transaction data efficiently. Cloud Dataflow, a serverless data processing service, can be used to process transactions in real-time and trigger fraud alerts based on specific criteria. This combination of BigQuery and Cloud Dataflow provides a robust and scalable solution for real-time fraud detection in the given scenario.</p><p><br></p><p>Use Cloud Datastore to store transaction data and use Cloud Functions to process transactions in real-time and trigger fraud alerts. -&gt;&nbsp;Incorrect. While Cloud Functions can be used for event-driven processing, Cloud Datastore may not be the best choice for storing and querying large volumes of transaction data required for fraud detection. Therefore, this option may not be the most suitable solution.</p><p><br></p><p>Use Cloud SQL to store transaction data and use Cloud Pub/Sub to process transactions in real-time and trigger fraud alerts. -&gt;&nbsp;Incorrect. While Cloud SQL is a fully managed relational database service, it may not provide the scalability and performance required for handling large volumes of transaction data in real-time. Therefore, this option may not be the most suitable solution.</p><p><br></p><p>Use Cloud Firestore to store transaction data and use Cloud Tasks to process transactions in real-time and trigger fraud alerts. -&gt;&nbsp;Incorrect. While Cloud Firestore is a NoSQL document database, it may not be the optimal choice for storing and querying large volumes of transaction data in a structured manner. Additionally, Cloud Tasks is primarily used for asynchronous task execution and may not be the ideal service for real-time fraud detection. Therefore, this option may not be the most suitable solution.</p>",
                "answers": [
                    "<p>Use Cloud Datastore to store transaction data and use Cloud Functions to process transactions in real-time and trigger fraud alerts.</p>",
                    "<p>Use BigQuery to store transaction data and use Cloud Dataflow to process transactions in real-time and trigger fraud alerts. </p>",
                    "<p>Use Cloud SQL to store transaction data and use Cloud Pub/Sub to process transactions in real-time and trigger fraud alerts.</p>",
                    "<p>Use Cloud Firestore to store transaction data and use Cloud Tasks to process transactions in real-time and trigger fraud alerts.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "A financial services company is looking to implement a new fraud detection system for their credit card transactions. The system should be able to detect fraud in real-time and provide alerts to security personnel. The company wants to leverage the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146166,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A multinational retail company has a large number of physical stores across multiple countries. The company wants to implement a centralized system to monitor the energy consumption of each store in real-time and generate reports to help optimize energy usage. The company wants to use the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud IoT Core to connect the energy meters in each store to the cloud, BigQuery to store energy consumption data, and Cloud Dataflow to process the data and generate reports. -&gt; Correct. Cloud IoT Core is a fully managed service for securely connecting and managing IoT devices, making it a suitable choice for connecting the energy meters in each store. BigQuery is a highly scalable and cost-effective data warehouse that can handle large volumes of data, making it a good option for storing energy consumption data. Cloud Dataflow is a fully managed data processing service that can handle batch and stream processing, making it suitable for processing real-time energy consumption data and generating reports.</p><p><br></p><p>Use Cloud Pub/Sub to connect the energy meters in each store to the cloud, Cloud Datastore to store energy consumption data, and Cloud Functions to process the data and generate reports. -&gt;&nbsp;Incorrect. Cloud Datastore is a NoSQL database that may not be the optimal choice for storing large amounts of time-series data. Cloud Functions can be used for processing data, but it might not be the most efficient solution for real-time data processing and generating reports.</p><p><br></p><p>Use Cloud IoT Edge to connect the energy meters in each store to the cloud, Cloud SQL to store energy consumption data, and Cloud Pub/Sub to process the data and generate reports. -&gt;&nbsp;Incorrect.&nbsp; Cloud SQL is a relational database service and may not be the most suitable option for storing time-series data. Cloud Pub/Sub can handle data streaming, but it may not be the optimal choice for real-time data processing and report generation.</p><p><br></p><p>Use Cloud IoT Edge to connect the energy meters in each store to the cloud, Cloud Firestore to store energy consumption data, and Cloud Tasks to process the data and generate reports. -&gt;&nbsp;Incorrect. Cloud Firestore is a NoSQL document database and may not be the best option for storing time-series data. Cloud Tasks is a task queue service and may not be the optimal solution for real-time data processing and generating reports.</p>",
                "answers": [
                    "<p>Use Cloud Pub/Sub to connect the energy meters in each store to the cloud, Cloud Datastore to store energy consumption data, and Cloud Functions to process the data and generate reports.</p>",
                    "<p>Use Cloud IoT Core to connect the energy meters in each store to the cloud, BigQuery to store energy consumption data, and Cloud Dataflow to process the data and generate reports.</p>",
                    "<p>Use Cloud IoT Edge to connect the energy meters in each store to the cloud, Cloud SQL to store energy consumption data, and Cloud Pub/Sub to process the data and generate reports.</p>",
                    "<p>Use Cloud IoT Edge to connect the energy meters in each store to the cloud, Cloud Firestore to store energy consumption data, and Cloud Tasks to process the data and generate reports.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "A multinational retail company has a large number of physical stores across multiple countries. The company wants to implement a centralized system to monitor the energy consumption of each store in real-time and generate reports to help optimize energy usage. The company wants to use the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146168,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A large media company wants to implement a system for processing user-generated content, such as images and videos, and automatically categorize them based on visual content. The system should be able to process large amounts of data in real-time and scale as needed. The company wants to use the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Storage to store the user-generated content, Cloud Dataproc to process the data, and Vertex AI to categorize the content. -&gt;&nbsp;Correct. The large media company wants to implement a system that processes user-generated content, such as images and videos, and automatically categorizes them based on visual content. The system should be able to process large amounts of data in real-time and scale as needed.</p><p>It is the best solution because it uses a combination of services to achieve the requirements of the media company.</p><ul><li><p>Cloud Storage provides a scalable and cost-effective way to store the user-generated content.</p></li><li><p>Cloud Dataproc is a fully-managed big data processing service that allows for the processing of large amounts of data in a cost-effective manner.</p></li><li><p>Vertex AI provides a set of pre-trained models and tools for machine learning, which can be used to categorize the content based on visual content.</p></li></ul><p><br></p><p>Use Cloud Storage to store the user-generated content, Cloud Functions to process the data and categorize the content. -&gt; Incorrect. It is not the best solution because Cloud Functions are not the best choice for processing large amounts of data in real-time. Cloud Functions are best suited for event-driven applications that require quick, short-lived computations.</p><p><br></p><p>Use Cloud Storage to store the user-generated content, Cloud Dataflow to process the data and categorize the content. -&gt; Incorrect. Cloud Dataflow is a fully managed service for stream and batch processing, but it lacks the machine learning capabilities to categorize content based on visual data, which is the key requirement in this scenario. Hence, this solution is incomplete.</p><p><br></p><p>Use Cloud Pub/Sub to ingest the user-generated content, BigQuery to store the data, and Vertex AI to categorize the content. -&gt; Incorrect. It is not the best solution because Pub/Sub and BigQuery are not designed for image and video processing, and Vertex AI provides a better solution for categorizing the content based on visual content.</p>",
                "answers": [
                    "<p>Use Cloud Storage to store the user-generated content, Cloud Functions to process the data and categorize the content.</p>",
                    "<p>Use Cloud Storage to store the user-generated content, Cloud Dataflow to process the data and categorize the content.</p>",
                    "<p>Use Cloud Storage to store the user-generated content, Cloud Dataproc to process the data, and Vertex AI to categorize the content.</p>",
                    "<p>Use Cloud Pub/Sub to ingest the user-generated content, BigQuery to store the data, and Vertex AI to categorize the content.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "A large media company wants to implement a system for processing user-generated content, such as images and videos, and automatically categorize them based on visual content. The system should be able to process large amounts of data in real-time and scale as needed. The company wants to use the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146170,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A large financial services company needs to build a secure and scalable platform for processing financial transactions. The platform must meet the following requirements:</p><ul><li><p>provide fast and secure transaction processing</p></li><li><p>ensure data privacy and security</p></li><li><p>enable real-time reporting and auditing</p></li><li><p>minimize downtime during maintenance and upgrades</p></li><li><p>minimize costs while still providing high performance</p></li></ul><p><br></p><p>Which solution would you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implementing a managed solution using Cloud Spanner for data storage and processing, Cloud Dataflow for real-time analytics, and Cloud Key Management Service (KMS) for secure payment processing. -&gt; Correct. Cloud Spanner is a globally distributed and strongly consistent relational database that can provide fast and secure transaction processing while ensuring data privacy and security. Cloud Dataflow is a fully managed service for real-time analytics, allowing real-time reporting and auditing capabilities. Cloud KMS enables secure payment processing by providing key management services. This option aligns well with all the given requirements, including scalability, security, real-time analytics, and cost-effectiveness.</p><p><br></p><p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling. -&gt; Incorrect. Using a custom-built solution introduces additional complexity and potential security risks. Bigtable may not be the most suitable choice for structured financial transaction data storage, as it is a wide-column NoSQL database. This option does not explicitly address data privacy and security requirements.</p><p><br></p><p>Implementing a serverless solution using Cloud Functions for transaction processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing. -&gt; Incorrect. While serverless solutions can provide scalability and ease of deployment, Cloud Functions may not be the most efficient choice for transaction processing in terms of performance and cost. Cloud Firestore, a NoSQL document database, may not be the optimal option for structured financial transaction data storage. This option does not explicitly address the need for real-time reporting and auditing.</p><p><br></p><p>Implementing a hybrid solution using Cloud SQL for data storage, Compute Engine for transaction processing, and BigQuery for real-time analytics. -&gt; Incorrect. While Cloud SQL can handle structured data storage and Compute Engine allows for transaction processing, this hybrid solution may not provide the same level of scalability, reliability, and cost-effectiveness as the managed solutions. BigQuery is a powerful analytics platform but may not be the most efficient choice for real-time analytics. This option does not explicitly address the need for secure payment processing.</p>",
                "answers": [
                    "<p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling.</p>",
                    "<p>Implementing a serverless solution using Cloud Functions for transaction processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing.</p>",
                    "<p>Implementing a managed solution using Cloud Spanner for data storage and processing, Cloud Dataflow for real-time analytics, and Cloud Key Management Service (KMS) for secure payment processing.</p>",
                    "<p>Implementing a hybrid solution using Cloud SQL for data storage, Compute Engine for transaction processing, and BigQuery for real-time analytics.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "A large financial services company needs to build a secure and scalable platform for processing financial transactions. The platform must meet the following requirements:provide fast and secure transaction processingensure data privacy and securityenable real-time reporting and auditingminimize downtime during maintenance and upgradesminimize costs while still providing high performanceWhich solution would you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146172,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A large healthcare organization needs to build a secure and scalable platform for storing and processing sensitive patient data. The platform must meet the following requirements:</p><ul><li><p>ensure secure storage and processing of sensitive patient data</p></li><li><p>enable fast and efficient data retrieval for medical professionals</p></li><li><p>support real-time data analysis for clinical decision-making</p></li><li><p>enable data sharing with authorized partners while ensuring data privacy and security</p></li><li><p>minimize costs while still providing high performance</p></li></ul><p><br></p><p>Which solution would you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implementing a managed solution using Cloud Healthcare API for data storage and processing, Cloud Dataflow for real-time analytics, and Cloud Identity Access Management (IAM) for secure data sharing. -&gt;&nbsp;Correct. The Cloud Healthcare API is designed specifically for healthcare data storage and processing, providing secure and compliant handling of sensitive patient data. Cloud Dataflow enables real-time analytics and data processing, allowing for efficient clinical decision-making. Cloud IAM ensures secure data sharing with authorized partners while maintaining data privacy and security. This managed solution offers a comprehensive set of services that meet the requirements of the healthcare organization.</p><p><br></p><p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling. -&gt; Incorrect. While these services are capable of handling aspects of the requirements, managing a custom-built solution can introduce additional complexity, maintenance overhead, and potential security risks. It may not be the most efficient and cost-effective approach.</p><p><br></p><p>Implementing a serverless solution using Cloud Functions for data processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing. -&gt; Incorrect. While serverless services offer scalability and ease of use, Cloud Firestore may not be the most suitable choice for secure and efficient storage of sensitive patient data. Additionally, a comprehensive solution that addresses all requirements may require additional services and configurations.</p><p><br></p><p>Implementing a hybrid solution using Cloud SQL for data storage, Compute Engine for data processing, and BigQuery for real-time analytics. -&gt; Incorrect. While each service individually has its strengths, this combination may not provide the most efficient and integrated solution for healthcare data storage, processing, and real-time analytics. It may introduce complexities and potential limitations in meeting all the specified requirements.</p><p><br></p><p>https://cloud.google.com/healthcare-api</p>",
                "answers": [
                    "<p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling.</p>",
                    "<p>Implementing a serverless solution using Cloud Functions for data processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing.</p>",
                    "<p>Implementing a managed solution using Cloud Healthcare API for data storage and processing, Cloud Dataflow for real-time analytics, and Cloud Identity Access Management (IAM) for secure data sharing.</p>",
                    "<p>Implementing a hybrid solution using Cloud SQL for data storage, Compute Engine for data processing, and BigQuery for real-time analytics.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "A large healthcare organization needs to build a secure and scalable platform for storing and processing sensitive patient data. The platform must meet the following requirements:ensure secure storage and processing of sensitive patient dataenable fast and efficient data retrieval for medical professionalssupport real-time data analysis for clinical decision-makingenable data sharing with authorized partners while ensuring data privacy and securityminimize costs while still providing high performanceWhich solution would you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146174,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A large logistics company is looking to build a secure and scalable platform for tracking shipments and managing delivery schedules. The platform must meet the following requirements:</p><ul><li><p>support real-time tracking of shipments with high accuracy</p></li><li><p>ensure secure storage and processing of sensitive shipment information</p></li><li><p>enable efficient coordination of delivery schedules across multiple locations</p></li><li><p>minimize downtime during maintenance and upgrades</p></li><li><p>minimize costs while still providing high performance</p></li></ul><p><br></p><p>Which solution would you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implementing a managed solution using Google Maps Platform for real-time tracking, Cloud Datastore for data storage, and Cloud Identity and Access Management (IAM) for secure access control. -&gt; Correct. Google Maps Platform provides real-time location tracking capabilities that can be used to track shipments. Cloud Datastore is a highly scalable, managed NoSQL database that can be used to store sensitive shipment information securely. And, Cloud Identity and Access Management (IAM) can be used to control access to the platform, ensuring that sensitive information is only accessible to authorized individuals. This solution would meet all the requirements while still providing high performance and minimizing costs.</p><p><br></p><p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling. -&gt; Incorrect. Using a custom-built solution introduces additional complexity and potential security risks. Bigtable may not be the most suitable choice for structured data storage and retrieval of sensitive shipment information. This option does not explicitly address the requirement for real-time tracking or secure access control.</p><p><br></p><p>Implementing a serverless solution using Cloud Functions for data processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing. -&gt;&nbsp;Incorrect. While serverless solutions can provide scalability and ease of deployment, Cloud Functions may not be the most efficient choice for real-time data processing and tracking of shipments. Cloud Firestore, a NoSQL document database, may not be the optimal option for structured data storage and secure processing of sensitive shipment information. This option does not explicitly address the requirement for efficient coordination of delivery schedules across multiple locations.</p><p><br></p><p>Implementing a hybrid solution using Cloud SQL for data storage, Compute Engine for data processing, and BigQuery for real-time analytics. -&gt;&nbsp;Incorrect. While Cloud SQL can handle structured data storage and Compute Engine allows for data processing, this hybrid solution may not provide the same level of scalability, reliability, and cost-effectiveness as the managed solutions. BigQuery is a powerful analytics platform but may not be the most efficient choice for real-time analytics or real-time tracking of shipments. This option does not explicitly address the requirement for secure access control.</p>",
                "answers": [
                    "<p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling.</p>",
                    "<p>Implementing a serverless solution using Cloud Functions for data processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing.</p>",
                    "<p>Implementing a managed solution using Google Maps Platform for real-time tracking, Cloud Datastore for data storage, and Cloud Identity and Access Management (IAM) for secure access control.</p>",
                    "<p>Implementing a hybrid solution using Cloud SQL for data storage, Compute Engine for data processing, and BigQuery for real-time analytics.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "A large logistics company is looking to build a secure and scalable platform for tracking shipments and managing delivery schedules. The platform must meet the following requirements:support real-time tracking of shipments with high accuracyensure secure storage and processing of sensitive shipment informationenable efficient coordination of delivery schedules across multiple locationsminimize downtime during maintenance and upgradesminimize costs while still providing high performanceWhich solution would you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146176,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are designing a cloud-based architecture for a company that requires a secure and scalable solution for its database. The database must be able to handle high volumes of transactions, and the data must be encrypted at rest and in transit. Which of the following options provides the best solution?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use a managed database service with automatic encryption and scale-up capability. -&gt; Correct. Using a managed database service with automatic encryption and scale-up capability provides the best solution for the company's requirements. Managed database services such as Amazon RDS, Google Cloud SQL, and Azure Database offer automatic encryption of data at rest and in transit, and they provide built-in scalability features that can automatically scale up or down the database depending on the workload. This solution also reduces the operational burden of managing a database and ensures that the database is always up to date with the latest security patches.</p><p><br></p><p>Use a self-managed database service with manual encryption and scale-out capability. -&gt; Incorrect. Using a self-managed database service with manual encryption and scale-out capability may provide scalability, but it requires manual setup and management of encryption, which can be error-prone and time-consuming.</p><p><br></p><p>Use a NoSQL database service with built-in encryption and automatic scaling. -&gt; Incorrect. Using a NoSQL database service with built-in encryption and automatic scaling may provide automatic encryption and scalability, but it may not be suitable for a company that requires a traditional SQL database.</p><p><br></p><p>Use a hybrid approach with a self-managed database service and a third-party encryption tool. -&gt; Incorrect. Using a hybrid approach with a self-managed database service and a third-party encryption tool may provide encryption, but it adds complexity to the architecture and requires more manual management.</p>",
                "answers": [
                    "<p>Use a managed database service with automatic encryption and scale-up capability.</p>",
                    "<p>Use a self-managed database service with manual encryption and scale-out capability.</p>",
                    "<p>Use a NoSQL database service with built-in encryption and automatic scaling.</p>",
                    "<p>Use a hybrid approach with a self-managed database service and a third-party encryption tool.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are designing a cloud-based architecture for a company that requires a secure and scalable solution for its database. The database must be able to handle high volumes of transactions, and the data must be encrypted at rest and in transit. Which of the following options provides the best solution?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146178,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with a media company to develop a web application for live streaming events. The application needs to handle high incoming traffic during popular events. It's also important for the company to keep costs as low as possible when there are no live events (low traffic). Which environment of App Engine should you choose for this scenario?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>App Engine Standard environment, because it can scale to zero instances when there's no traffic. -&gt;&nbsp;Correct. It automatically scales instances up and down, even down to zero when there's no traffic. This feature will help the company to keep costs low when there are no live events.</p><p><br></p><p>App Engine Flexible environment, because it can scale to zero instances when there's no traffic. -&gt; Incorrect. The App Engine Flexible environment doesn't scale down to zero instances. This means it will continue to accrue costs even when there's no traffic, which is not suitable for the scenario.</p><p><br></p><p>Both environments would suit this application equally well. -&gt; Incorrect. Both environments do not suit this application equally well due to the specific scaling and cost requirements of the scenario, making this option incorrect.</p><p><br></p><p>App Engine Flexible environment, because it supports third-party software. -&gt; Incorrect. Although the App Engine Flexible environment supports third-party software, it doesn't scale down to zero instances, which is a crucial requirement for the scenario.</p>",
                "answers": [
                    "<p>App Engine Standard environment, because it can scale to zero instances when there's no traffic.</p>",
                    "<p>App Engine Flexible environment, because it can scale to zero instances when there's no traffic.</p>",
                    "<p>Both environments would suit this application equally well.</p>",
                    "<p>App Engine Flexible environment, because it supports third-party software.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with a media company to develop a web application for live streaming events. The application needs to handle high incoming traffic during popular events. It's also important for the company to keep costs as low as possible when there are no live events (low traffic). Which environment of App Engine should you choose for this scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 82146180,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization uses a BigQuery data warehouse for analytics. You expect the volume of data and the complexity of queries to increase significantly over the next year. What could be a strategy to ensure query performance remains high?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use the BigQuery Reservation model to purchase dedicated query processing capacity. -&gt;&nbsp;Correct. The BigQuery Reservation model allows you to purchase dedicated query processing capacity (slots). This can ensure consistent performance even as query load increases.</p><p><br></p><p>Increase the amount of storage available to BigQuery. -&gt;&nbsp;Incorrect. BigQuery is serverless, so increasing storage doesn't directly affect query performance. BigQuery automatically manages resources like storage.</p><p><br></p><p>Migrate data from BigQuery to Cloud Spanner for increased performance. -&gt;&nbsp;Incorrect. Cloud Spanner is a relational database service designed for high transaction rates, and might not be as suitable for analytics as BigQuery.</p><p><br></p><p>Split your data into multiple BigQuery datasets to balance the load. -&gt;&nbsp;Incorrect. BigQuery's performance doesn't depend on the number of datasets. Therefore, splitting data into multiple datasets wouldn't necessarily improve query performance.</p>",
                "answers": [
                    "<p>Use the BigQuery Reservation model to purchase dedicated query processing capacity.</p>",
                    "<p>Increase the amount of storage available to BigQuery.</p>",
                    "<p>Migrate data from BigQuery to Cloud Spanner for increased performance.</p>",
                    "<p>Split your data into multiple BigQuery datasets to balance the load.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization uses a BigQuery data warehouse for analytics. You expect the volume of data and the complexity of queries to increase significantly over the next year. What could be a strategy to ensure query performance remains high?",
            "related_lectures": []
        }
    ]
}