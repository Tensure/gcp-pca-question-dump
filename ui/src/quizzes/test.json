[
  {
    "id": 82163914,
    "question_plain": "You want to have durable storage in a Kubernetes cluster. Pods are ephemeral, so they may be deleted or recreated. As a cloud architect, you want to decouple pods from persistent storage. What Kubernetes mechanism should you use?",
    "answers": [
      "<p>PersistentVolumes</p>",
      "<p>Deployments</p>",
      "<p>ReplicaSets</p>",
      "<p>DeamonSet</p>"
    ],
    "explanation": "<p>PersistentVolumes -&gt;&nbsp;Correct. PersistentVolumes in Kubernetes provide a way to decouple pods from persistent storage. They are an abstraction that allows pods to claim and use durable storage resources in a decoupled manner. PersistentVolumes can be created and managed separately from pods and can survive pod restarts or deletions. Pods can then request access to PersistentVolumes through PersistentVolumeClaims, ensuring durable storage is available even when pods are recreated or deleted.</p><p><br></p><p>Deployments -&gt;&nbsp;Incorrect. Deployments in Kubernetes are primarily used for managing and updating applications by controlling the creation and scaling of ReplicaSets. </p><p><br></p><p>ReplicaSets -&gt;&nbsp;Incorrect. ReplicaSets are used in Kubernetes to ensure a specified number of pod replicas are running at all times. They help maintain the desired pod count and handle scaling.</p><p><br></p><p>DeamonSet -&gt;&nbsp;Incorrect. DaemonSets are used to ensure that a specific pod runs on every node in a Kubernetes cluster. They are typically used for running background or system-level processes on every node.</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/concepts/persistent-volumes#persistentvolumes</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163916,
    "question_plain": "An e-commerce company runs all workload in the on-premises data center. In case the on-premises data center is not available, they want to use Google Cloud as a disaster recovery infrastructure. As a cloud architect, what network topology should you use in this case?",
    "answers": [
      "<p>Mirrored topology</p>",
      "<p>Meshed topology</p>",
      "<p>Handover topology</p>",
      "<p>Gated ingress and egress topology</p>"
    ],
    "explanation": "<p>Mirrored topology -&gt; Correct. A mirrored topology is a network topology that replicates the on-premises network in the cloud. This approach enables you to create an exact replica of your on-premises environment in the cloud, including virtual machines, storage, and network configurations. By doing so, the mirrored topology enables your workloads to failover to the cloud seamlessly in the event of a disaster, without any interruption in service.</p><p><br></p><p>Meshed topology -&gt; Incorrect. A meshed topology is a network topology that connects all devices in a network to one another. It is not specifically designed for disaster recovery.</p><p><br></p><p>Handover topology -&gt; Incorrect. A handover topology is a network topology that enables a device or service to switch from one network to another. It is not specifically designed for disaster recovery.</p><p><br></p><p>Gated ingress and egress topology -&gt; Incorrect. The idea of the mirrored topology is to have the cloud computing environment and private computing environment mirror each other. This is the correct answer.</p><p><br></p><p>https://cloud.google.com/architecture/hybrid-and-multi-cloud-network-topologies#mirrored</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163862,
    "question_plain": "As a cloud architect, you are managing a project that consists of a single Virtual Private Cloud (VPC) and a single subnetwork located in the us-west1 region. Within this subnetwork, there is a Compute Engine instance hosting an application. Now, your development team intends to deploy a new instance within the same project, but in the europe-central2 region. They require access to the application and wish to adhere to Google's best practices. As a cloud architect, what guidance should you provide in this situation?",
    "answers": [
      "<p>They should create a subnetwork in the same VPC, in <code>europe-central2</code> region. Than, create a new instance in the new subnetwork and use the first instance's private address as the endpoint.</p>",
      "<p>They should create a VPC and a subnetwork in <code>europe-central2</code> region. Than, expose the application with an internal load balancer, and finally create a new instance in the new subnetwork and use the load balancer's address as the endpoint.</p>",
      "<p>They should create a subnetwork in the same VPC, in <code>europe-central2</code> region. Than, use Cloud VPN to connect these two subnetworks, and finally create a new instance in the new subnetwork and use the first instance's private address as the endpoint.</p>",
      "<p>They should create a VPC and a subnetwork in <code>europe-central2</code> region. Than, peer the 2 VPCs, and finally create a new instance in the new subnetwork and use the first instance's private address as the endpoint.</p>"
    ],
    "explanation": "<p>They should create a subnetwork in the same VPC, in <code>europe-central2</code> region. Than, create a new instance in the new subnetwork and use the first instance's private address as the endpoint. -&gt;&nbsp;Correct. By creating a subnetwork in the same VPC but in a different region, the development team can deploy the new instance in <code>europe-central2</code> region while still maintaining connectivity to the existing application hosted in the us-west1 region. They can use the first instance's private address as the endpoint to access the application, leveraging the VPC network connectivity.</p><p><br></p><p>They should create a VPC and a subnetwork in <code>europe-central2</code> region. Than, expose the application with an internal load balancer, and finally create a new instance in the new subnetwork and use the load balancer's address as the endpoint. -&gt;&nbsp;Incorrect. It suggests creating a new VPC and a subnetwork in the <code>europe-central2</code> region, along with an internal load balancer. While load balancers can distribute traffic, in this case, it is not necessary as there is only one instance hosting the application.</p><p><br></p><p>They should create a subnetwork in the same VPC, in <code>europe-central2</code> region. Than, use Cloud VPN to connect these two subnetworks, and finally create a new instance in the new subnetwork and use the first instance's private address as the endpoint. -&gt;&nbsp;Incorrect. Cloud VPN is typically used to establish secure connections between on-premises networks and VPC networks, rather than connecting subnetworks within the same VPC.</p><p><br></p><p>They should create a VPC and a subnetwork in <code>europe-central2</code> region. Than, peer the 2 VPCs, and finally create a new instance in the new subnetwork and use the first instance's private address as the endpoint. -&gt;&nbsp;Incorrect. VPC peering is used to establish connectivity between different VPC networks, but in this case, there is a requirement to deploy the new instance within the same project and VPC.</p><p><br></p><p>https://cloud.google.com/vpc/docs/vpc#vpc_networks_and_subnets</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163864,
    "question_plain": "Your team has created an updated version of an application that is currently hosted on the App Engine Standard environment. You aim to migrate 2% of your users to the new version while minimizing complexity. What course of action would you recommend?",
    "answers": [
      "<p>You should deploy a new version in the same application and split the traffic (98% to 2%).</p>",
      "<p>You should deploy a new version in the same application and use the <code>--migrate</code> option.</p>",
      "<p>You should create a new App Engine application in the same project. Deploy a new version in that application. Use the App Engine to split the traffic.</p>",
      "<p>You should create a new App Engine application in the same project. Deploy a new version in that application. Configure your network load balancer to send 2% of the traffic to that new application.</p>"
    ],
    "explanation": "<p>You should deploy a new version in the same application and split the traffic (98% to 2%). -&gt; Correct. It suggests deploying the new version in the same application and splitting the traffic between the old and new versions. This approach is the simplest and requires minimal changes to the current deployment. The traffic splitting can be done using the App Engine's built-in traffic splitting feature, which allows directing a percentage of traffic to a specific version. This way, 2% of the traffic will be sent to the new version, and the remaining 98% will continue to use the old version until further changes are made.</p><p><br></p><p>You should deploy a new version in the same application and use the <code>--migrate</code> option. -&gt; Incorrect. The <code>--migrate</code> option is used for migrating traffic from one version to another within the same application, and it doesn't split the traffic.</p><p><br></p><p>You should create a new App Engine application in the same project. Deploy a new version in that application. Use the App Engine to split the traffic. -&gt; Incorrect. It suggest creating a new application which introduces unnecessary complexity and requires additional configuration and management effort.</p><p><br></p><p>You should create a new App Engine application in the same project. Deploy a new version in that application. Configure your network load balancer to send 2% of the traffic to that new application. -&gt; Incorrect. It suggest creating a new application or configuring a network load balancer, which introduces unnecessary complexity and requires additional configuration and management effort.</p><p><br></p><p>https://cloud.google.com/appengine/docs/standard/python/splitting-traffic</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163866,
    "question_plain": "A&nbsp;data science team wants to analyze very large data sets and prepares them for a machine learning model. As a cloud architect, which service should you recommend to use for interactive queries and online analytics?",
    "answers": [
      "<p>BigQuery</p>",
      "<p>Cloud Datastore</p>",
      "<p>Cloud Bigtable</p>",
      "<p>Cloud Spanner</p>"
    ],
    "explanation": "<p>BigQuery -&gt; Correct.&nbsp; BigQuery is a fully managed, highly scalable, and cost-effective cloud data warehouse that enables interactive analysis of large datasets using SQL-like queries. It is ideal for analyzing large datasets and preparing them for machine learning models. BigQuery allows you to store and query data using a pay-as-you-go model, and it supports real-time analysis with streaming data.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. It is a NoSQL document database that is optimized for storing and querying large amounts of semi-structured data. It is not specifically designed for interactive queries and online analytics.</p><p><br></p><p>Cloud Bigtable -&gt; Incorrect. It is a NoSQL wide-column database that is optimized for scalability and high-performance read/write operations. It is designed for use cases that require low latency and high throughput, such as AdTech and IoT, but it may not be the best option for interactive queries and online analytics.</p><p><br></p><p>Cloud Spanner -&gt; Incorrect. It is a globally distributed, horizontally scalable, and strongly consistent relational database. It is ideal for mission-critical, transaction-intensive applications that require high availability and scalability. However, it may not be the best option for interactive queries and online analytics.</p><p><br></p><p>https://cloud.google.com/bigquery/docs</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163868,
    "question_plain": "You have completed the deployment of a brand new regional Kubernetes Engine cluster, where the default pool in the first zone consists of four machines. Additionally, you have maintained the default number of zones during the deployment. How many Compute Engine instances have been deployed and are being billed to your account?",
    "answers": ["<p>12</p>", "<p>4</p>", "<p>8</p>", "<p>16</p>"],
    "explanation": "<p>12 -&gt;&nbsp;Correct. Four nodes are deployed in each of three zones. A control plane node is deployed in each zone but it is not billed against your account.</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/quickstart</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163870,
    "question_plain": "You are taking charge of the migration process for a legacy application, moving it from an on-premises data center to the Google Cloud Platform. This application is responsible for handling SSL encrypted traffic from clients across the globe on TCP port 443. Which GCP load balancing service should you employ to minimize latency for all clients?",
    "answers": [
      "<p>SSL Proxy Load Balancer</p>",
      "<p>Network TCP/UDP Load Balancer</p>",
      "<p>Internal TCP/UDP Load Balancer</p>",
      "<p>External HTTP(S) Load Balancer</p>"
    ],
    "explanation": "<p>SSL Proxy Load Balancer -&gt; Correct. The SSL Proxy Load Balancer is specifically designed to handle SSL-encrypted traffic. It terminates SSL connections at the load balancer and then forwards the decrypted traffic to the backend instances. By terminating SSL at the load balancer, it reduces the latency associated with SSL handshake and encryption for the backend instances, resulting in minimized latency for all clients.</p><p><br></p><p>Network TCP/UDP Load Balancer -&gt;&nbsp;Incorrect. The Network TCP/UDP Load Balancer operates at the transport layer (Layer 4) and does not provide SSL termination or decryption capabilities. It is designed for load balancing TCP or UDP traffic at the network level.</p><p><br></p><p>Internal TCP/UDP Load Balancer -&gt;&nbsp;Incorrect. The Internal TCP/UDP Load Balancer is used for internal load balancing within a VPC network and does not provide SSL termination capabilities for handling traffic from external clients.</p><p><br></p><p>External HTTP(S) Load Balancer -&gt;&nbsp;Incorrect. The External HTTP(S) Load Balancer is specifically designed for HTTP(S) traffic and operates at the application layer (Layer 7).</p><p><br></p><p>https://cloud.google.com/load-balancing/docs/ssl</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163872,
    "question_plain": "The game's backend APIs run on a fleet of virtual machines behind a Managed Instance Group with autoscaling enabled. The scaling policy on this group adds more instances if the CPU utilization is consistently over 80%, and to scale down when the CPU utilization is consistently lower than 60%. You've noticed that autoscaling adds more virtual machines than necessary when scaling up, and you suspect that this may be due to a misconfiguration in health checks - the initial health check latency is set to 30 seconds. Each virtual machine takes less than 3 minutes to be ready to process requests. What can you do to fix this issue?",
    "answers": [
      "<p>You can update the autoscaling health check to increase the initial delay to 200 seconds.</p>",
      "<p>You can update the autoscaling health check from HTTP to TCP.</p>",
      "<p>You can update the managed instances template to set the maximum instances to 3.</p>",
      "<p>You can update the managed instances template to set the minimum instances to 3.</p>"
    ],
    "explanation": "<p>You can update the autoscaling health check to increase the initial delay to 200 seconds. -&gt; Correct. By increasing the initial delay of the health check to 200 seconds, you allow more time for the virtual machines to become fully ready to process requests before autoscaling takes place. Since each virtual machine takes less than 3 minutes to be ready, increasing the initial delay will help prevent unnecessary scaling.</p><p><br></p><p>You can update the autoscaling health check from HTTP to TCP. -&gt;&nbsp;Incorrect. Changing the health check protocol from HTTP to TCP may have implications on the health monitoring and may not directly address the problem of unnecessary scaling due to misconfiguration in health checks.</p><p><br></p><p>You can update the managed instances template to set the maximum instances to 3. -&gt;&nbsp;Incorrect. Limiting the maximum instances to 3 would restrict the scaling, but it does not address the underlying problem of unnecessary scaling caused by the misconfiguration in health checks.</p><p><br></p><p>You can update the managed instances template to set the minimum instances to 3. -&gt;&nbsp;Incorrect. Setting the minimum instances to 3 would enforce a higher baseline number of instances, but it does not directly address the issue of unnecessary scaling caused by the misconfiguration in health checks.</p><p><br></p><p>https://cloud.google.com/compute/docs/instance-groups/autohealing-instances-in-migs</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163874,
    "question_plain": "After adding a new version of your application in App Engine Standard, users have reported experiencing slow performance issues. Your immediate objective is to revert to the previous version as quickly as possible in response to these complaints. What should you do?",
    "answers": [
      "<p>You should set the previous version as default to route all traffic in App Engine Console.</p>",
      "<p>You should deploy the previous version as a new App Engine Application and use traffic splitting feature to send all traffic to the new application.</p>",
      "<p>You should deploy the previous version in Flexible environment and use traffic splitting feature to send all traffic to the new application.</p>",
      "<p>You should deploy the previous version on a Kubernetes cluster and use traffic splitting feature to send all traffic to the new application.</p>"
    ],
    "explanation": "<p>You should set the previous version as default to route all traffic in App Engine Console. -&gt;&nbsp;Correct. Setting the previous version as default will immediately route all traffic back to the previous version of the application, which will effectively revert the application to the previous version. This can be done quickly and easily in the App Engine Console, without requiring any additional deployments or configuration changes.</p><p><br></p><p>You should deploy the previous version as a new App Engine Application and use traffic splitting feature to send all traffic to the new application. -&gt; Incorrect. It is not the best approach, as deploying the previous version as a new App Engine application and using traffic splitting would require more effort and resources than simply reverting to the previous version.</p><p><br></p><p>You should deploy the previous version in Flexible environment and use traffic splitting feature to send all traffic to the new application. -&gt; Incorrect. It is also not the best approach, as deploying the previous version in the Flexible environment would require additional configuration and maintenance compared to simply setting the previous version as the default.</p><p><br></p><p>You should deploy the previous version on a Kubernetes cluster and use traffic splitting feature to send all traffic to the new application. -&gt; Incorrect. It is not the best approach, as deploying the previous version on a Kubernetes cluster would also require additional configuration and maintenance compared to simply setting the previous version as the default. Additionally, using a Kubernetes cluster for this task may be overkill, as App Engine provides built-in functionality for managing multiple versions of an application.</p><p><br></p><p>https://cloud.google.com/appengine/docs/standard/python/tools/uploadinganapp</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163876,
    "question_plain": "A company wants to migrate on-premises data center to Google Cloud. As a cloud architect, you need to estimate monthly expenses that this company needs to run all their infrastructure in GCP. How can you calculate your expenses?",
    "answers": [
      "<p>You should use the Google Cloud Pricing Calculator to estimate the monthly expenses.</p>",
      "<p>You should capture the pricing from the products pricing page and manually calculate monthly expenses.</p>",
      "<p>You should migrate all applications to GCP and run them for a week. Then based on that, calculate monthly expenses.</p>",
      "<p>You should migrate all applications to GCP and run them for a day. Then based on that, calculate monthly expenses.</p>"
    ],
    "explanation": "<p>You should use the Google Cloud Pricing Calculator to estimate the monthly expenses. -&gt; Correct. It is the correct answer because the Google Cloud Pricing Calculator allows you to estimate the cost of running all infrastructure on GCP, including the cost of computing, storage, networking, and other services that the company might require. The calculator provides you with a breakdown of the estimated costs for each service and allows you to adjust parameters to simulate various scenarios.</p><p><br></p><p>You should capture the pricing from the products pricing page and manually calculate monthly expenses. -&gt;&nbsp;Incorrect. While the pricing information is available on the products pricing page, manually calculating the monthly expenses based on this information can be challenging and time-consuming, especially when dealing with multiple services and configurations. The Google Cloud Pricing Calculator provides a more convenient and accurate estimation.</p><p><br></p><p>You should migrate all applications to GCP and run them for a week. Then based on that, calculate monthly expenses. -&gt;&nbsp;Incorrect. Migrating applications and running them for a week does not provide a reliable basis for calculating the monthly expenses. This approach does not consider the ongoing costs, such as storage, network usage, or other services required for the infrastructure. It is more suitable for testing and performance evaluation rather than cost estimation.</p><p><br></p><p>You should migrate all applications to GCP and run them for a day. Then based on that, calculate monthly expenses. -&gt;&nbsp;Incorrect. Migrating applications and running them for a day does not provide a reliable basis for calculating the monthly expenses. This approach does not consider the ongoing costs, such as storage, network usage, or other services required for the infrastructure. It is more suitable for testing and performance evaluation rather than cost estimation.</p><p><br></p><p>https://cloud.google.com/products/calculator</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163878,
    "question_plain": "As a cloud architect, you are faced with the situation where you have recently deployed an application on a single Compute Engine virtual machine instance, but its popularity is not meeting your initial expectations. Your goal now is to minimize costs associated with this scenario. What would be the optimal deployment location for your application?",
    "answers": [
      "<p>You should containerize your application an deploy with Cloud Run.</p>",
      "<p>You should deploy your application with App Engine Flexible.</p>",
      "<p>You should deploy your application with Kubernetes Engine with horizontal pod autoscaling and cluster autoscaler enabled.</p>",
      "<p>In this case, it is not possible to reduce costs.</p>"
    ],
    "explanation": "<p>You should containerize your application an deploy with Cloud Run. -&gt; Correct. Cloud Run allows you to run stateless containers that are automatically scaled up and down based on incoming requests, which can help minimize costs. With Cloud Run, you only pay for the resources you use, and there are no instances to manage or scaling decisions to make. This makes it an ideal option for applications that are not as popular as expected and need to be run cost-effectively.</p><p><br></p><p>You should deploy your application with App Engine Flexible. -&gt; Incorrect. App Engine Flexible is also good option for scalable applications, but it may be more complex to set up and manage compared to Cloud Run. Additionally, it may require more resources and have higher costs, which can be a disadvantage if you are trying to minimize costs.</p><p><br></p><p>You should deploy your application with Kubernetes Engine with horizontal pod autoscaling and cluster autoscaler enabled. -&gt; Incorrect. Kubernetes Engine is also good option for scalable applications, but it may be more complex to set up and manage compared to Cloud Run. Additionally, it&nbsp; may require more resources and have higher costs, which can be a disadvantage if you are trying to minimize costs.</p><p><br></p><p>In this case, it is not possible to reduce costs. -&gt; Incorrect. You can almost always cut costs by optimizing your infrastructure, choosing the right services, and making sure you only use the resources you need.</p><p><br></p><p>https://cloud.google.com/run/docs/quickstarts</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163880,
    "question_plain": "An online marketing company has several critical applications running on its private data center. They want to use Compute Engine to handle traffic bursts and communicate via their internal IP addresses. What should you advise them?",
    "answers": [
      "<p>They should create a new VPC in GCP with an overlapping IP range and configure Cloud VPN between the private network and GCP.</p>",
      "<p>They should allow applications in the data center to scale to Google Cloud through the VPN tunnel.</p>",
      "<p>They should create a new GCP project and a new VPC and make this a shared VPC with the private network. Then, allow applications in the data center to scale to Google Cloud on the shared VPC.</p>",
      "<p>They should migrate all applications from private data center to the Google Cloud.</p>"
    ],
    "explanation": "<p>They should allow applications in the data center to scale to Google Cloud through the VPN tunnel. -&gt; Correct. By establishing a VPN tunnel between their private data center and GCP, the applications can scale out to Google Cloud during traffic bursts and communicate using their internal IP addresses. This satisfies the requirements without changing the current setup or introducing potential conflicts.</p><p><br></p><p>They should create a new VPC in GCP with an overlapping IP range and configure Cloud VPN between the private network and GCP. -&gt; Incorrect. Overlapping IP ranges can cause routing issues, and it's generally not recommended. Creating a VPC in GCP with an overlapping IP range would lead to IP conflicts and might render some systems unreachable.</p><p><br></p><p>They should create a new GCP project and a new VPC and make this a shared VPC with the private network. Then, allow applications in the data center to scale to Google Cloud on the shared VPC. -&gt; Incorrect. Shared VPCs in GCP are used to share networking resources across different GCP projects. The question doesn't suggest there's a need for multiple GCP projects, so this option adds unnecessary complexity.</p><p><br></p><p>They should migrate all applications from private data center to the Google Cloud. -&gt; Incorrect. This is a drastic move and isn't aligned with the scenario described. Migrating everything to Google Cloud might not be necessary if the company only wants to handle traffic bursts.</p><p><br></p><p>https://cloud.google.com/network-connectivity/docs/vpn</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163882,
    "question_plain": "A machine learning team needs to use a Kubernetes Engine cluster with specific GPUs to process long running jobs that cannot be restarted. In this case, how do you recommend configuring the Kubernetes Engine cluster?",
    "answers": [
      "<p>They should deploy the workload on a node pool with non-preemptible compute engine instances and GPUs attached. Enable cluster autoscaling and set min-nodes to 1.</p>",
      "<p>They should enable Vertical Pod autoscaling.</p>",
      "<p>They should enable Kubernetes Engine cluster node auto-provisioning.</p>",
      "<p>They should deploy the workload on a node pool with preemptible compute engine instances and GPUs attached.</p>"
    ],
    "explanation": "<p>They should deploy the workload on a node pool with non-preemptible compute engine instances and GPUs attached. Enable cluster autoscaling and set min-nodes to 1. -&gt;&nbsp;Correct. The reason for choosing this option is that the requirement specifies that the long running jobs cannot be restarted, and preemptible instances can be shut down at any time, which can cause the loss of data and progress. Therefore, it is recommended to use non-preemptible instances to ensure the stability and reliability of the workload. Moreover, since the workload requires specific GPUs, the team should deploy the workload on a node pool with GPUs attached. Cluster autoscaling can be enabled to ensure that the workload can scale to meet demand, and setting min-nodes to 1 can ensure that there is always at least one node available for the workload to run on.</p><p><br></p><p>They should enable Vertical Pod autoscaling. -&gt;&nbsp;Incorrect. Enabling Vertical Pod autoscaling, is not relevant to the requirements specified in the question.</p><p><br></p><p>They should enable Kubernetes Engine cluster node auto-provisioning. -&gt;&nbsp;Incorrect. Enabling Kubernetes Engine cluster node auto-provisioning, can help automate the creation of new nodes when the workload demands it, but it does not address the requirement for specific GPUs and non-preemptible instances.</p><p><br></p><p>They should deploy the workload on a node pool with preemptible compute engine instances and GPUs attached. -&gt; Incorrect. Deploying the workload on a node pool with preemptible compute engine instances and GPUs attached, is not recommended as it can result in data and progress loss due to the nature of preemptible instances.</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163884,
    "question_plain": "As a cloud architect, it is necessary for you to adhere to your organization's guidelines when utilizing Cloud Storage for the storage of company data. These guidelines dictate the archiving of data that is more than one year old and the deletion of data that is older than three years. What course of action should you take?",
    "answers": [
      "<p>You should set up Object Lifecycle Management policies.</p>",
      "<p>You should run a script every day. Copy data that is older than one year to an archive bucket, and delete data from three years ago.</p>",
      "<p>You should run a script every day. Set storage class to Archive for data that is older than one year, and delete data from three years ago.</p>",
      "<p>You should set up default storage class for two buckets with storage classes: Standard and Archive. Use a script to move the data in the appropriate bucket when its needed.</p>"
    ],
    "explanation": "<p>You should set up Object Lifecycle Management policies. -&gt; Correct. Object Lifecycle Management policies are specifically designed to automate the process of archiving and deleting objects based on their age. With Object Lifecycle Management policies, the cloud architect can set up rules to automatically transition data to archival storage after a specified period of time and to automatically delete data after a specified period of time. This ensures compliance with the organization's regulations without the need for manual intervention.</p><p><br></p><p>You should run a script every day. Copy data that is older than one year to an archive bucket, and delete data from three years ago. -&gt; Incorrect. Running a script every day to copy or move data to an archive bucket can be error-prone and may require manual intervention if something goes wrong. Also, it does not provide the same level of automation and control that Object Lifecycle Management policies offer.</p><p><br></p><p>You should run a script every day. Set storage class to Archive for data that is older than one year, and delete data from three years ago. -&gt; Incorrect. Running a script every day to change the storage class to Archive, can be error-prone and may require manual intervention if something goes wrong. Also, it does not provide the same level of automation and control that Object Lifecycle Management policies offer.</p><p><br></p><p>You should set up default storage class for two buckets with storage classes: Standard and Archive. Use a script to move the data in the appropriate bucket when its needed. -&gt; Incorrect. Setting up default storage class for two buckets, does not provide the same level of automation and control as Object Lifecycle Management policies. It would require manual intervention to move the data to the appropriate bucket, which can be cumbersome and prone to errors.</p><p><br></p><p>https://cloud.google.com/storage/docs/lifecycle</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163886,
    "question_plain": "As a cloud architect, you need to prepare a resource hierarchy for your company. Suppose your company has two different applications with development and production environment. With Google's best practices in mind, what should you do?",
    "answers": [
      "<p>You should create four different projects (for each application and environment). This isolates the environments from each other, so changes to the development project don't accidently impact production environment. This also gives you better access control.</p>",
      "<p>You should create all applications in one project.</p>",
      "<p>You should create two projects, each for one application.</p>",
      "<p>You should create two projects, each for one environment.</p>"
    ],
    "explanation": "<p>You should create four different projects (for each application and environment). This isolates the environments from each other, so changes to the development project don't accidently impact production environment. This also gives you better access control. -&gt; Correct. Google's best practice recommends using multiple projects, one for each environment and application, to isolate environments from each other, give better access control, and avoid changes to the development project accidentally impacting the production environment. Creating four different projects (for each application and environment) is the most effective way to achieve this. This also allows for better resource management, as each project has its own set of quotas, billing, and monitoring.</p><p><br></p><p>You should create all applications in one project. -&gt; Incorrect. Creating all applications in one project might result in resource sharing between development and production environments, increasing the risk of accidents or unintended changes. </p><p><br></p><p>You should create two projects, each for one application. -&gt;&nbsp;Incorrect. Creating two projects, each for one application, does not provide the needed separation of development and production environments. </p><p><br></p><p>You should create two projects, each for one environment. -&gt; Incorrect. Creating two projects, each for one environment, does not separate applications and may make resource management more challenging.</p><p><br></p><p>https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations#project-structure</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163888,
    "question_plain": "You provide a service that you need to open to everyone in your partner network and have a server and an IP address where the application is located. You do not want to have to change the IP address on your DNS server if your server goes down or is replaced. You also want to avoid downtime and deliver a solution with minimal cost and setup. What should you recommend?",
    "answers": [
      "<p>You should reserve a static external IP address, and assign it using Cloud DNS.</p>",
      "<p>You should create a script that updates the domain's IP address when the server goes down or is replaced.</p>",
      "<p>You should reserve a static internal IP address, and assign it using Cloud DNS.</p>",
      "<p>You should use the Bring Your Own IP (BYOIP) method to use your own IP address.</p>"
    ],
    "explanation": "<p>You should reserve a static external IP address, and assign it using Cloud DNS. -&gt; Correct.&nbsp; Reserving a static external IP address and assigning it using Cloud DNS can provide a stable IP address to your application and can eliminate the need for changing the IP address on your DNS server if your server goes down or is replaced. This solution can also avoid downtime and deliver a solution with minimal cost and setup. </p><p><br></p><p>You should create a script that updates the domain's IP address when the server goes down or is replaced. -&gt; Incorrect. Using a script to update the domain's IP address when the server goes down or is replaced may not be reliable and can lead to downtime. </p><p><br></p><p>You should reserve a static internal IP address, and assign it using Cloud DNS. -&gt; Incorrect. Reserving a static internal IP address may not be sufficient if you need to expose your service to the partner network. </p><p><br></p><p>You should use the Bring Your Own IP (BYOIP) method to use your own IP address. -&gt; Incorrect. Using Bring Your Own IP (BYOIP) method may require significant setup and configuration and may not be the most cost-effective solution.</p><p><br></p><p>https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163890,
    "question_plain": "Your team needs to create development, test and production environments for project in Google Cloud. As a cloud architect, you need to effectively implement and manage these environments and ensure their consistency. With Google's best practices in mind, what should you do?",
    "answers": [
      "<p>You should use the Cloud Foundation Toolkit to create a single deployment template that will work for all environments, and deploy with Terraform.</p>",
      "<p>You should create a Cloud Shell script that uses <code>gcloud</code> commands to deploy the environments.</p>",
      "<p>You should create a single Terraform configuration for all environments.</p>",
      "<p>You should create a Terraform configuration for each environment. Use them for repeated deployment.</p>"
    ],
    "explanation": "<p>You should use the Cloud Foundation Toolkit to create a single deployment template that will work for all environments, and deploy with Terraform. -&gt; Correct. The Cloud Foundation Toolkit provides a set of best practices, tools, and templates that help you implement consistent and repeatable infrastructure in Google Cloud. Using a single deployment template for all environments will ensure consistency and reduce errors. Deploying with Terraform is a good option as well, as it allows for infrastructure as code and version control.</p><p><br></p><p>You should create a Cloud Shell script that uses <code>gcloud</code> commands to deploy the environments. -&gt; Incorrect. It may work for small deployments but can be difficult to maintain and scale.</p><p><br></p><p>You should create a single Terraform configuration for all environments. -&gt; Incorrect. It can be challenging to maintain and update as it may require significant changes to accommodate differences between environments.</p><p><br></p><p>You should create a Terraform configuration for each environment. Use them for repeated deployment. -&gt; Incorrect. It can be a good option if there are significant differences between environments that require unique configuration. However, this can lead to inconsistency between environments if changes are not consistently applied to all configurations.</p><p><br></p><p>https://cloud.google.com/foundation-toolkit</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163892,
    "question_plain": "Your company is running several related applications on Compute Engine virtual machine instances and wants to expose each application through a DNS name. With Google's best practices in mind, what should you recommend to them?",
    "answers": [
      "<p>They should use Cloud DNS to translate their domain names into IP addresses.</p>",
      "<p>They should assign Google Cloud routes to their virtual machines, assign DNS names to routes, and make the DNS names public.</p>",
      "<p>They should assign an alias IP address range to each virtual machine, and then make the internal DNS names public.</p>",
      "<p>They should use Compute Engine's internal DNS service to assign DNS names to virtual machines and make them available to users.</p>"
    ],
    "explanation": "<p>They should use Cloud DNS to translate their domain names into IP addresses. -&gt; Correct. Cloud DNS is a scalable, reliable, and managed authoritative Domain Name System (DNS) service that allows you to easily publish and manage domain names to the internet. It translates human-readable domain names into the numeric IP addresses that computers use to connect to each other. Therefore, this is the standard and recommended way to expose applications on Compute Engine instances through DNS names, following Google's best practices.</p><p><br></p><p>They should assign Google Cloud routes to their virtual machines, assign DNS names to routes, and make the DNS names public. -&gt;&nbsp;Incorrect. Routes are not meant to be exposed to the internet. Instead, they define paths network traffic takes from a VM instance to other networks. They don't have direct relation with DNS.</p><p><br></p><p>They should assign an alias IP address range to each virtual machine, and then make the internal DNS names public. -&gt;&nbsp;Incorrect. Assigning an alias IP address range to each VM and making the internal DNS names public would not be a standard or recommended way to expose applications.</p><p><br></p><p>They should use Compute Engine's internal DNS service to assign DNS names to virtual machines and make them available to users. -&gt;&nbsp;Incorrect. Compute Engine's internal DNS service is primarily meant for internal communications within a network, not for exposing applications on the internet. This service enables instances to resolve the hostnames of other instances within the same network.</p><p><br></p><p>https://cloud.google.com/dns/docs</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163894,
    "question_plain": "Your organization stores sensitive customer data in Cloud Storage. You have been asked to design a solution that both prevents unauthorized data access and ensures regulatory compliance. Which of the following approaches would be the most appropriate?",
    "answers": [
      "<p>Encrypt data using customer-managed encryption keys (CMEK) and enforce fine-grained access control with Identity and Access Management (IAM).</p>",
      "<p>Implement Cloud Identity-Aware Proxy (IAP) to control access to Cloud Storage.</p>",
      "<p>Use VPC Service Controls to isolate Cloud Storage.</p>",
      "<p>Use Cloud Audit Logs to monitor access to Cloud Storage.</p>"
    ],
    "explanation": "<p>Encrypt data using customer-managed encryption keys (CMEK) and enforce fine-grained access control with Identity and Access Management (IAM). -&gt;&nbsp;Correct. By using CMEK, you can control the encryption and decryption keys. With IAM, you can enforce fine-grained access control, ensuring that only authorized entities have access to specific resources.</p><p><br></p><p>Implement Cloud Identity-Aware Proxy (IAP) to control access to Cloud Storage. -&gt;&nbsp;Incorrect. While IAP is an effective tool for controlling access to applications deployed on Google Cloud, it's not specifically designed to secure data stored in Cloud Storage.</p><p><br></p><p>Use VPC Service Controls to isolate Cloud Storage. -&gt;&nbsp;Incorrect. While VPC Service Controls can provide additional security measures, they are primarily used for preventing data exfiltration from Google Cloud services. They do not directly handle encryption or access management within Cloud Storage.</p><p><br></p><p>Use Cloud Audit Logs to monitor access to Cloud Storage. -&gt;&nbsp;Incorrect. While Cloud Audit Logs can provide valuable information on who did what, when, and where, they are more about visibility and accountability, and do not prevent unauthorized access to Cloud Storage.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163896,
    "question_plain": "You are planning to make your API publicly available and you have concerns about potential DDoS attacks targeting your service. Which GCP service should be taken into consideration to ensure the protection of your API?",
    "answers": [
      "<p>Cloud Armor</p>",
      "<p>Cloud IAM</p>",
      "<p>Cloud CDN</p>",
      "<p>Cloud VPN</p>"
    ],
    "explanation": "<p>Cloud Armor -&gt; Correct. Cloud Armor is a service provided by Google Cloud Platform (GCP) that offers security policies and defenses against distributed denial-of-service (DDoS) attacks. It allows you to define and enforce granular rules to control and filter incoming traffic to your API. With Cloud Armor, you can set up rules to block or allow traffic based on various parameters, such as IP addresses, geographical locations, or specific HTTP headers.</p><p><br></p><p>Cloud IAM -&gt; Incorrect. Cloud IAM (Identity and Access Management) is a service that manages access control and permissions for resources within GCP. While it is essential for managing and controlling user access to your GCP resources, it does not specifically address the protection of your API from DDoS attacks.</p><p><br></p><p>Cloud CDN -&gt; Incorrect. Cloud CDN (Content Delivery Network) is a service that caches and delivers content from Google's global network of edge locations. While Cloud CDN can improve the performance and latency of your API by caching and serving content closer to your users, it does not provide specific protections against DDoS attacks.</p><p> </p><p>Cloud VPN -&gt; Incorrect. Cloud VPN is a service that enables secure connections between your on-premises network and your GCP virtual private cloud (VPC) network. While it establishes a secure connection, it is primarily used for secure network connectivity rather than protecting your API from DDoS attacks.</p><p><br></p><p>https://cloud.google.com/armor/docs/cloud-armor-overview</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163898,
    "question_plain": "You are the lead cloud architect for a multinational firm which plans to deploy its complex microservices-based applications to the Google Cloud Platform (GCP). The firm has an SLA of 99.99% uptime and your application should be available all the time. The application also has varying loads throughout the day and spikes during the holiday season. Which is the most effective solution?",
    "answers": [
      "<p>Deploy the application on Kubernetes Engine (GKE) with clusters in multiple regions and with autoscaling enabled.</p>",
      "<p>Deploy the application on Compute Engine instances within a single region.</p>",
      "<p>Deploy the application on App Engine Standard with autoscaling enabled.</p>",
      "<p>Deploy the application on a combination of Compute Engine instances and Cloud Functions.</p>"
    ],
    "explanation": "<p>Deploy the application on Kubernetes Engine (GKE) with clusters in multiple regions and with autoscaling enabled. -&gt;&nbsp;Correct. Kubernetes provides a great platform for deploying and managing complex microservices-based applications. Autoscaling will handle varying loads, and multi-region clusters provide the redundancy necessary to meet the firm's high uptime SLA.</p><p><br></p><p>Deploy the application on Compute Engine instances within a single region. -&gt;&nbsp;Incorrect. This approach doesn't account for regional outages and doesn't automatically scale according to the load. It's unlikely to meet the firm's uptime SLA.</p><p><br></p><p>Deploy the application on App Engine Standard with autoscaling enabled. -&gt;&nbsp;Incorrect. While App Engine Standard can auto-scale and meet high load requirements, it's less suited for complex microservices-based applications. It also doesn't inherently provide multi-region redundancy.</p><p><br></p><p>Deploy the application on a combination of Compute Engine instances and Cloud Functions. -&gt;&nbsp;Incorrect. This combined approach is not optimal for managing complex microservices-based applications and does not provide automatic scaling or multi-region redundancy inherently.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163900,
    "question_plain": "One of your web application is becoming widely used. The frontend runs in App Engine and scales automatically. The backend runs in a Managed Instance Group on Compute Engine with the maximum number of instances set to 10. Sometimes the frontend sends more data than the backend can keep up and the data is lost, but you do not want to increase the maximum size of the MIG or change the VM instance type. As a cloud architect, what can you recommend to prevent data loss?",
    "answers": [
      "<p>You should have the frontend writing data to the Cloud Pub/Sub topic, and the backend read from that topic. This provide a managed and scalable message queue, and stores ingested data until the backend can process it.</p>",
      "<p>You should use an unmanaged instance group.</p>",
      "<p>You should store ingested data in Cloud Storage.</p>",
      "<p>You should store ingested data in BigQuery.</p>"
    ],
    "explanation": "<p>You should have the frontend writing data to the Cloud Pub/Sub topic, and the backend read from that topic. This provide a managed and scalable message queue, and stores ingested data until the backend can process it. -&gt; Correct. You should have the frontend writing data to the Cloud Pub/Sub topic, and the backend read from that topic. This provides a managed and scalable message queue, and stores ingested data until the backend can process it. When the frontend sends more data than the backend can handle, data is lost, and this is an issue. Cloud Pub/Sub is a messaging service that decouples senders and receivers of messages. Therefore, using Cloud Pub/Sub as a message queue ensures that the frontend can send data to a buffer that stores ingested data until the backend can process it, preventing data loss. Additionally, Cloud Pub/Sub is highly scalable and can handle large volumes of messages, making it a perfect solution for a widely used web application.</p><p><br></p><p>You should use an unmanaged instance group. -&gt; Incorrect. It is not a solution for the data loss problem. </p><p><br></p><p>You should store ingested data in Cloud Storage. -&gt; Incorrect. It does not provide a message queue or buffer to store ingested data until the backend can process it, so data loss is still possible.</p><p><br></p><p>You should store ingested data in BigQuery. -&gt; Incorrect. It does not provide a message queue or buffer to store ingested data until the backend can process it, so data loss is still possible.</p><p><br></p><p>https://cloud.google.com/pubsub/docs/create-topic-console</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163902,
    "question_plain": "As a cloud architect, your task is to transfer all on-premises workloads to Google Cloud. Specifically, you are interested in running a custom container within a managed service. What are the available options for you to consider?",
    "answers": [
      "<p>App Engine Flexible</p>",
      "<p>Kubernetes Engine</p>",
      "<p>App Engine Standard</p>",
      "<p>Compute Engine</p>",
      "<p>Cloud Functions</p>"
    ],
    "explanation": "<p>App Engine Flexible -&gt;&nbsp;Correct. It is a fully managed platform that allows you to build and run containerized applications. It supports a wide range of languages, including Java, Python, Node.js, and Go. App Engine Flexible is a good choice when you need to deploy and run your custom container in a fully managed environment with automatic scaling, load balancing, and monitoring.</p><p><br></p><p>Kubernetes Engine -&gt; Correct. It is a managed service for running containerized applications using Kubernetes. It allows you to create and manage Kubernetes clusters, and deploy and manage containers in a highly available and scalable environment. Kubernetes Engine is a good choice when you need a more flexible container environment, with advanced features such as automatic scaling, rolling updates, and load balancing.</p><p><br></p><p>App Engine Standard -&gt;&nbsp;Incorrect. App Engine Standard Environment is a platform-as-a-service (PaaS) offering on Google Cloud, but it does not support running custom containers. It provides a runtime environment for specific programming languages (such as Python, Java, Go, etc.) and imposes some restrictions on the application architecture and dependencies.</p><p><br></p><p>Compute Engine -&gt;&nbsp;Incorrect. Compute Engine is an infrastructure-as-a-service (IaaS) offering on Google Cloud that allows you to create and manage virtual machines (VMs). While you can run containers within VMs on Compute Engine, it does not provide a managed service specifically designed for running containers like the other options mentioned.</p><p><br></p><p>Cloud Functions -&gt;&nbsp;Incorrect. Cloud Functions is a serverless compute platform that enables you to run code in response to events. It is designed for small, event-driven functions and does not directly support running custom containers.</p><p><br></p><p>https://cloud.google.com/appengine/docs/flexible</p><p>https://cloud.google.com/kubernetes-engine/docs/quickstart</p>",
    "correct_response": ["a", "b"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 82163904,
    "question_plain": "As a cloud architect, you aim to enhance the processing speed of a web application that generates thumbnails from user-uploaded photos. Currently, the frontend application uploads photos to Cloud Storage, while the outdated backend relies on a cron job that checks Cloud Storage buckets every 20 minutes for new photos. Your objective is to optimize this application and process the photos as soon as possible. Which Google Cloud service would be the most suitable choice?",
    "answers": [
      "<p>Cloud Function</p>",
      "<p>App Engine Flexible</p>",
      "<p>Kubernetes pod</p>",
      "<p>A cron job that checks the bucket more often.</p>"
    ],
    "explanation": "<p>Cloud Function -&gt;&nbsp;Correct. Cloud Functions allow you to execute code in response to events, such as an uploaded photo to Cloud Storage. With Cloud Functions, you can trigger the thumbnail generation process immediately after a photo is uploaded, resulting in faster processing speed compared to the current cron job approach.</p><p><br></p><p>App Engine Flexible -&gt;&nbsp;Incorrect. App Engine Flexible is a platform-as-a-service (PaaS) offering that allows you to deploy and manage applications, but it may not provide the necessary event-driven capabilities for immediate processing of uploaded photos.</p><p><br></p><p>Kubernetes pod -&gt;&nbsp;Incorrect. Kubernetes pods are a fundamental unit of deployment in Kubernetes clusters, but they do not inherently provide event-driven functionality for processing uploaded photos in real-time.</p><p><br></p><p>A cron job that checks the bucket more often. -&gt;&nbsp;Incorrect. Increasing the frequency of the cron job to check the bucket more often reduces the delay between checks, but it still relies on periodic checks rather than immediate processing triggered by events.</p><p><br></p><p>https://cloud.google.com/functions/docs/how-to</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163906,
    "question_plain": "An online marketing company wants to migrate a local data warehouse to GCP and they want to use a managed cloud solution. As a cloud architect, you advise them which service they should use. What do you recommend?",
    "answers": [
      "<p>BigQuery</p>",
      "<p>Compute Engine</p>",
      "<p>Cloud Dataproc</p>",
      "<p>Cloud Bigtable</p>",
      "<p>Cloud Storage</p>"
    ],
    "explanation": "<p>BigQuery -&gt; Correct. BigQuery is Google Cloud's fully managed, petabyte-scale, and cost-effective analytics data warehouse that lets you run analytics over vast amounts of data in near real time. With BigQuery, there's no infrastructure to set up or manage, letting you focus on finding meaningful insights using standard SQL and taking advantage of flexible pricing models across on-demand and flat-rate options.</p><p><br></p><p>Compute Engine -&gt; Incorrect. Compute Engine isn't a managed service. </p><p><br></p><p>Cloud Dataproc -&gt; Incorrect. Dataproc is a managed Hadoop and Spark service.</p><p><br></p><p>Cloud Bigtable -&gt;&nbsp;Incorrect. Bigtable is a NoSQL database for low-latency writes and limited ranges of queries. </p><p><br></p><p>Cloud Storage -&gt; Incorrect. Cloud Storage isn't a good choice for data warehouse (object storage).</p><p><br></p><p>https://cloud.google.com/bigquery/docs/introduction</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163908,
    "question_plain": "As a cloud architect, it is your duty to oversee the expansion of a web application from North America to Europe. This web application collects Personally Identifiable Information (PII). What regulations must you bear in mind when entering the European market?",
    "answers": [
      "<p>GDPR - General Data Protection Regulation</p>",
      "<p>PCI&nbsp;DSS - Payment Card Industry Data Security Standard</p>",
      "<p>HIPAA - Health Insurance Portability &amp; Accountability Act</p>",
      "<p>SOX - Sarbanes-Oxley Act</p>"
    ],
    "explanation": "<p>GDPR - General Data Protection Regulation -&gt; Correct.&nbsp; The GDPR is a regulation that protects the privacy and personal data of individuals residing in the European Union (EU). It applies to all businesses that collect or process personal data of EU residents, regardless of whether the business is based within the EU or not. Therefore, as a cloud architect expanding a web application from North America to Europe and collecting Personally Identifiable Information (PII), you must comply with GDPR regulations.</p><p><br></p><p>PCI&nbsp;DSS - Payment Card Industry Data Security Standard -&gt; Incorrect. PCI DSS is a set of security standards designed to protect credit card information during transactions. It does not apply to PII in general.</p><p><br></p><p>HIPAA - Health Insurance Portability &amp; Accountability Act -&gt; Incorrect. HIPAA is a US regulation that sets the standards for the protection of health information. It does not apply to PII in general.</p><p><br></p><p>SOX - Sarbanes-Oxley Act -&gt; Incorrect. SOX is a US regulation that sets the standards for financial reporting and accountability. It does not apply to PII in general.</p><p><br></p><p>https://cloud.google.com/privacy/gdpr</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163910,
    "question_plain": "A small mobile gaming company wants to benefit from cloud solutions and migrate mobile game application that uses PostgreSQL database to Google Cloud. They have a small development team and want to minimize administrative tasks. What managed service do you recommend?",
    "answers": [
      "<p>Cloud SQL</p>",
      "<p>Cloud Spanner</p>",
      "<p>Cloud Dataproc</p>",
      "<p>Cloud Bigtable</p>"
    ],
    "explanation": "<p>Cloud SQL -&gt; Correct. Cloud SQL is a fully managed relational database service that automates administrative tasks such as database provisioning, patching, and backups, which can help minimize the administrative tasks for the small development team. Additionally, Cloud SQL supports PostgreSQL, which is the database currently being used by the mobile gaming company. </p><p><br></p><p>Cloud Spanner -&gt;&nbsp;Incorrect. It is a globally distributed, horizontally scalable, and strongly consistent database service that is best suited for mission-critical and complex applications. </p><p><br></p><p>Cloud Dataproc -&gt; Incorrect. It is a managed Spark and Hadoop service for processing large datasets. </p><p><br></p><p>Cloud Bigtable -&gt;&nbsp;Incorrect. It is a NoSQL database service that is best suited for large-scale and low-latency workloads.</p><p><br></p><p>https://cloud.google.com/sql/docs/postgres</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163912,
    "question_plain": "A healthcare company collects Personally Identifiable Information (PII) and they want to run a highly secure environment in Google Cloud. They want to use virtual machines to run workloads.&nbsp;What should you advise them?",
    "answers": [
      "<p>They should use Shielded VMs that only runs digitally verified boot components.</p>",
      "<p>They should use Managed Instance Group.</p>",
      "<p>They should use Cloud Functions.</p>",
      "<p>They should use Preemptible VMs.</p>"
    ],
    "explanation": "<p>They should use Shielded VMs that only runs digitally verified boot components. -&gt; Correct. The healthcare company collects Personally Identifiable Information (PII), and therefore, as a cloud architect, you should advise them to use Shielded VMs that only run digitally verified boot components. Shielded VMs provide verifiable integrity, which means that it ensures that the VM boots from a known, verified, and trusted state. Shielded VMs provide the necessary secure boot process by using UEFI firmware, which confirms the signature of the boot loader before booting it. Shielded VMs also protect VMs against rootkit and bootkit malware attacks.</p><p><br></p><p>They should use Managed Instance Group. -&gt; Incorrect. Managed Instance Groups (MIGs) is a tool that helps you manage a group of virtual machine instances. It doesn't provide the same level of security as Shielded VMs.</p><p><br></p><p>They should use Cloud Functions. -&gt; Incorrect. Cloud Functions is a serverless computing solution where you write and deploy small code snippets that run in response to events.</p><p><br></p><p>They should use Preemptible VMs. -&gt; Incorrect. Preemptible VMs is a type of VM that you can use for short-lived, fault-tolerant workloads. Preemptible VMs have a lower price but can be stopped by Google at any time, which may not be suitable for highly secure environments.</p><p><br></p><p>https://cloud.google.com/compute/shielded-vm/docs/shielded-vm</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163918,
    "question_plain": "As a cloud architect, you need to establish a connection between two Virtual Private Cloud networks that uses private IP address space. Traffic stays within Google's network and doesn't traverse the public internet. Which option should you choose?",
    "answers": [
      "<p>VPC&nbsp;Network Peering</p>",
      "<p>Custom subnets</p>",
      "<p>Firewall rules</p>",
      "<p>Cloud Interconnect</p>",
      "<p>Cloud VPN</p>"
    ],
    "explanation": "<p>VPC&nbsp;Network Peering -&gt; Correct. VPC Network Peering is a service that enables you to establish a direct, private connection between two VPC networks. When you use VPC Network Peering, traffic stays within Google's network and doesn't traverse the public internet. Additionally, VPC Network Peering allows you to use private IP addresses within your VPC networks, which is what is needed in this scenario.</p><p><br></p><p>Custom subnets -&gt; Incorrect. It is not a valid option for establishing a connection between two VPC networks. Custom subnets are simply a way to divide a VPC network's IP address space into smaller subnets.</p><p><br></p><p>Firewall rules -&gt; Incorrect. It is a security feature that controls traffic to and from instances in a VPC network, but it doesn't establish a connection between two VPC networks.</p><p><br></p><p>Cloud Interconnect -&gt; Incorrect. It is used to connect your on-premises network to your Google Cloud VPC network using a physical connection, and it doesn't apply to this scenario where the connection needs to be established between two VPC networks.</p><p><br></p><p>Cloud VPN -&gt; Incorrect. It is used to connect your on-premises network to your Google Cloud VPC network using an IPsec VPN connection, and it doesn't apply to this scenario where the connection needs to be established between two VPC networks.</p><p><br></p><p>https://cloud.google.com/vpc/docs/vpc-peering</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163920,
    "question_plain": "The stock market news application, which includes static content, is experiencing growing popularity. With users from various parts of the world, the application offers subscription plans. However, there is concern among the managers that extended page load times might lead to an increase in the churn rate. As a cloud architect, what recommendations should you provide in this situation?",
    "answers": [
      "<p>They should use Cloud CDN, which distributes static content globally.</p>",
      "<p>They should use Regional Cloud Storage.</p>",
      "<p>They should use Cloud VPN.</p>",
      "<p>They should use Managed Instance Group.</p>"
    ],
    "explanation": "<p>They should use Cloud CDN, which distributes static content globally. -&gt; Correct. The application is becoming more popular, and it has users all over the world. As a result, serving static content globally using a CDN (Content Delivery Network) can significantly reduce page load times, improving the user experience and reducing the churn rate. Cloud CDN is Google's solution for a global CDN, and it can cache content closer to the user, reducing the latency and improving the performance. Therefore, recommending Cloud CDN is a suitable solution for this scenario. </p><p><br></p><p>They should use Regional Cloud Storage. -&gt;&nbsp;Incorrect. Regional Cloud Storage is a storage option provided by Google Cloud, but it does not directly address the concern of extended page load times. Regional Cloud Storage may help with storing and retrieving data, but it does not provide the global distribution and caching capabilities that Cloud CDN offers.</p><p><br></p><p>They should use Cloud VPN. -&gt;&nbsp;Incorrect. Cloud VPN is a service that enables secure connections between your on-premises network and your Google Cloud virtual private network (VPC) network. While secure connectivity is important, it does not directly address the concern of page load times and user experience.</p><p><br></p><p>They should use Managed Instance Group. -&gt;&nbsp;Incorrect. Managed Instance Group is a feature in Google Cloud that allows you to manage a group of virtual machine instances. While it can help with scaling and managing instances, it does not specifically address the concern of extended page load times and improving user experience caused by latency.</p><p><br></p><p>https://cloud.google.com/cdn/docs/overview</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163922,
    "question_plain": "An insurance company uses several third-party enterprise applications that require special licenses. This company wants to migrate all workload to Google Cloud. As a cloud architect what should you check first?",
    "answers": [
      "<p>You should check if the licenses are transferable to the cloud.</p>",
      "<p>You should check the cost of such a migration using Cloud Pricing Calculator.</p>",
      "<p>You don't need to check if your licenses are transferable to the cloud. Everything is open source in the cloud.</p>",
      "<p>You should go straight to migration process.</p>"
    ],
    "explanation": "<p>You should check if the licenses are transferable to the cloud. -&gt;&nbsp;Correct. Before migrating enterprise applications to the cloud, it is crucial to check whether the licenses for those applications are transferable. Some software licenses have restrictions or limitations that may prevent their use in a cloud environment. By checking the license terms and consulting with the software vendors, you can ensure compliance and avoid any licensing issues during the migration process.</p><p><br></p><p>You should check the cost of such a migration using Cloud Pricing Calculator. -&gt;&nbsp;Incorrect. While it is important to consider the cost implications of the migration, checking the license transferability should be prioritized before estimating the migration costs. The cost of the migration can be assessed after confirming that the licenses are transferable.</p><p><br></p><p>You don't need to check if your licenses are transferable to the cloud. Everything is open source in the cloud. -&gt;&nbsp;Incorrect. While the cloud offers a wide range of open-source solutions, not all enterprise applications are open source. It is essential to verify the license transferability for third-party applications before migrating them to the cloud.</p><p><br></p><p>You should go straight to migration process. -&gt;&nbsp;Incorrect. Jumping directly into the migration process without verifying the license transferability could lead to complications and non-compliance issues. It is necessary to assess the license terms and ensure they allow for the migration to the cloud.</p><p><br></p><p>https://cloud.google.com/compute/docs/nodes/bringing-your-own-licenses</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163924,
    "question_plain": "As a cloud architect, you are responsible for preparing a cloud migration plan for services that include wide range of data. A company has 10 PB of different data in on-premises data center. This data need to be transferred to Cloud Storage and the network bandwidth between the on-premises data center and Google Cloud is 10 Gbps. What transfer option should you recommend?",
    "answers": [
      "<p>Transfer Appliance</p>",
      "<p><code>gcloud</code> </p>",
      "<p><code>gsutil</code> </p>",
      "<p>BigQuery Data Transfer</p>",
      "<p>Transfer Service</p>"
    ],
    "explanation": "<p>Transfer Appliance -&gt;&nbsp;Correct. Transfer Appliance is a high-capacity storage server that you can use to securely migrate your on-premises data to Google Cloud. It enables you to transfer large amounts of data quickly and easily, without being limited by your network bandwidth or data transfer costs.</p><p><br></p><p><code>gcloud</code> -&gt; Incorrect. <code>gcloud</code> is a command-line tool for managing Google Cloud resources, but it does not directly address the large-scale data transfer requirements for this scenario.</p><p><br></p><p><code>gsutil</code> -&gt; Incorrect. <code>gsutil</code> is acommand-line tool for managing data, but it does not directly address the large-scale data transfer requirements for this scenario.</p><p><br></p><p>BigQuery Data Transfer -&gt; Incorrect. BigQuery Data Transfer is a specialized service for transferring data into BigQuery, which is not relevant to this scenario.</p><p><br></p><p>Transfer Service -&gt; Incorrect. Transfer Service is a more general data transfer service for moving data into Google Cloud, but Transfer Appliance is better suited for this particular use case because of the large amount of data that needs to be transferred.</p><p><br></p><p>https://cloud.google.com/transfer-appliance/docs/4.0/overview</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163926,
    "question_plain": "A company is moving an enterprise application to the Google Cloud. This application runs on a cluster of virtual machines on private data center, and workloads are distributed by a load balancer. Select all true statements. (select 2)",
    "answers": [
      "<p>The migration team decided not to make unnecessary changes before moving this application to the cloud. This migration strategy is called Lift and Shift.</p>",
      "<p>The migration team decided to use containers and the Kubernetes Engine. This migration strategy is called Improve and Move.</p>",
      "<p>The migration team decided not to make unnecessary changes before moving this application to the cloud. This migration strategy is called Improve and Move.</p>",
      "<p>The migration team decided to use containers and the Kubernetes Engine. This migration strategy is called Lift and Shift.</p>",
      "<p>The migration team decided to use containers and the Kubernetes Engine. This migration strategy is called Remove and Replace.</p>"
    ],
    "explanation": "<p>The migration team decided not to make unnecessary changes before moving this application to the cloud. This migration strategy is called Lift and Shift. -&gt;&nbsp;Correct. Lift and Shift is a migration strategy where the company moves the existing workload to the cloud without making any significant changes. This is typically done when the workload is running on virtual machines on-premises and the company wants to take advantage of the cloud's benefits, such as scalability and availability.</p><p><br></p><p>The migration team decided to use containers and the Kubernetes Engine. This migration strategy is called Improve and Move. -&gt;&nbsp;Correct. Improve and Move is a migration strategy where the company optimizes the workload before moving it to the cloud. In this case, the migration team decided to use containers and Kubernetes Engine, which would involve making significant changes to the application architecture.</p><p><br></p><p>The migration team decided not to make unnecessary changes before moving this application to the cloud. This migration strategy is called Improve and Move. -&gt; Incorrect. It describes the Lift and Shift strategy.</p><p><br></p><p>The migration team decided to use containers and the Kubernetes Engine. This migration strategy is called Lift and Shift. -&gt;&nbsp;Incorrect. It describes the Improve and Move strategy.</p><p><br></p><p>The migration team decided to use containers and the Kubernetes Engine. This migration strategy is called Remove and Replace. -&gt;&nbsp;Incorrect. It describes a migration strategy that is not relevant to the scenario described in the question.</p><p><br></p><p>https://cloud.google.com/architecture/migration-to-gcp-getting-started#lift_and_shift</p><p>https://cloud.google.com/architecture/migration-to-gcp-getting-started#improve_and_move</p>",
    "correct_response": ["a", "b"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 82163928,
    "question_plain": "Your organization has a large monolithic application running on-premises that has become difficult to maintain and scale. The application has a relational database backend and a web-based frontend. You have been tasked with migrating this application to Google Cloud Platform (GCP) and breaking it down into microservices. Which of the following strategies should you use?",
    "answers": [
      "<p>Refactor the application into microservices on-premises, then migrate each microservice to GCP one at a time.</p>",
      "<p>Lift-and-shift migration, moving the application to GCP as-is, and then refactoring it into microservices.</p>",
      "<p>Use Anthos Migrate to containerize the application and migrate it to GCP.</p>",
      "<p>Use Cloud Endpoints to break down the monolithic application into microservices, then migrate to GCP.</p>"
    ],
    "explanation": "<p>Refactor the application into microservices on-premises, then migrate each microservice to GCP one at a time. -&gt;&nbsp;Correct. Refactoring the application into microservices before migration allows you to manage and mitigate issues on a smaller scale and reduces risk during the migration. Each microservice can be tested and validated independently.</p><p><br></p><p>Lift-and-shift migration, moving the application to GCP as-is, and then refactoring it into microservices. -&gt;&nbsp;Incorrect. While this strategy will get your application onto GCP quickly, refactoring a monolithic application into microservices while it is running could be disruptive and increase the complexity of the migration.</p><p><br></p><p>Use Anthos Migrate to containerize the application and migrate it to GCP. -&gt;&nbsp;Incorrect. While Anthos Migrate can be used to containerize and migrate applications, it doesn't break down monolithic applications into microservices.</p><p><br></p><p>Use Cloud Endpoints to break down the monolithic application into microservices, then migrate to GCP. -&gt;&nbsp;Incorrect. Cloud Endpoints is a tool for developing, deploying, protecting, and monitoring APIs, not for breaking down monolithic applications into microservices.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163930,
    "question_plain": "As a cloud architect, you set up the optimal combination of CPU and memory resources for nodes in a Kubernetes cluster. You want to be notified whenever CPU utilization exceeds 80% for 5 minutes or when memory utilization exceeds 90% for 1 minute. What do you need to specify to receive such notifications?",
    "answers": [
      "<p>An alerting policy</p>",
      "<p>An alerting condition</p>",
      "<p>Cloud Pub/Sub topic</p>",
      "<p>A logging message specification</p>"
    ],
    "explanation": "<p>An alerting policy -&gt; Correct. An alerting policy allows you to define conditions and thresholds for triggering notifications or alerts based on specific metrics or events. In this case, you would set up an alerting policy to monitor CPU utilization and memory utilization in the Kubernetes cluster and specify the conditions for triggering notifications when the thresholds are exceeded.</p><p><br></p><p>An alerting condition -&gt;&nbsp;Incorrect. While an alerting condition is related to the concept of alerts, it alone does not provide the mechanism to receive notifications. It is part of an alerting policy and defines the specific conditions that need to be met for an alert to be triggered.</p><p><br></p><p>Cloud Pub/Sub topic -&gt;&nbsp;Incorrect. Cloud Pub/Sub is a messaging service provided by Google Cloud Platform, and it is not directly related to receiving notifications for resource utilization in a Kubernetes cluster.</p><p><br></p><p>A logging message specification -&gt;&nbsp;Incorrect. A logging message specification is not directly related to receiving notifications for resource utilization. Logging messages are typically used for recording events and activities in a system, and they are not the appropriate mechanism for real-time notifications based on resource utilization thresholds.</p><p><br></p><p>https://cloud.google.com/monitoring/alerts</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163932,
    "question_plain": "Your company's finance department has notified you that logs generated by a financial application will need to be kept for five years. It is not likely to be accessed, but it has to be available if needed within three days. How would you recommend storing that data?",
    "answers": [
      "<p>You should export it to Cloud Storage and use Coldline class storage.</p>",
      "<p>You should store it in Cloud Logging.</p>",
      "<p>You should export it to BigQuery.</p>",
      "<p>You should export it to Cloud Pub/Sub.</p>"
    ],
    "explanation": "<p>You should export it to Cloud Storage and use Coldline class storage. -&gt; Correct. Cloud Storage provides durable and scalable object storage for various use cases. The Coldline storage class in Cloud Storage is specifically designed for long-term data archival with infrequent access. By exporting the logs to Cloud Storage and using the Coldline storage class, you can ensure cost-effective storage for the logs that need to be kept for five years while still maintaining availability if needed within three days.</p><p><br></p><p>You should store it in Cloud Logging. -&gt; Incorrect. Cloud Logging is a service provided by Google Cloud for collecting, storing, and analyzing logs and can be used for real-time monitoring and analysis. However, it may not be the most cost-effective or scalable solution for long-term archival storage of logs.</p><p><br></p><p>You should export it to BigQuery. -&gt; Incorrect. BigQuery is a fully managed data warehouse and analytics platform, which provides powerful querying and analysis capabilities. While it can handle large volumes of data and support long-term storage, it may not be the most cost-effective solution if the logs are not expected to be frequently accessed or analyzed.</p><p><br></p><p>You should export it to Cloud Pub/Sub. -&gt; Incorrect. Cloud Pub/Sub is a messaging service that provides reliable, real-time messaging between applications. While it can be used for distributing messages and events, it is not designed for long-term storage and retrieval of logs.</p><p><br></p><p>https://cloud.google.com/storage/docs/storage-classes#coldline</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163934,
    "question_plain": "A shipment tracking application receives data from sensors. Sometimes more data arrives than the virtual machines can process. As a cloud architect, you don't want to use additional virtual machines and you also need the most economical solution. What can you do to prevent data loss?",
    "answers": [
      "<p>You should write data to the Cloud Pub/Sub queue, and the application should read data from the queue.</p>",
      "<p>You should write data to local SSDs on the Compute Engine virtual machines.</p>",
      "<p>You should write data to Cloud Memorystore, and the application should read data from the cache.</p>",
      "<p>You should increase the CPU.</p>"
    ],
    "explanation": "<p>You should write data to the Cloud Pub/Sub queue, and the application should read data from the queue. -&gt; Correct. It provides a scalable and economical solution for handling spikes in data volume. Cloud Pub/Sub is a fully-managed messaging service that allows applications to send and receive messages between independent systems. It can handle a high volume of messages and can scale automatically to handle spikes in traffic. By writing data to a Cloud Pub/Sub queue, the application can read the data as it is able to process it. This ensures that data is not lost and that the application can keep up with incoming data even during spikes in volume.</p><p><br></p><p>You should write data to local SSDs on the Compute Engine virtual machines. -&gt; Incorrect.&nbsp; It is not a scalable solution because it does not provide a way to distribute the data across multiple machines. In addition, local SSDs are not persistent, so data can be lost if the virtual machine fails or is restarted.</p><p><br></p><p>You should write data to Cloud Memorystore, and the application should read data from the cache. -&gt; Incorrect. It is not a good solution for this scenario because it is a caching service, not a messaging service. Caching services are designed to store frequently-accessed data to improve application performance. They are not intended to handle spikes in data volume and are not a reliable mechanism for storing large volumes of data.</p><p><br></p><p>You should increase the CPU. -&gt; Incorrect. It is not a good solution for this scenario because it does not address the underlying problem of handling spikes in data volume. Increasing the CPU may help the application process data faster, but it does not provide a mechanism for handling large volumes of incoming data.</p><p><br></p><p>https://cloud.google.com/pubsub/docs/create-topic-console</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163936,
    "question_plain": "As a cloud architect responsible for preparing a migration strategy, you are confronted with a scenario where a company possesses sensitive data that must be encrypted using keys under their control. The objective is to store the data in GCP while minimizing costs and operational overhead. What recommendations would you propose in this scenario?",
    "answers": [
      "<p>You should use Cloud KMS for sensitive data.</p>",
      "<p>You should use Google default encryption for the data.</p>",
      "<p>You should use a custom encryption algorithm for the data.</p>",
      "<p>You cannot use your own keys with GCP.</p>"
    ],
    "explanation": "<p>You should use Cloud KMS for sensitive data. -&gt; Correct. Cloud Key Management Service (KMS) is a fully managed service that allows you to encrypt data at rest with your own encryption keys. By using Cloud KMS, you can encrypt sensitive data stored in GCP and manage your own encryption keys without the operational overhead of managing your own key management system.</p><p><br></p><p>You should use Google default encryption for the data. -&gt; Incorrect. Google default encryption, on the other hand, is a feature that encrypts data by default using a unique encryption key created by Google, and is useful for preventing unauthorized access to data in the event of a data breach. However, if you need to control the encryption keys and have the ability to rotate, revoke or manage access to them, then you should use Cloud KMS.</p><p><br></p><p>You should use a custom encryption algorithm for the data. -&gt; Incorrect. Using a custom encryption algorithm is not recommended, as it can be difficult to manage and may not be as secure as using a standard, tested encryption algorithm.</p><p><br></p><p>You cannot use your own keys with GCP. -&gt; Incorrect. It is important to note that GCP does allow you to bring your own encryption keys for many of its services, including Cloud Storage, Cloud SQL, and Cloud Bigtable, but not for all services.</p><p><br></p><p>https://cloud.google.com/kms/docs/create-encryption-keys</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163938,
    "question_plain": "Your organization has multiple applications deployed across different environments (development, staging, production) in Google Cloud. Recently, a misconfiguration in a configuration file led to a major issue in the production environment. To avoid such incidents in the future, what should you do?",
    "answers": [
      "<p>Use Google Cloud Build to automate builds and apply unit tests to configuration files.</p>",
      "<p>Enforce manual reviews for all configuration files before deployment.</p>",
      "<p>Utilize Google Cloud Deployment Manager to automate deployment of resources and configuration files.</p>",
      "<p>Utilize Google Cloud Shell to execute configuration changes.</p>"
    ],
    "explanation": "<p>Use Google Cloud Build to automate builds and apply automatic testing of configuration files. -&gt;&nbsp;Correct. Cloud Build allows for the creation of a continuous integration/continuous deployment (CI/CD) pipeline, which can include automatic testing of configuration files. This approach can identify and prevent errors before deployment to the production environment.</p><p><br></p><p>Enforce manual reviews for all configuration files before deployment. -&gt;&nbsp;Incorrect. While manual reviews can help, they are not foolproof, and errors can still be overlooked. This approach is also time-consuming and doesn't scale well.</p><p><br></p><p>Utilize Google Cloud Deployment Manager to automate deployment of resources and configuration files. -&gt;&nbsp;Incorrect. While Deployment Manager can automate the deployment process, it doesn't inherently prevent or mitigate the risk of configuration errors.</p><p><br></p><p>Utilize Google Cloud Shell to execute configuration changes. -&gt;&nbsp;Incorrect. Cloud Shell is more of an interactive shell environment and doesn't inherently provide mechanisms to prevent or catch configuration errors.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163940,
    "question_plain": "Your company has a hybrid cloud computing model and your current network connection is using 50% of&nbsp;bandwidth. As a cloud architect, you are concerned that you only have one connection that you might lose in the event of a failure. What would you do to minimize this risk?",
    "answers": [
      "<p>You should use redundant network connections between the on-premises data center and GCP.</p>",
      "<p>You should increase the bandwidth of your current network connection. </p>",
      "<p>You should increase the number of virtual machines for your workload.</p>",
      "<p>You should increase the performance of virtual machine disks.</p>"
    ],
    "explanation": "<p>You should use redundant network connections between the on-premises data center and GCP. -&gt; Correct. Using redundant network connections between the on-premises data center and Google Cloud Platform helps to minimize the risk of network failure. This can be achieved by configuring multiple connections and using load balancing to distribute the traffic across them. If one connection fails, the traffic is automatically rerouted to the other connection, ensuring that the network remains available.</p><p><br></p><p>You should increase the bandwidth of your current network connection.&nbsp; -&gt; Incorrect. It may help to improve network performance, but it does not address the risk of network failure.</p><p><br></p><p>You should increase the number of virtual machines for your workload. -&gt; Incorrect. It may improve workload performance, but it does not address the risk of network failure.</p><p><br></p><p>You should increase the performance of virtual machine disks. -&gt; Incorrect. It may help to improve disk performance, but it does not address the risk of network failure.</p><p><br></p><p>https://cloud.google.com/network-connectivity/docs/interconnect/how-to/dedicated/creating-redundant-interconnect</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163942,
    "question_plain": "A Managed Instance Group adds more virtual machines than necessary, and then shuts them down. This pattern is repeated many times. What would you do to stabilize adding and removing virtual machines?",
    "answers": [
      "<p>You should increase the time autoscalers consider when making decisions.</p>",
      "<p>You should decrease the time autoscalers consider when making decisions.</p>",
      "<p>You should increase the maximum number of virtual machines in a Managed Instance Group.</p>",
      "<p>You should decrease the maximum number of virtual machines in a Managed Instance Group.</p>"
    ],
    "explanation": "<p>You should increase the time autoscalers consider when making decisions. -&gt;&nbsp;Correct. It is correct because the autoscaler is adding and removing virtual machines too quickly, causing instability in the Managed Instance Group. By increasing the time the autoscaler considers when making decisions, it will allow for more stable and gradual changes in the number of virtual machines added and removed from the group.</p><p><br></p><p>You should decrease the time autoscalers consider when making decisions. -&gt; Incorrect. Decreasing the time autoscalers consider when making decisions, would further exacerbate the issue by causing even quicker changes in the number of virtual machines added and removed from the group.</p><p><br></p><p>You should increase the maximum number of virtual machines in a Managed Instance Group. -&gt; Incorrect. Increasing the maximum number of virtual machines in a Managed Instance Group, is not relevant to the issue at hand. The problem is not with the maximum number of virtual machines but with the instability caused by the rapid adding and removing of virtual machines.</p><p><br></p><p>You should decrease the maximum number of virtual machines in a Managed Instance Group. -&gt; Incorrect. Decreasing the maximum number of virtual machines in a Managed Instance Group, would limit the capacity of the group and not address the issue of instability caused by rapid changes in the number of virtual machines.</p><p><br></p><p>https://cloud.google.com/compute/docs/autoscaler</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163944,
    "question_plain": "As a cloud architect, you are responsible for implementing a new application using a microservices architecture. You would like to run each microservice in containers. In addition, you want to minimize DevOps overhead and benefit from autoscaling. What should you recommend?",
    "answers": [
      "<p>You should run the containers in Kubernetes Engine.</p>",
      "<p>You should run the containers in Managed Instance Group.</p>",
      "<p>You should run the containers in Unmanaged Instance Group.</p>",
      "<p>You should use App Engine for this.</p>"
    ],
    "explanation": "<p>You should run the containers in Kubernetes Engine. -&gt; Correct. Kubernetes Engine is a managed Kubernetes service provided by Google Cloud. It allows you to deploy and manage containers at scale, providing features like container orchestration, automatic scaling, load balancing, and automated rollout and rollback capabilities. Running microservices in containers on Kubernetes Engine enables you to minimize DevOps overhead and benefit from autoscaling, making it an ideal choice for a microservices architecture.</p><p><br></p><p>You should run the containers in Managed Instance Group. -&gt;&nbsp;Incorrect. Managed Instance Groups are primarily used for managing groups of virtual machine instances, providing features like autoscaling and load balancing at the virtual machine level. While you can run containers within virtual machines, it does not provide the native container orchestration and management capabilities of Kubernetes Engine.</p><p><br></p><p>You should run the containers in Unmanaged Instance Group. -&gt;&nbsp;Incorrect. Unmanaged Instance Groups are similar to Managed Instance Groups, but they require more manual configuration and management of virtual machine instances. They do not provide the native container orchestration and management capabilities required for running containers at scale.</p><p><br></p><p>You should use App Engine for this. -&gt;&nbsp;Incorrect. App Engine is a platform-as-a-service (PaaS) offering that provides a runtime environment for running applications without needing to manage the underlying infrastructure. While App Engine supports running applications in containers using the flexible environment, it may not provide the same level of control and flexibility required for a microservices architecture.</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/quickstart</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163946,
    "question_plain": "For your application to function properly, it necessitates a filesystem that can be mounted using operating system commands. Furthermore, this filesystem must be accessible from numerous virtual machine instances. Which Google Cloud Platform (GCP) service would you suggest?",
    "answers": [
      "<p>Cloud Filestore</p>",
      "<p>Cloud SQL</p>",
      "<p>Cloud Spanner</p>",
      "<p>Cloud Firestore</p>"
    ],
    "explanation": "<p>Cloud Filestore -&gt; Correct. Cloud Filestore is a managed file storage service that makes it easy for applications to access fully managed NFS file shares. The service can be used to create a filesystem that can be mounted using operating system commands and can be shared between multiple virtual machine instances. </p><p><br></p><p>Cloud SQL -&gt;&nbsp;Incorrect. Cloud SQL is a fully-managed relational database that supports SQL. It does not provide a file system that can be mounted using operating system commands, so it wouldn't be suitable for this use-case.</p><p><br></p><p>Cloud Spanner -&gt;&nbsp;Incorrect. This is a globally distributed, horizontally scalable database service. It's not a file system, and therefore doesn't fit the requirement of being mounted using OS commands.</p><p><br></p><p>Cloud Firestore -&gt; Incorrect. This is a NoSQL document database and, like Cloud Spanner and Cloud SQL, doesn't provide the file system capabilities required.</p><p><br></p><p>https://cloud.google.com/filestore/docs/create-instance-console</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163948,
    "question_plain": "As a cloud architect, you have the following three options for deploying MySQL database:1. Use Cloud SQL.2. Use Cloud Marketplace with click-to-deploy interface to install MySQL onto a Compute Engine instance.3. Manually install and customize MySQL on your Compute Engine instance.You want to minimize administrative duties as much as possible. Which option should you use?",
    "answers": [
      "<p>1</p>",
      "<p>2</p>",
      "<p>3</p>",
      "<p>All options have the same administrative duties.</p>"
    ],
    "explanation": "<p>1 -&gt; Correct. Using Cloud SQL minimizes administrative duties as it is a fully managed service, which means Google Cloud takes care of many of the administrative tasks, such as patching, backups, and updates. Other options require more administrative duties as they involve managing and maintaining the infrastructure of the Compute Engine instance that hosts MySQL.</p><p><br></p><p>https://cloud.google.com/architecture/setup-mysql?hl=en#how_to_choose_the_right_mysql_deployment_option</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163950,
    "question_plain": "Anonymous users from all over the world access a public information website hosted in an on-premises data center. The servers that host this website are older, and users are complaining about slow response times. There has also been an increase in distributed denial-of-service attacks targeting a website in recent times. Attacks always come from the same IP address ranges. The management has identified the public information website as an easy, low risk application to migrate to Google Cloud. You need to improve access latency and provide a security solution that will prevent the denial-of-service traffic from entering your Virtual Private Cloud (VPC) network. What should you do?",
    "answers": [
      "<p>You should deploy an external HTTP(S) load balancer, configure Google Cloud Armor, and move the application onto Compute Engine virtual machines.</p>",
      "<p>You should deploy an external HTTP(S) load balancer, configure VPC firewall rules, and move the applications onto Compute Engine virtual machines.</p>",
      "<p>You should containerize the application and move it into Google Kubernetes Engine (GKE). Create a GKE service to expose the pods within the cluster, and set up a GKE network policy.</p>",
      "<p>You should containerize the application and move it into Google Kubernetes Engine (GKE). Create an internal load balancer to expose the pods outside the cluster, and configure Identity-Aware Proxy (IAP) for access.</p>"
    ],
    "explanation": "<p>You should deploy an external HTTP(S) load balancer, configure Google Cloud Armor, and move the application onto Compute Engine virtual machines. -&gt;&nbsp;Correct. The external HTTP(s) load balancer will improve access latency and Cloud Armor can be configured to block the Distributed Denial-of-Service (DDoS) attack.</p><p><br></p><p>You should deploy an external HTTP(S) load balancer, configure VPC firewall rules, and move the applications onto Compute Engine virtual machines. -&gt; Incorrect. Firewall rules do not block malicious traffic into a VPC but rather block it at the VM level.</p><p><br></p><p>You should containerize the application and move it into Google Kubernetes Engine (GKE). Create a GKE service to expose the pods within the cluster, and set up a GKE network policy. -&gt; Incorrect. GKE service does not expose a set of pods outside of a cluster and a GKE network policy only filters traffic between pods and services.</p><p><br></p><p>You should containerize the application and move it into Google Kubernetes Engine (GKE). Create an internal load balancer to expose the pods outside the cluster, and configure Identity-Aware Proxy (IAP) for access.-&gt;Incorrect. GKE internal load balancer will not load balance external traffic and anonymous users need access to the website so IAP is not a fit.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163952,
    "question_plain": "Refer to the EHR Healthcare case study for this question: https://services.google.com/fh/files/blogs/master_case_study_ehr_healthcare.pdfEHR is interested in establishing a connection between one of its data centers and Google Cloud. However, the data center is situated in a remote location that is over 100 kilometers away from a Google-owned point of presence (POP). Budget constraints prevent them from acquiring new hardware, but&nbsp; an existing firewall can accommodate future throughput growth. Additionally, they provided the following information:communication is required between servers in their on-premises data center and Google Kubernetes Engine (GKE) resources in the cloudboth on-premises servers and cloud resource are set up with private RFC 1918 IP addressesthe service provider has notified the customer that basic internet connectivity is provided as a best-effort service and does not come with any service level agreement (SLA)In your role as a cloud architect, what connectivity option would you recommend?",
    "answers": [
      "<p>Provision a Partner Interconnect connection.</p>",
      "<p>Provision a new Internet connection.</p>",
      "<p>Provision Carrier Peering.</p>",
      "<p>Provision a Dedicated Interconnect connection.</p>"
    ],
    "explanation": "<p>Provision a Partner Interconnect connection. -&gt;&nbsp;Correct. It allows the customer to lower latency by connecting directly to a partner network that is directly connected to Google.</p><p><br></p><p>Provision Carrier Peering. -&gt;&nbsp;Incorrect. It does not give private IP addressing across the connection.</p><p><br></p><p>Provision a new Internet connection. -&gt; Incorrect. An additional Internet connection will not provide RFC1918 communications by itself.</p><p><br></p><p>Provision a Dedicated Interconnect connection. -&gt; Incorrect. Dedicated Interconnect would require the customer to buy new hardware to get a 10 gig interface for their firewall.</p><p><br></p><p>https://cloud.google.com/network-connectivity/docs/interconnect/concepts/partner-overview</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163954,
    "question_plain": "In your role as a cloud architect, you are in the process of designing a hybrid environment that is future-proof and necessitates a network connection between Google Cloud and your on-premises infrastructure. Your objective is to guarantee compatibility between the Google Cloud environment you are designing and your existing on-premises network environment. What course of action should you take?",
    "answers": [
      "<p>You should create a network plan for your VPC in Google Cloud that uses non-overlapping CIDR ranges with your on-premises environment. Use a Cloud Interconnect connection between your on-premises environment and Google Cloud.</p>",
      "<p>You should use the default VPC in your Google Cloud project. Use a Cloud VPN connection between your on-premises environment and Google Cloud.</p>",
      "<p>You should create a custom VPC in Google Cloud in auto mode. Use a Cloud VPN connection between your on-premises environment and Google Cloud.</p>",
      "<p>You should create a network plan for your VPC in Google Cloud that uses CIDR ranges that overlap with your on-premises environment. Use a Cloud Interconnect connection between your on-premises environment and Google Cloud.</p>"
    ],
    "explanation": "<p>You should create a network plan for your VPC in Google Cloud that uses non-overlapping CIDR ranges with your on-premises environment. Use a Cloud Interconnect connection between your on-premises environment and Google Cloud. -&gt;&nbsp;Correct. This ensures your on premises network is compatible with your Google Cloud VPC.</p><p><br></p><p>You should use the default VPC in your Google Cloud project. Use a Cloud VPN connection between your on-premises environment and Google Cloud. -&gt; Incorrect. The default VPC is a VPC with Auto Mode IP ranges.</p><p><br></p><p>You should create a custom VPC in Google Cloud in auto mode. Use a Cloud VPN connection between your on-premises environment and Google Cloud. -&gt; Incorrect. With Auto Mode IP Ranges there is no guarantee that the IP ranges will not overlap with your on premises environment, either now or in the future.</p><p><br></p><p>You should create a network plan for your VPC in Google Cloud that uses CIDR ranges that overlap with your on-premises environment. Use a Cloud Interconnect connection between your on-premises environment and Google Cloud. -&gt;&nbsp;Incorrect. To ensure correct routing, ranges cannot overlap between environments.</p><p><br></p><p>https://cloud.google.com/vpc/docs/vpc#ip-ranges</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163956,
    "question_plain": "In your role as a cloud architect, you are tasked with designing a large distributed application comprising 20 microservices. Each of these microservices must connect to the database backend. Your objective is to ensure the secure storage of credentials for these connections. What is the recommended storage location for the credentials?",
    "answers": [
      "<p>In a secret management system</p>",
      "<p>In the source code</p>",
      "<p>In an environment variable</p>",
      "<p>In a config file that has restricted access through ACLs</p>"
    ],
    "explanation": "<p>In a secret management system -&gt;&nbsp;Correct. A secret management system such as Secret Manager is a secure and convenient storage system for API keys, passwords, certificates, and other sensitive data. Secret Manager provides a central place and single source of truth to manage, access, and audit secrets across Google Cloud.</p><p><br></p><p>In the source code -&gt;&nbsp;Incorrect. Storing credentials in source code and source control is discoverable, in plain text, by anyone with access to the source code. This also introduces the requirement to update code and do a deployment each time the credentials are rotated.</p><p><br></p><p>In an environment variable -&gt; Incorrect. Consistently populating environment variables would require the credentials to be available, in plain text, when the session is started.</p><p><br></p><p>In a config file that has restricted access through ACLs -&gt; Incorrect. Instead of managing access to the config file and updating manually as keys are rotated, it would be better to leverage a key management system. Additionally, there is increased risk if the config file contains the credentials in plain text.</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/tutorials/workload-identity-secrets</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163958,
    "question_plain": "Refer to the TerramEarth case study for this question: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdfTo account for potential future use cases of the data collected by TerramEarth, a decision has been made to construct a system that captures and stores all raw data, ensuring its availability for future needs. As a cloud architect, what would be the most cost-effective approach to achieve this objective?",
    "answers": [
      "<p>You should have the vehicles in the field continue to dump data via FTP, and adjust the existing Linux machines to immediately upload it to Cloud Storage with <code>gsutil</code>.</p>",
      "<p>You should have the vehicles in the field stream the data directly into BigQuery.</p>",
      "<p>You should have the vehicles in the field pass the data to Cloud Pub/Sub and dump it into a Cloud Dataproc cluster that stores data in Apache Hadoop Distributed File System (HDFS) on persistent disks.</p>",
      "<p>You should have the vehicles in the field continue to dump data via FTP, adjust the existing Linux machines, and use a collector to upload them into Cloud Dataproc HDFS for storage</p>"
    ],
    "explanation": "<p>You should have the vehicles in the field continue to dump data via FTP, and adjust the existing Linux machines to immediately upload it to Cloud Storage with <code>gsutil</code>. -&gt;&nbsp;Several load-balanced Compute Engine VMs would suffice to ingest 9 TB per day, and Cloud Storage is the cheapest per-byte storage offered by Google. Depending on the format, the data could be available via BigQuery immediately, or shortly after running through an ETL job. Thus, this solution meets business and technical requirements while optimizing for cost.</p><p><br></p><p>You should have the vehicles in the field stream the data directly into BigQuery. -&gt; Incorrect. TerramEarth has cellular service for 200,000 vehicles, and each vehicle sends at least one row (120 fields) per second. This exceeds BigQuery's maximum rows per second per project quota[1]. Additionally, there are 20 million total vehicles, most of which perform uploads when connected by a maintenance port, which drastically exceeds the streaming project quota further.</p><p><br></p><p>You should have the vehicles in the field pass the data to Cloud Pub/Sub and dump it into a Cloud Dataproc cluster that stores data in Apache Hadoop Distributed File System (HDFS) on persistent disks. -&gt; Incorrect. Although Cloud Pub/Sub is a fine choice for this application, Cloud Dataproc is probably not. The question posed asks us to optimize for cost. Because Cloud Dataproc is optimized for ephemeral, job-scoped clusters, a long-running cluster with large amounts of HDFS storage could be very expensive to build and maintain when compared to managed and specialized storage solutions like Cloud Storage.</p><p><br></p><p>You should have the vehicles in the field continue to dump data via FTP, adjust the existing Linux machines, and use a collector to upload them into Cloud Dataproc HDFS for storage -&gt;&nbsp;Incorrect. The question asks us to optimize for cost, and because Cloud Dataproc is optimized for ephemeral, job-scoped clusters, a long-running cluster with large amounts of HDFS storage could be very expensive to build and maintain when compared to managed and specialized storage solutions like Cloud Storage.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163960,
    "question_plain": "To implement a disaster recovery plan, your company is currently working on replicating its production MySQL database from a private data center to a GCP project utilizing a Google Cloud VPN connection. However, there are latency issues and a minor packet loss occurring during the replication process, causing disruptions. What course of action should they take?",
    "answers": [
      "<p>Configure a Google Cloud Dedicated Interconnect.</p>",
      "<p>Configure their replication to use UDP.</p>",
      "<p>Restore their database daily using Google Cloud SQL.</p>",
      "<p>Add additional VPN connections and load balance them.</p>"
    ],
    "explanation": "<p>Configure a Google Cloud Dedicated Interconnect. -&gt; Correct. Google Cloud Dedicated Interconnect is a dedicated, high-speed, low-latency connectivity option that enables data replication between on-premises data centers and GCP projects. It provides a direct physical connection between your on-premises network and Google's network, which can reduce latency and packet loss. In this case, using Google Cloud Dedicated Interconnect would likely resolve the latency issues and packet loss that the company is experiencing with their VPN connection.</p><p><br></p><p>Configure their replication to use UDP. -&gt; Incorrect. It may help reduce latency, but it would not necessarily address the packet loss issue.</p><p><br></p><p>Restore their database daily using Google Cloud SQL. -&gt; Incorrect. It is not a real-time replication solution and would not provide the same level of protection as real-time replication.</p><p><br></p><p>Add additional VPN connections and load balance them. -&gt; Incorrect. It would not necessarily address the underlying latency and packet loss issues and could add additional complexity and cost.</p><p><br></p><p>https://cloud.google.com/network-connectivity/docs/interconnect/concepts/dedicated-overview</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163962,
    "question_plain": "In your Compute Engine managed instance group, an outage has occurred where all instances are continuously restarting every 6 seconds. Although you have a configured health check, autoscaling is currently disabled. To address this issue, your Linux expert colleague has offered to investigate. Your task is to ensure that your colleague has appropriate access to the VMs for troubleshooting purposes. What should you do?",
    "answers": [
      "<p>Disable the health check for the instance group. Add his SSH key to the project-wide SSH keys.</p>",
      "<p>Disable autoscaling for the instance group. Add his SSH key to the project-wide SSH keys.</p>",
      "<p>Perform a rolling restart on the instance group.</p>",
      "<p>Grant your colleague the IAM role of project Viewer.</p>"
    ],
    "explanation": "<p>Disable the health check for the instance group. Add his SSH key to the project-wide SSH Keys. -&gt; Correct. By disabling the health check, the instances will stop restarting every 6 seconds. This allows your colleague to access the VMs without being interrupted by the restart process. Adding the colleague's SSH key to the project-wide SSH keys grants them access to the VMs for troubleshooting purposes.</p><p><br></p><p>Disable autoscaling for the instance group. Add his SSH key to the project-wide SSH keys. -&gt; Incorrect. Disabling autoscaling does not directly address the issue of continuous restarts every 6 seconds. It is the health check that triggers the restarts, so disabling the health check is the more appropriate action. Adding the SSH key to the project-wide SSH keys is still valid for granting access to the colleague.</p><p><br></p><p>Perform a rolling restart on the instance group. -&gt; Incorrect. Performing a rolling restart on the instance group might not address the underlying cause of the continuous restarts. It could disrupt the VMs further and might not be necessary if the issue is related to the health check.</p><p><br></p><p>Grant your colleague the IAM role of project Viewer. -&gt; Incorrect. Granting the project Viewer role would provide read-only access to the project but may not necessarily address the issue at hand. It is more important to disable the health check and provide appropriate access for troubleshooting purposes.</p><p><br></p><p>https://cloud.google.com/compute/docs/connect/add-ssh-keys</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163964,
    "question_plain": "As a cloud architect, you are tasked with designing an architecture for an application that will operate on Compute Engine. It is essential to create a disaster recovery plan that ensures the application can seamlessly switch to another region in the event of a regional outage. What course of action should you take?",
    "answers": [
      "<p>You should deploy the application on two Compute Engine instance groups, each in the same project but in a different region. Use the first instance group to serve traffic, and use the HTTP load balancing service to fail over to the standby instance group in case of a disaster.</p>",
      "<p>You should deploy the application on two Compute Engine instances in the same project but in a different region. Use the first instance to serve traffic, and use the HTTP load balancing service to fail over to the standby instance in case of a disaster.</p>",
      "<p>You should deploy the application on a Compute Engine instance. Use the instance to serve traffic, and use the HTTP load balancing service to fail over to an instance on your premises in case of a disaster.</p>",
      "<p>You should deploy the application on two Compute Engine instance groups, each in a separate project and a different region. Use the first instance group to serve traffic, and use the HTTP load balancing service to fail over to the standby instance group in case of a disaster.</p>"
    ],
    "explanation": "<p>You should deploy the application on two Compute Engine instance groups, each in the same project but in a different region. Use the first instance group to serve traffic, and use the HTTP load balancing service to fail over to the standby instance group in case of a disaster. -&gt; Correct. It provides the best solution for regional disaster recovery with minimal downtime. By deploying the application on two Compute Engine instance groups in different regions, traffic can be routed to the healthy instance group if one region becomes unavailable. HTTP load balancing can automatically detect a regional outage and redirect traffic to the healthy instance group in another region.</p><p><br></p><p>You should deploy the application on two Compute Engine instances in the same project but in a different region. Use the first instance to serve traffic, and use the HTTP load balancing service to fail over to the standby instance in case of a disaster. -&gt;&nbsp;Incorrect. Deploying the application on two Compute Engine instances in the same project but in a different region, does not provide the same level of redundancy as deploying it on instance groups. Instance groups provide the ability to automatically manage and scale a set of VM instances as a single entity. Additionally, instance groups can automatically restart failed instances, which can help minimize downtime in case of a failure.</p><p><br></p><p>You should deploy the application on a Compute Engine instance. Use the instance to serve traffic, and use the HTTP load balancing service to fail over to an instance on your premises in case of a disaster. -&gt; Incorrect. Deploying the application on a Compute Engine instance and using the HTTP load balancing service to fail over to an instance on your premises in case of a disaster, introduces more complexity into the architecture, as it requires a network connection between your on-premises infrastructure and the Google Cloud Platform (GCP) infrastructure. It also introduces additional network latency that may affect the performance of the application.</p><p><br></p><p>You should deploy the application on two Compute Engine instance groups, each in a separate project and a different region. Use the first instance group to serve traffic, and use the HTTP load balancing service to fail over to the standby instance group in case of a disaster. -&gt; Incorrect. Deploying the application on two Compute Engine instance groups in separate projects, adds additional complexity to the architecture and requires cross-project network connectivity. This can increase the risk of misconfiguration or failure in the network, which could lead to additional downtime in case of a disaster.</p><p><br></p><p>https://cloud.google.com/architecture/disaster-recovery</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163966,
    "question_plain": "As a cloud architect, your role involves developing an application using various microservices that should remain internal to the cluster. Your objective is to configure each microservice with a designated number of replicas. Additionally, you aim to establish a uniform addressing mechanism where any microservice can access a specific microservice, irrespective of its scaling level. This solution needs to be implemented on Google Kubernetes Engine. What course of action should you take in this situation?",
    "answers": [
      "<p>Deploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service DNS name to address it from other microservices within the cluster.</p>",
      "<p>Deploy each microservice as a Deployment. Expose the Deployment in the cluster using an Ingress, and use the Ingress IP address to address the Deployment from other microservices within the cluster.</p>",
      "<p>Deploy each microservice as a Pod. Expose the Pod in the cluster using a Service, and use the Service DNS name to address the microservice from other microservices within the cluster.</p>",
      "<p>Deploy each microservice as a Pod. Expose the Pod in the cluster using an Ingress, and use the Ingress IP address name to address the Pod from other microservices within the cluster.</p>"
    ],
    "explanation": "<p>Deploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service DNS name to address it from other microservices within the cluster. -&gt; Correct. In a Kubernetes cluster, a Deployment is used to manage a set of identical Pods. Each Pod represents a single instance of a microservice. By deploying each microservice as a Deployment, the cloud architect can ensure that each microservice is replicated to the desired number of instances. To access the microservices, the cloud architect can expose each Deployment within the cluster using a Service. A Service provides a stable DNS name that can be used to address the microservice, regardless of the number of replicas. This allows other microservices within the cluster to access a specific microservice in a uniform way.</p><p><br></p><p>Deploy each microservice as a Deployment. Expose the Deployment in the cluster using an Ingress, and use the Ingress IP address to address the Deployment from other microservices within the cluster. -&gt; Incorrect. It is incorrect because an Ingress is used to expose HTTP and HTTPS routes from outside the cluster to services within the cluster. It is not appropriate for addressing microservices within the cluster.</p><p><br></p><p>Deploy each microservice as a Pod. Expose the Pod in the cluster using a Service, and use the Service DNS name to address the microservice from other microservices within the cluster. -&gt; Incorrect. It is incorrect because deploying each microservice as a Pod does not provide a way to ensure that the microservice is replicated to the desired number of instances. Using a Service to expose the Pod would provide a stable DNS name but would not ensure that the microservice is replicated.</p><p><br></p><p>Deploy each microservice as a Pod. Expose the Pod in the cluster using an Ingress, and use the Ingress IP address name to address the Pod from other microservices within the cluster. -&gt; Incorrect. It is incorrect because an Ingress is not appropriate for addressing microservices within the cluster.</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/how-to/deploying-workloads-overview</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163968,
    "question_plain": "Following your company's acquisition of another company, you have been assigned the task of integrating their existing Google Cloud environment with your company's data center. Upon examination, you uncover that certain RFC 1918 IP address ranges employed in the new company's Virtual Private Cloud (VPC) conflict with the IP address space utilized in your data center. What steps should you take to establish connectivity and prevent any routing conflicts once the connectivity is established?",
    "answers": [
      "<p>Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply new IP addresses so there is no overlapping IP space.</p>",
      "<p>Create a Cloud VPN connection from the new VPC to the data center, and create a Cloud NAT instance to perform NAT on the overlapping IP space.</p>",
      "<p>Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply a custom route advertisement to block the overlapping IP space.</p>",
      "<p>Create a Cloud VPN connection from the new VPC to the data center, and apply a firewall rule that blocks the overlapping IP space.</p>"
    ],
    "explanation": "<p>Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply new IP addresses so there is no overlapping IP space. -&gt;&nbsp;Correct. This approach ensures that both VPC networks can communicate with each other without any routing conflicts as the IP addresses are unique to each VPC network.</p><p><br></p><p>Create a Cloud VPN connection from the new VPC to the data center, and create a Cloud NAT instance to perform NAT on the overlapping IP space. -&gt; Incorrect. This solution is not recommended as it does not eliminate the routing conflict, but rather creates a workaround by performing network address translation.</p><p><br></p><p>Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply a custom route advertisement to block the overlapping IP space. -&gt; Incorrect. This option does not address the problem of routing conflicts and may prevent communication between the two VPC networks.</p><p><br></p><p>Create a Cloud VPN connection from the new VPC to the data center, and apply a firewall rule that blocks the overlapping IP space. -&gt; Incorrect. This approach also does not address the routing conflict problem and may prevent communication between the two VPC networks.</p><p><br></p><p>https://cloud.google.com/network-connectivity/docs/router</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163970,
    "question_plain": "You are in the process of setting up a single second-generation Cloud SQL MySQL database that holds crucial transactional data for the business. Your objective is to minimize data loss in the event of a significant failure. Which two functionalities should you incorporate in the solution?",
    "answers": [
      "<p>Binary logging</p>",
      "<p>Automated backups</p>",
      "<p>Sharding</p>",
      "<p>Read replicas</p>",
      "<p>Semisynchronous replication</p>"
    ],
    "explanation": "<p>Binary logging -&gt; Correct. It allows the database to record all changes to the data, and these logs can be used for backup, replication, or for auditing purposes. This feature can help in case of data loss by allowing the recovery of the database up to the point of the last recorded transaction.</p><p><br></p><p>Automated backups -&gt; Correct. It creates backups of the database at regular intervals, which can be used for disaster recovery in case of data loss. Automated backups allow for a fast and easy restore process, minimizing the amount of data that is lost in the event of a major failure.</p><p><br></p><p>Sharding -&gt; Incorrect. It splits the database into smaller, more manageable parts, allowing for improved performance and scalability. However, it does not directly address data loss.</p><p><br></p><p>Read replicas -&gt; Incorrect. It is used to improve read performance by creating copies of the database for read-only operations. It can help to improve performance, but it does not directly address data loss.</p><p><br></p><p>Semisynchronous replication -&gt; Incorrect. It provides increased data consistency by ensuring that at least one replica has received a transaction before it is committed. While it can help to minimize data loss, it does not directly address data loss in the event of a major failure.</p><p><br></p><p>https://cloud.google.com/sql/docs/mysql/backup-recovery/backups</p>",
    "correct_response": ["a", "b"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 82163972,
    "question_plain": "Your company has multiple GKE clusters running in different regions. You are tasked with setting up a centralized monitoring solution to have a single view of the health and performance of all the clusters. Which approach would you take?",
    "answers": [
      "<p>Use Anthos to centrally manage and monitor all GKE clusters.</p>",
      "<p>Deploy a Prometheus server in each GKE cluster and configure them to push metrics to a central Grafana dashboard.</p>",
      "<p>Use Google Cloud's Operations suite and set up a Workspace for each GKE cluster, then create a combined dashboard.</p>",
      "<p>Configure each GKE cluster to export logs and metrics to a central BigQuery dataset and create custom SQL queries to monitor the health.</p>"
    ],
    "explanation": "<p>Use Anthos to centrally manage and monitor all GKE clusters. -&gt;&nbsp;Correct. Anthos provides a centralized management platform for all GKE clusters, making it the best choice for this scenario.</p><p><br></p><p>Deploy a Prometheus server in each GKE cluster and configure them to push metrics to a central Grafana dashboard. -&gt; Incorrect. While Prometheus and Grafana are popular monitoring solutions, managing separate Prometheus instances for each cluster can be cumbersome.</p><p><br></p><p>Use Google Cloud's Operations suite and set up a Workspace for each GKE cluster, then create a combined dashboard. -&gt; Incorrect. Google Cloud's Operations suite allows creating Workspaces, but setting up a separate Workspace for each cluster doesn't provide a centralized view.</p><p><br></p><p>Configure each GKE cluster to export logs and metrics to a central BigQuery dataset and create custom SQL queries to monitor the health. -&gt; Incorrect. Exporting to BigQuery can be useful for long-term analysis but may not be the best real-time monitoring solution.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163974,
    "question_plain": "In your organization, there is an application that operates on multiple Compute Engine instances. The objective is to establish seamless communication between your application and an on-premises service, which demands high throughput, using internal IP addresses. The goal is to minimize latency as much as possible. As a cloud architect, what recommendations should you provide in this scenario?",
    "answers": [
      "<p>You should configure a Cloud Dedicated Interconnect connection between the on-premises environment and Google Cloud.</p>",
      "<p>You should use Cloud VPN to configure a VPN tunnel between the on-premises environment and Google Cloud.</p>",
      "<p>You should configure a direct peering connection between the on-premises environment and Google Cloud.</p>",
      "<p>You should use OpenVPN to configure a VPN tunnel between the on-premises environment and Google Cloud.</p>"
    ],
    "explanation": "<p>You should configure a Cloud Dedicated Interconnect connection between the on-premises environment and Google Cloud. -&gt; Correct. Cloud Dedicated Interconnect provides a direct, dedicated, and private network connection between your on-premises environment and Google Cloud. This connection offers high throughput and low latency, making it suitable for demanding workloads that require seamless communication with minimal delay. By using internal IP addresses, you can ensure that the communication between the application on Compute Engine instances and the on-premises service occurs over this dedicated and high-performance connection.</p><p><br></p><p>You should use Cloud VPN to configure a VPN tunnel between the on-premises environment and Google Cloud. -&gt; Incorrect. Cloud VPN establishes a secure virtual private network (VPN) tunnel over the public internet between your on-premises environment and Google Cloud. While it provides secure connectivity, it may introduce additional latency compared to a dedicated connection like Cloud Dedicated Interconnect.</p><p><br></p><p>You should configure a direct peering connection between the on-premises environment and Google Cloud. -&gt; Incorrect. Direct peering allows you to establish a direct network connection between your on-premises environment and Google Cloud using a peering provider. While direct peering can offer a fast and dedicated connection, it might not provide the same level of throughput and latency benefits as Cloud Dedicated Interconnect.</p><p><br></p><p>You should use OpenVPN to configure a VPN tunnel between the on-premises environment and Google Cloud. -&gt; Incorrect. OpenVPN is an open-source VPN software that can be used to establish a VPN tunnel between your on-premises environment and Google Cloud. While it provides secure connectivity, it may not offer the same level of performance, throughput, and low latency as Cloud Dedicated Interconnect.</p><p><br></p><p>https://cloud.google.com/network-connectivity/docs/interconnect</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163976,
    "question_plain": "You have been hired to design a disaster recovery solution for a customer's Google Cloud Platform (GCP) environment. The customer requires that the solution provide low Recovery Time Objective (RTO) and Recovery Point Objective (RPO) in the event of a disaster. Which of the following GCP services would you use to meet these requirements?",
    "answers": [
      "<p>Cloud Storage and Cloud Functions</p>",
      "<p>Cloud SQL and BigQuery</p>",
      "<p>Cloud Storage only</p>",
      "<p>Cloud Storage and Compute Engine with Persistent Disks</p>"
    ],
    "explanation": "<p>To answer this question, we need to understand the concepts of Recovery Time Objective (RTO) and Recovery Point Objective (RPO).</p><ol><li><p>RTO is the targeted duration of time within which a business process must be restored after a disaster (or disruption) in order to avoid unacceptable consequences associated with a break in that business process. Essentially, it's how quickly you need to be back up and running.</p></li><li><p>RPO is the maximum targeted period in which data (transactions) might be lost from an IT service due to a major incident. In other words, it's how much data you can afford to lose before it seriously impacts your operation.</p></li></ol><p><br></p><p>Cloud Storage and Compute Engine with Persistent Disks. -&gt;&nbsp;Correct.&nbsp;Cloud Storage is designed for 99.999999999% durability and has various storage class options (Multi-regional, regional, nearline, coldline, and archive) for different use cases and access frequencies. This will ensure that data is not lost (low RPO). Compute Engine with Persistent Disks is a service that provides virtual machines that can be quickly spun up and shut down. This is beneficial for disaster recovery because these instances can be quickly started to restore service (low RTO). Persistent disks are network storage devices that persist data independent of the life of an instance, which helps to prevent data loss.</p><p><br></p><p>Cloud Storage and Cloud Functions -&gt;&nbsp;Incorrect. Cloud Functions is a serverless execution environment for building and connecting cloud services but it doesn't directly support a disaster recovery solution.</p><p><br></p><p>Cloud SQL and BigQuery -&gt;&nbsp;Incorrect. Cloud SQL and BigQuery are focused on database services, but they alone cannot ensure low RTO and RPO.</p><p><br></p><p>Cloud Storage only -&gt;&nbsp;Incorrect. It would be good for ensuring low RPO (minimal data loss), but it doesn't address the RTO requirement to quickly restore services.</p><p><br></p><p>https://cloud.google.com/compute/docs/disks#pdspecs</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163978,
    "question_plain": "You are designing a cloud solution for a financial services company that requires strict compliance with data security regulations. Your application needs to handle sensitive customer data, and you must ensure all data at rest and in transit is appropriately encrypted. Which of the following approaches best aligns with security and compliance standards for managing encryption keys and secrets?",
    "answers": [
      "<p>Use Cloud Key Management Service (KMS) to manage encryption keys and Cloud Identity-Aware Proxy (IAP) to handle access to those keys.</p>",
      "<p>Store encryption keys in a regular cloud storage bucket and manage access using IAM roles.</p>",
      "<p>Embed encryption keys directly into application code to ensure they are readily available for use in encryption and decryption processes.</p>",
      "<p>Utilize self-managed encryption keys stored on-premises, and use VPN connections to access these keys from cloud services as needed.</p>"
    ],
    "explanation": "<p>Use Cloud Key Management Service (KMS) to manage encryption keys and Cloud Identity-Aware Proxy (IAP) to handle access to those keys. -&gt; Correct. Cloud KMS provides managed security and control over encryption keys, while IAP helps control access to applications and services based on identity, thus ensuring both key security and proper access management.</p><p><br></p><p>Store encryption keys in a regular cloud storage bucket and manage access using IAM roles. -&gt; Incorrect. Storing encryption keys directly in cloud storage without a dedicated security service is risky and does not comply with best practices for key management and data security.</p><p><br></p><p>Embed encryption keys directly into application code to ensure they are readily available for use in encryption and decryption processes. -&gt; Incorrect. Hard-coding encryption keys in the application code is insecure as it exposes the keys in source control and to anyone who has access to the application code.</p><p><br></p><p>Utilize self-managed encryption keys stored on-premises, and use VPN connections to access these keys from cloud services as needed. -&gt; Incorrect. While using on-premises keys adds a level of control, it complicates the architecture and can introduce latency and complexity in key retrieval during critical operations, potentially violating compliance requirements for quick data access.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163980,
    "question_plain": "You are designing a solution for a customer who needs to collect and analyze large amounts of log data from their web applications. The customer wants to store the log data for a minimum of 7 years for compliance purposes. Which of the following Google Cloud Platform (GCP) services would you use to meet this requirement?",
    "answers": [
      "<p>Bigtable</p>",
      "<p>Cloud Storage</p>",
      "<p>Cloud SQL</p>",
      "<p>Cloud Datastore</p>"
    ],
    "explanation": "<p>Cloud Storage -&gt; Correct. Cloud Storage allows world-wide storage and retrieval of any amount of data at any time. You can use Cloud Storage for a range of scenarios including serving website content, storing data for archival and disaster recovery, or distributing large data objects to users via direct download.</p><p><br></p><p>Bigtable -&gt; Incorrect. Bigtable is a high-performance, NoSQL database service provided by Google Cloud. While Bigtable is well-suited for applications that require high-speed data ingestion and low-latency queries, it may not be the most efficient choice for long-term storage and retention of log data for 7 years.</p><p><br></p><p>Cloud SQL -&gt; Incorrect. Cloud SQL is a fully managed relational database service provided by Google Cloud. It is optimized for transactional workloads and may not be the ideal choice for storing and retaining large amounts of log data for an extended period, especially if the data does not have a structured schema typically associated with relational databases.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. Cloud Datastore is a NoSQL document database provided by Google Cloud. It is designed for scalable applications and may not be the best choice for storing and retaining log data for compliance purposes, especially when dealing with large amounts of data over a long period.</p><p><br></p><p>https://cloud.google.com/storage/docs</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163982,
    "question_plain": "What is the recommended approach for securing access to Google Cloud resources for a multi-tier application architecture, such as a web frontend, application backend, and a database?",
    "answers": [
      "<p>Create separate service accounts for each tier, and use Google Cloud IAM roles to grant the minimum necessary permissions to each tier.</p>",
      "<p>Create a single service account with the necessary permissions and use it for all tiers.</p>",
      "<p>Use Google Cloud IAM roles to grant the necessary permissions to each Google Cloud user who requires access to the resources.</p>",
      "<p>Use Google Cloud IAM roles to grant the necessary permissions to each Google Cloud group, and add the appropriate users to the relevant groups.</p>"
    ],
    "explanation": "<p>Create separate service accounts for each tier, and use Google Cloud IAM roles to grant the minimum necessary permissions to each tier. -&gt;&nbsp;Correct. When securing access to Google Cloud resources for a multi-tier application architecture, it is recommended to create separate service accounts for each tier and use Google Cloud IAM roles to grant the minimum necessary permissions to each tier. This approach follows the principle of least privilege, which means that each tier has only the permissions required to perform its specific tasks and no more. By using separate service accounts and IAM roles, it is possible to enforce fine-grained access control and minimize the risk of unauthorized access to resources.</p><p><br></p><p>Create a single service account with the necessary permissions and use it for all tiers. -&gt;&nbsp;Incorrect. It is not recommended because using a single service account for all tiers creates a higher risk of unauthorized access and makes it difficult to enforce fine-grained access control.</p><p><br></p><p>Use Google Cloud IAM roles to grant the necessary permissions to each Google Cloud user who requires access to the resources. -&gt; Incorrect. It is also not recommended because granting permissions directly to users violates the principle of least privilege and makes it difficult to manage access control in a scalable way.</p><p><br></p><p>Use Google Cloud IAM roles to grant the necessary permissions to each Google Cloud group, and add the appropriate users to the relevant groups. -&gt;&nbsp;Incorrect. It is a possible solution, but it can become difficult to manage as the number of users and groups increases, and it may be less flexible than using service accounts and IAM roles.</p><p><br></p><p>https://cloud.google.com/iam/docs/creating-managing-service-accounts</p><p>https://cloud.google.com/iam/docs/service-accounts</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163984,
    "question_plain": "Your organization is developing a new multi-tier web application. The application architecture consists of a web front end, a REST API backend, and a relational database. The application is expected to experience heavy traffic, so it needs to be highly scalable and resilient. As a cloud architect, which of the following deployment strategies would you recommend for this application on Google Cloud?",
    "answers": [
      "<p>Deploy the web front end and REST API backend on separate App Engine services, and the database on Cloud Spanner.</p>",
      "<p>Deploy the web front end on Compute Engine, the REST API backend on GKE, and the database on Cloud Bigtable.</p>",
      "<p>Deploy the entire application on a single GKE cluster, using different namespaces for the front end, the backend, and the database.</p>",
      "<p>Deploy the web front end on Cloud Functions, the REST API backend on App Engine, and the database on Firestore.</p>"
    ],
    "explanation": "<p>Deploy the web front end and REST API backend on separate App Engine services, and the database on Cloud Spanner. -&gt;&nbsp;Correct. App Engine is well suited to serve both the front end and the REST API backend, as it can scale automatically based on the incoming traffic. Cloud Spanner is a good choice for the database tier as it is a fully managed, relational database that can scale horizontally while maintaining strong consistency across all regions.</p><p><br></p><p>Deploy the web front end on Compute Engine, the REST API backend on GKE, and the database on Cloud Bigtable. -&gt; Incorrect. While Compute Engine and GKE are robust solutions, they require more operational overhead compared to platform services like App Engine. Cloud Bigtable, while excellent for large analytical and operational workloads, does not provide the relational database structure typically required for web applications.</p><p><br></p><p>Deploy the entire application on a single GKE cluster, using different namespaces for the front end, the backend, and the database. -&gt;&nbsp;Incorrect. Deploying the entire application on a single GKE cluster does not provide the best level of isolation between application tiers.</p><p><br></p><p>Deploy the web front end on Cloud Functions, the REST API backend on App Engine, and the database on Firestore. -&gt;&nbsp;Incorrect. Cloud Functions is not the best choice for serving the web front end, especially in an application expected to experience heavy traffic. Firestore, although a scalable database solution, does not support the SQL queries and transactions as efficiently as Cloud Spanner does for complex web applications.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163986,
    "question_plain": "A company is planning to deploy a large-scale e-commerce platform on Google Cloud Platform (GCP) that must be able to handle millions of transactions per day and provide fast and reliable performance. The platform must also provide scalable and secure storage for product information, customer data, and transaction records. Additionally, the platform must be able to support real-time analytics and data processing. Which of the following options would be the most effective approach to meet these requirements while also optimizing cost?",
    "answers": [
      "<p>Use Cloud SQL to store product information and customer data, and use BigQuery for real-time analytics and data processing. Use Cloud Storage for transaction records and serving product images.</p>",
      "<p>Use Cloud Spanner to store product information, customer data, and transaction records, and use BigQuery for real-time analytics and data processing.</p>",
      "<p>Use Cloud Datastore to store product information and customer data, and use Cloud Dataflow for real-time analytics and data processing. Use Cloud Storage for transaction records and serving product images.</p>",
      "<p>Use Cloud Firestore to store product information, customer data, and transaction records, and use Cloud Dataproc for real-time analytics and data processing.</p>"
    ],
    "explanation": "<p>Use Cloud Spanner to store product information, customer data, and transaction records, and use BigQuery for real-time analytics and data processing. -&gt;&nbsp;Correct. The reason for this is that Cloud Spanner is a horizontally scalable, highly available, and globally distributed relational database that can handle large volumes of data and provide strong consistency. It can also provide high write throughput and low latency reads, making it an ideal choice for e-commerce platforms that need to handle millions of transactions per day. BigQuery, on the other hand, is a fast and scalable data warehouse that can handle real-time analytics and data processing. It allows for easy querying of large datasets and provides results in seconds, making it ideal for e-commerce platforms that need to analyze large volumes of data quickly. Using Cloud Spanner and BigQuery together can provide a scalable, reliable, and high-performance e-commerce platform that can handle millions of transactions per day while also supporting real-time analytics and data processing. This option also minimizes the use of additional services, which helps optimize costs.</p><p><br></p><p>Use Cloud SQL to store product information and customer data, and use BigQuery for real-time analytics and data processing. Use Cloud Storage for transaction records and serving product images. -&gt; Incorrect. Cloud SQL is a managed relational database service, but it may not provide the scalability and performance required to handle millions of transactions per day efficiently.</p><p><br></p><p>Use Cloud Datastore to store product information and customer data, and use Cloud Dataflow for real-time analytics and data processing. Use Cloud Storage for transaction records and serving product images. -&gt; Incorrect. Cloud Datastore is a NoSQL document database, which may not be the most suitable choice for storing product information and customer data in a large-scale e-commerce platform.</p><p><br></p><p>Use Cloud Firestore to store product information, customer data, and transaction records, and use Cloud Dataproc for real-time analytics and data processing. -&gt; Incorrect. Cloud Firestore is a NoSQL document database, but it may not offer the same scalability and performance as Cloud Spanner.</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163988,
    "question_plain": "Your organization is deploying a data-intensive application on GCP. The application requires 50 TB of storage for data that will be infrequently accessed - once per quarter. The data must be available for immediate access when needed, but cost optimization is a key requirement. Which storage class should you use?",
    "answers": [
      "<p>Coldline Storage Class for Cloud Storage</p>",
      "<p>Archive Storage Class for Cloud Storage</p>",
      "<p>Nearline Storage Class for Cloud Storage.</p>",
      "<p>Standard Storage Class for Cloud Storage.</p>"
    ],
    "explanation": "<p>Coldline Storage Class for Cloud Storage -&gt;&nbsp;Correct. Coldline storage is a cost-effective solution for storing infrequently accessed data that still requires immediate access. It offers lower costs than Nearline and Standard storage, while providing faster access than Archive storage.</p><p><br></p><p>Archive Storage Class for Cloud Storage -&gt; Incorrect. While Archive storage is the cheapest option, data retrieval can take several hours, which violates the requirement for immediate access.</p><p><br></p><p>Nearline Storage Class for Cloud Storage. -&gt; Incorrect. Nearline storage is more cost-effective than standard storage for data that's accessed less than once a month, but there are cheaper options for infrequently accessed data.</p><p><br></p><p>Standard Storage Class for Cloud Storage. -&gt; Incorrect. While this class offers high availability and performance, it's the most expensive for storing large amounts of infrequently accessed data.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163990,
    "question_plain": "Your company is working on a real-time analytics solution that processes vast amounts of data collected from IoT devices across the globe. The system must be capable of low-latency read and write access, high throughput, and horizontal scalability. The data will be analyzed using both batch and stream processing. Which Google Cloud service would be the most appropriate solution for this scenario?",
    "answers": [
      "<p>Google Bigtable</p>",
      "<p>BigQuery</p>",
      "<p>Cloud Firestore</p>",
      "<p>Cloud Spanner</p>"
    ],
    "explanation": "<p>Google Bigtable -&gt;&nbsp;Correct. Google Bigtable is an excellent choice for this scenario, as it is a NoSQL big data database service that is designed to handle the high-throughput, low-latency workloads that are common in real-time analytics. It also scales horizontally to handle large volumes of data, making it ideal for the scenario.</p><p><br></p><p>BigQuery -&gt; Incorrect. BigQuery is excellent for running fast, SQL-like queries against large datasets. However, it's not designed for the high volume, high-speed inserts for real-time analytics that the scenario demands.</p><p><br></p><p>Cloud Firestore -&gt; Incorrect. Cloud Firestore is designed for serverless applications, and while it is a NoSQL document database that can scale, it doesn't provide the high throughput and low latency capabilities that are necessary for handling real-time analytics with IoT data.</p><p><br></p><p>Cloud Spanner -&gt; Incorrect. Cloud Spanner is a relational database that is excellent for high availability and consistency, but it may not be ideal for the high throughput, low latency requirements of a real-time analytics solution with large amounts of data from IoT devices.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163992,
    "question_plain": "You are a cloud architect designing a new application for a global financial services company. The application will handle real-time transaction processing across multiple regions and should be resilient to regional outages. Your solution should also minimize latency. Which of the following is the most appropriate design for this use case?",
    "answers": [
      "<p>Use Google Cloud Spanner as your primary database with multi-region configuration and Cloud Load Balancing to distribute the traffic.</p>",
      "<p>Use Google Cloud Bigtable as your primary database with regional replication and Cloud Load Balancing for distributing traffic.</p>",
      "<p>Use Google Cloud Datastore as your primary database with multi-region distribution and Cloud CDN for cache optimization.</p>",
      "<p>Use Google Cloud SQL with multi-region replication and Cloud CDN for cache optimization.</p>"
    ],
    "explanation": "<p>Use Google Cloud Spanner as your primary database with multi-region configuration and Cloud Load Balancing to distribute the traffic. -&gt;&nbsp;Correct. Spanner provides strong consistency, global transaction support, and can be configured across multiple regions, making it an excellent choice for global real-time transaction processing. The use of Cloud Load Balancing helps to distribute traffic and reduce latency.</p><p><br></p><p>Use Google Cloud Bigtable as your primary database with regional replication and Cloud Load Balancing for distributing traffic. -&gt;&nbsp;Incorrect. While Bigtable is great for large analytical and operational workloads, it may not be ideal for real-time transaction processing needed by financial applications.</p><p><br></p><p>Use Google Cloud Datastore as your primary database with multi-region distribution and Cloud CDN for cache optimization. -&gt;&nbsp;Incorrect. While Google Cloud Datastore provides strong consistency and multi-region distribution, it's not optimized for real-time transaction processing and doesn't inherently support financial services transaction handling.</p><p><br></p><p>Use Google Cloud SQL with multi-region replication and Cloud CDN for cache optimization. -&gt; Incorrect. Cloud SQL provides traditional relational database services but lacks the horizontal scalability and global transaction support needed for this specific use case.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163994,
    "question_plain": "You are the cloud architect for a multinational corporation which has decided to migrate its on-premises data warehouse to Google Cloud. The data warehouse needs to handle several petabytes of data, with frequent, unpredictable spikes in query activity. What approach should you recommend for scalable data warehouse solution on GCP?",
    "answers": [
      "<p>Use BigQuery for both data storage and analysis.</p>",
      "<p>Use Cloud Storage for data storage, with scheduled queries running in Dataflow.</p>",
      "<p>Use Bigtable for data storage, with analysis using Data Studio.</p>",
      "<p>Use Cloud SQL for data storage, with Dataflow for analysis.</p>"
    ],
    "explanation": "<p>Use BigQuery for both data storage and analysis. -&gt;&nbsp;Correct. BigQuery is a fully managed, serverless data warehouse solution provided by Google Cloud. It is designed to handle massive datasets and can scale seamlessly to handle several petabytes of data. BigQuery also provides powerful querying capabilities, allowing for efficient analysis of the data. It is well-suited for handling frequent, unpredictable spikes in query activity, as it automatically scales resources to meet demand. Using BigQuery for both data storage and analysis simplifies the architecture and leverages the native capabilities of the platform.</p><p><br></p><p>Use Cloud Storage for data storage, with scheduled queries running in Dataflow. -&gt;&nbsp;Incorrect. While Cloud Storage is good for storing large amounts of data, it is not designed to serve as a scalable data warehouse. Dataflow is mainly used for processing data, not designed for querying large data sets.</p><p><br></p><p>Use Bigtable for data storage, with analysis using Data Studio. -&gt;&nbsp;Incorrect. Bigtable is designed for high throughput, low latency workloads, and while it can store large amounts of data, it does not have the SQL capabilities necessary for data warehousing. Data Studio is for visualizing data, not querying large datasets.</p><p><br></p><p>Use Cloud SQL for data storage, with Dataflow for analysis. -&gt;&nbsp;Incorrect. Cloud SQL is a managed relational database service, which may not be the ideal choice for handling several petabytes of data. It is optimized for online transaction processing (OLTP) workloads rather than large-scale data warehousing. While Dataflow is a flexible and scalable data processing service, it is primarily used for ETL (extract, transform, load) and batch processing tasks, and may not provide the same level of performance and efficiency as BigQuery for ad-hoc queries and analytics.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163996,
    "question_plain": "Consider a scenario where a large online retailer needs to implement a highly available and scalable NoSQL solution for its e-commerce platform. The solution must handle large amounts of traffic during peak periods, provide fast and reliable performance, and ensure the security of customer data. What is the most appropriate solution for the large online retailer to implement a highly available and scalable solution for its e-commerce platform while handling large amounts of traffic during peak periods, providing fast and reliable performance, and ensuring the security of customer data?",
    "answers": [
      "<p>Use App Engine for the e-commerce platform with a single instance and use Cloud Storage for storage.</p>",
      "<p>Use Compute Engine with autoscaling and load balancing for the e-commerce platform and use Cloud SQL for storage.</p>",
      "<p>Use Compute Engine VM single instance for the e-commerce platform and use Cloud&nbsp;SQL for storage.</p>",
      "<p>Use Google Kubernetes Engine for the e-commerce platform with autoscaling and use Cloud Firestore for storage.</p>"
    ],
    "explanation": "<p>Use Google Kubernetes Engine for the e-commerce platform with autoscaling and use Cloud Firestore for storage. -&gt;&nbsp;Correct. Google Kubernetes Engine (GKE) provides a highly available and scalable container orchestration platform based on Kubernetes. It allows for automatic scaling and load balancing, making it suitable for handling large amounts of traffic during peak periods. Cloud Firestore is a NoSQL document database provided by Google Cloud, offering scalability, fast and reliable performance, and built-in security features. It is well-suited for storing structured NoSQL data and ensuring the security of customer data.</p><p><br></p><p>Use App Engine for the e-commerce platform with a single instance and use Cloud Storage for storage. -&gt;&nbsp;Incorrect. App Engine is a platform-as-a-service (PaaS) offering that automatically scales based on demand, but it may not provide the flexibility and fine-grained control required for this scenario. Cloud Storage is an object storage service and may not be the best choice for storing structured data in a NoSQL format.</p><p><br></p><p>Use Compute Engine with autoscaling and load balancing for the e-commerce platform and use Cloud SQL for storage. -&gt;&nbsp;Incorrect. While Compute Engine with autoscaling and load balancing can handle traffic and provide scalability, Cloud SQL is a managed relational database service and may not be the best fit for a NoSQL solution.</p><p><br></p><p>Use Compute Engine VM single instance for the e-commerce platform and use Cloud&nbsp;SQL for storage. -&gt;&nbsp;Incorrect. A single Compute Engine VM instance does not provide the scalability and high availability required for handling large amounts of traffic during peak periods. Cloud SQL, being a managed relational database service, may not align well with the requirements of a NoSQL solution.</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82163998,
    "question_plain": "Which of the following steps should be taken to ensure maximum data transfer speed while migrating 5TB of private data from on-premises to Google Cloud Storage?",
    "answers": [
      "<p>Use a low-speed internet connection</p>",
      "<p>Use a migration tool to automate the transfer process</p>",
      "<p>Transfer all the data in one large piece</p>",
      "<p>Transfer data during peak network hours</p>"
    ],
    "explanation": "<p>Use a migration tool to automate the transfer process -&gt; Correct. Using a migration tool like Google Cloud Storage Transfer Service, gsutil, or Cloud Data Transfer can help to automate the transfer process and ensure that your data is secure and well-managed during the migration. This option is faster and more secure than manually copying files or using a low-speed internet connection. </p><p><br></p><p>Use a low-speed internet connection -&gt;&nbsp;Incorrect. It is incorrect as it suggests using a low-speed internet connection which would slow down the transfer process. </p><p><br></p><p>Transfer all the data in one large piece -&gt; Incorrect. It is incorrect as transferring all the data in one large piece can also slow down the transfer process and may even cause errors or data loss. </p><p><br></p><p>Transfer data during peak network hours -&gt; Incorrect. It is incorrect as transferring data during peak network hours can result in network congestion and may slow down the transfer process.</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82164000,
    "question_plain": "You are a cloud architect working on a large-scale application that leverages various Google Cloud services, including Bigtable, Firestore, and Pub/Sub. Your development team is transitioning from a monolithic architecture to a microservices architecture, and you are tasked with implementing a testing strategy that includes the use of cloud emulators to ensure each service is properly tested in isolation before integration. Which of the following strategies would be most effective for managing this implementation using Google Cloud emulators?",
    "answers": [
      "<p>Configure separate CI/CD pipelines to include stages that set up and tear down emulators for each service, ensuring isolated environment testing during development.</p>",
      "<p>Utilize the Google Cloud SDK to deploy emulators for Bigtable, Firestore, and Pub/Sub directly in the production environment for live testing.</p>",
      "<p>Implement a single emulator that mimics all services (Bigtable, Firestore, and Pub/Sub) to simplify the testing process and reduce resource usage.</p>",
      "<p>Only use local machine emulators for development and skip CI/CD integration, relying on developer discipline to conduct necessary tests.</p>"
    ],
    "explanation": "<p>Configure separate CI/CD pipelines to include stages that set up and tear down emulators for each service, ensuring isolated environment testing during development. -&gt;&nbsp;Correct. Using CI/CD pipelines to manage the lifecycle of emulators for each microservice allows isolated testing and ensures that each component interacts properly with emulated versions of its dependencies, simulating production conditions without affecting the live environment.</p><p><br></p><p>Utilize the Google Cloud SDK to deploy emulators for Bigtable, Firestore, and Pub/Sub directly in the production environment for live testing. -&gt;&nbsp;Incorrect. Deploying emulators in a production environment is not recommended as it could lead to performance issues and does not replicate the live environment accurately.</p><p><br></p><p>Implement a single emulator that mimics all services (Bigtable, Firestore, and Pub/Sub) to simplify the testing process and reduce resource usage. -&gt;&nbsp;Incorrect. No single emulator can mimic all these services, and using one might lead to insufficient testing as different services have unique characteristics and behaviors.</p><p><br></p><p>Only use local machine emulators for development and skip CI/CD integration, relying on developer discipline to conduct necessary tests. -&gt;&nbsp;Incorrect. Relying solely on local testing without integrating emulators into CI/CD pipelines misses opportunities for automated, repeatable testing and can lead to inconsistencies in how tests are run by different developers.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430128,
    "question_plain": "Your company is restructuring its operations and you've been tasked with reorganizing your Google Cloud resources to match the new business units. You currently have all resources in one project. Your company is divided into several departments, each containing multiple teams. You need to ensure that billing reports can be generated per department, and resources can be isolated per team. What is the most effective way to reorganize your resources?",
    "answers": [
      "<p>Create a folder for each department and then create projects under those folders for each team.</p>",
      "<p>Create a separate organization for each department and projects for each team under the organizations.</p>",
      "<p>Create a separate project for each team and use labels to identify the departments.</p>",
      "<p>Keep all resources in one project but use different network subnets and labels to differentiate between departments and teams.</p>"
    ],
    "explanation": "<p>Create a folder for each department and then create projects under those folders for each team. -&gt;&nbsp;Correct. By creating a folder for each department and a project for each team, you can manage access controls at the project level and isolate resources for each team. Billing can be set up at the project level, allowing you to generate billing reports per department.</p><p><br></p><p>Create a separate organization for each department and projects for each team under the organizations. -&gt;&nbsp;Incorrect. In GCP, organizations are intended to represent an entire company, so creating an organization for each department would be an inappropriate use of the resource hierarchy.</p><p><br></p><p>Create a separate project for each team and use labels to identify the departments. -&gt;&nbsp;Incorrect. Although this would isolate resources for each team, using labels to identify departments would not allow you to generate billing reports per department.</p><p><br></p><p>Keep all resources in one project but use different network subnets and labels to differentiate between departments and teams. -&gt;&nbsp;Incorrect. This approach doesn't properly utilize the resource hierarchy for isolation and would not allow for billing reports per department.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430130,
    "question_plain": "Compute Engine instance has a single boot persistent disk (PD) that contains the operating system by default. Suppose your application requires additional storage space and you should add additional storage to your instance. Which storage options can you use with Compute Engine virtual machines to do this? Select all that apply.",
    "answers": [
      "<p>Zonal/Regional persistent disk</p>",
      "<p>Local&nbsp;SSD</p>",
      "<p>Cloud Storage bucket</p>",
      "<p>BigQuery</p>",
      "<p>Bigtable</p>"
    ],
    "explanation": "<p>Compute Engine offers several types of storage options for your instances. Each of the following storage options has unique price and performance characteristics:</p><p>- Zonal persistent disk: Efficient, reliable block storage.</p><p>- Regional persistent disk: Regional block storage replicated in two zones.</p><p>- Local SSD: High performance, transient, local block storage.</p><p>- Cloud Storage buckets: Affordable object storage.</p><p>- Filestore: High performance file storage for Google Cloud users.</p><p><br></p><p>https://cloud.google.com/compute/docs/disks</p>",
    "correct_response": ["a", "b", "c"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 78430132,
    "question_plain": "Compute Engine instance has a single boot persistent disk (PD) that contains the operating system by default. Suppose your application requires additional storage space. As a cloud architect, you want to choose the best solution in terms of read/write IOPS per instance. Which storage options should you use?",
    "answers": [
      "<p>Local SSD</p>",
      "<p>Regional SSD</p>",
      "<p>Zonal SSD</p>",
      "<p>Zonal Standard Persistent Disk</p>",
      "<p>Regional Standard Persistent Disk</p>"
    ],
    "explanation": "<p>Local SSD -&gt; Correct. Local SSDs are physically attached to the instance's host server and provide high-speed, low-latency storage for applications that require high-performance storage. Local SSDs provide the highest read/write IOPS per instance compared to the other options listed in the question. However, local SSDs are not persistent, meaning that their data is lost when the instance is terminated.</p><p><br></p><p>Regional SSD -&gt; Incorrect. Regional SSD is network-attached disk that provide lower performance than local SSDs. However, it is persistent and can be used for storing data that needs to be retained even after the instance is terminated.</p><p><br></p><p>Zonal SSD -&gt; Incorrect. Zonal SSD provides higher performance than regional SSD, but it is only available in a single zone and cannot be attached to instances in other zones.</p><p><br></p><p>Zonal Standard Persistent Disk -&gt; Incorrect. Zonal standard persistent disk provides a lower performance than zonal SSD, but it is persistent and can be used for storing data that needs to be retained even after the instance is terminated.</p><p><br></p><p>Regional Standard Persistent Disk -&gt; Incorrect. Regional standard persistent disk is a network-attached disk that provide lower performance than local SSDs. However, it is persistent and can be used for storing data that needs to be retained even after the instance is terminated.</p><p><br></p><p>https://cloud.google.com/compute/docs/disks</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430134,
    "question_plain": "Suppose you run Spark jobs and machine learning models from time to time in your on-premises data center. As a cloud architect, you want to move to Google Cloud with your workload. Which service should you use?",
    "answers": [
      "<p>Dataproc</p>",
      "<p>BigQuery</p>",
      "<p>Bigtable</p>",
      "<p>Cloud Storage</p>"
    ],
    "explanation": "<p>Dataproc -&gt; Correct. Dataproc is a fully managed cloud service that provides Apache Spark and Apache Hadoop clusters on Google Cloud. It is an excellent choice for running Spark jobs and machine learning models on Google Cloud, especially if you are already using these technologies in your on-premises data center. Dataproc can help you take advantage of Google Cloud's scalability and ease of use while minimizing the need to re-architect your existing workloads.</p><p><br></p><p>BigQuery -&gt; Incorrect. It is a cloud data warehouse that can be used to analyze large datasets. However, it is not designed for running Spark jobs and machine learning models directly.</p><p><br></p><p>Bigtable -&gt; Incorrect. It is a NoSQL database service that is designed for storing and analyzing large datasets. It can be used for Spark and machine learning workloads, but it is not a direct replacement for Dataproc.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. It is a general-purpose object storage service that can be used to store and retrieve large datasets. It can be used as a data source for Spark and machine learning workloads, but it does not provide the Spark and Hadoop clusters that Dataproc offers.</p><p><br></p><p>https://cloud.google.com/dataproc/docs/concepts/overview</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430136,
    "question_plain": "A social media application allows users to upload pictures. You need to convert each image to your internal optimized binary format and store it. As a cloud architect, you want to use the most efficient, cost-effective solution. What should you recommend?",
    "answers": [
      "<p>You should save uploaded images in a Cloud Storage bucket, and monitor the bucket for uploads. Run a Cloud Function to convert the images and to store them in a Cloud Storage bucket.</p>",
      "<p>You should store uploaded images in Cloud Bigtable, monitor Bigtable entries, and then run a Cloud Function to convert the images and store them in Bigtable.</p>",
      "<p>You should store uploaded images in Firestore, monitor Firestore entries, and then run a Cloud Function to convert the images and store them in Firestore.</p>",
      "<p>You should store uploaded images in Filestore, monitor Filestore entries, and then run a Cloud Function to convert the images and store them in Filestore.</p>"
    ],
    "explanation": "<p>You should save uploaded images in a Cloud Storage bucket, and monitor the bucket for uploads. Run a Cloud Function to convert the images and to store them in a Cloud Storage bucket. -&gt; Correct. Storing uploaded images in a Cloud Storage bucket is a cost-effective and scalable solution. Cloud Storage provides durable and highly available storage for objects like images. By monitoring the Cloud Storage bucket for uploads using a Cloud Function, you can trigger the image conversion process and store the optimized binary format of the images back in the same or a different Cloud Storage bucket.</p><p><br></p><p>You should store uploaded images in Cloud Bigtable, monitor Bigtable entries, and then run a Cloud Function to convert the images and store them in Bigtable. -&gt;&nbsp;Incorrect. Cloud Bigtable is a NoSQL database optimized for high-performance, low-latency read and write operations on large datasets. While it can store binary data, it may not be the most efficient and cost-effective solution for storing and processing images.</p><p><br></p><p>You should store uploaded images in Firestore, monitor Firestore entries, and then run a Cloud Function to convert the images and store them in Firestore. -&gt;&nbsp;Incorrect. Firestore is a flexible, scalable NoSQL document database, but it is primarily designed for storing structured data rather than binary objects like images. Storing and processing images in Firestore may not provide the optimal performance and cost-effectiveness compared to using Cloud Storage.</p><p><br></p><p>You should store uploaded images in Filestore, monitor Filestore entries, and then run a Cloud Function to convert the images and store them in Filestore. -&gt;&nbsp;Incorrect. Filestore is a managed file storage service for applications that need a file system interface. While it can store binary files like images, it may not be the most efficient and cost-effective solution for storing and processing images compared to using Cloud Storage.</p><p><br></p><p>https://cloud.google.com/functions/docs/calling/storage</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430138,
    "question_plain": "To leverage Cloud Pub/Sub messages within your App Engine application, you find that the Cloud Pub/Sub API is currently disabled. In order to enable your application to utilize Cloud Pub/Sub, you plan to authenticate it to the API using a service account. What measures should you take to ensure seamless usage of Cloud Pub/Sub by your application?",
    "answers": [
      "<p>You should enable the Cloud Pub/Sub API in the API Library on the GCP Console.</p>",
      "<p>You should rely on the automatic enablement of the Cloud Pub/Sub API when the Service Account accesses it.</p>",
      "<p>You should use Deployment Manager to deploy your application. Rely on the automatic enablement of all APIs used by the application being deployed.</p>",
      "<p>You should grant the App Engine Default service account the role of Cloud Pub/Sub Admin. Have your application enable the API on the first connection to Cloud Pub/Sub.</p>"
    ],
    "explanation": "<p>You should enable the Cloud Pub/Sub API in the API Library on the GCP Console. -&gt; Correct. To use the Cloud Pub/Sub API in your App Engine application, you need to enable the API first. This can be done through the API Library on the GCP Console. Once the API is enabled, you can then use a service account to authenticate your application to the API and use Cloud Pub/Sub. </p><p><br></p><p>You should rely on the automatic enablement of the Cloud Pub/Sub API when the Service Account accesses it. -&gt; Incorrect. It is not a valid option, as the Cloud Pub/Sub API needs to be enabled before it can be accessed by the service account. </p><p><br></p><p>You should use Deployment Manager to deploy your application. Rely on the automatic enablement of all APIs used by the application being deployed. -&gt; Incorrect. It is not the best choice because it involves deploying the application with all APIs enabled, which is not a good security practice. </p><p><br></p><p>You should grant the App Engine Default service account the role of Cloud Pub/Sub Admin. Have your application enable the API on the first connection to Cloud Pub/Sub. -&gt; Incorrect. It is not necessary since granting the App Engine Default service account the Cloud Pub/Sub Admin role doesn't automatically enable the Cloud Pub/Sub API.</p><p><br></p><p>https://cloud.google.com/endpoints/docs/openapi/enable-api</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430140,
    "question_plain": "Your organization has two Google Cloud projects and you want them to share policies. What's the best practice in this case?",
    "answers": [
      "<p>Place both projects into a folder, and define the policies on this folder.</p>",
      "<p>You should duplicate all the policies on one project onto the other.</p>",
      "<p>Combine two projects into one.</p>",
      "<p>You cannot share policies across two different projects in GCP.</p>"
    ],
    "explanation": "<p>Place both projects into a folder, and define the policies on this folder. -&gt; Correct. In Google Cloud, policies can be defined at the project, folder, or organization level. By placing both projects into a folder and defining policies on the folder, those policies will apply to both projects within that folder.</p><p><br></p><p>You should duplicate all the policies on one project onto the other. -&gt; Incorrect. Duplicating policies onto the other project, is not the best practice as it can lead to inconsistencies if policies need to be updated or changed.</p><p><br></p><p>Combine two projects into one. -&gt; Incorrect. Combining two projects into one, may not be feasible or desirable depending on the specific needs and requirements of each project.</p><p><br></p><p>You cannot share policies across two different projects in GCP. -&gt; Incorrect. You can share policies across two different projects in GCP as long as they are in the same folder or organization.</p><p><br></p><p>https://cloud.google.com/resource-manager/docs/creating-managing-folders</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430142,
    "question_plain": "Your organization is moving its on-premises data center to Google Cloud. The existing setup involves multiple layers of firewalls and access control lists (ACLs) to ensure the security of the network. The organization wants to mirror the same level of security on Google Cloud. What should be your approach to designing the network on Google Cloud?",
    "answers": [
      "<p>Use Shared VPC with subnets for each layer and apply Firewall Rules and Cloud Armor policies.</p>",
      "<p>Implement multiple security groups to mirror the firewall layers.</p>",
      "<p>Create a flat network with Firewall Rules for each service.</p>",
      "<p>Use a single VPC for all resources and implement IAM policies for network control.</p>"
    ],
    "explanation": "<p>Use Shared VPC with subnets for each layer and apply Firewall Rules and Cloud Armor policies. -&gt;&nbsp;Correct. A Shared VPC allows for segregation of resources similar to a layered on-premises network. Firewall Rules and Cloud Armor can be used to control and protect the network traffic, similar to firewalls and ACLs in on-premises setups.</p><p><br></p><p>Implement multiple security groups to mirror the firewall layers. -&gt;&nbsp;Incorrect. Google Cloud doesn't use the concept of security groups. The correct approach would be to use Firewall Rules and VPC Service Controls.</p><p><br></p><p>Create a flat network with Firewall Rules for each service. -&gt;&nbsp;Incorrect. While Firewall Rules can be used to control traffic, a flat network doesn't provide the same level of segregation and control as a layered approach.</p><p><br></p><p>Use a single VPC for all resources and implement IAM policies for network control. -&gt;&nbsp;Incorrect. While IAM is an important part of Google Cloud security, it does not replace network-level controls such as Firewall Rules or Cloud Armor.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430144,
    "question_plain": "In your new e-commerce application you need to use Cloud Storage. As a cloud architect, you need to have the lowest-cost, highly durable storage service for data archiving, online backup, and disaster recovery (365-day minimum storage duration). Which storage class should you select?",
    "answers": [
      "<p>Archive</p>",
      "<p>Standard</p>",
      "<p>Nearline</p>",
      "<p>Coldline</p>"
    ],
    "explanation": "<p>Archive -&gt; Correct. The reason for choosing the Archive storage class is that it provides the lowest-cost, highly durable storage service for long-term data retention, archiving, online backup, and disaster recovery scenarios. The Archive storage class is designed for data that is accessed less frequently, stored for longer periods (minimum of 365 days), and stored primarily for backup, archiving, or compliance purposes. This storage class has the lowest storage cost per gigabyte and the highest durability of all storage classes in Google Cloud Storage.</p><p><br></p><p>Standard -&gt; Incorrect. Standard storage class is designed for frequently accessed data, where fast access times and low latency are important. This option is not suitable for long-term data retention, archiving, or backup purposes.</p><p><br></p><p>Nearline -&gt; Incorrect. Nearline storage class is designed for infrequently accessed data that can tolerate slightly higher latency and access times than Standard storage class. This option is suitable for data that may need to be accessed once or twice a month and stored for a minimum of 30 days.</p><p><br></p><p>Coldline -&gt; Incorrect. Coldline storage class is designed for infrequently accessed data that can tolerate higher latency and access times than Nearline storage class. This option is suitable for data that may need to be accessed once or twice a year and stored for a minimum of 90 days. However, it is more expensive than the Archive storage class.</p><p><br></p><p>https://cloud.google.com/storage/docs/storage-classes#archive</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430146,
    "question_plain": "As a cloud architect, you need to advise on how to store structured and unstructured binary data (images, media files) with Google Cloud. Which service should you recommend?",
    "answers": [
      "<p>Cloud Storage</p>",
      "<p>BigQuery</p>",
      "<p>Cloud Bigtable</p>",
      "<p>Cloud SQL</p>",
      "<p>Cloud Spanner</p>"
    ],
    "explanation": "<p>Cloud Storage -&gt; Correct. It is a fully managed object storage service that provides a durable and highly available way to store structured and unstructured data, including binary data like images and media files. Cloud Storage provides features such as versioning, lifecycle management, and access controls that make it easy to manage data at scale. Cloud Storage also integrates with other GCP services, such as Cloud Functions and Cloud Run, to enable serverless workflows.</p><p><br></p><p>BigQuery -&gt; Incorrect. It is a fully managed data warehouse service that is optimized for handling large datasets and performing analytics on them. BigQuery is not designed for storing binary data like images and media files.</p><p><br></p><p>Cloud Bigtable -&gt; Incorrect. It is a fully managed NoSQL database service that is optimized for handling large amounts of data with low latency. While Cloud Bigtable can store binary data, it is not designed for storing unstructured data like images and media files.</p><p><br></p><p>Cloud SQL -&gt; Incorrect. It is a fully managed relational database service that supports several popular database engines such as MySQL and PostgreSQL. While Cloud SQL can store binary data, it is not designed for storing unstructured data like images and media files.</p><p><br></p><p>Cloud Spanner -&gt; Incorrect. It is a fully managed relational database service that provides strong consistency and horizontal scalability. While Cloud Spanner can store binary data, it is not designed for storing unstructured data like images and media files.</p><p><br></p><p>https://cloud.google.com/storage/docs/json_api/v1/objects</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430148,
    "question_plain": "Personally Identifiable Information (PII) and sensitive information about your company's customers should be stored securely in Cloud Storage. Several people from your company's compliance department need access to some of this information. As a cloud architect, what should you do to follow Google's best practices?",
    "answers": [
      "<p>You should use granular ACLs on the bucket.</p>",
      "<p>You should grant Storage Object Viewer role to the entire compliance department.</p>",
      "<p>You should create additional bucket, enable public access, and provide specific file URLs to the compliance department.</p>",
      "<p>You should grant Storage Object Creator role to the entire compliance department.</p>"
    ],
    "explanation": "<p>You should use granular ACLs on the bucket. -&gt; Correct. The recommended best practice in this scenario is to use granular Access Control Lists (ACLs) on the bucket. You can use IAM to create custom roles that grant only the permissions that the compliance department needs to access the sensitive data, and then assign those roles to the appropriate users. By using granular ACLs, you can ensure that only authorized users have access to sensitive data, and you can limit the risk of data leaks or unauthorized access.</p><p><br></p><p>You should grant Storage Object Viewer role to the entire compliance department. -&gt; Incorrect. Granting the Storage Object Viewer role to the entire compliance department is not the best practice. Doing so would give them more permissions than they need to perform their tasks and could lead to unauthorized access to sensitive data.</p><p><br></p><p>You should create additional bucket, enable public access, and provide specific file URLs to the compliance department. -&gt; Incorrect. It is also not recommended because enabling public access to a bucket is a security risk, and providing specific file URLs to the compliance department can be difficult to manage and can lead to data leaks.</p><p><br></p><p>You should grant Storage Object Creator role to the entire compliance department. -&gt; Incorrect. Granting the Storage Object Viewer role to the Storage Object Creator role to the entire compliance department is not the best practice. Doing so would give them more permissions than they need to perform their tasks and could lead to unauthorized access to sensitive data.</p><p><br></p><p>https://cloud.google.com/storage/docs/access-control/lists</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430150,
    "question_plain": "An application is deployed using App Engine to serve production traffic. Your colleague have found a critical error in application and you need to fix this quickly, but you don't know if new solution will cause other problems. What do you recommend?",
    "answers": [
      "<p>You should deploy the new version of the application, and use traffic splitting to send a small percentage of traffic to it.</p>",
      "<p>You should set up a second App Engine service, and then update a subset of clients to hit the new version.</p>",
      "<p>You should deploy the new version of the application temporarily, capture logs and then roll it back to the previous version.</p>",
      "<p>You should create a second Google App Engine project with the new application code, and migrate users gradually to the new application.</p>"
    ],
    "explanation": "<p>You should deploy the new version of the application, and use traffic splitting to send a small percentage of traffic to it. -&gt;&nbsp;Correct. By deploying the new version of the application and using traffic splitting, you can gradually route a small percentage of production traffic to the new version while still serving the majority of traffic with the existing stable version. This allows you to test the new solution in a controlled manner and monitor its performance and impact on the application without affecting the entire user base.</p><p><br></p><p>You should set up a second App Engine service, and then update a subset of clients to hit the new version. -&gt;&nbsp;Incorrect. While this approach can be feasible in some cases, it may require additional configuration and maintenance overhead, especially if the application has a large number of clients or if fine-grained control is needed.</p><p><br></p><p>You should deploy the new version of the application temporarily, capture logs and then roll it back to the previous version. -&gt;&nbsp;Incorrect. While log capture can provide valuable insights, this approach does not address the need to test the new solution with live traffic and monitor its behavior in a controlled manner.</p><p><br></p><p>You should create a second Google App Engine project with the new application code, and migrate users gradually to the new application. -&gt;&nbsp;Incorrect. While this approach can be used for more complex migration scenarios, it may introduce additional complexity and overhead, especially when managing user migration and data synchronization between multiple projects.</p><p><br></p><p>https://cloud.google.com/appengine/docs</p><p>https://cloud.google.com/appengine/docs/standard/python/splitting-traffic</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430152,
    "question_plain": "Production Compute Engine workload is running in a small subnet, which can be expanded. The recent spike in traffic has caused problems, but there are no free IP addresses for Managed Instances Group to autoscale. What should you do?",
    "answers": [
      "<p>You should expand the subnet IP range.</p>",
      "<p>You should create a new subnet with a larger, non-overlapping range. Move all instances to the new subnet and remove the old subnet.</p>",
      "<p>You should create a new project and a new VPC. Share the new VPC with the existing project and configure all existing resources to use the new VPC.</p>",
      "<p>You should create a new subnet with a larger, overlapping range to automatically move all instances to the new subnet. Then, remove the old subnet.</p>"
    ],
    "explanation": "<p>You should expand the subnet IP range. -&gt;&nbsp;Correct. By expanding the subnet IP range, you can accommodate more IP addresses within the existing subnet. This allows for additional instances to be provisioned and scaled within the Managed Instance Group to handle the increased traffic. Expanding the subnet IP range is a straightforward solution that avoids the need to create new subnets or projects.</p><p><br></p><p>You should create a new subnet with a larger, non-overlapping range. Move all instances to the new subnet and remove the old subnet. -&gt;&nbsp;Incorrect. While it can provide more IP addresses, it also requires migrating all instances to the new subnet, which can be complex and disruptive. Expanding the existing subnet is generally a more practical and straightforward solution.</p><p><br></p><p>You should create a new project and a new VPC. Share the new VPC with the existing project and configure all existing resources to use the new VPC. -&gt;&nbsp;Incorrect. While this can provide additional resources, it involves significant overhead and complexity. It is not necessary to create a new project and VPC in this scenario.</p><p><br></p><p>You should create a new subnet with a larger, overlapping range to automatically move all instances to the new subnet. Then, remove the old subnet. -&gt;&nbsp;Incorrect. Automatically moving instances to the new subnet can be complex and may introduce disruption to the workload. Expanding the existing subnet is a simpler and more appropriate solution.</p><p><br></p><p>https://cloud.google.com/vpc/docs/using-vpc#expand-subnet</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430154,
    "question_plain": "As a cloud architect, you need to prepare a migration strategy for a company that wants to migrate all applications from its on-premise data center to GCP. The company's DevOps team currently use Jenkins to automate configuration updates. What should you recommend?",
    "answers": [
      "<p>Jenkins can be delivered using Google Marketplace.</p>",
      "<p>Download the Jenkins binary from Jenkins website and deploy to the new Compute Engine instance.</p>",
      "<p>Download the Jenkins binary from Jenkins website and deploy in App Engine Standard environment.</p>",
      "<p>Create a YAML Kubernetes Deployment file referencing the Jenkins docker image and deploy to the new GKE cluster.</p>"
    ],
    "explanation": "<p>Jenkins can be delivered using Google Marketplace. -&gt;&nbsp;Correct. Cloud Marketplace provides a wide range of pre-configured software solutions, including Jenkins, that can be easily deployed and managed on Google Cloud Platform (GCP). By using Jenkins from the marketplace, the company can quickly provision and configure Jenkins on GCP without the need for manual installation and configuration.</p><p><br></p><p>Download the Jenkins binary from Jenkins website and deploy to the new Compute Engine instance. -&gt;&nbsp;Incorrect. While this approach is technically possible, it requires manual installation, configuration, and ongoing maintenance of the Jenkins instance on Compute Engine, which may be more time-consuming and error-prone compared to using a managed solution.</p><p><br></p><p>Download the Jenkins binary from Jenkins website and deploy in App Engine Standard environment. -&gt;&nbsp;Incorrect. App Engine Standard environment is a platform-as-a-service (PaaS) offering that is designed for running web applications, and it has limitations in terms of the available runtime and configuration options, which may not align well with the requirements of running Jenkins.</p><p><br></p><p>Create a YAML Kubernetes Deployment file referencing the Jenkins docker image and deploy to the new GKE cluster. -&gt;&nbsp;Incorrect. While this approach is technically possible, it requires more advanced knowledge of Kubernetes and containerization compared to using a managed solution from the marketplace.</p><p><br></p><p>https://cloud.google.com/jenkins</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430156,
    "question_plain": "Your company has a multistage CI/CD pipeline for deploying applications on Google Cloud. Recently, multiple faulty deployments have made it to the production stage. What quality control measures should you implement to avoid such scenarios?",
    "answers": [
      "<p>Implement automated testing and validation at each stage of the pipeline.</p>",
      "<p>Implement more strict IAM policies to prevent unauthorized deployments.</p>",
      "<p>Enforce manual approval before any deployment to the production environment.</p>",
      "<p>Increase the frequency of deployments to identify and fix issues quickly.</p>"
    ],
    "explanation": "<p>Implement automated testing and validation at each stage of the pipeline. -&gt; Correct. Automated testing and validation at each stage can catch and prevent many issues before they reach production. It's a scalable and effective measure for maintaining high-quality deployments.</p><p><br></p><p>Implement more strict IAM policies to prevent unauthorized deployments. -&gt;&nbsp;Incorrect. While it's crucial to control who can deploy to production, stricter IAM policies would not necessarily prevent faulty deployments from authorized users.</p><p><br></p><p>Enforce manual approval before any deployment to the production environment. -&gt;&nbsp;Incorrect. While this measure can catch some issues, it heavily relies on human intervention, which may not scale well and can still overlook problems.</p><p><br></p><p>Increase the frequency of deployments to identify and fix issues quickly. -&gt;&nbsp;Incorrect. Simply increasing the deployment frequency doesn't necessarily improve quality control. Without proper testing and validation, this might lead to more faulty deployments.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430158,
    "question_plain": "A mission-critical application runs on several virtual machines in on-premise data center and needs to be migrated to GCP. As a cloud architect, you need to prepare a migration strategy. The company wants to benefit from Lift and Shift approach, and this application needs to be scaled automatically and efficiently based on the CPU utilization. What do you recommend?",
    "answers": [
      "<p>This company should deploy the application to Compute Engine Managed Instance Group with autoscaling enabled based on CPU utilization.</p>",
      "<p>This company should deploy the application to GKE cluster with Horizontal Pod Autoscaling enabled based on CPU utilization.</p>",
      "<p>This company should deploy the application to Google Compute Engine Managed Instance Group with time-based autoscaling based on last months traffic patterns.</p>",
      "<p>This company should deploy the application to Compute Engine Unmanaged Instance Group with autoscaling enabled based on CPU utilization.</p>"
    ],
    "explanation": "<p>This company should deploy the application to Compute Engine Managed Instance Group with autoscaling enabled based on CPU utilization. -&gt;&nbsp;Correct. The Lift and Shift approach is a migration strategy where the applications are moved as they are, without any changes, to the cloud. The question states that the company wants to benefit from the Lift and Shift approach, which means that the application will be migrated without any changes. Since the application needs to be scaled automatically and efficiently based on the CPU utilization, a Managed Instance Group (MIG) with autoscaling enabled based on CPU utilization is the best option. Compute Engine MIG provides automatic scaling of virtual machines based on demand. Autoscaling based on CPU utilization will help the application to scale up or down automatically depending on the CPU usage, which will ensure that the application is running efficiently.</p><p><br></p><p>This company should deploy the application to GKE cluster with Horizontal Pod Autoscaling enabled based on CPU utilization. -&gt; Incorrect. It is incorrect because GKE is not the best option for Lift and Shift approach. GKE is a container orchestration system, which requires a re-architecture of the application to run in containers.</p><p><br></p><p>This company should deploy the application to Google Compute Engine Managed Instance Group with time-based autoscaling based on last months traffic patterns. -&gt; Incorrect. It is also incorrect because time-based autoscaling will not provide the required efficiency in scaling. Autoscaling based on last month's traffic patterns may not be the best predictor of future demand.</p><p><br></p><p>This company should deploy the application to Compute Engine Unmanaged Instance Group with autoscaling enabled based on CPU utilization. -&gt; Incorrect. It is incorrect because unmanaged instance groups require manual management and do not provide the automatic scaling based on demand.</p><p><br></p><p>https://cloud.google.com/compute/docs/autoscaler</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430160,
    "question_plain": "An application needs to be migrated from your on-premise data center to Google Cloud App Engine. You modified your application to use Cloud Pub/Sub with a specific service account which has the necessary permissions to publish and subscribe on Pub/Sub. However, Cloud Pub/Sub API has not yet been enabled. What should you do?",
    "answers": [
      "<p>You should navigate to the APIs &amp; Services section in Google Console and enable Cloud Pub/Sub API.</p>",
      "<p>You should use Deployment Manager to configure the App Engine to use a specific service account with the necessary permissions and rely on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe.</p>",
      "<p>You should configure the App Engine to use a specific service account with the necessary permissions and rely on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe.</p>",
      "<p>You should grant <code>roles/pubsub.admin</code> IAM role to the service account and modify the application code to enable the API before publishing or subscribing.</p>"
    ],
    "explanation": "<p>You should navigate to the APIs &amp; Services section in Google Console and enable Cloud Pub/Sub API. -&gt;&nbsp;Correct. The user should navigate to the APIs &amp; Services section in Google Console and enable the Cloud Pub/Sub API. Cloud Pub/Sub is a managed messaging service that enables communication between different components of an application. Before an application can use Pub/Sub, the Cloud Pub/Sub API must be enabled in the Google Cloud Console.</p><p><br></p><p>You should use Deployment Manager to configure the App Engine to use a specific service account with the necessary permissions and rely on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe. -&gt; Incorrect. This option is incorrect because it mentions relying on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe. However, this automatic enablement does not occur until the Cloud Pub/Sub API is explicitly enabled.</p><p><br></p><p>You should configure the App Engine to use a specific service account with the necessary permissions and rely on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe. -&gt; Incorrect. This option is incorrect because it mentions relying on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe. However, this automatic enablement does not occur until the Cloud Pub/Sub API is explicitly enabled.</p><p><br></p><p>You should grant <code>roles/pubsub.admin</code> IAM role to the service account and modify the application code to enable the API before publishing or subscribing. -&gt; Incorrect. It is incorrect because granting the <code>roles/pubsub.admin</code> IAM role to the service account would give it too many permissions, and modifying the application code to enable the API before publishing or subscribing is unnecessary. The API can be enabled in the Google Cloud Console.</p><p><br></p><p>https://cloud.google.com/endpoints/docs/openapi/enable-api</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430162,
    "question_plain": "A social media company stores images in a Cloud Storage bucket for long term. Images older than 30 days are accessed only in exceptional circumstances, and images older than six months are no longer needed. As a cloud architect, how can you optimize lifecycle management policy to reduce costs?",
    "answers": [
      "<p>You should configure a lifecycle management policy to transition objects older than 30 days to Coldline storage class. Than, configure another lifecycle management policy to delete objects older than six months.</p>",
      "<p>You should configure a lifecycle management policy to transition objects older than 30 days to Coldline storage class. Than, configure another lifecycle management policy to delete objects older than 365 days.</p>",
      "<p>You should configure a lifecycle management policy to transition objects older than 30 days to Archive storage class. Than, configure another lifecycle management policy to delete objects older than six months.</p>",
      "<p>You should use a Cloud Function to change the storage class to Coldline for objects older than 30 days. And use another Cloud Function to delete objects older than 6 months from Coldline Storage Class.</p>"
    ],
    "explanation": "<p>You should configure a lifecycle management policy to transition objects older than 30 days to Coldline storage class. Than, configure another lifecycle management policy to delete objects older than six months. -&gt;&nbsp;Correct. In this scenario, the images are stored in the cloud for long term, but they are accessed only in exceptional circumstances. This means that the data has a low access rate, and it is essential to optimize the storage cost. The company needs to keep the images for up to six months, after which they are no longer required. Google Cloud Storage provides different storage classes with different costs and performance characteristics. The Coldline storage class is designed for long-term storage of infrequently accessed data, and it provides a lower storage cost but with a longer retrieval time. Therefore, the first step is to configure a lifecycle management policy to transition objects older than 30 days to Coldline storage class, which is a cost-effective option for storing data that is not accessed frequently. Then, another lifecycle management policy should be configured to delete objects older than six months, which ensures that the data is retained for the required duration while minimizing storage costs.</p><p><br></p><p>You should configure a lifecycle management policy to transition objects older than 30 days to Coldline storage class. Than, configure another lifecycle management policy to delete objects older than 365 days. -&gt; Incorrect. It suggests to delete objects older than 365 days, which will cause additional costs.</p><p><br></p><p>You should configure a lifecycle management policy to transition objects older than 30 days to Archive storage class. Than, configure another lifecycle management policy to delete objects older than six months. -&gt; Incorrect. It suggests to transition objects to Archive storage class, which is intended for archiving data for disaster recovery. Also Archive storage class has a 365-day minimum storage duration.</p><p><br></p><p>You should use a Cloud Function to change the storage class to Coldline for objects older than 30 days. And use another Cloud Function to delete objects older than 6 months from Coldline Storage Class. -&gt; Incorrect. It proposes to use Cloud Functions to change the storage class and delete objects, which adds complexity and costs to the solution.</p><p><br></p><p>https://cloud.google.com/storage/docs/lifecycle</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430164,
    "question_plain": "As a cloud architect, you need to advise what to do in the following situation. A development team needs to directly connect their on-premises resources to several virtual machines inside a VPC. They want fast and secure access to these virtual machines with minimal maintenance and cost. What do you recommend?",
    "answers": [
      "<p>They should use Cloud VPN to create a bridge between the VPC and their network.</p>",
      "<p>They should set up Cloud Interconnect.</p>",
      "<p>They should assign a public IP address to each virtual machine and assign a strong password to each of them.</p>",
      "<p>They should start a Compute Engine virtual machine, install a software router, and create a direct tunnel to each virtual machine.</p>"
    ],
    "explanation": "<p>They should use Cloud VPN to create a bridge between the VPC and their network. -&gt; Correct. Cloud VPN provides a secure and cost-effective way to establish a secure connection between the on-premises resources and the virtual machines inside a VPC. Cloud VPN uses the internet to connect to the VPC, and this eliminates the need for dedicated connections or expensive hardware. Additionally, Cloud VPN provides a fast and secure connection by encrypting all traffic between the on-premises resources and the VPC. </p><p><br></p><p>They should assign a public IP address to each virtual machine and assign a strong password to each of them. -&gt;&nbsp;Incorrect. Assigning public IP addresses to virtual machines is not a secure solution because it exposes the virtual machines to the public internet. </p><p><br></p><p>They should set up Cloud Interconnect. -&gt; Incorrect. Cloud Interconnect is a dedicated connection service that can provide higher bandwidth and lower latency, but it's more expensive and requires more maintenance. </p><p><br></p><p>They should start a Compute Engine virtual machine, install a software router, and create a direct tunnel to each virtual machine. -&gt; Incorrect. It is also not the best solution because it adds unnecessary complexity and maintenance overhead.</p><p><br></p><p>https://cloud.google.com/network-connectivity/docs/vpn</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430166,
    "question_plain": "Your company needs to store audit logs of a project for at least 5 years. From time to time you would like to do some log analytics. What should you do in this case?",
    "answers": [
      "<p>You should route audit logs to BigQuery.</p>",
      "<p>You should route audit logs to Cloud Storage.</p>",
      "<p>You should route audit logs to Bigtable.</p>",
      "<p>You should route audit logs to Pub/Sub.</p>"
    ],
    "explanation": "<p>You should route audit logs to BigQuery. -&gt; Correct. BigQuery is a fully-managed, highly-scalable, and cost-effective data warehouse designed for large-scale data analytics. It allows you to store and query large amounts of data quickly and easily using SQL-like syntax. By routing audit logs to BigQuery, you can store the logs for at least 5 years, and you can also perform log analytics using BigQuery's powerful querying capabilities.</p><p><br></p><p>You should route audit logs to Cloud Storage. -&gt; Incorrect. Cloud Storage is also a valid option for storing audit logs, but it may not be as well-suited for log analytics as BigQuery. Cloud Storage is a simple, durable, and highly available object storage service that can be used for storing large amounts of unstructured data. However, it does not provide the advanced querying capabilities that BigQuery offers.</p><p><br></p><p>You should route audit logs to Bigtable. -&gt; Incorrect. Bigtable is a NoSQL database that can be used for real-time analytics and high-performance, low-latency applications. While it can be used for storing audit logs, it may not be the best option for long-term storage and analytics.</p><p><br></p><p>You should route audit logs to Pub/Sub. -&gt; Incorrect. Pub/Sub is a messaging service that can be used for asynchronous communication between different components of a system. It may be used for routing audit logs to other services for processing or storage, but it is not designed for long-term storage or analytics.</p><p><br></p><p>https://cloud.google.com/resource-manager/docs/audit-logging#exporting_audit_logs</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430168,
    "question_plain": "As a cloud architect, you have been tasked with architecting a complex deployment scenario for SAP HANA on Google Cloud. The client requires high availability (HA) and disaster recovery (DR) for their mission-critical SAP HANA databases. They also need the deployment to be in multiple regions for serving international customers. How would you design this deployment?",
    "answers": [
      "<p>Deploy a multi-node SAP HANA cluster in each region using Google Compute Engine and SAP HANA System Replication for HA and DR.</p>",
      "<p>Deploy a single-node SAP HANA instance in each region using Google Compute Engine and replicate data between them.</p>",
      "<p>Deploy a multi-node SAP HANA cluster in a single region using Google Compute Engine and Google Cloud Storage for backups.</p>",
      "<p>Deploy SAP HANA on Google Kubernetes Engine in multiple regions.</p>"
    ],
    "explanation": "<p>Deploy a multi-node SAP HANA cluster in each region using Google Compute Engine and SAP HANA System Replication for HA and DR. -&gt;&nbsp;Correct. A multi-node SAP HANA cluster in each region would provide high availability. SAP HANA System Replication enables database replication across multiple nodes and regions, providing both high availability (HA) and disaster recovery (DR) for SAP HANA databases.</p><p><br></p><p>Deploy a single-node SAP HANA instance in each region using Google Compute Engine and replicate data between them. -&gt; Incorrect. While this deployment would provide multiple instances in different regions, a single-node deployment would not provide the high availability required by the client.</p><p><br></p><p>Deploy a multi-node SAP HANA cluster in a single region using Google Compute Engine and Google Cloud Storage for backups. -&gt; Incorrect. Deploying a multi-node SAP HANA cluster in a single region would not fulfill the requirement for multi-region deployment to serve international customers.</p><p><br></p><p>Deploy SAP HANA on Google Kubernetes Engine in multiple regions. -&gt; Incorrect. SAP HANA is not typically deployed on Kubernetes. SAP HANA is a high-performance in-memory database that requires specialized, high-memory instances which are provided by Google Compute Engine, not Google Kubernetes Engine.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430170,
    "question_plain": "Suppose you work as a cloud architect and you want to create a group of virtual machines for processing big data (Hadoop, Spark). You can afford to interrupt jobs as they can be easily recreated. The solution must be as low cost as possible. What should you do?",
    "answers": [
      "<p>You should create a Managed Instance Group that uses Preemptible VMs.</p>",
      "<p>You should create a Managed Instance Group with VMs in a single zone.</p>",
      "<p>You should create a Managed Instance Group with VMs in multiple zones.</p>",
      "<p>You should create a Unmanaged Instance Group.</p>"
    ],
    "explanation": "<p>You should create a Managed Instance Group that uses Preemptible VMs. -&gt;&nbsp;Correct. Preemptible VMs are a type of virtual machine on Google Cloud Platform that can be used at a significantly lower cost compared to regular VMs. The tradeoff is that Google Cloud can reclaim these instances with only a 30-second warning, but since the jobs can be easily recreated, this is not a problem. Managed instance groups allow you to create and manage groups of virtual machines that work together to run your workloads, and they can automatically create new VMs to replace the preemptible ones that get terminated.</p><p><br></p><p>You should create a Managed Instance Group with VMs in a single zone. -&gt; Incorrect. Creating a Managed Instance Group with VMs in a single zone, would not be the best solution because if there is an outage in that zone, the entire group would be affected.</p><p><br></p><p>You should create a Managed Instance Group with VMs in multiple zones. -&gt; Incorrect. Creating a Managed Instance Group with VMs in multiple zones, would be more resilient to outages but would also increase costs.</p><p><br></p><p>You should create a Unmanaged Instance Group. -&gt; Incorrect. Creating an unmanaged instance group, would not provide the automatic replacement of terminated instances that managed instance groups with preemptible VMs can provide.</p><p><br></p><p>https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances#create_managed_group</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430172,
    "question_plain": "As a cloud architect, you are responsible for the user management service for your global company. This service will perform basic operations such as viewing, adding, updating, deleting addresses. Each of these operations is implemented by a Docker container microservice. The processing load can vary from low to very high. You want to deploy the service on Google Cloud for scalability and minimal administration. What should you do?",
    "answers": [
      "<p>You should deploy your Docker containers into Cloud Run.</p>",
      "<p>You should start each Docker container as a Managed Instance Group (MIG).</p>",
      "<p>You should deploy your Docker containers into GKE.</p>",
      "<p>You should combine four microservices into a single Docker image, and deploy it to the App Engine instance.</p>"
    ],
    "explanation": "<p>You should deploy your Docker containers into Cloud Run. -&gt; Correct. The user management service needs to be scalable and easy to administer. Cloud Run is a managed compute platform that automatically scales containerized applications based on incoming requests. This makes it an excellent choice for deploying microservices, as each microservice can be containerized and deployed on its own. In addition, Cloud Run is serverless, which means that you don't have to worry about provisioning or managing any servers. You simply need to upload your container images and Cloud Run will handle the rest.</p><p><br></p><p>You should start each Docker container as a Managed Instance Group (MIG). -&gt;&nbsp;Incorrect. It is not the best approach for deploying microservices, as MIGs are designed for stateful applications that require a fixed number of instances.</p><p><br></p><p>You should deploy your Docker containers into GKE. -&gt; Incorrect. It is also a good option, but it requires more administration and management than Cloud Run. You would need to set up and manage the Kubernetes cluster, including nodes and pods.</p><p><br></p><p>You should combine four microservices into a single Docker image, and deploy it to the App Engine instance. -&gt;&nbsp;Incorrect. It is not a good approach for microservices architecture, as it goes against the microservices principle of each service being independent and scalable.</p><p><br></p><p>https://cloud.google.com/run/docs/deploying</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430174,
    "question_plain": "A legacy systems run in your on-premises data center. As a cloud architect, you are responsible for migration these systems to the cloud and you plan to decommission all existing applications and completely redesign and rewrite them as cloud-native applications. Which approach should you choose?",
    "answers": [
      "<p>Remove and Replace</p>",
      "<p>Lift and Shift</p>",
      "<p>Improve and Move</p>",
      "<p>Blue-green</p>"
    ],
    "explanation": "<p>Remove and Replace -&gt; Correct. In this scenario, the goal is to completely redesign and rewrite the legacy applications as cloud-native applications. The \"Remove and Replace\" approach involves decommissioning the existing applications and building new cloud-native applications from scratch. This approach provides the opportunity to take full advantage of cloud-native technologies and design patterns, and can lead to more efficient and scalable applications that are better suited to the cloud environment.</p><p><br></p><p>Lift and Shift -&gt; Incorrect. This approach involves moving the existing applications to the cloud with minimal changes. It is not well-suited for completely redesigning and rewriting applications as cloud-native applications.</p><p><br></p><p>Improve and Move -&gt; Incorrect. This approach involves improving the existing applications and then migrating them to the cloud. While it may be suitable for some scenarios, it is not the best fit for completely redesigning and rewriting applications as cloud-native applications.</p><p><br></p><p>Blue-green -&gt; Incorrect. This approach involves deploying two identical environments (blue and green) and switching between them for upgrades or rollbacks. While it can be useful for reducing downtime during upgrades or rollbacks, it is not well-suited for completely redesigning and rewriting applications as cloud-native applications.</p><p><br></p><p>https://cloud.google.com/architecture/migration-to-gcp-getting-started#types_of_migrations</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430176,
    "question_plain": "Your organization, a multinational bank, has decided to migrate its operations to Google Cloud. The applications include a transaction processing system, a customer relationship management (CRM) tool, an intranet site, a data warehouse, and an email server. Given the need for continuous high availability and security compliance, which application should be migrated first?",
    "answers": [
      "<p>The intranet site</p>",
      "<p>The transaction processing system</p>",
      "<p>The customer relationship management (CRM) tool</p>",
      "<p>The data warehouse</p>"
    ],
    "explanation": "<p>The intranet site -&gt;&nbsp;Correct. The intranet site is a good candidate to be migrated first. It's typically less critical than the other systems and could allow the organization to gain familiarity with Google Cloud without risking core operational systems.</p><p><br></p><p>The transaction processing system -&gt;&nbsp;Incorrect. Migrating this system first might expose the organization to significant risk, as transaction systems are usually critical for day-to-day operations. It would be safer to start with less critical applications.</p><p><br></p><p>The customer relationship management (CRM) tool -&gt;&nbsp;Incorrect. The CRM tool is important for maintaining customer relationships and contains sensitive data. Migrating this system first could be risky.</p><p><br></p><p>The data warehouse -&gt;&nbsp;Incorrect. Migrating the data warehouse first might expose the organization to risks related to data loss or compliance issues. It would be safer to start with less critical applications.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430178,
    "question_plain": "Your company runs a mission-critical application on Google Cloud with multiple microservices. Recently, the application experienced downtime due to a service failure, which had a cascading effect. As a Cloud Architect, what should you do to enhance the solution's reliability and prevent such incidents in the future?",
    "answers": [
      "<p>Implement circuit breakers and retries with exponential backoff and jitter for inter-service communications.</p>",
      "<p>Implement regular disaster recovery drills to ensure the readiness of the backup system.</p>",
      "<p>Move the application to a monolithic architecture to simplify the complexity.</p>",
      "<p>Implement load balancing and auto-scaling for all microservices.</p>"
    ],
    "explanation": "<p>Implement circuit breakers and retries with exponential backoff and jitter for inter-service communications. -&gt; Correct. Circuit breakers can prevent a single service failure from bringing down the whole system by stopping the flow of traffic to the failed service. Retries with exponential backoff and jitter can give the service time to recover, reducing the chances of cascading failure.</p><p><br></p><p>Implement regular disaster recovery drills to ensure the readiness of the backup system. -&gt;&nbsp;Incorrect. While disaster recovery drills are good practice for ensuring system recovery, they do not inherently prevent service failures or cascading effects.</p><p><br></p><p>Move the application to a monolithic architecture to simplify the complexity. -&gt;&nbsp;Incorrect. This approach does not inherently improve reliability and might in fact reduce the ability to scale and manage the application effectively.</p><p><br></p><p>Implement load balancing and auto-scaling for all microservices. -&gt;&nbsp;Incorrect. While load balancing and auto-scaling can help maintain service availability and performance, they do not inherently prevent cascading failures from a service failure.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430180,
    "question_plain": "You are a cloud architect for a large media corporation that has decided to adopt Google Cloud. They have an extensive infrastructure including a content management system (CMS), a user registration system, an ad serving platform, an analytics platform, and an internal communication platform. Which one of these systems would you consider as a good candidate for the first migration?",
    "answers": [
      "<p>The analytics platform</p>",
      "<p>The ad serving platform</p>",
      "<p>The user registration system</p>",
      "<p>The content management system (CMS)</p>"
    ],
    "explanation": "<p>The analytics platform -&gt;&nbsp;Correct. The analytics platform is a good choice for a first migration because it typically doesn't directly affect the user experience. Also, there are robust tools in Google Cloud for analytics, making it a relatively straightforward migration.</p><p><br></p><p>The ad serving platform -&gt;&nbsp;Incorrect. This system directly impacts the organization's revenue, so any disruption during migration could lead to significant revenue loss.</p><p><br></p><p>The user registration system -&gt;&nbsp;Incorrect. The user registration system likely contains sensitive user data, and any disruption or data loss during migration could have significant consequences.</p><p><br></p><p>The content management system (CMS) -&gt;&nbsp;Incorrect. A CMS is often tightly integrated with other systems and might have significant dependencies, making it a challenging choice for a first migration.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430182,
    "question_plain": "A mobile gaming company has a new mobile game with features that allow users to accumulate points by playing the game. Points can be use to make in-game purchases. You want to prevent bot activity (playing the game much faster than humans) and you don't want to ban users (if not necessary). What should you do?",
    "answers": [
      "<p>You should modify the game API to prevent more than 5-10 function calls per user, per minute.</p>",
      "<p>You should ban users with bot activity. Some users may be wrongly banned.</p>",
      "<p>You should alert users with bot activity. Some users may be wrongly alerted.</p>",
      "<p>There is nothing you can do in this case.</p>"
    ],
    "explanation": "<p>You should modify the game API to prevent more than 5-10 function calls per user, per minute. -&gt; Correct. By implementing rate limiting on the game API, you can restrict the number of function calls a user can make within a specified time period. This helps prevent bot activity by limiting the speed at which requests can be made. By setting a reasonable limit, you can strike a balance between preventing excessive activity from bots and allowing genuine users to enjoy the game without being unfairly restricted.</p><p><br></p><p>You should ban users with bot activity. Some users may be wrongly banned. -&gt;&nbsp;Incorrect. While banning users may effectively address the bot activity concern, there is a risk of false positives, where legitimate users may be wrongly identified as bots and banned. This approach may result in a negative user experience and potential loss of genuine users.</p><p><br></p><p>You should alert users with bot activity. Some users may be wrongly alerted. -&gt;&nbsp;Incorrect. While it's important to be transparent and inform users about any potential suspicious activity, relying solely on alerts may not be sufficient to prevent bot activity. It may result in some legitimate users receiving false alerts, leading to confusion and frustration.</p><p><br></p><p>There is nothing you can do in this case. -&gt;&nbsp;Incorrect. This answer choice implies that there are no actions that can be taken to address the bot activity issue. However, there are proactive measures that can be implemented, such as rate limiting, to mitigate the impact of bot activity and protect the integrity of the game.</p><p><br></p><p>https://cloud.google.com/compute/docs/api-rate-limits</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430184,
    "question_plain": "As a cloud architect, it is your duty to establish industry best practices within your company. In the context of API error messages, what specific data and information would you suggest including?",
    "answers": [
      "<p>An API error message should return a status code form with the standard 400s and 500s HTTP status codes along with additional error details in the payload.</p>",
      "<p>An API error message should return HTTP status 200 with additional error details in the payload.</p>",
      "<p>An API error message should return error details in the payload, and don't return a status code.</p>",
      "<p>An API error message should define your own set of application-specific error codes.</p>"
    ],
    "explanation": "<p>An API error message should return a status code form with the standard 400s and 500s HTTP status codes along with additional error details in the payload. -&gt;&nbsp;Correct. An API error message is a response that a client receives when an error occurs while making an API request. It is important to provide clear and useful information in the error message to help developers understand what went wrong and how to fix it. Returning an HTTP status code is essential to indicate whether the request was successful or not. Standard HTTP status codes such as 400s and 500s should be used to indicate the type of error that occurred, such as 400 for client-side errors or 500 for server-side errors. In addition to the status code, the API error message should also include additional error details in the payload, such as a message explaining what went wrong, and potentially additional information to help developers diagnose the problem.</p><p><br></p><p>An API error message should return HTTP status 200 with additional error details in the payload. -&gt; Incorrect. It is not recommended as returning HTTP status 200 would indicate a successful request, which is misleading in the case of an error. </p><p><br></p><p>An API error message should return error details in the payload, and don't return a status code. -&gt; Incorrect. It is not recommended as it would not provide any indication of whether the request was successful or not. </p><p><br></p><p>An API error message should define your own set of application-specific error codes. -&gt; Incorrect. It may be appropriate in some cases, but it is generally better to use standard HTTP status codes as they are widely recognized and understood.</p><p><br></p><p>https://cloud.google.com/apis/design/errors</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430186,
    "question_plain": "Your company operates a large-scale e-commerce platform on Google Cloud with millions of daily users. You have been tasked with introducing a new feature to the platform. The feature has been thoroughly tested but has never been deployed in production. Given the potential for unforeseen issues, you have been asked to implement a deployment strategy that minimizes potential disruptions. Which approach should you take?",
    "answers": [
      "<p>Use a Canary deployment strategy.</p>",
      "<p>Use a Blue/Green deployment strategy.</p>",
      "<p>Use a Red/Black deployment strategy.</p>",
      "<p>Deploy the feature to all users at once without any gradual rollout.</p>"
    ],
    "explanation": "<p>Use a Canary deployment strategy. -&gt;&nbsp;Correct. Canary deployments allow for the new version of the application to be rolled out gradually to a small percentage of users. This allows you to monitor the new version in production, reducing the potential impact of any issues.</p><p><br></p><p>Use a Blue/Green deployment strategy. -&gt; Incorrect. While Blue/Green deployment strategy allows for quick rollback, it doesn't allow for gradual rollout of features, meaning if an issue arises, it could impact a large number of users immediately.</p><p><br></p><p>Use a Red/Black deployment strategy. -&gt; Incorrect. Red/Black deployment strategy is essentially the same as Blue/Green deployment strategy - it allows for easy rollback but doesn't provide a gradual rollout of the new feature.</p><p><br></p><p>Deploy the feature to all users at once without any gradual rollout. -&gt; Incorrect. Deploying a new, untested feature to all users at once is likely to lead to major disruptions if any unforeseen issues arise.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430188,
    "question_plain": "Your organization has just migrated their production environment to Google Cloud. As part of regulatory compliance, all administrative activities must be audited, and logs must be kept for at least one year. As a cloud architect, how would you ensure this requirement is met?",
    "answers": [
      "<p>Enable Cloud Logging on all resources and export logs to Cloud Storage with a lifecycle policy of one year.</p>",
      "<p>Use Cloud Monitoring to audit all administrative activities.</p>",
      "<p>Enable VPC Flow Logs for auditing all administrative activities.</p>",
      "<p>Use Cloud Functions to write log entries into BigQuery for long-term storage.</p>"
    ],
    "explanation": "<p>Enable Cloud Logging on all resources and export logs to Cloud Storage with a lifecycle policy of one year. -&gt;&nbsp;Correct. This approach meets the requirement to audit all administrative activities and keep logs for at least one year. Cloud Logging will capture the logs, and exporting them to Cloud Storage allows for cost-effective long-term storage.</p><p><br></p><p>Use Cloud Monitoring to audit all administrative activities. -&gt;&nbsp;Incorrect. Cloud Monitoring is useful for performance metrics and uptime, but it does not capture administrative activities in the same way as Cloud Logging does.</p><p><br></p><p>Enable VPC Flow Logs for auditing all administrative activities. -&gt;&nbsp;Incorrect. VPC Flow Logs capture network traffic, but do not capture other administrative activities, such as changes to IAM policies or resource configurations.</p><p><br></p><p>Use Cloud Functions to write log entries into BigQuery for long-term storage. -&gt;&nbsp;Incorrect. This approach might be too complex and costly. While BigQuery is a good option for querying large datasets, it's not typically used for long-term log storage.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430190,
    "question_plain": "As a cloud architect, you are working with a client who wants to store their Git repositories within the Google Cloud environment. The client also requires functionality for code review, branch management, and collaborative development. Which service in Google Cloud should you recommend for these requirements?",
    "answers": [
      "<p>Cloud Source Repositories</p>",
      "<p>Cloud Storage with object versioning enabled</p>",
      "<p>Cloud Pub/Sub</p>",
      "<p>Cloud Git</p>"
    ],
    "explanation": "<p>Cloud Source Repositories -&gt; Correct. Cloud Source Repositories is a single place for your team to store, manage, and track code, providing features like code review and collaboration, making it an ideal choice for storing Git repositories.</p><p><br></p><p>Cloud Storage with object versioning enabled -&gt; Incorrect. Cloud Storage is a service for storing and retrieving any amount of data at any time, but it's not optimized for storing Git repositories and providing collaboration and code review features.</p><p><br></p><p>Cloud Pub/Sub -&gt; Incorrect. Cloud Pub/Sub is a messaging service for exchanging event data among services and applications, not for storing Git repositories.</p><p><br></p><p>Cloud Git -&gt; Incorrect. There is no such service in Google Cloud.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430192,
    "question_plain": "As a cloud architect, you are managing a complex deployment scenario that requires high availability when using Google Cloud Storage. Which of the following practices would provide the highest level of data durability and availability?",
    "answers": [
      "<p>Store all data in a multi-region Cloud Storage bucket.</p>",
      "<p>Store all data in a single-region Cloud Storage bucket.</p>",
      "<p>Store all data in a dual-region Cloud Storage bucket.</p>",
      "<p>Store all data in a local SSD.</p>"
    ],
    "explanation": "<p>Store all data in a multi-region Cloud Storage bucket. -&gt; Correct. Multi-region Cloud Storage buckets offer the highest availability as data is automatically stored redundantly in multiple geographically distant regions. This helps ensure that your data is still accessible even if one entire region goes offline.</p><p><br></p><p>Store all data in a single-region Cloud Storage bucket. -&gt; Incorrect. A single-region Cloud Storage bucket keeps your data in one region. While this offers lower latency and costs when the users are mainly in the same region, it does not provide the same level of high availability as multi-region storage.</p><p><br></p><p>Store all data in a dual-region Cloud Storage bucket. -&gt; Incorrect. Dual-region Cloud Storage provides higher availability compared to a single-region, but not as high as a multi-region bucket. Data in a dual-region bucket is stored in two specific regions.</p><p><br></p><p>Store all data in a local SSD. -&gt;&nbsp;Incorrect. Local SSDs provide high-performance storage but they are physically attached to the server hosting the VM instance. They do not offer the same durability and availability as replicated cloud storage, and data can be lost if the VM is stopped or if the SSD fails.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430194,
    "question_plain": "As a cloud architect, you are responsible for a Lift and Shift migration to the Google Cloud. You have several load-balanced clusters that use virtual machines that are not identically configured. You don't want to make unnecessary changes when moving to the cloud. Which GCP feature should you use?",
    "answers": [
      "<p>Unmanaged Instance Groups</p>",
      "<p>Managed Instance Groups</p>",
      "<p>Kubernetes clusters</p>",
      "<p>App Engine</p>"
    ],
    "explanation": "<p>Unmanaged Instance Groups -&gt; Correct. Unmanaged instance groups allow you to bring together virtual machines with non-identical configurations, which is required for your Lift and Shift migration as per the scenario given in the question. You do not want to make unnecessary changes and the virtual machines are not identically configured. Unmanaged instance groups provide this flexibility.</p><p><br></p><p>Managed Instance Groups -&gt;&nbsp;Incorrect. Managed Instance Groups (MIGs) in Google Cloud are a collection of identical instances. This would require your load-balanced clusters to have identically configured virtual machines, which contradicts the premise of your question.</p><p><br></p><p>Kubernetes clusters -&gt;&nbsp;Incorrect. Kubernetes is an open-source platform for managing containerized workloads and services. While it's a powerful tool, using it would entail changes to your existing VM-based architecture, which you are trying to avoid according to the question.</p><p><br></p><p>App Engine -&gt;&nbsp;Incorrect. App Engine is a fully managed serverless platform for developing and hosting web applications. Like Kubernetes, using App Engine would necessitate significant changes to your existing setup, which contradicts the desire to avoid unnecessary changes mentioned in the question.</p><p><br></p><p>https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-unmanaged-instances</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430196,
    "question_plain": "As a cloud architect, you are responsible for migrating stateless application written in Python to the Google Cloud. You want to avoid managing servers or containers and this application must be scalable. Which GCP&nbsp;service do you recommend?",
    "answers": [
      "<p>App Engine</p>",
      "<p>Compute Engine</p>",
      "<p>Kubernetes Engine</p>",
      "<p>Cloud Function</p>"
    ],
    "explanation": "<p>App Engine -&gt; Correct. App Engine is a fully managed Platform-as-a-Service (PaaS) offering from Google Cloud that allows you to deploy web applications and APIs written in several programming languages, including Python, without worrying about managing servers or containers. App Engine automatically scales your application based on traffic and provides a high level of availability.</p><p><br></p><p>Compute Engine -&gt; Incorrect. Compute Engine is an Infrastructure-as-a-Service (IaaS) offering that allows you to deploy virtual machines and manage them yourself. While Compute Engine provides a lot of flexibility, it requires more management and configuration than App Engine, making it less suitable for stateless applications that need to be scalable.</p><p><br></p><p>Kubernetes Engine -&gt; Incorrect. Kubernetes Engine is a managed Kubernetes service that allows you to deploy, manage, and scale containerized applications. While Kubernetes Engine provides a lot of flexibility and scalability, it requires more management and configuration than App Engine, making it less suitable for stateless applications that need to be scalable.</p><p><br></p><p>Cloud Function -&gt; Incorrect. Cloud Functions is a serverless compute service that allows you to run code in response to events without worrying about managing servers or containers. However, Cloud Functions is designed for short-lived, event-driven functions rather than stateless applications, making it less suitable for this scenario.</p><p><br></p><p>https://cloud.google.com/appengine/docs</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430198,
    "question_plain": "A monolithic Python application needs to be migrated to the Google Cloud. As a cloud architect, you are planning to use the Lift and Shift strategy. You have a Dockerfile created with this application and an image container. You want to minimize effort to maintain it. What should you do?",
    "answers": [
      "<p>You should use App Engine Flexible to run the container image.</p>",
      "<p>You should use App Engine Standard to run the container image.</p>",
      "<p>You should create a Compute Engine instance and configure it from scratch.</p>",
      "<p>You should use Managed Instance Group to run the container image.</p>"
    ],
    "explanation": "<p>You should use App Engine Flexible to run the container image. -&gt; Correct. App Engine Flexible is the best option for running a Docker container on Google Cloud using the Lift and Shift strategy. App Engine Flexible allows developers to package and deploy their applications in a Docker container without the need to manage infrastructure or worry about scalability, while also providing access to many GCP services.</p><p><br></p><p>You should use App Engine Standard to run the container image. -&gt; Incorrect. App Engine Standard is not suitable in this case as it does not support running Docker containers directly. </p><p><br></p><p>You should create a Compute Engine instance and configure it from scratch. -&gt; Incorrect. Creating a Compute Engine instance from scratch would require manual configuration and maintenance, which is not ideal for minimizing effort. </p><p><br></p><p>You should use Managed Instance Group to run the container image. -&gt; Incorrect. Managed Instance Group could be an option, but it would require additional setup and configuration, making it less optimal than App Engine Flexible.</p><p><br></p><p>https://cloud.google.com/appengine/docs/flexible</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430200,
    "question_plain": "As a cloud architect, you are managing a complex deployment scenario for a multinational enterprise. The company requires that all data at rest be encrypted using encryption keys that the company manages itself. The company also needs to use different encryption keys for its branches in North America, Europe, and Asia. How can you implement this requirement in Google Cloud?",
    "answers": [
      "<p>Use customer-managed encryption keys (CMEK), create separate keys for each region in Cloud KMS, and assign them accordingly.</p>",
      "<p>Use Google-managed encryption keys and assign different keys to different regions.</p>",
      "<p>Use Cloud Storage with Uniform bucket-level access and provide different keys for different regions.</p>",
      "<p>Use Cloud Datastore and provide different keys for different regions.</p>"
    ],
    "explanation": "<p>Use customer-managed encryption keys (CMEK), create separate keys for each region in Cloud KMS, and assign them accordingly. -&gt; Correct. The use of customer-managed encryption keys (CMEK) allows the company to create, control, and manage its encryption keys through Google Cloud KMS. The company can create separate keys for each region and assign them to resources accordingly, which fulfills its requirements.</p><p><br></p><p>Use Google-managed encryption keys and assign different keys to different regions. -&gt; Incorrect. Google-managed encryption keys do not give the company the control it needs over its keys and the ability to use different keys in different regions.</p><p><br></p><p>Use Cloud Storage with Uniform bucket-level access and provide different keys for different regions. -&gt; Incorrect. Cloud Storage with Uniform bucket-level access is a storage solution that controls access to the bucket as a whole, but this alone doesn't answer the need for the company to manage its own encryption keys and use different keys in different regions.</p><p><br></p><p>Use Cloud Datastore and provide different keys for different regions. -&gt; Incorrect. Cloud Datastore is a NoSQL document database built for automatic scaling and high performance, but it doesn't directly address the requirement for managing custom encryption keys per region.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430202,
    "question_plain": "A data scientist has prepared a query in BigQuery and expects to process a large amount of data. Before executing the query, he wants to know how much data will be processed. What should a data scientist do?",
    "answers": [
      "<p>When he runs a query in the <code>bq</code> command-line tool, he should use <code>--dry_run</code> flag to estimate the number of bytes read by the query.</p>",
      "<p>When he runs a query in the <code>bq</code> command-line tool, he should use <code>--cost</code> flag to estimate the number of bytes read by the query.</p>",
      "<p>He should use Google Cloud Pricing Calculator to estimate the number of bytes read by the query.</p>",
      "<p>The number of bytes cannot be estimated without executing the query.</p>"
    ],
    "explanation": "<p>The correct answer is to use <code>--dry_run</code> flag. The Google Cloud Pricing Calculator cannot estimate the number of bytes read by the query. -&gt; Correct. When the data scientist runs a query in the bq command-line tool, they should use the <code>--dry_run</code> flag to estimate the number of bytes read by the query. The <code>--dry_run</code> flag in BigQuery's <code>bq</code> command-line tool is designed to simulate a query's execution, including the amount of data read, without actually executing the query. By using this flag, the data scientist can estimate the size of the data that the query will process before executing it.</p><p><br></p><p>When he runs a query in the <code>bq</code> command-line tool, he should use <code>--cost</code> flag to estimate the number of bytes read by the query. -&gt; Incorrect. It is incorrect because the <code>--cost</code> flag is used to estimate the cost of a query based on the amount of data processed, not the size of the data.</p><p><br></p><p>He should use Google Cloud Pricing Calculator to estimate the number of bytes read by the query. -&gt; Incorrect. It is incorrect because the Google Cloud Pricing Calculator is used to estimate the cost of running a workload on Google Cloud Platform, including BigQuery, but it does not estimate the amount of data processed by a specific query.</p><p><br></p><p>The number of bytes cannot be estimated without executing the query. -&gt; Incorrect. It is incorrect because the amount of data processed can be estimated using the <code>--dry_run</code> flag before executing the query.</p><p><br></p><p>https://cloud.google.com/bigquery/docs/dry-run-queries</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430204,
    "question_plain": "You are a cloud architect at a multinational corporation that has recently decided to move its infrastructure to Google Cloud Platform. The company has several departments globally, with each department having multiple teams. You need to design a resource hierarchy that allows you to apply company-wide policies, segregate resources at the department level, and isolate resources at the team level. What would be the best way to set up your GCP resource hierarchy?",
    "answers": [
      "<p>Create an organization for the company, a project for each team, and segregate departments using folders.</p>",
      "<p>Create a project for each department and use IAM roles to segregate resources at the team level.</p>",
      "<p>Create an organization for the company, a project for each department, and folders for each team.</p>",
      "<p>Create an organization for each department and projects for each team under those organizations.</p>"
    ],
    "explanation": "<p>Create an organization for the company, a project for each team, and segregate departments using folders. -&gt;&nbsp;Correct. The organization node is the root node in the Google Cloud resource hierarchy and represents your company. Under the organization, creating folders for each department and then projects for each team would allow you to apply company-wide policies, segregate resources at the department level, and isolate resources at the team level.</p><p><br></p><p>Create a project for each department and use IAM roles to segregate resources at the team level. -&gt;&nbsp;Incorrect. This approach won't provide the level of isolation required at the team level, as resources within the same project are part of the same trust boundary.</p><p><br></p><p>Create an organization for the company, a project for each department, and folders for each team. -&gt;&nbsp;Incorrect. Folders should not be created under projects in GCP resource hierarchy. Folders can only be created under an organization or other folders.</p><p><br></p><p>Create an organization for each department and projects for each team under those organizations. -&gt;&nbsp;Incorrect. In GCP, an organization is typically used to represent a company rather than a department. Using an organization for each department is an inappropriate use of the resource hierarchy.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430206,
    "question_plain": "As a cloud architect, you are involved in a project that requires the utilization of numerous virtual machines in Compute Engine. These virtual machines can exist in various states such as Running, Suspended, Stopped, or Deleted. Please select all accurate statements that demonstrate how each state influences the billing aspect.",
    "answers": [
      "<p>In Running state, you pay for the vCPU, memory, and disk utilization.</p>",
      "<p>In Stopped state, you only pay for disk utilization.</p>",
      "<p>In Deleted state, you only pay for disk utilization.</p>",
      "<p>In Suspended state, you pay for the vCPU, memory, and disk utilization.</p>"
    ],
    "explanation": "<p>In Running state, you pay for the vCPU, memory, and disk utilization. -&gt;&nbsp;Correct. When a virtual machine is running, you are billed for the resources it consumes, including vCPU, memory, and disk utilization. The billing is based on the machine type and usage.</p><p><br></p><p>In Stopped state, you only pay for disk utilization. -&gt;&nbsp;Correct. When a virtual machine is stopped, you are not billed for vCPU and memory usage. However, you are still billed for the disk storage that is associated with the stopped virtual machine. This allows you to retain the disk data while not incurring costs for the compute resources.</p><p><br></p><p>In Deleted state, you only pay for disk utilization. -&gt;&nbsp;Incorrect. When a virtual machine is deleted, you are not billed for any resources, including disk utilization. Deleting a virtual machine removes all associated resources, and you are no longer charged for them.</p><p><br></p><p>In Suspended state, you pay for the vCPU, memory, and disk utilization. -&gt;&nbsp;Incorrect. During suspension, compute resources are generally not billed.</p><p><br></p><p>https://cloud.google.com/architecture/cost-optimization-using-automated-vm-management?hl=en#stopping_suspending_and_deleting_instances</p>",
    "correct_response": ["a", "b"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 78430208,
    "question_plain": "Refer to the EHR Healthcare case study for this question: https://services.google.com/fh/files/blogs/master_case_study_ehr_healthcare.pdfEHR's client in the healthcare sector is a world-renowned research and hospital facility. A significant number of the patients at this renowned research and hospital facility are prominent public figures. There have been consistent attempts from both internal and external sources to illicitly access the health information of these patients. To safeguard patient privacy, the hospital has implemented a policy that restricts the movement of patient information stored in Cloud Storage buckets beyond the geographic boundaries where the buckets are located. It is crucial for you to ensure that the data stored in Cloud Storage buckets within the europe-west2 region remains confined within this specific region and does not get transferred elsewhere. What actions are recommended?",
    "answers": [
      "<p>You should enable Virtual Private Network Service Controls, and create a service perimeter around the Cloud Storage resources.</p>",
      "<p>You should encrypt the data in the application on-premises before the data is stored in the <code>europe-west2</code> region.</p>",
      "<p>You should assign the Identity and Access Management (IAM) <code>storage.objectViewer</code> role only to users and service accounts that need to use the data.</p>",
      "<p>You should create an access control list (ACL) that limits access to the bucket to authorized users only, and apply it to the buckets in the <code>europe-west2</code> region.</p>"
    ],
    "explanation": "<p>You should enable Virtual Private Network Service Controls, and create a service perimeter around the Cloud Storage resources. -&gt; Correct. VPC Service Controls improves your ability to mitigate the risk of data exfiltration from Google Cloud services.</p><p><br></p><p>You should encrypt the data in the application on-premises before the data is stored in the <code>europe-west2</code> region. -&gt; Incorrect. Encrypting the data does not stop data exfiltration.</p><p><br></p><p>You should assign the Identity and Access Management (IAM) <code>storage.objectViewer</code> role only to users and service accounts that need to use the data. -&gt; Incorrect. IAM roles deal with identity-based access control, not context-aware perimeter security.</p><p><br></p><p>You should create an access control list (ACL) that limits access to the bucket to authorized users only, and apply it to the buckets in the <code>europe-west2</code> region. -&gt; Incorrect. Cloud Storage ACLs are a mechanism you can use to define who has access to your buckets and objects, as well as their level of access. ACLs do not stop data exfiltration.</p><p><br></p><p>https://cloud.google.com/vpc-service-controls/docs/overview</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430210,
    "question_plain": "Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdfAs a cloud architect, your task is to deploy Virtual Private Cloud (VPC) Service Controls for Mountkirk Games. The objective is to allow developers to utilize Cloud Shell while ensuring they do not have unrestricted access to managed services. It is crucial to find a balance between these conflicting goals while considering the business requirements of Mountkirk Games. What steps should you take in this scenario?",
    "answers": [
      "<p>You should create a service perimeter around only the projects that handle sensitive data, and do not grant your developers access to it.</p>",
      "<p>You should use VPC Service Controls for the entire platform.</p>",
      "<p>You should prioritize VPC Service Controls implementation over Cloud Shell usage for the entire platform.</p>",
      "<p>You should include all developers in an access level associated with the service perimeter, and allow them to use Cloud Shell. </p>"
    ],
    "explanation": "<p>You should create a service perimeter around only the projects that handle sensitive data, and do not grant your developers access to it. -&gt;&nbsp;Correct. VPC Service Controls protects data, and Cloud Shell facilitates rapid iteration of Google Cloud architecture changes.</p><p><br></p><p>You should use VPC Service Controls for the entire platform. -&gt;&nbsp;Incorrect. VPC Service Controls do not directly affect scaling of Google Cloud architecture.</p><p><br></p><p>You should prioritize VPC Service Controls implementation over Cloud Shell usage for the entire platform. -&gt;&nbsp;Incorrect. A security perimeter is not a business requirement, but rapid iteration is.</p><p><br></p><p>You should include all developers in an access level associated with the service perimeter, and allow them to use Cloud Shell. -&gt;&nbsp;Incorrect. It works but does not scale, and we do not want to give users this much access.</p><p><br></p><p>https://cloud.google.com/vpc-service-controls/docs/overview</p><p>https://cloud.google.com/vpc-service-controls/docs/supported-products#shell</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430212,
    "question_plain": "The objective of your company is to monitor the presence of individuals in meeting rooms that have been booked for scheduled meetings. There are a total of 5,000 conference rooms spread across six offices located on four continents. Each room is equipped with a motion sensor that provides status updates every second. As a cloud architect, your goal is to establish a data ingestion system that can handle the requirements of this sensor network. The receiving infrastructure needs to consider the fact that the devices may experience inconsistent connectivity. Which solution is worth designing?",
    "answers": [
      "<p>Have devices poll for connectivity to Pub/Sub and publish the latest messages on a regular interval to a shared topic for all devices.</p>",
      "<p>Have each device create a persistent connection to a Compute Engine instance and write messages to a custom application.</p>",
      "<p>Have devices poll for connectivity to Cloud SQL and insert the latest messages on a regular interval to a device specific table.</p>",
      "<p>Have devices create a persistent connection to an App Engine application fronted by Cloud Endpoints, which ingest messages and write them to Datastore.</p>"
    ],
    "explanation": "<p>Have devices poll for connectivity to Pub/Sub and publish the latest messages on a regular interval to a shared topic for all devices. -&gt;&nbsp;Correct. Pub/Sub can handle the frequency of this data, and consumers of the data can pull from the shared topic for further processing.</p><p><br></p><p>Have each device create a persistent connection to a Compute Engine instance and write messages to a custom application. -&gt; Incorrect. Having a persistent connection does not handle the case where the device is disconnected.</p><p><br></p><p>Have devices poll for connectivity to Cloud SQL and insert the latest messages on a regular interval to a device specific table. -&gt; Incorrect. Cloud SQL is a regional, relational database and not the best fit for sensor data. Additionally, the frequency of the writes has the potential to exceed the supported number of concurrent connections.</p><p><br></p><p>Have devices create a persistent connection to an App Engine application fronted by Cloud Endpoints, which ingest messages and write them to Datastore. -&gt;&nbsp;Incorrect. Having a persistent connection does not handle the case where the device is disconnected.</p><p><br></p><p>https://cloud.google.com/pubsub/docs/overview</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430214,
    "question_plain": "As a cloud architect, you are managing a complex deployment scenario involving several Google Cloud projects under a single organization. You have a new team member who needs to view metadata about the organization and projects, but should not be allowed to modify resources. Which combination of IAM roles would best fulfill this requirement?",
    "answers": [
      "<p>Assign 'Organization Viewer' at the organization level and 'Project Viewer' at the project level.</p>",
      "<p>Assign 'Organization Viewer' at the organization level and 'Project Editor' at the project level.</p>",
      "<p>Assign 'Project Viewer' at both the organization and project levels.</p>",
      "<p>Assign 'Project Viewer' at the organization level and 'Organization Viewer' at the project level.</p>"
    ],
    "explanation": "<p>Assign 'Organization Viewer' at the organization level and 'Project Viewer' at the project level. -&gt; Correct. The 'Organization Viewer' role at the organization level gives the user the ability to view metadata about the organization. The 'Project Viewer' role at the project level allows the user to view but not modify project resources. This combination fulfills the requirement without granting additional permissions.</p><p><br></p><p>Assign 'Organization Viewer' at the organization level and 'Project Editor' at the project level. -&gt; Incorrect. The 'Project Editor' role at the project level would give the user the ability to modify project resources, which is not required in this scenario.</p><p><br></p><p>Assign 'Project Viewer' at both the organization and project levels. -&gt; Incorrect. 'Project Viewer' cannot be assigned at the organization level.</p><p><br></p><p>Assign 'Project Viewer' at the organization level and 'Organization Viewer' at the project level. -&gt;&nbsp;Incorrect. IAM roles are specific to resources. 'Project Viewer' cannot be assigned at the organization level, and 'Organization Viewer' cannot be assigned at the project level.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430216,
    "question_plain": "In order to cut down expenses, the chief engineering officer mandated that all developers shift their development infrastructure resources from on-premises virtual machines (VMs) to Google Cloud. These resources undergo frequent start/stop occurrences throughout the day and necessitate the persistence of their state. As a cloud architect, you have been tasked with designing a plan for running a development environment on Google Cloud that also allows the finance department to have clear visibility into the costs involved. Which two steps should you take?",
    "answers": [
      "<p>Use persistent disks to store the state. Start and stop the VM as needed.</p>",
      "<p>Use BigQuery billing export and labels to relate cost to groups.</p>",
      "<p>Use the <code>gcloud --auto-delete</code> flag on all persistent disks before stopping the VM.</p>",
      "<p>Apply VM CPU utilization label and include it in the BigQuery billing export.</p>",
      "<p>Store all state in a Local SSD, snapshot the persistent disks, and terminate the VM.</p>"
    ],
    "explanation": "<p>Use persistent disks to store the state. Start and stop the VM as needed. -&gt;&nbsp;Correct. Persistent disks will not be deleted when an instance is stopped.</p><p><br></p><p>Use BigQuery billing export and labels to relate cost to groups. -&gt;&nbsp;Correct. Exporting daily usage and cost estimates automatically throughout the day to a BigQuery dataset is a good way of providing visibility to the finance department. Labels can then be used to group the costs based on team or cost center.</p><p><br></p><p>Use the <code>gcloud --auto-delete</code> flag on all persistent disks before stopping the VM. -&gt; Incorrect. The <code>--auto-delete</code> flag has no effect unless the instance is deleted. Stopping an instance does not delete the instance or the attached persistent disks.</p><p><br></p><p>Apply VM CPU utilization label and include it in the BigQuery billing export. -&gt;&nbsp;Incorrect. Labels are used to organize instances, not to monitor metrics.</p><p><br></p><p>Store all state in a Local SSD, snapshot the persistent disks, and terminate the VM. -&gt; Incorrect. The state stored in local SSDs will be lost when the instance is stopped.</p><p><br></p><p>https://cloud.google.com/compute/docs/instances/instance-life-cycle</p><p>https://cloud.google.com/compute/docs/disks/local-ssd#data_persistence</p><p>https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-disk-auto-delete#--auto-delete</p>",
    "correct_response": ["a", "b"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 78430218,
    "question_plain": "Refer to the TerramEarth case study for this question: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdfAt present, maintenance personnel for TerramEarth obtain interactive performance graphs covering the past 24 hours (24 hours x 60 minutes x 60 seconds = 86,400 events) by connecting their maintenance tablets directly to the vehicle. The support group aims to enable remote viewing of this data by support technicians, while ensuring minimal latency for graph loading. What approach should be taken to offer this functionality?",
    "answers": [
      "<p>Perform queries on data indexed by <code>vehicle_id.timestamp</code> in Cloud Bigtable.</p>",
      "<p>Perform queries on data stored in a Cloud SQL.</p>",
      "<p>Perform queries against data stored on daily partitioned BigQuery tables.</p>",
      "<p>Execute queries against BigQuery with data stored in Cloud Storage via BigQuery federation.</p>"
    ],
    "explanation": "<p>Perform queries on data indexed by <code>vehicle_id.timestamp</code> in Cloud Bigtable. -&gt; Correct. Cloud Bigtable is optimized for time-series data. It is cost-efficient, highly available, and low-latency. It scales well. Best of all, it is a managed service that does not require significant operations work to keep running.</p><p><br></p><p>Perform queries on data stored in a Cloud SQL. -&gt; Incorrect. Cloud SQL provides relational database services that are well-suited to OLTP workloads, but not storage and low-latency retrieval of time-series data.</p><p><br></p><p>Perform queries against data stored on daily partitioned BigQuery tables. -&gt; Incorrect. BigQuery is fast for wide-range queries, but it is not as well-optimized for narrow-range queries as Cloud Bigtable is. Latency will be an order of magnitude shorter with Cloud Bigtable for this use.</p><p><br></p><p>Execute queries against BigQuery with data stored in Cloud Storage via BigQuery federation. -&gt; Incorrect. The objective is to minimize latency, and although BigQuery federation offers tremendous flexibility, it doesn't perform as well as native BigQuery storage[2], and will have longer latency than Cloud Bigtable for narrow-range queries. </p><p><br></p><p>https://cloud.google.com/bigtable/docs/schema-design-time-series#time-series-cloud-bigtable</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430220,
    "question_plain": "Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdfMountkirk Games has implemented their latest backend on the Google Cloud Platform. As a cloud architect, you aim to establish a comprehensive testing procedure for new iterations of the backend prior to their public release. Your objective is to design a testing environment that can scale effectively while keeping costs under control. How should you approach the process design?",
    "answers": [
      "<p>Create a scalable environment in GCP for simulating production load.</p>",
      "<p>Use the existing infrastructure to test the GCP-based backend at scale.</p>",
      "<p>Build stress tests into each component of your application and use resources from the already deployed production backend to simulate load.</p>",
      "<p>Create a set of static environments in GCP to test different levels of load—for example, high, medium, and low.</p>"
    ],
    "explanation": "<p>Create a scalable environment in GCP for simulating production load. -&gt; Correct. Simulating production load in GCP can scale in an economical way.</p><p><br></p><p>Use the existing infrastructure to test the GCP-based backend at scale. -&gt; Incorrect. One of the pain points about the existing infrastructure was precisely that the environment did not scale well.</p><p><br></p><p>Build stress tests into each component of your application and use resources from the already deployed production backend to simulate load. -&gt; Incorrect. It is a best practice to have a clear separation between test and production environments. Generating test load should not be done from a production environment.</p><p><br></p><p>Create a set of static environments in GCP to test different levels of load—for example, high, medium, and low. -&gt; Incorrect. Mountkirk Games wants the testing environment to scale as needed. Defining several static environments for specific levels of load goes against this requirement.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430222,
    "question_plain": "In your customer support tool, all email and chat conversations are logged to Bigtable for storage and analysis purposes. However, to ensure data privacy and compliance, it is essential to sanitize this data by removing any Personally Identifiable Information (PII) or payment card information before it is initially stored. What approach would you recommend for accomplishing this task?",
    "answers": [
      "<p>De-identify the data with the Cloud Data Loss Prevention API.</p>",
      "<p>Hash all data using SHA256.</p>",
      "<p>Encrypt all data using elliptic curve cryptography.</p>",
      "<p>Use regular expressions to find and redact phone numbers, email addresses, and credit card numbers.</p>"
    ],
    "explanation": "<p>De-identify the data with the Cloud Data Loss Prevention API. -&gt; Correct. The Cloud Data Loss Prevention (DLP) API is a tool that can be used to scan, de-identify, and classify sensitive data in real-time. It has pre-built detectors for PII such as social security numbers, credit card numbers, email addresses, phone numbers, etc., and it can automatically mask, tokenize, or delete the sensitive data before it is stored in Bigtable. By using the Cloud DLP API, the data can be sanitized in a scalable and efficient way, without having to write custom code to handle each type of sensitive data.</p><p><br></p><p>Hash all data using SHA256. -&gt; Incorrect. It would not be recommended because hashing is a one-way function, and the original data cannot be recovered from the hash value. This may be suitable for some use cases, but it would not allow for any meaningful analysis of the data.</p><p><br></p><p>Encrypt all data using elliptic curve cryptography. -&gt; Incorrect. It would not be recommended because encryption only protects the data while it is in transit or at rest, and it would not prevent authorized users from seeing the sensitive data if they have access to the decryption key.</p><p><br></p><p>Use regular expressions to find and redact phone numbers, email addresses, and credit card numbers. -&gt; Incorrect. It would be a manual and error-prone process and may not capture all sensitive data. Additionally, it would not be scalable for large amounts of data.</p><p><br></p><p>https://cloud.google.com/architecture/pci-dss-compliance-in-gcp#using_data_loss_prevention_api_to_sanitize_data</p><p>https://cloud.google.com/dlp</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430224,
    "question_plain": "As a cloud architect, you are tasked with designing a complex deployment scenario involving a GKE cluster that needs to be PCI DSS-compliant. Which of the following considerations is most critical to ensure that the deployment meets the PCI DSS compliance requirements?",
    "answers": [
      "<p>Implement network segmentation and isolate the Cardholder Data Environment (CDE) using network policies.</p>",
      "<p>Enable Cloud Logging at all times.</p>",
      "<p>Use GKE Autopilot for managing the GKE cluster.</p>",
      "<p>Deploy all applications in the cluster in the default namespace.</p>"
    ],
    "explanation": "<p>Implement network segmentation and isolate the Cardholder Data Environment (CDE) using network policies. -&gt; Correct. PCI DSS requires network segmentation to isolate the Cardholder Data Environment (CDE) from other networks. Implementing network policies in GKE allows you to control the network traffic to and from your containerized applications and is a critical step in creating a PCI DSS-compliant deployment.</p><p><br></p><p>Enable Cloud Logging at all times. -&gt; Incorrect. While enabling Cloud Logging is a good practice for monitoring and debugging, it is not the most critical consideration for achieving PCI DSS compliance in a GKE cluster.</p><p><br></p><p>Use GKE Autopilot for managing the GKE cluster. -&gt; Incorrect. While GKE Autopilot can simplify the management of GKE clusters, it does not directly ensure PCI DSS compliance.</p><p><br></p><p>Deploy all applications in the cluster in the default namespace. -&gt; Incorrect. Deploying applications in the default namespace does not help ensure PCI DSS compliance. It's generally a good practice to separate different applications or different parts of an application into different namespaces.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430226,
    "question_plain": "In the process of deploying an application on App Engine that requires integration with an on-premises database, you encounter a security constraint where the on-premises database cannot be accessed through the public Internet. What steps would you recommend taking to address this issue?",
    "answers": [
      "<p>Deploy your application on App Engine Flexible environment and use Cloud VPN to limit access to the on-premises database.</p>",
      "<p>Deploy your application on App Engine Standard environment and use App Engine firewall rules to limit access to the open on-premises database.</p>",
      "<p>Deploy your application on App Engine Standard environment and use Cloud SQL for SQL&nbsp;Server to limit access to the on-premises database.</p>",
      "<p>Deploy your application on App Engine Flexible environment and use App Engine firewall rules to limit access to the on-premises database.</p>"
    ],
    "explanation": "<p>Deploy your application on App Engine Flexible environment and use Cloud VPN to limit access to the on-premises database. -&gt; Correct. App Engine Flexible environment allows for more flexibility in configuring networking and infrastructure compared to the Standard environment. By deploying the application on App Engine Flexible, you can set up a secure connection between the App Engine environment and the on-premises database using Cloud VPN. Cloud VPN provides secure, encrypted connectivity over the public Internet, allowing you to limit access to the on-premises database while ensuring data privacy and security.</p><p><br></p><p>Deploy your application on App Engine Standard environment and use App Engine firewall rules to limit access to the open on-premises database. -&gt;&nbsp;Incorrect. App Engine Standard environment does not provide the same level of networking flexibility as the Flexible environment. While you can use firewall rules in App Engine Standard, they are primarily designed to restrict access to the application itself rather than controlling access to an on-premises database.</p><p><br></p><p>Deploy your application on App Engine Standard environment and use Cloud SQL for SQL&nbsp;Server to limit access to the on-premises database. -&gt;&nbsp;Incorrect. Cloud SQL for SQL Server is a managed database service provided by Google Cloud, but it is not specifically designed to integrate with on-premises databases. It may not provide the necessary connectivity and access control mechanisms required to connect to an on-premises database securely.</p><p><br></p><p>Deploy your application on App Engine Flexible environment and use App Engine firewall rules to limit access to the on-premises database. -&gt;&nbsp;Incorrect. While deploying on App Engine Flexible environment allows for more flexibility, App Engine firewall rules are designed to control access to the application itself rather than providing secure connectivity to an on-premises database.</p><p><br></p><p>https://cloud.google.com/appengine/docs/flexible</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430228,
    "question_plain": "As a cloud architect, you've implemented an autoscaling group for a client's application on Google Cloud Compute Engine. The client now wants to increase the maximum number of instances in the autoscaling group to handle peak demand. Which of the following steps would you take to achieve this?",
    "answers": [
      "<p>Increase the value of the <code>maxNumReplicas</code> parameter in the Autoscaler configuration.</p>",
      "<p>Increase the maximum number of vCPUs for the project.</p>",
      "<p>Increase the disk size for the instance template used by the autoscaling group.</p>",
      "<p>Increase the number of zones in the region where the autoscaling group is deployed.</p>"
    ],
    "explanation": "<p>Increase the value of the <code>maxNumReplicas</code> parameter in the Autoscaler configuration. -&gt; Correct. To increase the maximum number of instances in an autoscaling group, you need to increase the value of the <code>maxNumReplicas</code> parameter in the Autoscaler configuration.</p><p><br></p><p>Increase the maximum number of vCPUs for the project. -&gt; Incorrect. Increasing the maximum number of vCPUs for the project increases the project's quota for vCPUs but doesn't directly affect the number of instances in the autoscaling group.</p><p><br></p><p>Increase the disk size for the instance template used by the autoscaling group. -&gt; Incorrect. Increasing the disk size for the instance template would provide more storage for each instance but would not increase the number of instances in the autoscaling group.</p><p><br></p><p>Increase the number of zones in the region where the autoscaling group is deployed.-&gt; Incorrect. Increasing the number of zones in the region would potentially increase the availability of the application but doesn't directly affect the number of instances in the autoscaling group.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430230,
    "question_plain": "As a cloud architect, you are tasked with storing sensitive data in Google Cloud Storage buckets. What is the most secure approach to protect this data while at rest in the buckets?",
    "answers": [
      "<p>Use Customer-Managed Encryption Keys (CMEK) to encrypt the data.</p>",
      "<p>Use Google Cloud IAM to restrict bucket access only to authorized service accounts.</p>",
      "<p>Enable Object Versioning in the Cloud Storage bucket.</p>",
      "<p>Enable Public Access Prevention on the Cloud Storage bucket.</p>"
    ],
    "explanation": "<p>Use Customer-Managed Encryption Keys (CMEK) to encrypt the data. -&gt; Correct. Customer-Managed Encryption Keys (CMEK) is a feature where you can generate and control your encryption keys using Cloud Key Management Service (KMS). This provides the highest level of control over the encryption and decryption of your data stored in Cloud Storage.</p><p><br></p><p>Use Google Cloud IAM to restrict bucket access only to authorized service accounts. -&gt; Incorrect. hile restricting bucket access with IAM to authorized service accounts is a good practice, it does not protect the data while it's at rest in the bucket. Encryption is required for that purpose.</p><p><br></p><p>Enable Object Versioning in the Cloud Storage bucket. -&gt; Incorrect. Object Versioning helps protect your data against accidental deletions and overwrites, but it does not protect your data while it's at rest in the bucket.</p><p><br></p><p>Enable Public Access Prevention on the Cloud Storage bucket. -&gt; Incorrect. Enabling Public Access Prevention is a good practice to prevent public access to a bucket, but it does not protect your data while it's at rest in the bucket.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430232,
    "question_plain": "You are employed at a multinational corporation with employees aged from 18 to 60. The corporation maintains extensive personal data, including health conditions, in BigQuery. In compliance with current privacy regulations, the corporation must be able to delete such data when requested by an employee. How can you architect a solution to accommodate such requests?",
    "answers": [
      "<p>Assign a distinct identifier to every employee. When a deletion request is made, remove all rows from BigQuery that correspond to this identifier.</p>",
      "<p>Establish a BigQuery view on top of the table that holds all data. In the event of a deletion request, omit the rows related to the employee's data from this view. Employ this view for all analytical operations instead of the base table.</p>",
      "<p>Implement a data archiving strategy to move the personal data to a separate storage system when a deletion request is made.</p>",
      "<p>During the ingestion of new data into BigQuery, process the data via the Data Loss Prevention (DLP) API to detect any personal information. During the DLP scan, store the result in the Data Catalog. In response to a deletion request, consult the Data Catalog to identify the column containing personal information.</p>"
    ],
    "explanation": "<p>Assign a distinct identifier to every employee. When a deletion request is made, remove all rows from BigQuery that correspond to this identifier. -&gt;&nbsp;Correct. It aligns with the requirement of deleting personal data when requested by an employee while complying with privacy regulations. By assigning a distinct identifier to every employee, you can easily identify and locate the specific rows in BigQuery that correspond to the employee's data. When a deletion request is received, you can simply remove all rows from BigQuery that have the identifier associated with the employee making the request. This ensures that the personal data is effectively deleted from the database.</p><p><br></p><p>Establish a BigQuery view on top of the table that holds all data. In the event of a deletion request, omit the rows related to the employee's data from this view. Employ this view for all analytical operations instead of the base table. -&gt; Incorrect. It suggests using a view on top of the table to omit rows related to the employee's data. While this may seem like a viable option, it doesn't actually delete the data from the underlying table. The data remains stored in BigQuery, which could potentially be a privacy violation.</p><p><br></p><p>Implement a data archiving strategy to move the personal data to a separate storage system when a deletion request is made. -&gt; Incorrect. It suggests moving the personal data to a separate storage system instead of deleting it from BigQuery. The requirement is to delete the data when requested by an employee, not just move it to another location.</p><p><br></p><p>During the ingestion of new data into BigQuery, process the data via the Data Loss Prevention (DLP) API to detect any personal information. During the DLP scan, store the result in the Data Catalog. In response to a deletion request, consult the Data Catalog to identify the column containing personal information. -&gt; Incorrect. It suggests using the Data Loss Prevention (DLP) API and Data Catalog to identify and delete personal information. While this approach may be useful for detecting and cataloging personal information, it doesn't specifically address the requirement of deleting the data from BigQuery when requested by an employee.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430234,
    "question_plain": "As a cloud architect, you are designing a complex deployment scenario for a client that involves using an unmanaged instance group with an active instance and a standby instance in different zones. You are required to use a regional persistent disk and a network load balancer in front of the instances. How do you ensure that in the event of a failure, traffic will be redirected to the standby instance?",
    "answers": [
      "<p>Implement a health check and use a regional network load balancer.</p>",
      "<p>Use Google Cloud Armor for traffic redirection.</p>",
      "<p>Implement Cloud CDN for traffic redirection.</p>",
      "<p>Use Cloud NAT for traffic redirection.</p>"
    ],
    "explanation": "<p>Implement a health check and use a regional network load balancer. -&gt;&nbsp;Correct. In this scenario, it's important to set up a health check for the network load balancer to determine the health of the instances. When the active instance fails, the load balancer would detect it via the health check, and traffic would then be redirected to the standby instance in the different zone.</p><p><br></p><p>Use Google Cloud Armor for traffic redirection. -&gt; Incorrect. Cloud Armor is a distributed denial of service (DDoS) defense, and web application firewall (WAF) service. It is not responsible for traffic redirection in the context of load balancing.</p><p><br></p><p>Implement Cloud CDN for traffic redirection. -&gt; Incorrect. Cloud CDN (Content Delivery Network) uses Google's global edge network to serve content closer to users, improving performance. However, it is not used for traffic redirection in the event of instance failure.</p><p><br></p><p>Use Cloud NAT for traffic redirection. -&gt; Incorrect. Cloud NAT (Network Address Translation) enables instances without external IP addresses to access the internet, but it is not responsible for traffic redirection between active and standby instances.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430236,
    "question_plain": "As a cloud architect, you've been asked to perform a \"lift-and-shift\" migration of a Linux RHEL virtual machine from an on-premises data center to Google Cloud. Which Google Cloud service would be most appropriate to facilitate this migration?",
    "answers": [
      "<p>Google Cloud Migrate for Compute Engine</p>",
      "<p>Cloud Dataflow</p>",
      "<p>Cloud Storage</p>",
      "<p>Cloud Pub/Sub</p>"
    ],
    "explanation": "<p>Google Cloud Migrate for Compute Engine -&gt; Correct. Google Cloud Migrate for Compute Engine is a tool designed to assist with migrating physical servers and virtual machines (from on-premises or other clouds) into Google Compute Engine. It supports different types of operating systems, including Linux RHEL.</p><p><br></p><p>Cloud Dataflow -&gt; Incorrect. Cloud Dataflow is a service for stream and batch processing of data. It is not specifically designed for migrating virtual machines.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. Cloud Storage is used for storing and retrieving data, not for migrating virtual machines.</p><p><br></p><p>Cloud Pub/Sub -&gt; Incorrect. Cloud Pub/Sub is a messaging service designed for building event-driven systems and real-time data processing, rather than facilitating VM migrations.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430238,
    "question_plain": "You are architecting a cloud-based solution for a financial institution that handles sensitive customer data. The solution requires encryption at various levels to ensure data security. Which encryption techniques should you consider for this scenario?",
    "answers": [
      "<p>Database-level encryption for data at rest and transport layer encryption for data in transit</p>",
      "<p>Homomorphic encryption for data in transit and data at rest</p>",
      "<p>Asymmetric encryption for data at rest and hashing for data in transit</p>",
      "<p>Tokenization for data at rest and symmetric encryption for data in transit</p>"
    ],
    "explanation": "<p>Database-level encryption for data at rest and transport layer encryption for data in transit -&gt;&nbsp;Correct. Database-level encryption encrypts data at rest within the database, while transport layer encryption (such as SSL/TLS) secures data during transit. This combination ensures data security at both stages and is a recommended option.</p><p><br></p><p>Homomorphic encryption for data in transit and data at rest -&gt;&nbsp;Incorrect. Homomorphic encryption allows computations to be performed on encrypted data without decrypting it, but it is computationally intensive and may not be practical for data in transit or large-scale data storage.</p><p><br></p><p>Asymmetric encryption for data at rest and hashing for data in transit -&gt; Incorrect. Asymmetric encryption uses public and private key pairs for encryption, but it is not commonly used for encrypting large amounts of data at rest. Hashing is a one-way function that generates a fixed-size output based on input data and is not suitable for data in transit encryption.</p><p><br></p><p>Tokenization for data at rest and symmetric encryption for data in transit -&gt; Incorrect. Tokenization replaces sensitive data with randomly generated tokens but does not provide encryption for data at rest. Symmetric encryption uses a single key to encrypt and decrypt data and is suitable for data in transit. However, this combination does not fully address the encryption requirements for data at rest.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430240,
    "question_plain": "Your customer is planning to run a stateful, multi-tier application in Google Cloud. The customer has the following requirements:the application must store its data in a highly available, highly durable transactional storage systemthe storage system must be able to handle random read/write workloads and scale dynamicallythe storage system must be able to recover from failures quicklythe storage system must be transparent to the application, which should not have to modify its code to use itWhich Google Cloud service should you recommend to meet these requirements?",
    "answers": [
      "<p>Cloud SQL</p>",
      "<p>Cloud Spanner</p>",
      "<p>Cloud Datastore</p>",
      "<p>Cloud Bigtable</p>"
    ],
    "explanation": "<p>Cloud Spanner -&gt; Correct. Cloud Spanner is a fully managed, mission-critical, relational database service that offers transactional consistency at global scale, automatic, synchronous replication for high availability, and support for two SQL dialects: Google Standard SQL (ANSI 2011 with extensions) and PostgreSQL.</p><p><br></p><p>Cloud SQL -&gt; Incorrect. Cloud SQL is a managed relational database service that provides high availability and durability, but it may not scale horizontally as well as Cloud Spanner.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. Cloud Datastore is a NoSQL document database that can handle large amounts of structured and semi-structured data, but it may not provide strong consistency or transactional semantics.</p><p><br></p><p>Cloud Bigtable -&gt;&nbsp;Incorrect. Cloud Bigtable is a high-performance, NoSQL columnar database that can handle large-scale analytical and operational workloads, but it may not provide strong consistency or transactional semantics either.</p><p><br></p><p>https://cloud.google.com/spanner/docs</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430242,
    "question_plain": "You are the lead cloud architect for a global company that is migrating its data processing workloads to Google Cloud Platform (GCP). The company's primary requirements include scalability, cost-effectiveness, and the ability to process and analyze data in real-time. You need to choose the most appropriate GCP service to meet these requirements. Which of the following would you recommend?",
    "answers": [
      "<p>Dataflow </p>",
      "<p>Compute Engine</p>",
      "<p>App Engine</p>",
      "<p>Cloud Functions</p>"
    ],
    "explanation": "<p>Dataflow -&gt; Correct. Dataflow is a fully managed service for stream and batch data processing. It is ideal for real-time data analytics and processing as it scales automatically, handles the provisioning of resources, and optimizes for performance and cost. Dataflow integrates seamlessly with other GCP data analytics products, offering a comprehensive solution for complex data processing scenarios.</p><p><br></p><p>Compute Engine -&gt;&nbsp;Incorrect. Compute Engine provides scalable and customizable VMs. However, it requires manual scaling and management, making it less suitable for real-time data processing without extensive automation.</p><p><br></p><p>App Engine -&gt;&nbsp;Incorrect. App Engine automatically scales based on demand and is great for web applications but might not offer the granular control needed for complex real-time data processing and analytics tasks.</p><p><br></p><p>Cloud Functions -&gt;&nbsp;Incorrect. Cloud Functions is a serverless execution environment for building and connecting cloud services. While it is suitable for event-driven applications and can scale automatically, it might not be the best fit for heavy data processing tasks due to execution time limits and potential cost at scale.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430244,
    "question_plain": "Your customer wants to deploy a highly available and scalable web application in Google Cloud. The customer has the following requirements:the application must be able to handle incoming traffic spikes and scale dynamicallythe application must be highly available and recover from failures automaticallythe application must be easy to deploy and managethe application must be cost-effectiveWhich Google Cloud service should you recommend to meet these requirements?",
    "answers": [
      "<p>Cloud Load Balancer</p>",
      "<p>Cloud Functions</p>",
      "<p>Google Kubernetes Engine</p>",
      "<p>App Engine</p>"
    ],
    "explanation": "<p>App Engine -&gt;&nbsp;Correct. App Engine is a fully managed, serverless platform for developing and hosting web applications at scale. You can choose from several popular languages, libraries, and frameworks to develop your apps, and then let App Engine take care of provisioning servers and scaling your app instances based on demand.</p><p><br></p><p>Google Kubernetes Engine -&gt; Incorrect. While Google Kubernetes Engine is a highly scalable and reliable service, it may not be the most cost-effective solution to meet all of the customer's requirements.</p><p><br></p><p>Cloud Load Balancer -&gt; Incorrect. While Cloud Load Balancer is a useful service for load balancing incoming traffic, it may not be the best option for meeting all of the customer's requirements.</p><p><br></p><p>Cloud Functions -&gt; Incorrect. While Cloud Functions are a useful service for event-driven computing and small workloads, they may not be the best option for meeting all of the customer's requirements for a highly available and scalable web application.</p><p><br></p><p>https://cloud.google.com/appengine/docs</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430246,
    "question_plain": "Your customer is planning to store a large amount of log data in Google Cloud. The customer has the following requirements:the data must be easily searchable and filterablethe data must be stored in a highly durable and scalable storage systemthe data must be accessible by multiple teams within the customer's organizationthe data must be cost-effectiveWhich Google Cloud service should you recommend to meet these requirements?",
    "answers": [
      "<p>Cloud Storage</p>",
      "<p>Cloud Bigtable</p>",
      "<p>Cloud SQL</p>",
      "<p>Cloud Logging</p>"
    ],
    "explanation": "<p>Cloud Logging -&gt;&nbsp;Correct. Cloud Logging is a fully managed log management and analysis service that provides centralized storage and analysis capabilities for log data. It offers powerful search, filter, and analysis capabilities, allowing easy querying and visualization of log data. It is highly scalable, durable, and designed specifically for handling log data. Multiple teams within the organization can access and analyze the log data, ensuring data accessibility. Cloud Logging is cost-effective, as you only pay for the storage and analysis usage.</p><p><br></p><p>Cloud Storage -&gt;&nbsp;Incorrect. While Cloud Storage provides highly durable and scalable storage, it is primarily designed for storing objects like files and may not be the most suitable choice for storing and searching large amounts of log data. It may not offer the same level of search and filter capabilities as Cloud Logging.</p><p><br></p><p>Cloud Bigtable -&gt;&nbsp;Incorrect. Cloud Bigtable is a NoSQL database optimized for high-performance, low-latency read and write operations on large datasets. While it can handle large-scale data storage, it may not be the most efficient solution for storing and searching log data with search and filter capabilities.</p><p><br></p><p>Cloud SQL -&gt;&nbsp;Incorrect. Cloud SQL is a fully managed relational database service. While it provides excellent support for structured data and SQL queries, it may not be the most suitable choice for storing and searching log data, especially when dealing with large volumes and specific search requirements.</p><p><br></p><p>https://cloud.google.com/logging/docs</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430248,
    "question_plain": "A global automotive manufacturer is planning to deploy a new car platform that will collect and process data from vehicles in real-time. The platform must be able to handle a large volume of incoming data, support real-time data processing and analytics, and provide secure and reliable access to data for internal and external stakeholders. The platform must also be scalable and flexible to accommodate future growth and changes in business requirements. Which of the following options would be the most effective approach to meet these requirements?",
    "answers": [
      "<p>Use Cloud Pub/Sub to collect data from vehicles and Cloud Dataflow to process and analyze the data. Store the data in Cloud Bigtable and provide access to stakeholders using Cloud Dataproc and Cloud Data Studio.</p>",
      "<p>Use Cloud IoT Core to collect data from vehicles and Cloud Functions to process and analyze the data. Store the data in Cloud Firestore and provide access to stakeholders using Cloud Storage and Cloud BigQuery.</p>",
      "<p>Use Cloud IoT Core to collect data from vehicles and Cloud Dataproc to process and analyze the data. Store the data in Cloud Bigtable and provide access to stakeholders using Cloud Data Studio and Cloud Pub/Sub.</p>",
      "<p>Use Cloud IoT Core to collect data from vehicles and Cloud Dataflow to process and analyze the data. Store the data in Cloud Datastore and provide access to stakeholders using Cloud BigQuery and Cloud Storage.</p>"
    ],
    "explanation": "<p>Use Cloud Pub/Sub to collect data from vehicles and Cloud Dataflow to process and analyze the data. Store the data in Cloud Bigtable and provide access to stakeholders using Cloud Dataproc and Cloud Data Studio. -&gt;&nbsp;Correct. Cloud Pub/Sub is a messaging service that can handle a large volume of incoming data from vehicles. Cloud Dataflow allows for real-time data processing and analytics. Cloud Bigtable is a scalable NoSQL database that can handle large amounts of data and provide high-performance access. Cloud Dataproc is a managed Spark and Hadoop service that can be used for further data processing and analysis. Cloud Data Studio provides a platform for visualizing and sharing data insights with stakeholders. This approach covers all the requirements stated in the question.</p><p><br></p><p>Use Cloud IoT Core to collect data from vehicles and Cloud Functions to process and analyze the data. Store the data in Cloud Firestore and provide access to stakeholders using Cloud Storage and Cloud BigQuery. -&gt;&nbsp;Incorrect. Cloud Firestore is a document database and may not be the most suitable choice for storing large volumes of incoming data. Cloud Storage and Cloud BigQuery can provide access to stakeholders, but Cloud Dataproc and Cloud Data Studio are better suited for real-time data processing and analysis.</p><p><br></p><p>Use Cloud IoT Core to collect data from vehicles and Cloud Dataproc to process and analyze the data. Store the data in Cloud Bigtable and provide access to stakeholders using Cloud Data Studio and Cloud Pub/Sub. -&gt;&nbsp;Incorrect. Cloud Bigtable can handle large amounts of data, but Cloud Data Studio and Cloud Pub/Sub are not the most suitable choices for providing access to stakeholders. Cloud Data Studio is primarily used for visualizing data, while Cloud Pub/Sub is a messaging service and may not be optimized for direct stakeholder access.</p><p><br></p><p>Use Cloud IoT Core to collect data from vehicles and Cloud Dataflow to process and analyze the data. Store the data in Cloud Datastore and provide access to stakeholders using Cloud BigQuery and Cloud Storage. -&gt;&nbsp;Incorrect. Cloud Datastore is a NoSQL document database and may not be the best choice for storing large volumes of data. Cloud BigQuery and Cloud Storage can provide access to stakeholders, but Cloud Dataproc and Cloud Data Studio are better suited for real-time data processing and analysis.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430250,
    "question_plain": "A large e-commerce company is planning to migrate its existing monolithic application to a microservices architecture on GCP. The company wants to ensure that the microservices are scalable, highly available, and can handle sudden spikes in traffic. The company also wants to minimize downtime during the migration and ensure that the data remains secure and compliant with industry regulations. Which of the following options would be the most effective approach to meet these requirements?",
    "answers": [
      "<p>Use Google Kubernetes Engine to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud SQL to store data and Cloud IAM to control access to the data.</p>",
      "<p>Use App Engine to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud Firestore to store data and Cloud IAM to control access to the data.</p>",
      "<p>Use Cloud Functions to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud Bigtable to store data and Cloud IAM to control access to the data.</p>",
      "<p>Use Compute Engine to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud Datastore to store data and Cloud KMS to control access to the data.</p>"
    ],
    "explanation": "<p>Use Google Kubernetes Engine to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud SQL to store data and Cloud IAM to control access to the data. -&gt;&nbsp;Correct. Google Kubernetes Engine (GKE) is a managed Kubernetes environment that provides scalable and highly available microservices. Kubernetes has built-in features for scaling, load balancing, and high availability, making it an ideal choice for microservices. Using GKE will also help to minimize downtime during migration as the application can be gradually migrated to the new architecture without disruption to the end-users. Cloud Load Balancer is a global load balancing service that distributes traffic across multiple regions, making it an ideal choice for handling sudden spikes in traffic. Cloud Load Balancer can route traffic to the closest instance of the microservice, ensuring low latency and high performance. Cloud SQL is a managed database service that supports MySQL and PostgreSQL. It provides high availability and automatic replication, making it an ideal choice for storing data for microservices. Cloud SQL also provides features such as automated backups and encryption, ensuring data security and compliance with industry regulations. Cloud IAM is a cloud identity and access management service that enables administrators to manage access to resources in the cloud. Using Cloud IAM will ensure that access to data is controlled and that compliance with industry regulations is maintained.</p><p><br></p><p>Use App Engine to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud Firestore to store data and Cloud IAM to control access to the data. -&gt; Incorrect. It is incorrect because App Engine is a Platform-as-a-Service (PaaS) that does not provide as much control as Kubernetes, making it less suitable for microservices. Cloud Firestore is a NoSQL document database that does not provide the same level of transactional consistency as Cloud SQL, making it less suitable for storing data.</p><p><br></p><p>Use Cloud Functions to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud Bigtable to store data and Cloud IAM to control access to the data. -&gt; Incorrect. It is incorrect because Cloud Functions is a Function-as-a-Service (FaaS) that is designed for small, event-driven functions, and may not be suitable for a monolithic application to microservices migration. Cloud Bigtable is a NoSQL database that is designed for high-performance, low-latency workloads, but it may not provide the same level of consistency as Cloud SQL.</p><p><br></p><p>Use Compute Engine to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud Datastore to store data and Cloud KMS to control access to the data. -&gt; Incorrect. It is incorrect because Compute Engine requires more manual management and does not provide the built-in features for scaling and high availability that Kubernetes provides. Cloud Datastore is a NoSQL document database that does not provide the same level of consistency as Cloud SQL. While Cloud KMS can control access to data, it does not provide the same level of fine-grained access control as Cloud IAM.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430252,
    "question_plain": "Your company is operating a multi-tier web application on Google Cloud. The frontend servers should be globally available and scale automatically to handle traffic, while the backend servers should be accessed only by the frontend servers and internal systems. The data transfer between backend and frontend servers needs to be encrypted. As a cloud architect, how should you design your network to fulfill these requirements?",
    "answers": [
      "<p>Deploy the frontend on App Engine and backend on Compute Engine within a shared VPC, and use SSL/TLS for encrypted communication.</p>",
      "<p>Deploy both frontend and backend servers on Compute Engine instances within the same subnet.</p>",
      "<p>Use two separate VPCs for frontend and backend servers, and connect them using VPN.</p>",
      "<p>Use Kubernetes Engine for both frontend and backend servers, and segregate them using network policies.</p>"
    ],
    "explanation": "<p>Deploy the frontend on App Engine and backend on Compute Engine within a shared VPC, and use SSL/TLS for encrypted communication. -&gt; Correct. App Engine provides global availability and auto-scaling capabilities, while Compute Engine can be used for backend servers. Using a shared VPC can isolate backend servers, and SSL/TLS can secure the data transfer between the frontend and backend servers.</p><p><br></p><p>Deploy both frontend and backend servers on Compute Engine instances within the same subnet. -&gt;&nbsp;Incorrect. This option doesn't provide the required isolation between the frontend and backend servers, and doesn't ensure global availability or auto-scaling for the frontend servers.</p><p><br></p><p>Use two separate VPCs for frontend and backend servers, and connect them using VPN. -&gt;&nbsp;Incorrect. While this approach separates the frontend and backend servers, the VPN adds unnecessary complexity for inter-service communication and doesn't provide auto-scaling for the frontend servers.</p><p><br></p><p>Use Kubernetes Engine for both frontend and backend servers, and segregate them using network policies. -&gt;&nbsp;Incorrect. While Kubernetes Engine and network policies can provide a level of segregation, this option doesn't provide global availability without additional configuration and doesn't inherently encrypt communication between frontend and backend servers.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430254,
    "question_plain": "Your organization has a mission-critical application running on Google Kubernetes Engine (GKE). You have to push frequent updates to this application without causing any downtime or disrupting the user experience. As a cloud architect, which deployment strategy would you recommend?",
    "answers": [
      "<p>Implement a rolling update strategy with readiness and liveness probes.</p>",
      "<p>Use the recreate deployment strategy for updating the application.</p>",
      "<p>Use a blue-green deployment strategy with a manual switch.</p>",
      "<p>Implement a canary deployment strategy.</p>"
    ],
    "explanation": "<p>Implement a rolling update strategy with readiness and liveness probes. -&gt;&nbsp;Correct. A rolling update strategy will incrementally update the Pods with the new application version, while readiness and liveness probes will ensure that the new Pods are ready to serve traffic and are healthy before they replace the old ones. This leads to no downtime during the update.</p><p><br></p><p>Use the recreate deployment strategy for updating the application. -&gt;&nbsp;Incorrect. This strategy kills all existing pods before creating new ones. This would result in downtime which is not desirable for a mission-critical application.</p><p><br></p><p>Use a blue-green deployment strategy with a manual switch. -&gt;&nbsp;Incorrect. Blue-green deployments involve running two separate environments: Blue (the currently running version) and Green (the new version). Once the Green environment is ready and tested, traffic is switched from Blue to Green. While this strategy does allow for easy rollback and zero-downtime deployments, the manual switch could introduce a chance of human error. Moreover, maintaining two environments might be more resource-intensive.</p><p><br></p><p>Implement a canary deployment strategy. -&gt;&nbsp;Incorrect. Canary deployments release the new version to a small subset of users before rolling it out to everyone. This is useful for testing new features with a smaller audience before a full release. While canary deployments can be useful in many scenarios, they don't guarantee zero-downtime by themselves and can be more complex to set up and manage.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430256,
    "question_plain": "You're designing a real-time analytics solution for a global company. The solution must ingest clickstream data from a worldwide user base in real-time, process it, and make it available for real-time querying. Which solution should you recommend?",
    "answers": [
      "<p>Use Cloud Pub/Sub for data ingestion, Cloud Dataflow for processing, and BigQuery for real-time querying.</p>",
      "<p>Use Cloud Datastore for real-time data ingestion and querying.</p>",
      "<p>Use Cloud SQL for real-time data ingestion and querying.</p>",
      "<p>Store the data in Cloud Storage, then process it using Dataproc, and store the results in BigTable for real-time querying.</p>"
    ],
    "explanation": "<p>Use Cloud Pub/Sub for data ingestion, Cloud Dataflow for processing, and BigQuery for real-time querying. -&gt;&nbsp;Correct. This combination of services will effectively handle high-throughput data ingestion (Cloud Pub/Sub), real-time data processing (Cloud Dataflow), and provide the ability to run real-time SQL-like queries on the processed data (BigQuery).</p><p><br></p><p>Use Cloud Datastore for real-time data ingestion and querying. -&gt;&nbsp;Incorrect. Cloud Datastore is not designed for high-throughput real-time data ingestion or real-time analytics querying. It's more suited for web and mobile applications requiring a NoSQL database.</p><p><br></p><p>Use Cloud SQL for real-time data ingestion and querying. -&gt;&nbsp;Incorrect. Cloud SQL is not designed for high-throughput real-time data ingestion or real-time analytics. It's a fully managed relational database service for MySQL, PostgreSQL, and SQL Server.</p><p><br></p><p>Store the data in Cloud Storage, then process it using Dataproc, and store the results in BigTable for real-time querying. -&gt;&nbsp;Incorrect. This solution doesn't support real-time data ingestion and processing as data needs to be stored first before processing, which doesn't meet the real-time requirement.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430258,
    "question_plain": "A large financial institution is migrating their trading platform to the cloud to handle increased volume and improve performance. The platform must meet the following requirements:provide real-time access to market data and trade execution capabilitiesensure data privacy and compliance with regulatory requirementshave the ability to handle peak loads during trading hours while minimizing costsensure high availability and disaster recoveryenable auditing and compliance reportingWhich solution would you recommend to meet these requirements?",
    "answers": [
      "<p>Implementing a custom-built solution using Compute Engine instances, Cloud Storage, and Cloud Pub/Sub.</p>",
      "<p>Implementing a managed solution using Cloud Bigtable for real-time market data and trade execution, Cloud Storage for auditing and reporting, and Cloud Pub/Sub for real-time messaging.</p>",
      "<p>Implementing a hybrid solution using Cloud Bigtable for real-time market data and trade execution, Cloud SQL for auditing and reporting, and Cloud VPN to connect with on-premises storage.</p>",
      "<p>Implementing a serverless solution using Cloud Functions for real-time market data and trade execution, BigQuery for auditing and reporting, and Cloud Pub/Sub for real-time messaging.</p>"
    ],
    "explanation": "<p>Implementing a managed solution using Cloud Bigtable for real-time market data and trade execution, Cloud Storage for auditing and reporting, and Cloud Pub/Sub for real-time messaging. -&gt; Correct. Cloud Bigtable provides a high-performance, low-latency database solution for real-time market data and trade execution, while Cloud Storage can be used for auditing and compliance reporting, and Cloud Pub/Sub provides real-time messaging capabilities. This solution is designed to handle high loads during trading hours while minimizing costs and ensuring high availability and disaster recovery. It also provides data privacy and compliance with regulatory requirements, making it an appropriate choice for a large financial institution.</p><p><br></p><p>Implementing a custom-built solution using Compute Engine instances, Cloud Storage, and Cloud Pub/Sub. -&gt; Incorrect. It only addresses some of the requirements while missing some critical aspects. Compute Engine instances and Cloud Storage can handle peak loads, but they may not provide real-time access to market data and trade execution capabilities. Additionally, custom-built solutions can be more difficult to manage and may not offer the same level of disaster recovery as managed solutions.</p><p><br></p><p>Implementing a hybrid solution using Cloud Bigtable for real-time market data and trade execution, Cloud SQL for auditing and reporting, and Cloud VPN to connect with on-premises storage. -&gt; Incorrect. It involves a hybrid solution that uses on-premises storage in addition to cloud services. While this solution may be useful in certain cases, it does not necessarily meet all the requirements listed in the question. Using Cloud SQL for auditing and reporting may not be the most cost-effective solution, and using a VPN to connect to on-premises storage may not be the most efficient or scalable solution for handling peak loads during trading hours.</p><p><br></p><p>Implementing a serverless solution using Cloud Functions for real-time market data and trade execution, BigQuery for auditing and reporting, and Cloud Pub/Sub for real-time messaging. -&gt; Incorrect. Cloud Functions have a maximum execution time limit, which might not be suitable for long-running tasks or complex calculations often required in financial trading systems. While serverless solutions like Cloud Functions can be cost-effective for sporadic or event-driven workloads, they may not be the most cost-efficient choice for sustained workloads, such as those required for trading platforms that must handle peak loads during trading hours.</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430260,
    "question_plain": "A large multinational retail company is looking to improve their customer experience by providing real-time, personalized recommendations to customers using a recommendation engine. The solution must meet the following requirements:handle millions of requests per second with low latencystore and process petabytes of customer data in real-timeprovide the ability to easily update and test recommendation algorithmsensure data privacy and securityminimize costs while still providing high performanceWhich solution would you recommend to meet these requirements?",
    "answers": [
      "<p>Implementing a custom-built solution using Cloud Dataflow for data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling.</p>",
      "<p>Implementing a serverless solution using Cloud Functions for recommendation algorithms, Cloud Pub/Sub for real-time data processing, and BigQuery for data storage.</p>",
      "<p>Implementing a managed solution using Vertex AI Platform for recommendation algorithms, Cloud Dataproc for data processing, and Cloud Bigtable for data storage.</p>",
      "<p>Implementing a hybrid solution using Cloud Dataflow for data processing, Cloud Storage for data storage, and Compute Engine for recommendation algorithms deployment and scaling.</p>"
    ],
    "explanation": "<p>Implementing a managed solution using Vertex AI Platform for recommendation algorithms, Cloud Dataproc for data processing, and Cloud Bigtable for data storage.-&gt; Correct. Vertex AI Platform provides managed services for building and deploying machine learning models, making it suitable for recommendation algorithms. Cloud Dataproc is a managed Apache Spark and Hadoop service that can handle large-scale data processing with low latency. Cloud Bigtable is a highly scalable NoSQL database that can store and process petabytes of customer data in real-time. This managed solution provides the necessary scalability, real-time processing capabilities, and storage capacity to meet the requirements.</p><p><br></p><p>Implementing a custom-built solution using Cloud Dataflow for data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling. -&gt;&nbsp;Incorrect. While this answer choice includes scalable components like Cloud Dataflow and GKE, it requires custom development and management. The custom-built solution may require additional effort for algorithm development, deployment, and scaling, which could be more complex and time-consuming than other managed solutions.</p><p><br></p><p>Implementing a serverless solution using Cloud Functions for recommendation algorithms, Cloud Pub/Sub for real-time data processing, and BigQuery for data storage. -&gt;&nbsp;Incorrect. This answer choice utilizes serverless components, but it may not be the best fit for handling millions of requests per second with low latency. Cloud Functions and Cloud Pub/Sub have certain limitations in terms of scalability and real-time processing performance. While BigQuery can handle large-scale data storage, it may not be optimized for real-time data processing needs.</p><p><br></p><p>Implementing a hybrid solution using Cloud Dataflow for data processing, Cloud Storage for data storage, and Compute Engine for recommendation algorithms deployment and scaling. -&gt;&nbsp;Incorrect. While this answer choice includes scalable components like Cloud Dataflow and Compute Engine, it requires managing infrastructure with Compute Engine for recommendation algorithms deployment and scaling. This hybrid solution introduces more complexity compared to a fully managed solution and may not be the most cost-effective option while still providing high performance.</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430262,
    "question_plain": "Your company is deploying a critical application on Google Cloud Platform that requires high availability and a robust disaster recovery strategy. The application will handle large-scale, global user traffic and store sensitive data. What is the best approach to design the application’s infrastructure for these requirements?",
    "answers": [
      "<p>Deploy the application across multiple GCP regions, use Multi-regional storage for data, and implement a global load balancer.</p>",
      "<p>Host the application in a single region using standard storage and manual scaling to handle traffic spikes.</p>",
      "<p>Utilize a single-region, single-zone approach with regular data backups to a different region.</p>",
      "<p>Use a hybrid cloud approach, hosting the application on GCP and another cloud provider simultaneously.</p>"
    ],
    "explanation": "<p>Deploy the application across multiple GCP regions, use Multi-regional storage for data, and implement a global load balancer. -&gt;&nbsp;Correct. This design ensures high availability by distributing the application across various regions, reducing the risk of regional outages. Multi-regional storage provides redundancy for data, and a global load balancer efficiently distributes user traffic.</p><p><br></p><p>Host the application in a single region using standard storage and manual scaling to handle traffic spikes. -&gt;&nbsp;Incorrect. This approach does not offer high availability or effective disaster recovery, as it relies on a single region and lacks automated scalability.</p><p><br></p><p>Utilize a single-region, single-zone approach with regular data backups to a different region. -&gt;&nbsp;Incorrect. While backups are good, relying on a single zone and region can lead to significant downtime during outages, which is not ideal for critical applications.</p><p><br></p><p>Use a hybrid cloud approach, hosting the application on GCP and another cloud provider simultaneously. -&gt;&nbsp;Incorrect. While this might offer redundancy, it can lead to increased complexity and potential issues with data consistency and latency.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430264,
    "question_plain": "You are a cloud architect tasked with planning the migration of an enterprise's legacy systems to Google Cloud Platform (GCP). The enterprise has a mix of on-premises databases, applications, and storage systems that need to be moved to the cloud. Your plan needs to minimize downtime, ensure data integrity, and provide scalability. What would be the most effective migration plan for this scenario?",
    "answers": [
      "<p>Conduct a pilot migration with a small, non-critical system before fully migrating.</p>",
      "<p>Migrate all systems simultaneously to minimize total migration time.</p>",
      "<p>Move only the databases to GCP and keep the applications on-premises.</p>",
      "<p>Start by moving the largest data sets first to reduce complexity.</p>"
    ],
    "explanation": "<p>Conduct a pilot migration with a small, non-critical system before fully migrating. -&gt; Correct. A pilot migration allows for testing the migration process, addressing issues on a smaller scale, and building confidence for the full migration.</p><p><br></p><p>Migrate all systems simultaneously to minimize total migration time. -&gt; Incorrect. Migrating all systems simultaneously can lead to significant risks, including potential downtime and data loss.</p><p><br></p><p>Move only the databases to GCP and keep the applications on-premises. -&gt; Incorrect. Migrating only databases and keeping applications on-premises may lead to performance issues and doesn't fully leverage cloud benefits.</p><p><br></p><p>Start by moving the largest data sets first to reduce complexity. -&gt; Incorrect. Moving the largest data sets first can be more complex and risky. It's better to start small and scale up.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 78430266,
    "question_plain": "As a cloud architect, you are designing a solution for a client who wants to leverage Google Cloud SQL for their relational database needs. The client's application requires full transaction support with multi-version concurrency control, along with procedural languages like PL/pgSQL. Which database engine should you recommend for Cloud SQL?",
    "answers": [
      "<p>PostgreSQL</p>",
      "<p>Microsoft SQL Server</p>",
      "<p>MySQL</p>",
      "<p>Oracle Database</p>"
    ],
    "explanation": "<p>PostgreSQL -&gt; Correct. PostgreSQL is a database engine supported by Cloud SQL that offers full transaction support with multi-version concurrency control. It also supports procedural languages like PL/pgSQL, meeting the client's requirements.</p><p><br></p><p>Microsoft SQL Server -&gt; Incorrect. Microsoft SQL Server is a database engine supported by Cloud SQL, but it does not support procedural languages like PL/pgSQL.</p><p><br></p><p>MySQL -&gt;&nbsp;Incorrect. MySQL supports full transaction capabilities but uses a different procedural language, SQL/PSM, which might not meet the client's requirements for PL/pgSQL.</p><p><br></p><p>Oracle Database -&gt; Incorrect. Oracle Database is not a supported database engine on Cloud SQL.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146042,
    "question_plain": "When handling Personally Identifiable Information (PII) in Google Cloud Platform (GCP), which of the following statements regarding Customer-managed encryption keys (CMEK) is correct?",
    "answers": [
      "<p>CMEK provides customers with full control and ownership of encryption keys, allowing them to manage the encryption of all types of data within GCP.</p>",
      "<p>CMEK is a feature exclusively available for Google-managed services and cannot be utilized for customer-owned resources.</p>",
      "<p>CMEK ensures compliance with data protection regulations, eliminating the need for customers to implement additional security measures.</p>",
      "<p>CMEK is automatically enabled by default for all GCP services, providing enhanced encryption and protection for PII.</p>"
    ],
    "explanation": "<p>CMEK provides customers with full control and ownership of encryption keys, allowing them to manage the encryption of all types of data within GCP. -&gt;&nbsp;Correct. CMEK provides customers with full control and ownership of encryption keys, allowing them to manage the encryption of all types of data within GCP. This statement accurately represents the purpose and functionality of CMEK, empowering customers to maintain control over their encryption keys and data security.</p><p><br></p><p>CMEK is a feature exclusively available for Google-managed services and cannot be utilized for customer-owned resources. -&gt;&nbsp;Incorrect. It falsely claims that CMEK is exclusively available for Google-managed services. CMEK can be used to encrypt data for both Google-managed services and customer-owned resources within GCP.</p><p><br></p><p>CMEK ensures compliance with data protection regulations, eliminating the need for customers to implement additional security measures. -&gt; Incorrect. It wrongly suggests that CMEK alone ensures compliance with data protection regulations. While CMEK plays a crucial role in data security and encryption, additional security measures are often required to achieve full compliance with regulations.</p><p><br></p><p>CMEK is automatically enabled by default for all GCP services, providing enhanced encryption and protection for PII. -&gt; Incorrect. It incorrectly states that CMEK is automatically enabled by default for all GCP services. CMEK needs to be explicitly configured and enabled by the customer for each specific service and resource that requires customer-managed encryption keys.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146044,
    "question_plain": "You are a cloud architect tasked with designing a system to store petabytes of NoSQL data for a large-scale, global e-commerce company. The system needs to support high-speed writes and reads, provide seamless scalability, and ensure near real-time consistency. The data will be heavily used for both transactional and analytical workloads. Which Google Cloud service should you choose to meet these requirements?",
    "answers": [
      "<p>Cloud Bigtable</p>",
      "<p>BigQuery</p>",
      "<p>Cloud Spanner</p>",
      "<p>Cloud Datastore</p>"
    ],
    "explanation": "<p>Cloud Bigtable -&gt;&nbsp;Correct. It's designed to handle massive workloads at consistent low latency, can scale to petabytes of data, and is suitable for both transactional and analytical workloads. It provides strong consistency within a single row, but eventual consistency for reading multiple rows (which can be mitigated by proper design).</p><p><br></p><p>BigQuery -&gt;&nbsp;Incorrect. While it can store petabytes of data and is excellent for analytical workloads, it isn't designed for high-speed transactional workloads or real-time consistency.</p><p><br></p><p>Cloud Spanner -&gt;&nbsp;Incorrect. While it provides global transactions, strong consistency, and can handle massive amounts of data, it is a relational database, not a NoSQL database.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. While it is a NoSQL document database built for automatic scaling, high performance, and ease of application development, it is not designed to handle petabyte-scale data and has limitations for certain transactional workloads.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146046,
    "question_plain": "Your organization operates a large-scale web application on Google Cloud, and you need to design a solution to analyze logs from the application in near real-time to detect any potential issues or anomalies. You also want to do some transformation of log data before storing. Which of the following approaches should you recommend?",
    "answers": [
      "<p>Use Cloud Pub/Sub to ingest logs, Cloud Dataflow to transform, and then BigQuery to analyze.</p>",
      "<p>Store logs in Cloud Storage, then use Cloud Dataflow to move them to BigQuery for analysis.</p>",
      "<p>Use Cloud Functions to process each log entry and send it to BigQuery.</p>",
      "<p>Stream logs directly from the application to BigQuery.</p>"
    ],
    "explanation": "<p>Use Cloud Pub/Sub to ingest logs, Cloud Dataflow to transform, and then BigQuery to analyze. -&gt;&nbsp;Correct. Using Cloud Pub/Sub for log ingestion, Cloud Dataflow for transformation, and then storing and analyzing in BigQuery is a common pattern for handling and analyzing large-scale, real-time data in Google Cloud.</p><p><br></p><p>Store logs in Cloud Storage, then use Cloud Dataflow to move them to BigQuery for analysis. -&gt; Incorrect. This approach doesn't support real-time or near real-time analysis as there would be a delay in moving data from Cloud Storage to BigQuery.</p><p><br></p><p>Use Cloud Functions to process each log entry and send it to BigQuery. -&gt; Incorrect. While Cloud Functions could be used for this purpose, it might not be the most cost-effective or efficient way to process large-scale log data in real-time.</p><p><br></p><p>Stream logs directly from the application to BigQuery. -&gt; Incorrect. Streaming logs directly to BigQuery may be expensive and does not support advanced parsing or transformation that may be needed before analysis.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146048,
    "question_plain": "A social media startup deployed their image-sharing application using App Engine in the us-west1 region. After a few months, they notice that most of their users are based in Brazil. They want to minimize latency for their users. As a cloud architect, what should you advise them?",
    "answers": [
      "<p>They should move this application deployment to <code>southamerica-east1</code> region and create a new GCP project. Then, they should create a new App Engine application in the new GCP project and set its region to <code>southamerica-east1</code>. Finally, they should remove the old App Engine application.</p>",
      "<p>They should use a global CDN to cache their content closer to their users in Brazil.</p>",
      "<p>They should update the default region to <code>southamerica-east1</code> in the App Engine.</p>",
      "<p>They should create a ticket to Google Support to change application deployment region in App Engine.</p>"
    ],
    "explanation": "<p>They should move this application deployment to <code>southamerica-east1</code> region and create a new GCP project. Then, they should create a new App Engine application in the new GCP project and set its region to <code>southamerica-east1</code>. Finally, they should remove the old App Engine application. -&gt;&nbsp;Correct. App Engine applications are region-specific and cannot be moved between regions. Creating a new project and deploying the application in the desired region is necessary to minimize latency for the users in Brazil.</p><p><br></p><p>They should use a global CDN to cache their content closer to their users in Brazil. -&gt; Incorrect. While using a CDN can help reduce latency for static content, it doesn't address the need for the application itself to be closer to users. Moving the App Engine deployment is still necessary for the best performance.</p><p><br></p><p>They should update the default region to <code>southamerica-east1</code> in the App Engine. -&gt; Incorrect. You cannot update the default region of an existing App Engine application. Regions are fixed once set during the creation of the App Engine application.</p><p><br></p><p>They should create a ticket to Google Support to change application deployment region in App Engine. -&gt; Incorrect. Google Support cannot change the region of an existing App Engine application. A new deployment in the desired region within a new project is necessary.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146050,
    "question_plain": "One of your applications is deployed to the GKE&nbsp;cluster as a Kubernetes workload with DaemonSets and is gaining popularity. As a cloud architect, you want to add more pods to your workload and want to make sure the cluster scales up and down automatically based on the volume. What should you do?",
    "answers": [
      "<p>You should enable autoscaling on Kubernetes Engine.</p>",
      "<p>You should perform a rolling update to modify machine type to a higher one.</p>",
      "<p>You should enable Horizontal Pod Autoscaling for the Kubernetes deployment.</p>",
      "<p>You should create another identical Kubernetes workload and split traffic between the two workloads.</p>"
    ],
    "explanation": "<p>You should enable autoscaling on Kubernetes Engine. -&gt; Correct. Enabling autoscaling on Kubernetes Engine allows the cluster to automatically adjust the number of nodes based on the demand. This means that as the number of pods in the workload increases, the cluster will automatically add more nodes to handle the load. Similarly, if the workload decreases, the cluster will scale down and remove unnecessary nodes, helping to save costs. This is a more efficient solution than manually modifying the machine type or creating another workload. </p><p><br></p><p>You should perform a rolling update to modify machine type to a higher one. -&gt;&nbsp;Incorrect. It is incorrect because modifying the machine type will not scale the cluster automatically.</p><p><br></p><p>You should enable Horizontal Pod Autoscaling for the Kubernetes deployment. -&gt;&nbsp;Incorrect. It is incorrect because Horizontal Pod Autoscaling scales the number of pods within a deployment, but not the number of nodes in the cluster. </p><p><br></p><p>You should create another identical Kubernetes workload and split traffic between the two workloads. -&gt; Incorrect. It is also incorrect because creating another workload is not necessary and may result in unnecessary resource usage.</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146052,
    "question_plain": "As a cloud architect, it is your responsibility to monitor any modifications made to the Cloud Storage bucket. For each change, you are required to trigger an action that will promptly validate the compliance of the modification in near real-time. What action should you take?",
    "answers": [
      "<p>You should use Cloud Function events, and call your security script from the Cloud Function triggers.</p>",
      "<p>You should use the built-in triggering mechanism of Cloud Storage to run your security script.</p>",
      "<p>You should use a Python script to get appropriate logs, analyze them, and run the security script.</p>",
      "<p>You should use Crone Scheduler to schedule your security script.</p>"
    ],
    "explanation": "<p>You should use Cloud Function events, and call your security script from the Cloud Function triggers. -&gt;&nbsp;Correct. Cloud Functions allow you to execute your code in response to an event in Cloud Storage. You can use Cloud Storage triggers to execute your Cloud Function whenever an object is created or updated in the Cloud Storage bucket. This will allow you to verify the compliance of the change in near real-time. The security script can be called from the Cloud Function triggers.</p><p><br></p><p>You should use the built-in triggering mechanism of Cloud Storage to run your security script. -&gt; Incorrect. It is incorrect because while Cloud Storage does have a built-in triggering mechanism, it does not provide the ability to verify the compliance of the change in near real-time.</p><p><br></p><p>You should use a Python script to get appropriate logs, analyze them, and run the security script. -&gt; Incorrect. It is incorrect because although you can use Python scripts to get logs and analyze them, this solution would not be real-time, and it would be less efficient and less scalable than using Cloud Functions.</p><p><br></p><p>You should use Crone Scheduler to schedule your security script. -&gt; Incorrect. It is incorrect because Cron Scheduler is a time-based job scheduler and is not suitable for monitoring changes in real-time.</p><p><br></p><p>https://cloud.google.com/functions/docs/how-to</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146054,
    "question_plain": "Suppose you run a small business and need to grant a role for your accountant to view billing reports and approve invoices. With Google's best practices in mind, which Billing IAM role should you grant to your accountant?",
    "answers": [
      "<p>Billing Account Viewer</p>",
      "<p>Billing Account Administrator</p>",
      "<p>Billing Account User Project Creator</p>",
      "<p>Billing Account Creator</p>"
    ],
    "explanation": "<p>Billing Account Viewer -. Correct. The Billing Account Viewer role allows the user to view billing reports, cost trends, and related information without being able to make any changes or perform any actions. This is the least privileged role that grants access to billing information.</p><p><br></p><p>Billing Account Administrator -&gt; Incorrect. The Billing Account Administrator role provides full control over billing information, including the ability to make changes, modify budgets, and manage payment methods. </p><p><br></p><p>Billing Account User Project Creator -&gt; Incorrect. The Billing Account User Project Creator role allows users to create projects within a billing account, but does not provide access to billing information.</p><p><br></p><p>Billing Account Creator -&gt; Incorrect. The Billing Account Creator role is a highly privileged role that allows the user to create new billing accounts, but it does not provide access to billing information or the ability to manage existing accounts.</p><p><br></p><p>https://cloud.google.com/billing/docs/how-to/billing-access</p><p>https://cloud.google.com/billing/docs/how-to/billing-access#overview-of-cloud-billing-roles-in-cloud-iam</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146056,
    "question_plain": "Your company has an existing monolithic application running on-premises and you've been tasked to migrate this application to Google Cloud Platform (GCP). The company wants to start taking advantage of microservices for scalability and maintainability. What strategy should you use?",
    "answers": [
      "<p>Refactor the monolithic application into microservices and deploy using GKE.</p>",
      "<p>Refactor the monolithic application into microservices and deploy each one as a Cloud Function.</p>",
      "<p>Migrate the monolithic application to App Engine Standard, then refactor for microservices.</p>",
      "<p>Migrate the application to Cloud Run without refactoring, then move individual services to GKE as they are broken out.</p>"
    ],
    "explanation": "<p>Refactor the monolithic application into microservices and deploy using GKE. -&gt;&nbsp;Correct. This is a good strategy because Google Kubernetes Engine (GKE) is designed to manage, scale, and deploy containerized applications, which are well-suited to a microservices architecture.</p><p><br></p><p>Refactor the monolithic application into microservices and deploy each one as a Cloud Function. -&gt;&nbsp;Incorrect. While Cloud Functions can run individual pieces of code in response to events, they are typically used for lightweight, event-driven processing and may not be suitable for larger microservices or long-running processes.</p><p><br></p><p>Migrate the monolithic application to App Engine Standard, then refactor for microservices. -&gt;&nbsp;Incorrect. App Engine Standard is primarily designed for simple, stateless applications. While it could run a monolithic application, it may not be well-suited to a complex microservices architecture.</p><p><br></p><p>Migrate the application to Cloud Run without refactoring, then move individual services to GKE as they are broken out. -&gt;&nbsp;Incorrect. While Cloud Run can host containerized applications, it's better suited to stateless, request-driven workloads. It might not be the best fit for complex, stateful microservices.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146058,
    "question_plain": "Suppose your company has two VPC networks and they want to communicate internally between these networks. So that traffic stays within Google's network and doesn't traverse the public internet. Which service should you advise?",
    "answers": [
      "<p>VPC Network Peering</p>",
      "<p>Cloud VPN</p>",
      "<p>Cloud DNS</p>",
      "<p> Hybrid Connectivity</p>"
    ],
    "explanation": "<p>VPC Network Peering -&gt; Correct. It is the recommended option for establishing private communication between two VPC networks within Google Cloud. This service allows VPC networks to communicate with each other using private IP addresses, and the traffic stays within Google's network, which provides faster and more secure communication between the two networks. </p><p><br></p><p>Cloud VPN -&gt; Incorrect. It is used to establish secure connections between on-premises networks and Google Cloud VPC networks, and it uses the public internet to transmit traffic. </p><p><br></p><p>Cloud DNS -&gt;&nbsp;Incorrect. It is a service used for managing DNS records and resolving domain names to IP addresses. </p><p><br></p><p>Hybrid Connectivity -&gt;&nbsp;Incorrect. It refers to the ability to connect on-premises infrastructure to cloud-based infrastructure.</p><p><br></p><p>https://cloud.google.com/vpc/docs/vpc-peering</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146060,
    "question_plain": "As a cloud architect, you are tasked with creating a signed URL for a Google Cloud Storage (GCS) bucket. The requirement is to allow access to a certain object in the bucket for a third-party service, but only for 30 minutes. The solution should ensure minimum privileges and security. Which of the following options should you choose to implement this?",
    "answers": [
      "<p>Create a signed URL using a user-managed service account with read access on the bucket and a 30-minute expiration.</p>",
      "<p>Create a signed URL using the bucket's default service account with a 30-minute expiration.</p>",
      "<p>Create a signed URL using a user-managed service account with full access on the bucket and a 30-minute expiration.</p>",
      "<p>Create a signed URL using the bucket's default service account with no expiration, and manually invalidate the URL after 30 minutes.</p>"
    ],
    "explanation": "<p>Create a signed URL using a user-managed service account with read access on the bucket and a 30-minute expiration. -&gt; Correct. Creating a signed URL using a user-managed service account that only has read access to the bucket follows the principle of least privilege. The 30-minute expiration limits the availability of the object, providing further security.</p><p><br></p><p>Create a signed URL using the bucket's default service account with a 30-minute expiration. -&gt; Incorrect. The bucket's default service account should not be used to create a signed URL because it typically has more permissions than necessary for this specific task. This could potentially expose the bucket to risks if the signed URL is compromised.</p><p><br></p><p>Create a signed URL using a user-managed service account with full access on the bucket and a 30-minute expiration. -&gt; Incorrect. Granting full access to a service account for the purpose of creating a signed URL to read an object violates the principle of least privilege, even if the URL is set to expire after 30 minutes.</p><p><br></p><p>Create a signed URL using the bucket's default service account with no expiration, and manually invalidate the URL after 30 minutes. -&gt;&nbsp;Incorrect. Signed URLs should always have an expiration time to minimize the potential impact of a security compromise. Manually invalidating the URL after 30 minutes is not reliable or efficient.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146062,
    "question_plain": "As a cloud architect, you are working on a complex scenario where you have to deploy a monitoring pod in a DaemonSet object across a GKE cluster for a client's application. You want the monitoring pod to be deployed on every node of the GKE cluster. Which approach will allow you to efficiently achieve this objective?",
    "answers": [
      "<p>Deploy the monitoring pod using a DaemonSet across the nodes in the GKE cluster.</p>",
      "<p>Deploy the monitoring pod individually on each node in the GKE cluster.</p>",
      "<p>Deploy the monitoring pod as a ReplicaSet across the nodes in the GKE cluster.</p>",
      "<p>Deploy the monitoring pod using a StatefulSet across the nodes in the GKE cluster.</p>"
    ],
    "explanation": "<p>Deploy the monitoring pod using a DaemonSet across the nodes in the GKE cluster. -&gt; Correct. A DaemonSet ensures that all (or some) nodes run a copy of a pod. This is suitable for deploying system daemons such as log collectors, monitoring services etc. When a node is added to the cluster, the pod gets added to the new node, making it ideal for our scenario.</p><p><br></p><p>Deploy the monitoring pod individually on each node in the GKE cluster. -&gt; Incorrect. Deploying the monitoring pod individually on each node would not be efficient, as you would have to manually handle scaling and the addition of new nodes.</p><p><br></p><p>Deploy the monitoring pod as a ReplicaSet across the nodes in the GKE cluster. -&gt;&nbsp;Incorrect. A ReplicaSet ensures that a specified number of pod \"replicas\" are running at any given time. However, it doesn't ensure that pods are run on every node in the cluster.</p><p><br></p><p>Deploy the monitoring pod using a StatefulSet across the nodes in the GKE cluster. -&gt; Incorrect. StatefulSet is used for workloads that require stable network identifiers, stable persistent storage, and orderly, graceful deployment and scaling. It's not specifically designed to ensure that a pod runs on every node in a cluster.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146064,
    "question_plain": "You're a cloud architect and have been assigned a task to develop a model that will predict the type of product a customer is most likely to purchase next, based on their past purchases and behavior. The client has a massive amount of historic data available but lacks machine learning expertise. Furthermore, the solution needs to be able to constantly learn from new data. Which of the following would be the best approach for this situation?",
    "answers": [
      "<p>Use AutoML Tables, retrain the model periodically with new data.</p>",
      "<p>Use AutoML Vision, retrain the model periodically with new data.</p>",
      "<p>Use BigQuery ML, retrain the model manually with new data.</p>",
      "<p>Use Cloud Machine Learning Engine with TensorFlow and retrain the model manually with new data.</p>"
    ],
    "explanation": "<p>Use AutoML Tables, retrain the model periodically with new data. -&gt; Correct. AutoML Tables is the right solution because it is specifically designed for tabular or structured data, like historic purchase data. AutoML allows for automatic model training, tuning, and deployment, which suits the lack of machine learning expertise. It also allows the model to be retrained periodically with new data, which meets the requirement of constantly learning from new data.</p><p><br></p><p>Use AutoML Vision, retrain the model periodically with new data. -&gt; Incorrect. AutoML Vision is incorrect because it is designed for image analysis tasks, not for analyzing structured tabular data.</p><p><br></p><p>Use BigQuery ML, retrain the model manually with new data. -&gt; Incorrect. BigQuery ML allows machine learning models to be built using SQL queries, but in this case, it might not be suitable as it requires manual retraining and might not be as flexible or automated as AutoML for complex predictive models.</p><p><br></p><p>Use Cloud Machine Learning Engine with TensorFlow and retrain the model manually with new data. -&gt;&nbsp;Incorrect. Cloud Machine Learning Engine with TensorFlow might be a viable solution for some scenarios, but in this case, it requires manual model training and deep machine learning expertise, which is not available according to the scenario.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146066,
    "question_plain": "Your organization has an existing system on-premise and plans to migrate to Google Cloud. However, they plan to keep some parts of their system on-premises for the foreseeable future. The goal is to have the on-premise and cloud systems communicate securely and efficiently. What should be your strategy?",
    "answers": [
      "<p>Use Cloud VPN to establish a secure connection between the on-premise system and Google Cloud</p>",
      "<p>Use VPC Network Peering to connect the on-premise system to the VPC in Google Cloud</p>",
      "<p>Use Cloud Interconnect to connect the on-premise system to Google Cloud</p>",
      "<p>Use Cloud Endpoints to create APIs for the on-premise system and access them from Google Cloud</p>"
    ],
    "explanation": "<p>Use Cloud VPN to establish a secure connection between the on-premise system and Google Cloud -&gt;&nbsp;Correct. Cloud VPN can create a secure connection between an on-premise system and Google Cloud over the internet, which is ideal for hybrid systems that span on-premise and cloud environments.</p><p><br></p><p>Use VPC Network Peering to connect the on-premise system to the VPC in Google Cloud -&gt;&nbsp;Incorrect. VPC Network Peering is used to connect two VPCs, even across projects or organizations. It's not meant for connecting on-premise systems directly to Google Cloud.</p><p><br></p><p>Use Cloud Interconnect to connect the on-premise system to Google Cloud -&gt;&nbsp;Incorrect. Cloud Interconnect provides a direct connection to Google's network but may not be necessary or cost-effective for all businesses, especially if the traffic volume does not warrant it.</p><p><br></p><p>Use Cloud Endpoints to create APIs for the on-premise system and access them from Google Cloud -&gt;&nbsp;Incorrect. Cloud Endpoints is a development tool used to develop, deploy, protect, and monitor APIs. It's not primarily used for connecting on-premise systems with Google Cloud.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146068,
    "question_plain": "You are a cloud architect working for a fintech company that wants to deploy a critical, containerized application in a microservices architecture. The application needs to support high levels of incoming traffic with low latency, ensure zero-downtime deployments, allow for automatic scaling based on CPU utilization, and maintain end-to-end encryption. Which of the following deployment methods should you choose for this scenario?",
    "answers": [
      "<p>Deploy the application on Google Kubernetes Engine (GKE) with an HTTPS load balancer.</p>",
      "<p>Deploy the application on Compute Engine instances behind a load balancer.</p>",
      "<p>Deploy the application on App Engine Standard environment.</p>",
      "<p>Deploy the application on App Engine Flexible environment.</p>"
    ],
    "explanation": "<p>Deploy the application on Google Kubernetes Engine (GKE) with an HTTPS load balancer. -&gt; Correct. It's designed for deploying, scaling, and managing containerized applications. It supports zero-downtime deployments through rolling updates, autoscaling based on CPU utilization, and end-to-end encryption can be achieved through an HTTPS load balancer.</p><p><br></p><p>Deploy the application on Compute Engine instances behind a load balancer. -&gt; Incorrect. Compute Engine instances could be a potential choice for containerized applications, but managing zero-downtime deployments and automatic scaling can be challenging and complex without orchestration.</p><p><br></p><p>Deploy the application on App Engine Standard environment. -&gt; Incorrect. App Engine Standard environment does not natively support containerized applications. Although it supports automatic scaling and zero-downtime deployments, this option is not suitable for this scenario.</p><p><br></p><p>Deploy the application on App Engine Flexible environment. -&gt; Incorrect. While the App Engine Flexible environment does support containerized applications, it may not provide the same level of control over deployments, scaling, and management as GKE does, making it less suitable for this specific scenario. </p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146070,
    "question_plain": "As a cloud architect, you are working for a company that needs to perform a large-scale data processing task that is highly parallelizable and not time-sensitive. This operation should be completed within a reasonable time frame, but it doesn't require an immediate result. The cost of operation is a major consideration. The company has asked you for the most cost-effective solution that matches these requirements. What would be your recommendation?",
    "answers": [
      "<p>Use Google Cloud Dataproc with preemptible Compute Engine machines.</p>",
      "<p>Use Google Cloud Dataflow with non-preemptible Compute Engine machines.</p>",
      "<p>Use Google Kubernetes Engine with preemptible Compute Engine machines.</p>",
      "<p>Use Google Cloud Functions for the data processing task.</p>"
    ],
    "explanation": "<p>Use Google Cloud Dataproc with preemptible Compute Engine machines. -&gt;&nbsp;Correct. Dataproc is designed for fast, easy, and cost-efficient processing of big data workloads. Using preemptible Compute Engine machines, which are significantly cheaper than standard machines, can save costs. Since the task is not time-sensitive, the fact that preemptible instances can be terminated at any time (up to a maximum of 24 hours) is not a significant issue.</p><p><br></p><p>Use Google Cloud Dataflow with non-preemptible Compute Engine machines. -&gt;&nbsp;Incorrect. While Dataflow is a great option for large-scale data processing tasks, using non-preemptible Compute Engine machines may not be the most cost-effective solution for a non-time-sensitive task.</p><p><br></p><p>Use Google Kubernetes Engine with preemptible Compute Engine machines. -&gt;&nbsp;Incorrect. Kubernetes Engine can run large-scale data processing tasks, but it might be overkill for this specific scenario. Moreover, using Kubernetes Engine introduces additional overhead in managing and orchestrating containers.</p><p><br></p><p>Use Google Cloud Functions for the data processing task. -&gt; Incorrect. Cloud Functions is designed for executing event-driven functions and it's not suitable for large-scale data processing tasks.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146072,
    "question_plain": "You have application data that you need to access on a monthly basis, with the requirement that data older than five years is no longer necessary. What steps can you take to minimize storage expenses? Select all that apply.",
    "answers": [
      "<p>You should set an object lifecycle management policy to remove data older than 5 years.</p>",
      "<p>You should store infrequently accessed data in a Nearline storage class.</p>",
      "<p>You should store infrequently accessed data in a Standard storage class.</p>",
      "<p>You should store infrequently accessed data in a multi-regional bucket.</p>",
      "<p>You should set an object lifecycle management policy to change the storage class to Archive for data older than 5 years.</p>"
    ],
    "explanation": "<p>You should set an object lifecycle management policy to remove data older than 5 years. -&gt; Correct. By setting an object lifecycle management policy, you can define rules that automatically delete data that is older than a certain threshold, in this case, 5 years. This ensures that you only retain and pay for the necessary data, minimizing storage expenses.</p><p><br></p><p>You should store infrequently accessed data in a Nearline storage class. -&gt; Correct. The Nearline storage class is designed for data that is accessed less frequently but still requires relatively fast access times. By storing infrequently accessed data in the Nearline storage class, you can benefit from lower storage costs compared to the Standard storage class, while still maintaining reasonable access times for the data.</p><p><br></p><p>You should store infrequently accessed data in a Standard storage class. -&gt;&nbsp;Incorrect. In this scenario where the data is accessed on a monthly basis, storing it in the Standard storage class might result in higher storage costs, as you would be paying for a higher storage tier without utilizing the frequent access benefits.</p><p><br></p><p>You should store infrequently accessed data in a multi-regional bucket. -&gt;&nbsp;Incorrect. Storing infrequently accessed data in a multi-regional bucket might not be the most cost-effective option, as multi-regional storage typically comes with higher costs compared to regional or nearline storage options. It is more suitable for data that requires low-latency access from multiple regions.</p><p><br></p><p>You should set an object lifecycle management policy to change the storage class to Archive for data older than 5 years. -&gt;&nbsp;Incorrect. The requirement states that the data needs to be accessed on a monthly basis, and the Archive storage class is intended for long-term storage with infrequent access. Retrieving data from the Archive storage class incurs additional retrieval costs and longer access times, which may not align with the requirement of monthly access.</p><p><br></p><p>https://cloud.google.com/storage/docs/lifecycle</p><p>https://cloud.google.com/storage/docs/storage-classes#nearline</p>",
    "correct_response": ["a", "b"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 82146074,
    "question_plain": "Every year, an audit takes place within your company, and it is necessary to grant external auditors access to the audit logs from the past five years. What should you do to minimize the cost and operational overhead? (select 3)",
    "answers": [
      "<p>Export audit logs to Cloud Storage.</p>",
      "<p>Grant external auditors the role of Storage Object Viewer on the logs storage bucket.</p>",
      "<p>Configure a lifecycle management policy on the logs bucket to delete objects older than 5 years.</p>",
      "<p>Export audit logs to BigQuery.</p>",
      "<p>Export audit logs to Cloud Filestore.</p>"
    ],
    "explanation": "<p>Export audit logs to Cloud Storage. -&gt; Correct. Exporting audit logs to Cloud Storage is a common practice to store large amounts of data in a cost-effective manner. It allows external auditors to access the audit logs easily without having to access the underlying infrastructure directly.</p><p><br></p><p>Grant external auditors the role of Storage Object Viewer on the logs storage bucket. -&gt; Correct. Granting external auditors the role of Storage Object Viewer on the logs storage bucket ensures they can access the audit logs in the Cloud Storage bucket without granting them additional permissions or access to other resources.</p><p><br></p><p>Configure a lifecycle management policy on the logs bucket to delete objects older than 5 years. -&gt; Correct. Configuring a lifecycle management policy on the logs bucket to delete objects older than 5 years can help minimize storage costs and operational overhead. It ensures that the storage cost is optimized by deleting objects that are no longer required, and it also reduces the chances of security breaches by removing outdated logs that might be subject to data privacy and regulatory requirements.</p><p><br></p><p>Export audit logs to BigQuery. -&gt; Incorrect.</p><p>Export audit logs to Cloud Filestore. -&gt; Incorrect.</p><p><br></p><p>https://cloud.google.com/storage/docs/audit-logging</p><p>https://cloud.google.com/storage/docs/access-control/iam-roles#standard-roles</p><p>https://cloud.google.com/storage/docs/lifecycle</p>",
    "correct_response": ["a", "b", "c"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 82146076,
    "question_plain": "A web application uses Cloud SQL to store user data which is very critical for the application flow. As a cloud architect, you want to protect your data from zone failures. What should you do?",
    "answers": [
      "<p>You should configure High Availability for Cloud SQL.</p>",
      "<p>You should create a Read replica in a different region.</p>",
      "<p>You should create a Read replica in the same region but in a different zone.</p>",
      "<p>You cannot increase availability of your application in this case.</p>"
    ],
    "explanation": "<p>You should configure High Availability for Cloud SQL. -&gt; Correct. High Availability (HA) is the recommended configuration for critical data in Cloud SQL. It provides a failover replica in another zone in the same region to ensure data protection in the event of a zone outage. With HA, Cloud SQL automatically replicates data to a standby replica in a different zone within the same region. If there is a zone failure, Cloud SQL automatically fails over to the standby replica, ensuring that your application remains available. Therefore, configuring High Availability for Cloud SQL is the best option for protecting critical data from zone failures.</p><p><br></p><p>You should create a Read replica in a different region. -&gt; Incorrect. It suggest creating Read replica, which can help improve read performance, but it does not provide protection against zone failures.</p><p><br></p><p>You should create a Read replica in the same region but in a different zone. -&gt; Incorrect. It suggest creating Read replica, which can help improve read performance, but it does not provide protection against zone failures.</p><p><br></p><p>You cannot increase availability of your application in this case. -&gt; Incorrect. It is incorrect because you can increase the availability of the application by configuring High Availability for Cloud SQL.</p><p><br></p><p>https://cloud.google.com/sql/docs/mysql/high-availability</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146078,
    "question_plain": "As a cloud architect, you are tasked with designing a solution for a social media application that stores user-uploaded images in a Cloud Storage bucket. For each uploaded image, a thumbnail version needs to be generated for displaying in the application. The solution should be highly scalable to handle sudden spikes in uploads, cost-effective, and able to process images as soon as they are uploaded. What architecture would you suggest for this?",
    "answers": [
      "<p>Use Cloud Functions triggered by Cloud Storage events to generate thumbnails.</p>",
      "<p>Use App Engine to constantly check for new images and generate thumbnails.</p>",
      "<p>Use Compute Engine with a startup script to generate thumbnails.</p>",
      "<p>Use Kubernetes Engine to constantly check for new images and generate thumbnails.</p>"
    ],
    "explanation": "<p>Use Cloud Functions triggered by Cloud Storage events to generate thumbnails. -&gt;&nbsp;Correct. Cloud Functions is a serverless execution environment that can automatically run your function in response to events from Cloud Storage, such as when a new image is uploaded. This makes it an ideal choice for creating a scalable, event-driven, and cost-effective solution to generate thumbnails.</p><p><br></p><p>Use App Engine to constantly check for new images and generate thumbnails. -&gt; Incorrect. App Engine constantly checking for new images could lead to inefficiencies and unnecessary costs, as it would need to run continuously, even when there are no new images to process.</p><p><br></p><p>Use Compute Engine with a startup script to generate thumbnails. -&gt; Incorrect. Compute Engine could technically handle the task, but it may not be as scalable, cost-effective, or event-driven as a serverless option like Cloud Functions. It would also require additional management and configuration.</p><p><br></p><p>Use Kubernetes Engine to constantly check for new images and generate thumbnails. -&gt;&nbsp;Incorrect. Like the App Engine option, Kubernetes Engine constantly checking for new images can lead to inefficiencies and unnecessary costs. It also introduces complexity that may not be needed for this particular task.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146080,
    "question_plain": "The raw format of CCTV footage videos is stored by a machine learning startup in a Cloud Storage bucket. During the initial two-week period, the footage undergoes consistent processing to identify and detect potential threats. What storage approach would you suggest to minimize costs when storing the videos?",
    "answers": [
      "<p>You should use Standard storage class for the first two weeks, and use lifecycle rules to transition to Coldline.</p>",
      "<p>You should use Standard storage class for the first two weeks, and use lifecycle rules to transition to Nearline.</p>",
      "<p>You should use Standard storage class for the first two weeks, and then move videos to Persistent Disk.</p>",
      "<p>You should use Standard storage class for the first 30 days, and use lifecycle rules to transition to Coldline.</p>"
    ],
    "explanation": "<p>You should use Standard storage class for the first two weeks, and use lifecycle rules to transition to Coldline. -&gt; Correct. Using the Standard storage class for the initial two-week period allows for immediate access and processing of the CCTV footage videos when they are actively being analyzed for potential threats. After this initial processing period, you can leverage lifecycle rules to automatically transition the videos to the Coldline storage class. Coldline storage is designed for long-term storage of infrequently accessed data and offers lower storage costs compared to Standard storage, helping to minimize costs for storing the videos.</p><p><br></p><p>You should use Standard storage class for the first two weeks, and use lifecycle rules to transition to Nearline. -&gt;&nbsp;Incorrect. While using the Standard storage class for the initial two weeks is a good approach for immediate access and processing, transitioning the videos to the Nearline storage class may not be the most cost-effective option. Nearline storage is designed for data that is accessed less frequently than Standard storage but more frequently than Coldline storage. Since the scenario mentions that the footage undergoes consistent processing, it suggests that the videos might not be frequently accessed after the initial two-week period, making Coldline a more suitable and cost-effective option.</p><p><br></p><p>You should use Standard storage class for the first two weeks, and then move videos to Persistent Disk. -&gt;&nbsp;Incorrect. Using Persistent Disk for storing CCTV footage videos may not be the most cost-effective approach. Persistent Disk is primarily designed for block storage and is more suitable for hosting operating system files and application data within virtual machines. It may not offer the same cost optimization and data durability as storage classes specifically designed for long-term object storage like Coldline.</p><p><br></p><p>You should use Standard storage class for the first 30 days, and use lifecycle rules to transition to Coldline. -&gt;&nbsp;Incorrect. While transitioning to Coldline storage using lifecycle rules is a recommended approach, using the Standard storage class for the first 30 days may not be necessary if the processing period mentioned in the scenario is only two weeks. By using Standard storage for the entire 30 days, you would incur higher storage costs compared to using it for just the initial two weeks. Transitioning to Coldline storage after the two-week processing period would be more cost-effective.</p><p><br></p><p>https://cloud.google.com/storage/docs/storage-classes#standard</p><p>https://cloud.google.com/storage/docs/lifecycle</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146082,
    "question_plain": "As a cloud architect, you are migrating an application to GCP&nbsp;using overnight batch jobs that take approximately one hour. Which service should you recommend to do this with minimal cost?",
    "answers": [
      "<p>You should run the batch jobs in a preemptible compute engine instance.</p>",
      "<p>You should run the batch jobs in a GKE cluster.</p>",
      "<p>You should run the batch jobs in a normal virtual machine instance.</p>",
      "<p>You should run the batch jobs in a virtual machine instance with GPUs.</p>"
    ],
    "explanation": "<p>You should run the batch jobs in a preemptible compute engine instance. -&gt; Correct. Preemptible Compute Engine instances are a cost-effective option for running batch jobs. Preemptible instances are regular instances that are priced lower than standard instances and can run for up to 24 hours. However, these instances can be terminated at any time if Google needs to reclaim the resources, and their availability is not guaranteed. Since the batch jobs are running overnight, and the workloads are not mission-critical, it is acceptable to use preemptible instances. Additionally, the batch jobs are not GPU-intensive, so there is no need to use GPU-enabled instances.</p><p><br></p><p>You should run the batch jobs in a GKE cluster. -&gt; Incorrect. It is not ideal since setting up a Kubernetes cluster incurs additional management overhead and cost, and is best suited for running long-running services. It is also not the most cost-effective solution for running batch jobs that only run for a short period.</p><p><br></p><p>You should run the batch jobs in a normal virtual machine instance. -&gt; Incorrect. It is a viable option, but using preemptible instances is a more cost-effective solution.</p><p><br></p><p>You should run the batch jobs in a virtual machine instance with GPUs. -&gt; Incorrect. It is not necessary since the batch jobs are not GPU-intensive. Using GPU-enabled instances would add unnecessary cost.</p><p><br></p><p>https://cloud.google.com/compute/docs/instances/preemptible#what_is_a_preemptible_instance</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146084,
    "question_plain": "A mobile gaming company decided to migrate its analytics to BigQuery. Their analytics team needs access to perform queries against the data in BigQuery to improve user acquisition expenses for marketing campaigns. This analytics team members may change frequently. With Google best practices in mind, how do you grant access?",
    "answers": [
      "<p>You should create a Cloud Identity account for each analyst and add them all to a group. Then, grant BigQuery Data Viewer role to this group.</p>",
      "<p>You should create a Cloud Identity account for each analyst. Then, grant BigQuery Data Viewer role to each account.</p>",
      "<p>You should create a Cloud Identity account for each analyst and add them all to a group. Then, grant BigQuery Data Owner role to this group.</p>",
      "<p>You should create a Cloud Identity account for each analyst. Then, grant BigQuery Data Owner role to each account.</p>"
    ],
    "explanation": "<p>You should create a Cloud Identity account for each analyst and add them all to a group. Then, grant BigQuery Data Viewer role to this group. -&gt; Correct. Creating a Cloud Identity account for each analyst and adding them to a group provides a centralized approach for managing access to BigQuery. By granting the BigQuery Data Viewer role to the group, all analysts within the group will inherit the necessary permissions to perform queries against the data in BigQuery. This approach allows for easy management of access and ensures that new analysts can be added or removed from the group as needed, without individually granting or revoking access for each analyst.</p><p><br></p><p>You should create a Cloud Identity account for each analyst. Then, grant BigQuery Data Viewer role to each account. -&gt; Incorrect. While creating a Cloud Identity account for each analyst is a good step, granting the BigQuery Data Viewer role to each individual account can become cumbersome to manage as the number of analysts changes over time. It may require manual adjustments each time an analyst joins or leaves the team. This approach lacks the scalability and efficiency of managing access through groups.</p><p><br></p><p>You should create a Cloud Identity account for each analyst and add them all to a group. Then, grant BigQuery Data Owner role to this group. -&gt; Incorrect. Granting the BigQuery Data Owner role to a group may provide more permissions than necessary for the analytics team members. The Data Owner role includes privileges that go beyond performing queries, such as the ability to modify datasets and manage access controls. It is generally recommended to grant the least privilege required to perform the job, and in this case, the Data Viewer role is more appropriate.</p><p><br></p><p>You should create a Cloud Identity account for each analyst. Then, grant BigQuery Data Owner role to each account. -&gt; Incorrect. Granting the BigQuery Data Owner role to each individual account can give excessive privileges to analysts. This approach could lead to potential security risks and potential accidental modifications to datasets or access controls. It is preferable to assign more specific and limited roles that align with the actual requirements of the analytics team members.</p><p><br></p><p>https://cloud.google.com/identity/docs/set-up-cloud-identity-admin</p><p>https://cloud.google.com/bigquery/docs/access-control#bigquery</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146086,
    "question_plain": "In your company, each employee has a credit card assigned to their account. As a cloud architect, you want to consolidate the billing of all GCP projects into a new billing account. With Google best practices in mind, how should you do it?",
    "answers": [
      "<p>In the GCP Console, move all projects to the root organization in the Resource Manager.</p>",
      "<p>You should create a new Billing account and set up a payment method with company credit card.</p>",
      "<p>You should send an email to Google Billing Support and request them to create a new billing account and link all the projects to the billing account.</p>",
      "<p>Once a credit card is assigned to the project, it cannot be changed. You have to create all the projects from scratch with new billing account.</p>"
    ],
    "explanation": "<p>In the GCP Console, move all projects to the root organization in the Resource Manager. -&gt;&nbsp;Correct. Moving all projects to the root organization in the Resource Manager allows you to create a new billing account at the organization level and link all the projects to it. This approach is recommended by Google as it helps to centralize billing and provides better visibility into the costs and usage of all GCP projects. Once the projects are linked to the new billing account, you can remove the credit cards associated with individual accounts and use a single payment method for all projects. </p><p><br></p><p>You should create a new Billing account and set up a payment method with company credit card. -&gt;&nbsp;Incorrect. It is incorrect because it only addresses the creation of a new billing account but doesn't mention the steps needed to link the existing projects to the new account. </p><p><br></p><p>You should send an email to Google Billing Support and request them to create a new billing account and link all the projects to the billing account. -&gt; Incorrect. It is incorrect because it's not necessary to contact Google Billing Support for this task, and you can do it using the GCP Console. </p><p><br></p><p>Once a credit card is assigned to the project, it cannot be changed. You have to create all the projects from scratch with new billing account. -&gt;&nbsp;Incorrect. It is incorrect because it's not true that you cannot change the payment method associated with a GCP project.</p><p><br></p><p>https://cloud.google.com/resource-manager/docs/quickstart-organizations</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146088,
    "question_plain": "You need to migrate your legacy on-premises applications to Google Cloud that are written in C++ and you want to use the serverless approach. What GCP compute services should you use?",
    "answers": [
      "<p>You should deploy the containerized version of the application in Cloud Run.</p>",
      "<p>You should deploy the containerized version of the application in Google Kubernetes Engine.</p>",
      "<p>You should deploy the containerized version of the application in App Engine Flexible.</p>",
      "<p>You should deploy this application using a Managed Instance Group.</p>"
    ],
    "explanation": "<p>You should deploy the containerized version of the application in Cloud Run. -&gt; Correct. Cloud Run is a serverless compute platform that allows you to run stateless containers in a fully managed environment. It supports containerized applications built on any language or framework, including C++. By deploying the containerized version of the application to Cloud Run, you can take advantage of the serverless approach, where you only pay for the actual usage of resources, and the environment is fully managed by Google Cloud.</p><p><br></p><p>You should deploy the containerized version of the application in Google Kubernetes Engine. -&gt; Incorrect. It requires more management overhead and may not be the best fit for a serverless approach. Kubernetes Engine provides a managed environment for running containerized applications, but it requires more management overhead and may not be as cost-effective as a serverless solution.</p><p><br></p><p>You should deploy the containerized version of the application in App Engine Flexible. -&gt; Incorrect. It may not be the best fit for applications that require more customization and control over the infrastructure.</p><p><br></p><p>You should deploy this application using a Managed Instance Group. -&gt; Incorrect. It is not the best option for the given requirement because Managed Instance Groups are primarily used for managing groups of virtual machine instances, which require more management overhead and may not be as cost-effective as a serverless solution like Cloud Run.</p><p><br></p><p>https://cloud.google.com/run/docs/concepts</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146090,
    "question_plain": "An e-commerce company has petabytes of customer behavior data stored in private data center. Due to storage limitations in private data center, this company decided to migrate this data to GCP. The data must be available for your analysts, who have strong SQL background. How should you store the data to meet these requirements?",
    "answers": [
      "<p>You should import data into BigQuery.</p>",
      "<p>You should import flat files into Cloud Storage.</p>",
      "<p>You should import data into Cloud Datastore.</p>",
      "<p>You should import data into Cloud SQL.</p>"
    ],
    "explanation": "<p>You should import data into BigQuery. -&gt;&nbsp;Correct. BigQuery is a fully-managed, cloud-native data warehouse that enables super-fast SQL queries using the processing power of Google's infrastructure. It can handle petabytes of data and is designed for analyzing large datasets with ease. It is also highly scalable and supports real-time analysis of data. Analysts can use their existing SQL skills to query data stored in BigQuery without the need for any special software or hardware, making it a perfect fit for the requirements of the e-commerce company in this scenario.</p><p><br></p><p>You should import flat files into Cloud Storage. -&gt; Incorrect. It is not the best answer as flat files stored in Cloud Storage would require additional processing to make them queryable with SQL.</p><p>You should import data into Cloud Datastore. -&gt;&nbsp;Incorrect. It is not the best answer as Cloud Datastore is a NoSQL document database, which is not ideal for SQL queries.</p><p><br></p><p>You should import data into Cloud SQL. -&gt; Incorrect. It is not the best answer as Cloud SQL is a relational database service which requires data to be structured in a specific way and has limitations on data size, making it less suitable for the petabyte-scale data in this scenario.</p><p><br></p><p>https://cloud.google.com/bigquery/docs/introduction</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146092,
    "question_plain": "An application processes a significant number of transactions that exceed the capabilities of a single virtual machine. As a cloud architect, you want to spread transactions across multiple servers in real time and in the most cost-effective way. What should you do?",
    "answers": [
      "<p>You should send transactions to Pub/Sub and process them in virtual machines in a Managed Instance Group.</p>",
      "<p>You should send transactions to Cloud Bigtable, and poll for new transactions from the VMs.</p>",
      "<p>You should set up Cloud SQL with a memory cache for speed. On your multiple servers, poll for transactions that do not have the ‘processed’ key, and mark them ‘processed’ when done.</p>",
      "<p>You should send transactions to BigQuery. On the virtual machines, poll for transactions that don't have the ‘processed’ key, and mark them ‘processed’ when done.</p>"
    ],
    "explanation": "<p>You should send transactions to Pub/Sub and process them in virtual machines in a Managed Instance Group. -&gt;&nbsp;Correct. Pub/Sub is a messaging service that allows decoupling between the senders and receivers of messages, enabling the sender to send messages without worrying about who will receive them. A Managed Instance Group is a group of virtual machine instances that are managed together and can be scaled up or down automatically based on the workload. By using Pub/Sub and a Managed Instance Group, the transactions can be sent to Pub/Sub, and the virtual machines can subscribe to the relevant Pub/Sub topic to receive and process the transactions. This approach ensures that the transactions are spread across multiple servers, allowing for parallel processing and improved performance.</p><p><br></p><p>You should send transactions to Cloud Bigtable, and poll for new transactions from the VMs. -&gt; Incorrect. Cloud Bigtable is a NoSQL database for storing large amounts of data, and it may not be the most suitable solution for real-time transaction processing. </p><p><br></p><p>You should set up Cloud SQL with a memory cache for speed. On your multiple servers, poll for transactions that do not have the ‘processed’ key, and mark them ‘processed’ when done. -&gt; Incorrect. Cloud SQL is a managed relational database service that can be used for storing transaction data, but it does not provide the scalability or real-time processing capabilities required for this scenario. </p><p><br></p><p>You should send transactions to BigQuery. On the virtual machines, poll for transactions that don't have the ‘processed’ key, and mark them ‘processed’ when done. -&gt; Incorrect. BigQuery is a data warehouse that can be used for batch processing and analytics, but it may not be the best solution for real-time transaction processing.</p><p><br></p><p>https://cloud.google.com/pubsub/docs/quickstart-console</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146094,
    "question_plain": "Your organization provides a video streaming service to global users. Recently, users are complaining about the high latency when watching videos. As a cloud architect, how would you improve the streaming experience?",
    "answers": [
      "<p>Use Cloud CDN to cache and serve video content.</p>",
      "<p>Store all video content in a single region in Cloud Storage.</p>",
      "<p>Use Cloud Pub/Sub to push video content to users.</p>",
      "<p>Use Cloud Spanner to store and serve video content.</p>"
    ],
    "explanation": "<p>Use Cloud CDN to cache and serve video content. -&gt;&nbsp;Correct. Cloud CDN leverages Google's global edge network to cache and serve content closer to users, which can significantly reduce latency and improve the streaming experience.</p><p><br></p><p>Store all video content in a single region in Cloud Storage. -&gt;&nbsp;Incorrect. Storing content in a single region can cause latency for users that are geographically distant. This solution would not address the latency issue.</p><p><br></p><p>Use Cloud Pub/Sub to push video content to users. -&gt;&nbsp;Incorrect. Cloud Pub/Sub is a messaging service and not suited for video streaming, and would likely introduce more latency than current setup.</p><p><br></p><p>Use Cloud Spanner to store and serve video content. -&gt;&nbsp;Incorrect. Cloud Spanner is a globally distributed, horizontally scalable, relational database service and is not the best choice for storing and serving video content, as it might not reduce latency.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146096,
    "question_plain": "Your company runs several critical applications on Google Cloud. There has been a significant increase in the number of user-reported incidents recently, indicating performance issues. As a cloud architect, how would you improve the monitoring to proactively identify and address performance issues?",
    "answers": [
      "<p>Set up custom metrics in Cloud Monitoring for all critical applications and configure alerting based on those metrics.</p>",
      "<p>Use Cloud Profiler to continuously profile the applications.</p>",
      "<p>Use Cloud Trace to trace all requests to the applications.</p>",
      "<p>Use Cloud Debugger to debug the applications in real-time.</p>"
    ],
    "explanation": "<p>Set up custom metrics in Cloud Monitoring for all critical applications and configure alerting based on those metrics. -&gt;&nbsp;Correct. Cloud Monitoring allows you to set up custom metrics for monitoring specific aspects of your applications, and you can configure alerts to be notified when these metrics cross certain thresholds. This proactive approach can help you address performance issues before they impact users.</p><p><br></p><p>Use Cloud Profiler to continuously profile the applications. -&gt; Incorrect. Cloud Profiler is used to analyze the performance of your applications and understand where resources are being spent. However, it's primarily used for analyzing the performance of individual services or functions, rather than overall application monitoring.</p><p><br></p><p>Use Cloud Trace to trace all requests to the applications. -&gt; Incorrect. Cloud Trace allows you to analyze the latency of your application but it doesn't provide comprehensive application monitoring. While it's part of the overall solution, it should be used in conjunction with other tools.</p><p><br></p><p>Use Cloud Debugger to debug the applications in real-time. -&gt; Incorrect. Cloud Debugger allows you to inspect the state of your application in real time, but it's not a comprehensive monitoring solution and wouldn't be the most efficient tool for proactively identifying performance issues.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146098,
    "question_plain": "You plan to migrate your on-premises MySQL and PostgreSQL databases to the Google Cloud using Lift and Shift approach. Which Google Cloud service should you use?",
    "answers": [
      "<p>Database Migration Service</p>",
      "<p>Migrate for Anthos</p>",
      "<p>BigQuery Data Transfer Service</p>",
      "<p>Storage Transfer Service</p>"
    ],
    "explanation": "<p>Database Migration Service -&gt; Correct. Google's Database Migration Service is specifically designed to migrate databases to Google Cloud, including MySQL and PostgreSQL. It supports various migration source types, including on-premises databases, and provides a simple and automated way to migrate databases with minimal downtime. </p><p><br></p><p>Migrate for Anthos -&gt; Incorrect. Migrate for Anthos is used to modernize the applications.</p><p><br></p><p>BigQuery Data Transfer Service -&gt;&nbsp;Incorrect. BigQuery Data Transfer Service is used to transfer data to BigQuery.</p><p><br></p><p>Storage Transfer Service -&gt; Incorrect. Storage Transfer Service is used to transfer data to Cloud Storage.</p><p><br></p><p>https://cloud.google.com/database-migration/docs</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146100,
    "question_plain": "Your organization uses BigQuery extensively for data analysis and you are developing a new solution to automate some recurring tasks. As part of this solution, you need to create a new table in BigQuery and load data into it from a CSV file in Cloud Storage, all from a Compute Engine instance. What would be the correct approach?",
    "answers": [
      "<p>Use the <code>bq</code> command-line tool to both create the table and load the data from Cloud Storage.</p>",
      "<p>Use the <code>bq</code> command-line tool to create the table and then the <code>gsutil cp</code> command to copy the data from Cloud Storage to BigQuery.</p>",
      "<p>Use the <code>gsutil</code> command to create the table in BigQuery and then the <code>bq</code> command-line tool to load the data.</p>",
      "<p>Use the <code>gcloud</code> command-line tool to create the table and load the data into BigQuery.</p>"
    ],
    "explanation": "<p>Use the <code>bq</code> command-line tool to both create the table and load the data from Cloud Storage. -&gt; Correct. The <code>bq</code> tool can be used to perform a variety of BigQuery operations, including creating tables and loading data from Cloud Storage.</p><p><br></p><p>Use the <code>bq</code> command-line tool to create the table and then the <code>gsutil cp</code> command to copy the data from Cloud Storage to BigQuery. -&gt; Incorrect. <code>gsutil cp</code> command is used to copy files to and from Cloud Storage, not for loading data into BigQuery tables.</p><p><br></p><p>Use the <code>gsutil</code> command to create the table in BigQuery and then the <code>bq</code> command-line tool to load the data. -&gt; Incorrect. The <code>gsutil</code> tool is not used to interact with BigQuery, it's designed for working with Cloud Storage.</p><p><br></p><p>Use the <code>gcloud</code> command-line tool to create the table and load the data into BigQuery. -&gt; Incorrect. While <code>gcloud</code> is a comprehensive command-line tool for Google Cloud, specific BigQuery operations like creating tables and loading data are typically performed with the <code>bq</code> tool.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146102,
    "question_plain": "Your organization operates a multi-tier web application on Google Cloud. You have been tasked with implementing a solution to ensure operational reliability and quickly identify any issues that might affect the application's performance. Which of the following would be the best approach?",
    "answers": [
      "<p>Enable Cloud Logging and Monitoring for the entire project.</p>",
      "<p>Configure Cloud Pub/Sub to publish messages whenever there is a change in the application.</p>",
      "<p>Enable Cloud Debugger on all application instances.</p>",
      "<p>Use Cloud Dataflow to process logs in real-time.</p>"
    ],
    "explanation": "<p>Enable Cloud Logging and Monitoring for the entire project. -&gt; Correct. This option includes both Logging and Monitoring, which can be used to collect logs, metrics, events, and metadata from your Cloud project. This will allow you to monitor the performance of your application and quickly identify any issues.</p><p><br></p><p>Configure Cloud Pub/Sub to publish messages whenever there is a change in the application. -&gt;&nbsp;Incorrect. Cloud Pub/Sub is a messaging service that is not specifically designed for monitoring applications. While it could be used in conjunction with other services for monitoring, it is not a complete solution on its own.</p><p><br></p><p>Enable Cloud Debugger on all application instances. -&gt;&nbsp;Incorrect. Cloud Debugger allows you to inspect the state of an application at any code location without affecting the performance of the production application. However, it's not meant for general application performance monitoring, and using it for this purpose could lead to unnecessary complexity and overhead.</p><p><br></p><p>Use Cloud Dataflow to process logs in real-time. -&gt;&nbsp;Incorrect. While Cloud Dataflow can process and analyze real-time data, it's not specifically designed for monitoring applications. It would be overkill for this use case and may not provide the insights you need without additional tools.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146104,
    "question_plain": "You're designing a CI/CD pipeline for an application hosted on GCP. As part of the pipeline, you need to programmatically deploy and manage several GCP resources. Which approach should you use?",
    "answers": [
      "<p>Google Cloud SDK and REST APIs</p>",
      "<p>Google Cloud Console</p>",
      "<p>Cloud Shell</p>",
      "<p>Cloud Deployment Manager</p>"
    ],
    "explanation": "<p>Google Cloud SDK and REST APIs -&gt;&nbsp;Correct. The Google Cloud SDK and GCP REST APIs provide the programmatic interfaces needed to manage GCP resources within a CI/CD pipeline. They allow for the automation of resource management tasks.</p><p><br></p><p>Google Cloud Console -&gt;&nbsp;Incorrect. The Google Cloud Console is a web interface and not suitable for programmatically managing GCP resources within a CI/CD pipeline.</p><p><br></p><p>Cloud Shell -&gt;&nbsp;Incorrect. While Cloud Shell can be used to interact with GCP resources programmatically, it's an interactive, browser-based tool not suitable for use within a CI/CD pipeline.</p><p><br></p><p>Cloud Deployment Manager -&gt;&nbsp;Incorrect. Cloud Deployment Manager can be used to automate the creation and management of GCP resources, but it's more focused on infrastructure as code (IaC) and less suitable for integrating with a CI/CD pipeline compared to the Google Cloud SDK and GCP REST APIs.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146106,
    "question_plain": "You are developing an application that needs to interact with Google Cloud Storage to store and retrieve data. You need to authenticate the application programmatically with GCP without user intervention. Which method would you use?",
    "answers": [
      "<p>Service Account</p>",
      "<p>OAuth 2.0 Client ID</p>",
      "<p>Cloud Identity</p>",
      "<p>Cloud Shell</p>"
    ],
    "explanation": "<p>Service Account -&gt; Correct. Service accounts are used for server-to-server interactions and are not associated with individual end users. These are ideal for authenticating an application that needs to interact with Google Cloud Storage programmatically.</p><p><br></p><p>OAuth 2.0 Client ID -&gt; Incorrect. OAuth 2.0 Client IDs are used for web applications that need to interact with Google APIs on behalf of users. This does not suit an application that needs to authenticate with GCP without user intervention.</p><p><br></p><p>Cloud Identity -&gt; Incorrect. Cloud Identity is used for managing users, groups, and devices, not for programmatically authenticating applications with GCP.</p><p><br></p><p>Cloud Shell -&gt; Incorrect. Cloud Shell provides a shell environment for managing resources and applications hosted on Google Cloud. It does not provide a method to programmatically authenticate an application with GCP.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146108,
    "question_plain": "You have an application with static content that is deployed in US region. What can you do to bring your content closer to European users?",
    "answers": [
      "<p>You should distribute static content using Cloud CDN.</p>",
      "<p>You should scale up the size of the web server.</p>",
      "<p>You should move the server to Europe.</p>",
      "<p>You should distribute static content using Cloud VPN.</p>"
    ],
    "explanation": "<p>You should distribute static content using Cloud CDN. -&gt;&nbsp;Correct. Using Cloud CDN will cache static content in edge locations worldwide, including in Europe, and deliver it from the nearest location to the user, reducing latency and improving performance. </p><p><br></p><p>You should scale up the size of the web server. -&gt; Incorrect. Scaling up the size of the web server will not have an impact on the user's latency.</p><p><br></p><p>You should move the server to Europe. -&gt; Incorrect. Moving the server to Europe would increase the latency for users in the US. Using </p><p><br></p><p>You should distribute static content using Cloud VPN. -&gt; Incorrect. Cloud VPN is a way to establish a secure connection between two networks over the public internet but not for content distribution.</p><p><br></p><p>https://cloud.google.com/cdn/docs/overview</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146110,
    "question_plain": "You are a cloud architect for a company that has a multi-tier application running on GCP. The application includes a load balancer, several web servers, and a back-end database. As part of your security strategy, you want to allow HTTP(S) traffic only from the load balancer to the web servers. Which of the following would be the best approach?",
    "answers": [
      "<p>Create a firewall rule with a high priority (low numeric value) to allow traffic from the load balancer, and a default rule to deny all other traffic.</p>",
      "<p>Create a firewall rule with a low priority (high numeric value) to allow traffic from the load balancer, and a default rule to deny all other traffic.</p>",
      "<p>Create a firewall rule with a high priority (low numeric value) to deny all traffic, and a rule with a low priority (high numeric value) to allow traffic from the load balancer.</p>",
      "<p>Create two firewall rules with the same priority, one to allow traffic from the load balancer, and one to deny all other traffic.</p>"
    ],
    "explanation": "<p>Create a firewall rule with a high priority (low numeric value) to allow traffic from the load balancer, and a default rule to deny all other traffic. -&gt;&nbsp;Correct. Firewall rules with a lower numeric value have higher priority and are evaluated before rules with a higher numeric value. This setup ensures that the allowed traffic from the load balancer is not blocked by the default rule.</p><p><br></p><p>Create a firewall rule with a low priority (high numeric value) to allow traffic from the load balancer, and a default rule to deny all other traffic. -&gt; Incorrect. This approach would not work as expected because the default deny rule (which typically has a higher priority/lower numeric value) would block the traffic before the allow rule for the load balancer (with lower priority/higher numeric value) is evaluated.</p><p><br></p><p>Create a firewall rule with a high priority (low numeric value) to deny all traffic, and a rule with a low priority (high numeric value) to allow traffic from the load balancer. -&gt; Incorrect. This setup will not work as intended because the high priority rule denying all traffic would be evaluated before the rule allowing traffic from the load balancer, thus blocking all traffic.</p><p><br></p><p>Create two firewall rules with the same priority, one to allow traffic from the load balancer, and one to deny all other traffic. -&gt; Incorrect. If two rules have the same priority, GCP uses other criteria (like network ranges and protocols) to determine which rule applies, so this setup is not guaranteed to work as intended.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146112,
    "question_plain": "Your company has several applications running on GCP and on-premises. You need to create a hybrid cloud strategy to allow applications running on GCP to access data from your on-premises data center with low latency. What is the best strategy?",
    "answers": [
      "<p>Use Cloud Interconnect to create a dedicated connection from your on-premises network to your VPC network on Google Cloud.</p>",
      "<p>Use Cloud VPN to create an IPsec VPN tunnel from the on-premises network to your VPC network on Google Cloud.</p>",
      "<p>Use VPC Network Peering to connect the on-premises network to your VPC network on Google Cloud.</p>",
      "<p>Use Cloud Storage to store a copy of your on-premises data and access it from your applications on GCP.</p>"
    ],
    "explanation": "<p>Use Cloud Interconnect to create a dedicated connection from your on-premises network to your VPC network on Google Cloud. -&gt;&nbsp;Correct. Cloud Interconnect provides a direct, enterprise-grade connection to Google's network, which can offer lower latency than VPN connections over the public internet.</p><p><br></p><p>Use Cloud VPN to create an IPsec VPN tunnel from the on-premises network to your VPC network on Google Cloud. -&gt;&nbsp;Incorrect. Cloud VPN allows secure communication between your on-premise network and GCP over the public internet. However, it may not provide the low latency required by the applications.</p><p><br></p><p>Use VPC Network Peering to connect the on-premises network to your VPC network on Google Cloud. -&gt;&nbsp;Incorrect. VPC Network Peering is used to connect two VPC networks within Google Cloud or with other cloud providers. It doesn't apply to on-premise networks.</p><p><br></p><p>Use Cloud Storage to store a copy of your on-premises data and access it from your applications on GCP. -&gt;&nbsp;Incorrect. While this approach might provide data to GCP applications, it doesn't maintain real-time consistency with on-premise data sources and may not meet low latency requirements.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146114,
    "question_plain": "As a cloud architect, you are responsible for preparing a migration strategy. You need to deploy a disaster recovery infrastructure with the same design and configuration as your production environment using Google Cloud. Which topology would you use?",
    "answers": [
      "<p>Mirrored topology</p>",
      "<p>Gated egress topology</p>",
      "<p>Handover topology</p>",
      "<p>Gated ingress topology</p>"
    ],
    "explanation": "<p>Mirrored topology -&gt; Correct. A mirrored topology is a disaster recovery strategy that involves creating a mirror of the production environment in a separate location. In this strategy, the disaster recovery environment is an exact replica of the production environment, including the same design and configuration. If a disaster occurs in the production environment, traffic can be quickly redirected to the disaster recovery environment.</p><p><br></p><p>Gated egress topology -&gt; Incorrect. A gated egress topology involves routing traffic through a set of security controls before it leaves the network. This topology is not related to disaster recovery.</p><p><br></p><p>Gated ingress topology -&gt; Incorrect. A gated ingress topology involves routing traffic through a set of security controls before it enters the network. This topology is not related to disaster recovery.</p><p><br></p><p>Handover topology -&gt; Incorrect. A handover topology is a strategy in which traffic is handed over from one network to another, such as from a public cloud to a private cloud. While this can be part of a disaster recovery strategy, it does not involve creating a mirror of the production environment.</p><p><br></p><p>https://cloud.google.com/architecture/hybrid-and-multi-cloud-network-topologies</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146116,
    "question_plain": "Your company is deploying a new application on GCP that requires global availability and has heavy content, such as video streaming. The application is designed to handle varying levels of traffic, and you expect users from around the world. What type of load balancer would be the best choice?",
    "answers": [
      "<p>HTTP(S) Load Balancer</p>",
      "<p>Internal HTTP(S) Load Balancer</p>",
      "<p>SSL Proxy Load Balancer</p>",
      "<p>Network Load Balancer</p>"
    ],
    "explanation": "<p>HTTP(S) Load Balancer -&gt;&nbsp;Correct. HTTP(S) Load Balancer is a global, proxy-based load balancer that supports content-based routing. It integrates with Cloud CDN and Google's global edge network, making it suitable for applications with global reach and heavy content like video streaming.</p><p><br></p><p>Internal HTTP(S) Load Balancer -&gt; Incorrect. The Internal HTTP(S) Load Balancer is for private, internal load balancing and not suitable for applications that require global availability.</p><p><br></p><p>SSL Proxy Load Balancer -&gt; Incorrect. The SSL Proxy Load Balancer is used for SSL (TLS) traffic. It is not designed for global load balancing or heavy content like video streaming.</p><p><br></p><p>Network Load Balancer -&gt; Incorrect. The Network Load Balancer is a regional, non-proxy load balancer that distributes traffic based on IP protocol data, such as address, port, and protocol type. It is not optimized for global applications or heavy content.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146118,
    "question_plain": "You're designing a high-traffic web application on Google Cloud. The application involves dynamic content that changes frequently, but some parts of the data remain the same across different user sessions. You need to improve the application's performance and user experience by reducing the load on your databases. Which strategy should you adopt?",
    "answers": [
      "<p>Implement Cloud MemoryStore with Redis as an in-memory data store.</p>",
      "<p>Implement a global Cloud Load Balancer.</p>",
      "<p>Use Cloud CDN to cache dynamic content.</p>",
      "<p>Utilize Cloud Storage to store and retrieve commonly accessed data.</p>"
    ],
    "explanation": "<p>Implement Cloud MemoryStore with Redis as an in-memory data store. -&gt;&nbsp;Correct. Cloud MemoryStore provides a fully managed in-memory data store service built on scalable, secure, and highly available infrastructure managed by Google. Using Redis for caching can significantly improve the application's performance by providing fast access to frequently used data.</p><p><br></p><p>Implement a global Cloud Load Balancer. -&gt;&nbsp;Incorrect. Although a global Cloud Load Balancer can distribute incoming traffic, it won't serve the purpose of caching data to reduce database load.</p><p><br></p><p>Use Cloud CDN to cache dynamic content. -&gt;&nbsp;Incorrect. While Cloud CDN is an excellent tool for caching static content at the edge, it is not designed to cache frequently changing dynamic content.</p><p><br></p><p>Utilize Cloud Storage to store and retrieve commonly accessed data. -&gt;&nbsp;Incorrect. Cloud Storage is not suitable for caching as it is designed for long-term unstructured data storage. The latency for data access may not meet the performance needs of a caching solution.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146120,
    "question_plain": "As a cloud architect, you are responsible for preparing a migration plan for your company. You want to migrate Apache Kafka (real-time streaming data pipelines) to the Google Cloud. Which Google Cloud service should you use?",
    "answers": [
      "<p>Cloud Pub/Sub</p>",
      "<p>Cloud Bigtable</p>",
      "<p>Cloud Firestore</p>",
      "<p>Cloud Datastore</p>"
    ],
    "explanation": "<p>Cloud Pub/Sub -&gt; Correct. It is a fully managed real-time messaging service that enables you to send and receive messages between independent applications. It provides a scalable, durable message queue that can handle millions of messages per second. Apache Kafka is a popular messaging system used for real-time streaming data pipelines. Cloud Pub/Sub provides similar functionality as Kafka, such as message ordering and at-least-once delivery, making it a good choice for migrating Kafka workloads to the cloud.</p><p><br></p><p>Cloud Bigtable -&gt;&nbsp;Incorrect. It is a fully managed NoSQL database service that is optimized for handling large amounts of data with low latency. While Cloud Bigtable can store real-time streaming data, it is not designed for real-time messaging or queuing.</p><p><br></p><p>Cloud Firestore -&gt; Incorrect. It is a fully managed NoSQL document database service that is optimized for mobile and web application development. While Cloud Firestore can store real-time streaming data, it is not designed for real-time messaging or queuing.</p><p><br></p><p>Cloud Datastore -&gt;&nbsp;Incorrect. It is a fully managed NoSQL document database service that is optimized for handling large amounts of data with high query throughput. While Cloud Datastore can store real-time streaming data, it is not designed for real-time messaging or queuing.</p><p><br></p><p>https://cloud.google.com/architecture/migrating-from-kafka-to-pubsub?hl=en</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146122,
    "question_plain": "Refer to the EHR Healthcare case study for this question: https://services.google.com/fh/files/blogs/master_case_study_ehr_healthcare.pdfThe sales employees of EHR work remotely and travel to various locations for their job. These employees require access to web-based sales tools located in the EHR data center. EHR has made the decision to retire their existing Virtual Private Network (VPN) infrastructure, necessitating the migration of the web-based sales tools to a BeyondCorp access model. Each sales employee possesses a Google Workspace account, which they utilize for single sign-on (SSO) purposes. What should you do?",
    "answers": [
      "<p>You should create an Identity-Aware Proxy (IAP) connector that points to the sales tool application.</p>",
      "<p>You should create a Google group for the sales tool application, and upgrade that group to a security group.</p>",
      "<p>You should deploy an external HTTP(S) load balancer and create a custom Cloud Armor policy for the sales tool application.</p>",
      "<p>For every sales employee who needs access to the sales tool application, you should give their Google Workspace user account the predefined AppEngine Viewer role.</p>"
    ],
    "explanation": "<p>You should create an Identity-Aware Proxy (IAP) connector that points to the sales tool application. -&gt; Correct.</p><p><br></p><p>You should create a Google group for the sales tool application, and upgrade that group to a security group. -&gt; Incorrect. Google groups by themselves do not grant access to an application nor do they move an application to a BeyondCorp model.</p><p><br></p><p>You should deploy an external HTTP(S) load balancer and create a custom Cloud Armor policy for the sales tool application. -&gt;&nbsp;Incorrect. Cloud Armor does not authenticate or authorize application access.</p><p><br></p><p>For every sales employee who needs access to the sales tool application, you should give their Google Workspace user account the predefined AppEngine Viewer role. -&gt; Incorrect. The application is installed in the datacenter, not in the AppEngine environment.</p><p><br></p><p>https://cloud.google.com/iap/docs/cloud-iap-for-on-prem-apps-overview</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146124,
    "question_plain": "Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdfAs your new game is currently in public beta on the Google Cloud platform, it is essential to establish significant service level objectives (SLOs) before its official release to the public. What steps should you take to accomplish this?",
    "answers": [
      "<p>You should define one SLO as 99% HTTP requests return the 2xx status code. Define the other SLO as 99% requests return within 100 ms.</p>",
      "<p>You should define one SLO as 99.9% game server availability. Define the other SLO as less than 100-ms latency.</p>",
      "<p>You should define one SLO as service availability that is the same as Google Cloud's availability. Define the other SLO as 100-ms latency.</p>",
      "<p>You should define one SLO as total uptime of the game server within a week. Define the other SLO as the mean response time of all HTTP requests that are less than 100 ms.</p>"
    ],
    "explanation": "<p>You should define one SLO as 99% HTTP requests return the 2xx status code. Define the other SLO as 99% requests return within 100 ms. -&gt; Correct. It clearly defines the service level indicators and how to measure them.</p><p><br></p><p>You should define one SLO as 99.9% game server availability. Define the other SLO as less than 100-ms latency. -&gt;&nbsp;Incorrect. It doesn't clearly define how to measure both the availability and latency.</p><p><br></p><p>You should define one SLO as service availability that is the same as Google Cloud's availability. Define the other SLO as 100-ms latency. -&gt;&nbsp;Incorrect. Google Cloud availability has an impact on customer availability but it is only one factor. Also, for different Google Cloud products, the availability could be different.</p><p><br></p><p>You should define one SLO as total uptime of the game server within a week. Define the other SLO as the mean response time of all HTTP requests that are less than 100 ms. -&gt;&nbsp;Incorrect. There is no objective for the server uptime.</p><p><br></p><p>https://sre.google/workbook/implementing-slos/</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146126,
    "question_plain": "As a cloud architect, you work for a company that wants to try out the cloud with low risk. They want to archive approximately 500 TB of their log data to the cloud and test the serverless analytics features available to them there, while also retaining that data as a long-term disaster recovery backup. Which two steps should they take?",
    "answers": [
      "<p>Load logs into BigQuery.</p>",
      "<p>Upload log files into Cloud Storage.</p>",
      "<p>Load logs into Cloud SQL.</p>",
      "<p>Import logs into Cloud Logging.</p>",
      "<p>Insert logs into Cloud Bigtable.</p>"
    ],
    "explanation": "<p>Load logs into BigQuery. -&gt;&nbsp;Correct. BigQuery is a serverless cloud data warehouse for analytics and supports the volume and analytics requirement.</p><p><br></p><p>Upload log files into Cloud Storage. -&gt; Correct. Cloud Storage provides the Coldline and Archive storage classes to support long-term storage with infrequent access, which would support the long-term disaster recovery backup requirement.</p><p><br></p><p>Load logs into Cloud SQL. -&gt; Incorrect. Cloud SQL does not support the expected 100 TB. Additionally, Cloud SQL is a relational database and not the best fit for time-series log data formats.</p><p><br></p><p>Import logs into Cloud Logging. -&gt; Incorrect. Cloud Logging is optimized for monitoring, error reporting, and debugging instead of analytics queries.</p><p><br></p><p>Insert logs into Cloud Bigtable. -&gt;&nbsp;Incorrect. Cloud Bigtable is optimized for read-write latency and analytics throughput, not analytics querying and reporting.</p><p><br></p><p>https://cloud.google.com/bigquery/</p>",
    "correct_response": ["a", "b"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 82146128,
    "question_plain": "The database administration team has asked you to help them improve the performance of a new database server running on Compute Engine. The database is used to import and normalize company performance statistics. It is built with MySQL running on Debian Linux. They have an n1-standard-8 VM with 80 GB of SSD zonal persistent disk which they can't restart until the next maintenance event. What should they change to get better performance from this system as soon as possible and in a cost-effective manner?",
    "answers": [
      "<p>Dynamically resize the SSD persistent disk to 500 GB.</p>",
      "<p>Increase the virtual machine’s memory to 64 GB.</p>",
      "<p>Create a new virtual machine running PostgreSQL.</p>",
      "<p>Migrate their performance metrics warehouse to BigQuery.</p>"
    ],
    "explanation": "<p>Dynamically resize the SSD persistent disk to 500 GB. -&gt; Correct. Persistent disk performance is based on the total persistent disk capacity attached to an instance and the number of vCPUs that the instance has. Incrementing the persistent disk capacity will increment its throughput and IOPS, which in turn improve the performance of MySQL.</p><p><br></p><p>Increase the virtual machine’s memory to 64 GB. -&gt; Increasing the memory size requires a VM restart.</p><p><br></p><p>Create a new virtual machine running PostgreSQL. -&gt;&nbsp;Incorrect. The DB administration team is requesting help with their MySQL instance. Migration to a different product should not be the solution when other optimization techniques can still be applied first.</p><p><br></p><p>Migrate their performance metrics warehouse to BigQuery. -&gt; Incorrect. The DB administration team is requesting help with their MySQL instance. Migration to a different product should not be the solution when other optimization techniques can still be applied first.</p><p><br></p><p>https://cloud.google.com/compute/docs/disks/#pdspecs</p><p>https://cloud.google.com/compute/docs/disks/performance</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146130,
    "question_plain": "Refer to the TerramEarth case study for this question: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdfAs TerramEarth increases its adoption of the Google Cloud Platform, which specific legacy enterprise processes within the company will undergo substantial modifications?",
    "answers": [
      "<p>Capacity planning, TCO calculations, OPEX/CAPEX allocation</p>",
      "<p>OPEX/CAPEX allocation, LAN change management, capacity planning</p>",
      "<p>Capacity planning, utilization measurement, data center expansion</p>",
      "<p>Data center expansion, TCO calculations, utilization measurement</p>"
    ],
    "explanation": "<p>Capacity planning, TCO calculations, OPEX/CAPEX allocation -&gt; Correct. All of these tasks are big changes when moving to the cloud. Capacity planning for cloud is different than for on-premises data centers; TCO calculations are adjusted because TerramEarth is using services, not leasing/buying servers; OPEX/CAPEX allocation is adjusted as services are consumed vs. using capital expenditures.</p><p><br></p><p>OPEX/CAPEX allocation, LAN change management, capacity planning -&gt; Incorrect. LAN change management processes don't need to change significantly. TerramEarth can easily peer their on-premises LAN with their Google Cloud Platform VPCs, and as devices and subnets move to the cloud, the LAN team's implementation will change, but the change management process doesn't have to.</p><p><br></p><p>Capacity planning, utilization measurement, data center expansion -&gt; Incorrect. Measuring utilization can be done in the same way, often with the same tools (along with some new ones). Data center expansion is not a concern for cloud customers; it is part of the undifferentiated heavy lifting that is taken care of by the cloud provider.</p><p><br></p><p>Data center expansion, TCO calculations, utilization measurement -&gt; Incorrect. Data center expansion is not a concern for cloud customers; it is part of the undifferentiated heavy lifting that is taken care of by the cloud provider. Measuring utilization can be done in the same way, often with the same tools (along with some new ones).</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146132,
    "question_plain": "Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdfMountkirk Games aims to establish a continuous delivery pipeline for their architecture, which comprises numerous small services requiring quick updates and rollbacks. The following requirements are specified by Mountkirk Games:redundant deployment of services across multiple regions in the US and Europe.only the frontend services are accessible via the public internetallocation of a single frontend IP address for their entire service fleetdeployment artifacts are immutableWhich product combination would be most suitable for their needs?",
    "answers": [
      "<p>Container Registry, Google Kubernetes Engine, Cloud Load Balancing</p>",
      "<p>Cloud Storage, Cloud Dataflow, Compute Engine</p>",
      "<p>Cloud Storage, App Engine, Cloud Load Balancing</p>",
      "<p>Cloud Functions, Cloud Pub/Sub, Cloud Deployment Manager</p>"
    ],
    "explanation": "<p>Container Registry, Google Kubernetes Engine, Cloud Load Balancing -&gt;&nbsp;Correct. Google Kubernetes Engine is ideal for deploying small services that can be updated and rolled back quickly. It is a best practice to manage services using immutable containers. Cloud Load Balancing supports globally distributed services across multiple regions. It provides a single global IP address that can be used in DNS records. Using URL Maps, the requests can be routed to only the services that Mountkirk wants to expose. Container Registry is a single place for a team to manage Docker images for the services.</p><p><br></p><p>Cloud Storage, Cloud Dataflow, Compute Engine -&gt; Incorrect. Mountkirk Games wants to set up a continuous delivery pipeline, not a data processing pipeline. Cloud Dataflow is a fully managed service for creating data processing pipelines.</p><p><br></p><p>Cloud Storage, App Engine, Cloud Load Balancing -&gt; Incorrect. Cloud Load Balancer distributes traffic to Compute Engine instances. App Engine and Cloud Load Balancer are parts of different solutions.</p><p><br></p><p>Cloud Functions, Cloud Pub/Sub, Cloud Deployment Manager -&gt; Incorrect. You cannot reserve a single frontend IP for cloud functions. When deployed, an HTTP-triggered cloud function creates an endpoint with an automatically assigned IP.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146134,
    "question_plain": "In your role as a cloud architect, you are using Cloud Shell and have the requirement to install a custom utility that will be utilized at a later stage. Where can you place the file to ensure it is in the default execution path and remains persistent across multiple sessions?",
    "answers": [
      "<p><code>~/bin</code> </p>",
      "<p>In a Cloud Storage bucket</p>",
      "<p><code>/google/scripts/bin</code> </p>",
      "<p><code>/usr/local/bin</code> </p>"
    ],
    "explanation": "<p><code>~/bin</code> -&gt; Correct. Cloud Shell is a web-based command-line tool provided by Google Cloud Platform (GCP) for managing resources and interacting with various services. When you work within Cloud Shell, you have a persistent home directory (represented by the tilde \"~\"), which is associated with your user account. By placing the file in the <code>~/bin</code> directory, you ensure that it is within the default execution path. This directory is typically included in the default execution path for user accounts in Linux-based systems. When you install a utility in this directory, it becomes accessible from anywhere within the command-line environment without specifying the full path to the utility. Additionally, the file will remain persistent across multiple sessions because the home directory is associated with your user account and persists between Cloud Shell sessions. This means that the file will be available even if you close and reopen Cloud Shell or switch to a different GCP project.</p><p><br></p><p>In a Cloud Storage bucket -&gt;&nbsp;Incorrect. Cloud Storage buckets are designed for storing objects (files) in a distributed and scalable manner. While you can certainly store files in a Cloud Storage bucket, they won't be in the default execution path within Cloud Shell. You would need to download the file from the bucket and specify its full path to execute it.</p><p><br></p><p><code>/usr/local/bin</code> -&gt; Incorrect. This is a system-level directory typically used for installing programs or utilities that are available to all users on the system. However, in Cloud Shell, you don't have administrative privileges to write directly to this directory. You would need root access to install files in this location, which is not available in Cloud Shell.</p><p><br></p><p><code>/google/scripts/bin</code> -&gt; Incorrect. This directory path does not exist by default in Cloud Shell.</p><p><br></p><p>https://cloud.google.com/shell/docs/how-cloud-shell-works</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146136,
    "question_plain": "As a cloud architect, you're working for a large e-commerce company that wants to detect anomalies in their sales data in real-time. The data is stored in BigQuery and is updated every few minutes with new sales information from around the globe. The goal is to quickly identify unusual spikes or drops in sales, so the business can take immediate actions. What would be the most efficient way to architect this solution?",
    "answers": [
      "<p>Use BigQuery ML to create a machine learning model and use scheduled queries to periodically perform anomaly detection on the recent data.</p>",
      "<p>Use Cloud DLP (Data Loss Prevention) to monitor the sales data in BigQuery for anomalies.</p>",
      "<p>Use Cloud Storage to store the sales data and periodically run batch processing jobs using Dataflow to detect anomalies.</p>",
      "<p>Use Cloud Monitoring to monitor the sales data in BigQuery for anomalies.</p>"
    ],
    "explanation": "<p>Use BigQuery ML to create a machine learning model and use scheduled queries to periodically perform anomaly detection on the recent data. -&gt; Correct. BigQuery ML enables users to create and execute machine learning models in BigQuery using SQL queries, which is perfect for the given requirement. Scheduled queries can be used to perform anomaly detection on the latest data periodically.</p><p><br></p><p>Use Cloud DLP (Data Loss Prevention) to monitor the sales data in BigQuery for anomalies. -&gt; Incorrect. Cloud DLP (Data Loss Prevention) is designed for discovering, classifying, and protecting sensitive data. It is not meant for anomaly detection in the context of sales data.</p><p><br></p><p>Use Cloud Storage to store the sales data and periodically run batch processing jobs using Dataflow to detect anomalies. -&gt; Incorrect. It introduces delays, complexity, and additional costs associated with batch processing using Dataflow, while not leveraging the real-time analytics capabilities of BigQuery, which is already being used to store the sales data.</p><p><br></p><p>Use Cloud Monitoring to monitor the sales data in BigQuery for anomalies. -&gt; Incorrect. Cloud Monitoring is not the optimal tool for monitoring sales data and performing real-time anomaly detection in this specific use case. BigQuery provides more appropriate functionalities for analyzing the sales data efficiently.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146138,
    "question_plain": "You are operating in a high-security environment where Compute Engine VMs are not permitted to access the public internet. Currently, you lack a VPN connection to access a local network file server. You have a necessity to deploy a certain software on a Compute Engine instance. What is the recommended method for installing the software?",
    "answers": [
      "<p>Upload the necessary installation files to Cloud Storage. Configure the VM on a subnet with Private Google Access. Ensure the VM is assigned only an internal IP address. Use gsutil to download the installation files to the VM.</p>",
      "<p>Upload the necessary installation files to Cloud Storage, and implement firewall rules to block all traffic except the IP address range specific to Cloud Storage. Utilize gsutil to download the files to the VM.</p>",
      "<p>Upload the necessary installation files to Cloud Source Repositories. Set up the VM on a subnet with Private Google Access. Assign only an internal IP address to the VM. Use gcloud to download the installation files to the VM.</p>",
      "<p>Upload the necessary installation files to Cloud Source Repositories, and establish firewall rules to block all traffic except the IP address range for Cloud Source Repositories. Use gsutil to download the files to the VM.</p>"
    ],
    "explanation": "<p>Upload the necessary installation files to Cloud Storage. Configure the VM on a subnet with Private Google Access. Ensure the VM is assigned only an internal IP address. Use gsutil to download the installation files to the VM. -&gt; Correct. It provides a suitable solution for installing the software in a high-security environment where Compute Engine VMs are not allowed to access the public internet. In this scenario, uploading the necessary installation files to Cloud Storage allows you to store the files securely in Google Cloud. By configuring the VM on a subnet with Private Google Access, you ensure that the VM can access Google APIs and services like Cloud Storage without requiring access to the public internet. Assigning only an internal IP address to the VM further enforces the restriction on accessing the internet. Using gsutil, which is a command-line tool for interacting with Cloud Storage, you can download the installation files directly to the VM from Cloud Storage. This method allows you to retrieve the necessary files securely and efficiently within the restricted environment.</p><p><br></p><p>Upload the necessary installation files to Cloud Storage, and implement firewall rules to block all traffic except the IP address range specific to Cloud Storage. Utilize gsutil to download the files to the VM. -&gt; Incorrect. While this may provide some level of isolation, it may not be the most effective solution in a high-security environment where VMs are not permitted to access the public internet.</p><p><br></p><p>Upload the necessary installation files to Cloud Source Repositories. Set up the VM on a subnet with Private Google Access. Assign only an internal IP address to the VM. Use gcloud to download the installation files to the VM. -&gt; Incorrect. It suggests uploading the installation files to Cloud Source Repositories and using gcloud to download them to the VM. However, Cloud Source Repositories are primarily used for version control and collaborative development, not for hosting installation files. It is not the recommended approach for software installation.</p><p><br></p><p>Upload the necessary installation files to Cloud Source Repositories, and establish firewall rules to block all traffic except the IP address range for Cloud Source Repositories. Use gsutil to download the files to the VM. -&gt; Incorrect. It suggests blocking all traffic except the IP address range for Cloud Source Repositories and using gsutil to download the files. Using Cloud Source Repositories for software installation is not the appropriate use case, and the solution does not align with the given requirements.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146140,
    "question_plain": "You are a cloud architect and have been tasked with creating a data retention policy on a Google Cloud Storage (GCS) bucket that holds sensitive client information. This policy should ensure that objects older than 90 days are not accessible, even if they haven't been deleted. Also, to ensure business continuity, deleted objects should be restorable within 5 days. Which of the following methods would be the most suitable for implementing this policy?",
    "answers": [
      "<p>Apply a 90-day lifecycle rule to delete objects, enable versioning, and use a 5-day lifecycle rule to delete previous versions of objects.</p>",
      "<p>Apply a 90-day lifecycle rule to delete objects and a 5-day retention policy to retain deleted objects.</p>",
      "<p>Apply a 90-day lifecycle rule to archive objects and enable versioning with a 5-day lifecycle rule to delete previous versions of objects.</p>",
      "<p>Apply a 90-day retention policy to the bucket and a 5-day lifecycle rule to delete older versions of objects.</p>"
    ],
    "explanation": "<p>Apply a 90-day lifecycle rule to delete objects, enable versioning, and use a 5-day lifecycle rule to delete previous versions of objects. -&gt; Correct. A lifecycle rule with a 90-day condition will ensure objects older than 90 days are deleted. Bucket versioning will keep all versions of an object, including those that have been deleted. A second lifecycle rule with a 5-day condition on previous versions will ensure that deleted objects can be restored within 5 days.</p><p><br></p><p>Apply a 90-day lifecycle rule to delete objects and a 5-day retention policy to retain deleted objects. -&gt; Incorrect. A retention policy in Google Cloud Storage prevents an object's deletion, not its access. Additionally, retention policies do not retain deleted objects; they prevent an object's deletion.</p><p><br></p><p>Apply a 90-day lifecycle rule to archive objects and enable versioning with a 5-day lifecycle rule to delete previous versions of objects. -&gt; Incorrect. Lifecycle rules to archive objects in Google Cloud Storage move the objects to a colder storage class like \"Nearline\", \"Coldline\", or \"Archive\" to reduce costs. It does not prevent access to the objects. Also, lifecycle rules do not apply to previous versions of objects.</p><p><br></p><p>Apply a 90-day retention policy to the bucket and a 5-day lifecycle rule to delete older versions of objects. -&gt; Incorrect. A retention policy prevents an object's deletion for a specified period, it doesn't prevent access to it. Also, lifecycle rules on Google Cloud Storage do not apply to older versions of objects.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146142,
    "question_plain": "As a cloud architect, while utilizing the Google Network Intelligence Center's Firewall Insights feature, you observe that there are no log rows available for viewing when accessing the Firewall Insights page in the Google Cloud console. This prompts the need for assessing the effectiveness of the applied firewall ruleset, considering the existence of multiple firewall rules associated with the Compute Engine instance. To troubleshoot the issue, what steps should you take?",
    "answers": [
      "<p>Enable Firewall Rules Logging for the firewall rules you want to monitor.</p>",
      "<p>Enable Virtual Private Cloud (VPC) flow logging.</p>",
      "<p>Verify that your user account is assigned the <code>compute.networkAdmin</code> Identity and Access Management (IAM) role.</p>",
      "<p>Install the Google Cloud SDK, and verify that there are no Firewall logs in the command line output.</p>"
    ],
    "explanation": "<p>Enable Firewall Rules Logging for the firewall rules you want to monitor. -&gt;&nbsp;Correct. Enabling Firewall Rules Logging allows you to capture logs for the specified firewall rules. By enabling logging for the relevant firewall rules, you can troubleshoot the issue of missing log rows in Firewall Insights. This step ensures that the necessary logs are generated and available for analysis, providing visibility into the effectiveness of the firewall ruleset.</p><p><br></p><p>Enable Virtual Private Cloud (VPC) flow logging. -&gt;&nbsp;Incorrect. While VPC flow logging can be useful for overall network traffic analysis, it may not specifically address the issue of missing log rows in Firewall Insights. Enabling VPC flow logging alone does not guarantee the availability of firewall-specific log data required for assessing the firewall ruleset.</p><p><br></p><p>Verify that your user account is assigned the <code>compute.networkAdmin</code> Identity and Access Management (IAM) role. -&gt;&nbsp;Incorrect. While having the necessary IAM role is important for managing and configuring firewall rules, it does not directly address the issue of missing log rows in Firewall Insights. Verifying the IAM role is more relevant to ensuring the appropriate permissions for managing firewall rules, rather than troubleshooting log availability.</p><p><br></p><p>Install the Google Cloud SDK, and verify that there are no Firewall logs in the command line output. -&gt;&nbsp;Incorrect. Installing the Google Cloud SDK and checking for Firewall logs in the command-line output can help validate if Firewall logs are being generated. However, this step does not directly address the issue of missing log rows in Firewall Insights, nor does it provide a solution for troubleshooting or resolving the issue.</p><p><br></p><p>https://cloud.google.com/network-intelligence-center/docs/firewall-insights/how-to/view-understand-insights</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146144,
    "question_plain": "As a cloud architect, you are assisting a global organization in migrating its petabyte-scale on-premises data to Google Cloud. The organization has an extremely slow internet connection, strict compliance and security standards, and cannot tolerate any downtime during working hours. The migration should be completed as quickly as possible without interrupting business operations. Which of the following strategies would you recommend?",
    "answers": [
      "<p>Use Transfer Appliance to ship data to Google Cloud.</p>",
      "<p>Use Cloud Dataflow to process and migrate data.</p>",
      "<p>Use <code>gsutil</code> command-line tool to upload data to Cloud Storage.</p>",
      "<p>Use Storage Transfer Service for online transfer.</p>"
    ],
    "explanation": "<p>Use Transfer Appliance to ship data to Google Cloud. -&gt; Correct. This hardware appliance can be loaded with data and physically shipped to Google, where the data can be uploaded to the cloud. This approach is particularly useful in scenarios where an organization has a slow or unreliable internet connection, needs to meet strict compliance and security standards, and requires a quick data migration process.</p><p><br></p><p>Use Cloud Dataflow to process and migrate data. -&gt; Incorrect. Cloud Dataflow is primarily used for processing and transforming large datasets. It is not the most suitable tool for data migration, particularly when internet connectivity is slow.</p><p><br></p><p>Use <code>gsutil</code> command-line tool to upload data to Cloud Storage. -&gt;&nbsp;Incorrect. The <code>gsutil</code> command-line tool could be used to upload data to Cloud Storage, but this method would rely heavily on the organization's slow internet connection. It would not be an efficient solution for migrating petabyte-scale data.</p><p><br></p><p>Use Storage Transfer Service for online transfer. -&gt; Incorrect. Storage Transfer Service is a powerful tool for moving data between online storage services. However, the success of this method is also dependent on the organization's internet connection speed.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146146,
    "question_plain": "You are a cloud architect tasked with designing an application to regularly fetch and process social media data every 15 minutes. The processed data is then stored in Cloud Storage and made available to an analytics team. This operation must be reliable and scalable to handle periods of high demand, while minimizing costs. Which of the following is the most suitable approach to handle this scenario?",
    "answers": [
      "<p>Use App Engine Standard with the Cron service to trigger a Cloud Run service every 15 minutes. The service retrieves and processes the data and then stores it in Cloud Storage.</p>",
      "<p>Use Compute Engine with a Cron job installed on the instance to fetch, process, and store the data.</p>",
      "<p>Use App Engine Flexible with the Cron service to run a dedicated service for fetching and processing the data every 15 minutes.</p>",
      "<p>Use Cloud Scheduler to trigger a Pub/Sub topic every 15 minutes, which then triggers a Dataflow job to fetch, process, and store the data.</p>"
    ],
    "explanation": "<p>Use App Engine Standard with the Cron service to trigger a Cloud Run service every 15 minutes. The service retrieves and processes the data and then stores it in Cloud Storage. -&gt;&nbsp;Correct. App Engine's Cron service can reliably trigger tasks on a schedule. Cloud Run can handle processing tasks and automatically manages and scales the underlying infrastructure, allowing it to handle high-demand periods. Cloud Run services only run when they're needed, which can help minimize costs.</p><p><br></p><p>Use Compute Engine with a Cron job installed on the instance to fetch, process, and store the data. -&gt; Incorrect. Compute Engine could handle the task, but it might not scale as well during high demand periods. Also, managing Cron jobs directly on Compute Engine instances can add unnecessary complexity.</p><p><br></p><p>Use App Engine Flexible with the Cron service to run a dedicated service for fetching and processing the data every 15 minutes. -&gt; Incorrect. App Engine Flexible could handle the task, but it might not be the most cost-effective solution, as instances of App Engine Flexible are running continuously, even if the service only needs to operate every 15 minutes.</p><p><br></p><p>Use Cloud Scheduler to trigger a Pub/Sub topic every 15 minutes, which then triggers a Dataflow job to fetch, process, and store the data. -&gt; Incorrect. This approach might be overkill for the given task. Cloud Dataflow is more suited to handling complex real-time or batch data processing workloads, which might not be required in this case.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146148,
    "question_plain": "In your role as a cloud architect, your task involves deploying PHP App Engine Standard alongside Cloud SQL as the backend, with the goal of reducing the number of queries to the database. What actions should you take?",
    "answers": [
      "<p>Set the memcache service level to dedicated. Create a key from the hash of the query, and return database values from memcache before issuing a query to Cloud SQL.</p>",
      "<p>Set the memcache service level to dedicated. Create a cron task that runs every minute to populate the cache with keys containing query results.</p>",
      "<p>Set the memcache service level to shared. Create a cron task that runs every minute to save all expected queries to a key called cached_queries.</p>",
      "<p>Set the memcache service level to shared. Create a key called cached_queries, and return database values from the key before using a query to Cloud SQL.</p>"
    ],
    "explanation": "<p>Set the memcache service level to dedicated. Create a key from the hash of the query, and return database values from memcache before issuing a query to Cloud SQL. -&gt;&nbsp;Correct. By setting the memcache service level to dedicated, you ensure that you have a dedicated cache for your application, which can provide better performance and reliability. Creating a key from the hash of the query allows you to uniquely identify the result of each query. By checking if the result exists in memcache before issuing a query to Cloud SQL, you can reduce the number of queries to the database and retrieve the data directly from the cache if it's available.</p><p><br></p><p>Set the memcache service level to dedicated. Create a cron task that runs every minute to populate the cache with keys containing query results. -&gt;&nbsp;Incorrect. While setting the memcache service level to dedicated is a good step, creating a cron task to populate the cache every minute may not be the most efficient approach. It may lead to outdated or stale data in the cache, as it populates the cache at fixed intervals rather than dynamically based on the actual queries made by the application.</p><p><br></p><p>Set the memcache service level to shared. Create a cron task that runs every minute to save all expected queries to a key called cached_queries. -&gt;&nbsp;Incorrect. Setting the memcache service level to shared may limit the performance and reliability of the cache, as it is shared among multiple applications. Additionally, saving all expected queries to a single key called cached_queries may not be an efficient approach, as it does not provide individual caching for each query. It also does not address the goal of reducing the number of queries to the database.</p><p><br></p><p>Set the memcache service level to shared. Create a key called cached_queries, and return database values from the key before using a query to Cloud SQL. -&gt;&nbsp;Incorrect. Setting the memcache service level to shared may not provide the best performance and reliability. Creating a single key called cached_queries is not an efficient approach for caching individual query results. It does not effectively reduce the number of queries to the database or retrieve specific query results from the cache.</p><p><br></p><p>https://cloud.google.com/appengine/docs/legacy/standard/python/memcache/using</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146150,
    "question_plain": "Your customer is planning to run a machine learning workload on Google Cloud. The customer has the following requirements:the workload must be able to scale dynamically to handle large amounts of datathe workload must be able to process data in parallelthe workload must be able to handle failures and recover gracefullythe workload must be cost-effectiveWhich Google Cloud service should you recommend to meet these requirements?",
    "answers": [
      "<p>Cloud Machine Learning Engine</p>",
      "<p>Cloud AI Platform</p>",
      "<p>Cloud AutoML</p>",
      "<p>TensorFlow</p>"
    ],
    "explanation": "<p>Cloud AI Platform -&gt;&nbsp;Correct. Cloud AI Platform is a comprehensive platform for building, training, and deploying machine learning models. It offers scalability and the ability to process data in parallel through distributed training. It also provides fault tolerance and automatic model versioning, which allows graceful recovery from failures. Additionally, Cloud AI Platform offers cost-effective options such as using preemptible virtual machines for training to reduce costs.</p><p><br></p><p>Cloud Machine Learning Engine -&gt;&nbsp;Incorrect. While it provides scalability and fault tolerance, it is not specifically designed to handle large amounts of data or process data in parallel. It is more focused on model training and deployment rather than handling the entire workload.</p><p><br></p><p>Cloud AutoML -&gt;&nbsp;Incorrect. While it may offer some scalability and fault tolerance features, it is primarily focused on simplifying the model-building process rather than handling the entire workload with large amounts of data.</p><p><br></p><p>TensorFlow&nbsp; -&gt;&nbsp;Incorrect. TensorFlow is an open-source machine learning framework developed by Google. It is designed to facilitate the development and deployment of machine learning models, particularly deep learning models.</p><p><br></p><p>https://cloud.google.com/ai-platform/docs</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146152,
    "question_plain": "Your customer is planning to store and analyze a large amount of time-series data in Google Cloud. The customer has the following requirements:the data must be stored in a highly available and scalable storage systemthe data must be easily accessible for real-time analyticssingle-digit millisecond latencyWhich Google Cloud service should you recommend to meet these requirements?",
    "answers": [
      "<p>Cloud Storage</p>",
      "<p>Cloud Bigtable</p>",
      "<p>Cloud SQL</p>",
      "<p>Cloud Firestore</p>"
    ],
    "explanation": "<p>Cloud Bigtable -&gt;&nbsp;It is a NoSQL database that is designed to handle large-scale workloads and is optimized for storing and querying large amounts of structured data. Cloud Bigtable supports the storage of time-series data and can be easily accessed and analyzed using time-series databases like InfluxDB, as well as tools like Apache Beam and Cloud Dataflow. Additionally, Cloud Bigtable provides a cost-effective pricing model based on usage and can scale to meet the storage and query requirements of time-series data.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. It can also provide highly available and scalable storage for time-series data, but it does not have built-in analytics features for time-series data analysis, which may make it less suitable for the given requirements.</p><p><br></p><p>Cloud SQL -&gt; Incorrect. It is a fully managed relational database service that offers high availability, scalability, and supports SQL-like syntax for querying. However, it may not be the most cost-effective solution for storing and analyzing time-series data, especially for large-scale workloads.</p><p><br></p><p>Cloud Firestore -&gt; Incorrect. It is a fully managed NoSQL document database that can store, sync, and query data for mobile and web applications. It is optimized for real-time updates and serverless access and may not be the most suitable option for storing and analyzing time-series data.</p><p><br></p><p>https://cloud.google.com/bigtable/docs/schema-design-time-series</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146154,
    "question_plain": "Your customer is planning to run a big data analytics workload in Google Cloud. The customer has the following requirements:the workload must be able to handle a large volume of data and scale dynamicallythe workload must be able to process data in parallelthe workload must be able to handle failures and recover gracefullythe data must be easily queryable using SQL-like syntaxWhich Google Cloud service should you recommend to meet these requirements?",
    "answers": [
      "<p>Cloud Dataflow</p>",
      "<p>Cloud Dataproc</p>",
      "<p>Cloud BigQuery</p>",
      "<p>Cloud SQL</p>"
    ],
    "explanation": "<p>Cloud BigQuery -&gt;&nbsp;Correct. It is a fully-managed, serverless data warehouse that can handle large volumes of data and scale dynamically as required. It is designed for processing data in parallel, and can handle failures and recover gracefully. It supports querying data using SQL-like syntax, which makes it easy for users to interact with the data.</p><p><br></p><p>Cloud Dataflow -&gt; Incorrect. It is a fully-managed service for building and executing data processing pipelines, which can also handle large volumes of data and scale dynamically. However, it may not be as well-suited for SQL-like queries as Cloud BigQuery.</p><p><br></p><p>Cloud Dataproc -&gt; Incorrect. It is a fully-managed service for running Apache Hadoop and Apache Spark clusters in Google Cloud. While it can also handle large volumes of data and scale dynamically, it may not be as well-suited for SQL-like queries as Cloud BigQuery.</p><p><br></p><p>Cloud SQL -&gt; Incorrect. It is a fully-managed relational database service, which may not be the best fit for a big data analytics workload that requires handling large volumes of data and processing it in parallel.</p><p><br></p><p>https://cloud.google.com/bigquery/docs</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146156,
    "question_plain": "Your customer is planning to store and analyze large amounts of structured and unstructured data in Google Cloud. The customer has the following requirements:the data must be stored in a highly available and scalable storage systemthe data must be easily searchable and filterablethe data must be easily queryable using SQL-like syntaxthe data must be easily integratable with other Google Cloud servicesWhich Google Cloud service should you recommend to meet these requirements?",
    "answers": [
      "<p>Cloud Storage</p>",
      "<p>Cloud Datastore</p>",
      "<p>Cloud Bigtable</p>",
      "<p>Cloud Firestore</p>"
    ],
    "explanation": "<p>Cloud Firestore -&gt; Correct. Cloud Firestore is a fully managed NoSQL document database that is designed to store and manage structured and semi-structured data. It provides a highly available and scalable storage system with automatic sharding and load balancing, making it a good choice for storing large amounts of data. Firestore also provides powerful querying capabilities, with support for SQL-like syntax and complex queries on both structured and unstructured data. It also provides a flexible data model, making it easy to store and retrieve data in a variety of formats. In addition, Firestore is easily integratable with other Google Cloud services such as App Engine, Cloud Functions, and BigQuery, making it a good choice for integrating with other services as required by the customer.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. It is primarily designed for object storage, and while it is highly available and scalable, it does not provide the query capabilities required by the customer.</p><p><br></p><p>Cloud Datastore -. Incorrect. It is a NoSQL database, but it is optimized for storing small entities and may not be suitable for storing large amounts of data or complex queries.</p><p><br></p><p>Cloud Bigtable -&gt; Incorrect. It is designed for handling large volumes of structured data with low latency, but it may not be the most suitable for unstructured data or complex queries. It also does not support SQL-like syntax.</p><p><br></p><p>https://firebase.google.com/docs/firestore</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146158,
    "question_plain": "Your customer is planning to deploy a web application that requires high performance and low latency. The customer has the following requirements:the application must be easily deployable and manageablethe application must be highly available and recover from failures automaticallythe application must be able to handle incoming traffic spikes and scale dynamicallythe application must be cost-effectiveWhich Google Cloud service should you recommend to meet these requirements?",
    "answers": [
      "<p>App Engine</p>",
      "<p>Google Kubernetes Engine</p>",
      "<p>Cloud Functions</p>",
      "<p>Cloud Load Balancer</p>"
    ],
    "explanation": "<p>App Engine -&gt; Correct. The customer's requirements suggest that they need a serverless platform that can scale dynamically, be highly available, and cost-effective. App Engine meets all these requirements as it is a fully managed platform that can automatically scale up or down based on incoming traffic and provides high availability and fault tolerance. Additionally, it offers a simple and easy way to deploy and manage web applications. </p><p><br></p><p>Cloud Functions -&gt; Incorrect. Cloud Functions could be an alternative, but it has a cold start issue and may not be suitable for long-running services. </p><p><br></p><p>Google Kubernetes Engine -&gt; Incorrect. Google Kubernetes Engine would require more management and maintenance than App Engine. </p><p><br></p><p>Cloud Load Balancer -&gt; Incorrect. Cloud Load Balancer is not a service that can fulfill all the requirements mentioned.</p><p><br></p><p>https://cloud.google.com/appengine/docs</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146160,
    "question_plain": "A financial services company is planning to implement a high-performance computing solution on GCP to support its algorithmic trading operations. The company wants to ensure that the solution is scalable, low-latency, and able to handle a large volume of data. The company also wants to ensure that the solution is secure and that sensitive financial data is protected. Which of the following options would be the most effective approach to meet these requirements?",
    "answers": [
      "<p>Use Cloud Functions to stream the data and Cloud Dataflow to process the data. Use BigQuery to store the data and Cloud IAM to control access to the data. Implement security using Cloud KMS.</p>",
      "<p>Use Cloud Pub/Sub to stream the data and Cloud Dataproc to process the data. Use BigQuery to store the data and Cloud IAM to control access to the data. Implement security using Cloud IAP.</p>",
      "<p>Use Gogle Cloud Pub/Sub to stream the data and Cloud Dataflow to process the data. Use Cloud Bigtable to store the data and Cloud IAM to control access to the data. Implement security using Cloud KMS.</p>",
      "<p>Use Cloud Pub/Sub to stream the data and Cloud Dataproc to process the data. Use Cloud Bigtable to store the data and Cloud IAM to control access to the data. Implement security using Cloud IAP.</p>"
    ],
    "explanation": "<p>Use Gogle Cloud Pub/Sub to stream the data and Cloud Dataflow to process the data. Use Cloud Bigtable to store the data and Cloud IAM to control access to the data. Implement security using Cloud KMS. -&gt;&nbsp;Correct. Google Cloud Pub/Sub is designed for scalable, low-latency data streaming, while Cloud Dataflow offers powerful data processing capabilities. Cloud Bigtable is a high-performance NoSQL database that can handle large volumes of data with low latency. Cloud IAM provides access control and authorization mechanisms, ensuring the security of the sensitive financial data. Cloud KMS (Key Management Service) can be used to secure data encryption keys and provide additional security measures.</p><p><br></p><p>Use Cloud Functions to stream the data and Cloud Dataflow to process the data. Use BigQuery to store the data and Cloud IAM to control access to the data. Implement security using Cloud KMS. -&gt; Incorrect. While Cloud Functions and Cloud Dataflow offer streaming and data processing capabilities, BigQuery may not be the most suitable choice for handling large volumes of data with low-latency requirements. </p><p><br></p><p>Use Cloud Pub/Sub to stream the data and Cloud Dataproc to process the data. Use BigQuery to store the data and Cloud IAM to control access to the data. Implement security using Cloud IAP. -&gt; Incorrect. While Cloud Pub/Sub and Cloud Dataproc can handle streaming and data processing, BigQuery may not provide the low-latency requirements needed for algorithmic trading operations. Cloud IAP (Identity-Aware Proxy) provides security measures, but it may not fully address the specific security requirements for protecting sensitive financial data.</p><p><br></p><p>Use Cloud Pub/Sub to stream the data and Cloud Dataproc to process the data. Use Cloud Bigtable to store the data and Cloud IAM to control access to the data. Implement security using Cloud IAP. -&gt; Incorrect. Pub/Sub and Dataproc are suitable, and Bigtable is good for low-latency storage. However, Cloud IAP is not the best option for data security compared to Cloud KMS.</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146162,
    "question_plain": "A multinational retail company has stores in multiple countries and wants to leverage GCP to manage its inventory and ordering processes. The company wants to implement a mobile application that provides real-time visibility into inventory levels and enables employees to place orders for out-of-stock items from any location. The solution should also be scalable and be able to handle high volume transactions. Which of the following options would be the most effective approach to meet these requirements?",
    "answers": [
      "<p>Use Cloud Storage to store the inventory data and Cloud Functions to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data.</p>",
      "<p>Use Cloud SQL to store the inventory data and App Engine to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data.</p>",
      "<p>Use Cloud Bigtable to store the inventory data and Cloud Dataflow to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data.</p>",
      "<p>Use Cloud BigQuery to store the inventory data and Cloud Functions to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data.</p>"
    ],
    "explanation": "<p>Use Cloud SQL to store the inventory data and App Engine to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data. -&gt; Correct. Cloud SQL is a fully-managed database service that provides a highly scalable and available relational database engine for storing and managing data. It is a good option for storing inventory data because it supports high transaction rates and can handle large volumes of data. App Engine, on the other hand, is a serverless platform for building and deploying web and mobile applications, which can be used to trigger the ordering process in response to requests from the mobile application. This makes App Engine a good choice for implementing the ordering process because it can handle high volumes of transactions and scale automatically.</p><p><br></p><p>Use Cloud Storage to store the inventory data and Cloud Functions to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data. -&gt; Incorrect. It may not be the best solution for this use case, as Cloud Functions is more suitable for processing small, event-driven tasks, rather than processing large amounts of data in real-time.</p><p><br></p><p>Use Cloud Bigtable to store the inventory data and Cloud Dataflow to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data. -&gt; Incorrect. It may not be the best fit for this scenario. Cloud Bigtable is designed for very large amounts of data that require high throughput and scalability, which might be more than necessary for typical inventory data. Cloud Dataflow is a stream and batch data processing service that's more aligned with data analytics than triggering specific transactional operations like ordering.</p><p><br></p><p>Use Cloud BigQuery to store the inventory data and Cloud Functions to trigger the ordering process. Use Cloud Pub/Sub to notify employees of inventory changes and Cloud IAM to control access to the data. -&gt; Incorrect. It may not be the best solution for this use case, as BigQuery is more suited for querying and analyzing structured data rather than processing large volumes of data in real-time. Additionally, Cloud Functions may not be the best choice for handling high volumes of transactions, which is required for the ordering process in this scenario.</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146164,
    "question_plain": "A financial services company is looking to implement a new fraud detection system for their credit card transactions. The system should be able to detect fraud in real-time and provide alerts to security personnel. The company wants to leverage the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?",
    "answers": [
      "<p>Use Cloud Datastore to store transaction data and use Cloud Functions to process transactions in real-time and trigger fraud alerts.</p>",
      "<p>Use BigQuery to store transaction data and use Cloud Dataflow to process transactions in real-time and trigger fraud alerts. </p>",
      "<p>Use Cloud SQL to store transaction data and use Cloud Pub/Sub to process transactions in real-time and trigger fraud alerts.</p>",
      "<p>Use Cloud Firestore to store transaction data and use Cloud Tasks to process transactions in real-time and trigger fraud alerts.</p>"
    ],
    "explanation": "<p>Use BigQuery to store transaction data and use Cloud Dataflow to process transactions in real-time and trigger fraud alerts.&nbsp; -&gt; Correct. BigQuery is a powerful and scalable data warehousing solution that can handle large volumes of transaction data efficiently. Cloud Dataflow, a serverless data processing service, can be used to process transactions in real-time and trigger fraud alerts based on specific criteria. This combination of BigQuery and Cloud Dataflow provides a robust and scalable solution for real-time fraud detection in the given scenario.</p><p><br></p><p>Use Cloud Datastore to store transaction data and use Cloud Functions to process transactions in real-time and trigger fraud alerts. -&gt;&nbsp;Incorrect. While Cloud Functions can be used for event-driven processing, Cloud Datastore may not be the best choice for storing and querying large volumes of transaction data required for fraud detection. Therefore, this option may not be the most suitable solution.</p><p><br></p><p>Use Cloud SQL to store transaction data and use Cloud Pub/Sub to process transactions in real-time and trigger fraud alerts. -&gt;&nbsp;Incorrect. While Cloud SQL is a fully managed relational database service, it may not provide the scalability and performance required for handling large volumes of transaction data in real-time. Therefore, this option may not be the most suitable solution.</p><p><br></p><p>Use Cloud Firestore to store transaction data and use Cloud Tasks to process transactions in real-time and trigger fraud alerts. -&gt;&nbsp;Incorrect. While Cloud Firestore is a NoSQL document database, it may not be the optimal choice for storing and querying large volumes of transaction data in a structured manner. Additionally, Cloud Tasks is primarily used for asynchronous task execution and may not be the ideal service for real-time fraud detection. Therefore, this option may not be the most suitable solution.</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146166,
    "question_plain": "A multinational retail company has a large number of physical stores across multiple countries. The company wants to implement a centralized system to monitor the energy consumption of each store in real-time and generate reports to help optimize energy usage. The company wants to use the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?",
    "answers": [
      "<p>Use Cloud Pub/Sub to connect the energy meters in each store to the cloud, Cloud Datastore to store energy consumption data, and Cloud Functions to process the data and generate reports.</p>",
      "<p>Use Cloud IoT Core to connect the energy meters in each store to the cloud, BigQuery to store energy consumption data, and Cloud Dataflow to process the data and generate reports.</p>",
      "<p>Use Cloud IoT Edge to connect the energy meters in each store to the cloud, Cloud SQL to store energy consumption data, and Cloud Pub/Sub to process the data and generate reports.</p>",
      "<p>Use Cloud IoT Edge to connect the energy meters in each store to the cloud, Cloud Firestore to store energy consumption data, and Cloud Tasks to process the data and generate reports.</p>"
    ],
    "explanation": "<p>Use Cloud IoT Core to connect the energy meters in each store to the cloud, BigQuery to store energy consumption data, and Cloud Dataflow to process the data and generate reports. -&gt; Correct. Cloud IoT Core is a fully managed service for securely connecting and managing IoT devices, making it a suitable choice for connecting the energy meters in each store. BigQuery is a highly scalable and cost-effective data warehouse that can handle large volumes of data, making it a good option for storing energy consumption data. Cloud Dataflow is a fully managed data processing service that can handle batch and stream processing, making it suitable for processing real-time energy consumption data and generating reports.</p><p><br></p><p>Use Cloud Pub/Sub to connect the energy meters in each store to the cloud, Cloud Datastore to store energy consumption data, and Cloud Functions to process the data and generate reports. -&gt;&nbsp;Incorrect. Cloud Datastore is a NoSQL database that may not be the optimal choice for storing large amounts of time-series data. Cloud Functions can be used for processing data, but it might not be the most efficient solution for real-time data processing and generating reports.</p><p><br></p><p>Use Cloud IoT Edge to connect the energy meters in each store to the cloud, Cloud SQL to store energy consumption data, and Cloud Pub/Sub to process the data and generate reports. -&gt;&nbsp;Incorrect.&nbsp; Cloud SQL is a relational database service and may not be the most suitable option for storing time-series data. Cloud Pub/Sub can handle data streaming, but it may not be the optimal choice for real-time data processing and report generation.</p><p><br></p><p>Use Cloud IoT Edge to connect the energy meters in each store to the cloud, Cloud Firestore to store energy consumption data, and Cloud Tasks to process the data and generate reports. -&gt;&nbsp;Incorrect. Cloud Firestore is a NoSQL document database and may not be the best option for storing time-series data. Cloud Tasks is a task queue service and may not be the optimal solution for real-time data processing and generating reports.</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146168,
    "question_plain": "A large media company wants to implement a system for processing user-generated content, such as images and videos, and automatically categorize them based on visual content. The system should be able to process large amounts of data in real-time and scale as needed. The company wants to use the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?",
    "answers": [
      "<p>Use Cloud Storage to store the user-generated content, Cloud Functions to process the data and categorize the content.</p>",
      "<p>Use Cloud Storage to store the user-generated content, Cloud Dataflow to process the data and categorize the content.</p>",
      "<p>Use Cloud Storage to store the user-generated content, Cloud Dataproc to process the data, and Vertex AI to categorize the content.</p>",
      "<p>Use Cloud Pub/Sub to ingest the user-generated content, BigQuery to store the data, and Vertex AI to categorize the content.</p>"
    ],
    "explanation": "<p>Use Cloud Storage to store the user-generated content, Cloud Dataproc to process the data, and Vertex AI to categorize the content. -&gt;&nbsp;Correct. The large media company wants to implement a system that processes user-generated content, such as images and videos, and automatically categorizes them based on visual content. The system should be able to process large amounts of data in real-time and scale as needed.</p><p>It is the best solution because it uses a combination of services to achieve the requirements of the media company.</p><ul><li><p>Cloud Storage provides a scalable and cost-effective way to store the user-generated content.</p></li><li><p>Cloud Dataproc is a fully-managed big data processing service that allows for the processing of large amounts of data in a cost-effective manner.</p></li><li><p>Vertex AI provides a set of pre-trained models and tools for machine learning, which can be used to categorize the content based on visual content.</p></li></ul><p><br></p><p>Use Cloud Storage to store the user-generated content, Cloud Functions to process the data and categorize the content. -&gt; Incorrect. It is not the best solution because Cloud Functions are not the best choice for processing large amounts of data in real-time. Cloud Functions are best suited for event-driven applications that require quick, short-lived computations.</p><p><br></p><p>Use Cloud Storage to store the user-generated content, Cloud Dataflow to process the data and categorize the content. -&gt; Incorrect. Cloud Dataflow is a fully managed service for stream and batch processing, but it lacks the machine learning capabilities to categorize content based on visual data, which is the key requirement in this scenario. Hence, this solution is incomplete.</p><p><br></p><p>Use Cloud Pub/Sub to ingest the user-generated content, BigQuery to store the data, and Vertex AI to categorize the content. -&gt; Incorrect. It is not the best solution because Pub/Sub and BigQuery are not designed for image and video processing, and Vertex AI provides a better solution for categorizing the content based on visual content.</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146170,
    "question_plain": "A large financial services company needs to build a secure and scalable platform for processing financial transactions. The platform must meet the following requirements:provide fast and secure transaction processingensure data privacy and securityenable real-time reporting and auditingminimize downtime during maintenance and upgradesminimize costs while still providing high performanceWhich solution would you recommend to meet these requirements?",
    "answers": [
      "<p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling.</p>",
      "<p>Implementing a serverless solution using Cloud Functions for transaction processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing.</p>",
      "<p>Implementing a managed solution using Cloud Spanner for data storage and processing, Cloud Dataflow for real-time analytics, and Cloud Key Management Service (KMS) for secure payment processing.</p>",
      "<p>Implementing a hybrid solution using Cloud SQL for data storage, Compute Engine for transaction processing, and BigQuery for real-time analytics.</p>"
    ],
    "explanation": "<p>Implementing a managed solution using Cloud Spanner for data storage and processing, Cloud Dataflow for real-time analytics, and Cloud Key Management Service (KMS) for secure payment processing. -&gt; Correct. Cloud Spanner is a globally distributed and strongly consistent relational database that can provide fast and secure transaction processing while ensuring data privacy and security. Cloud Dataflow is a fully managed service for real-time analytics, allowing real-time reporting and auditing capabilities. Cloud KMS enables secure payment processing by providing key management services. This option aligns well with all the given requirements, including scalability, security, real-time analytics, and cost-effectiveness.</p><p><br></p><p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling. -&gt; Incorrect. Using a custom-built solution introduces additional complexity and potential security risks. Bigtable may not be the most suitable choice for structured financial transaction data storage, as it is a wide-column NoSQL database. This option does not explicitly address data privacy and security requirements.</p><p><br></p><p>Implementing a serverless solution using Cloud Functions for transaction processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing. -&gt; Incorrect. While serverless solutions can provide scalability and ease of deployment, Cloud Functions may not be the most efficient choice for transaction processing in terms of performance and cost. Cloud Firestore, a NoSQL document database, may not be the optimal option for structured financial transaction data storage. This option does not explicitly address the need for real-time reporting and auditing.</p><p><br></p><p>Implementing a hybrid solution using Cloud SQL for data storage, Compute Engine for transaction processing, and BigQuery for real-time analytics. -&gt; Incorrect. While Cloud SQL can handle structured data storage and Compute Engine allows for transaction processing, this hybrid solution may not provide the same level of scalability, reliability, and cost-effectiveness as the managed solutions. BigQuery is a powerful analytics platform but may not be the most efficient choice for real-time analytics. This option does not explicitly address the need for secure payment processing.</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146172,
    "question_plain": "A large healthcare organization needs to build a secure and scalable platform for storing and processing sensitive patient data. The platform must meet the following requirements:ensure secure storage and processing of sensitive patient dataenable fast and efficient data retrieval for medical professionalssupport real-time data analysis for clinical decision-makingenable data sharing with authorized partners while ensuring data privacy and securityminimize costs while still providing high performanceWhich solution would you recommend to meet these requirements?",
    "answers": [
      "<p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling.</p>",
      "<p>Implementing a serverless solution using Cloud Functions for data processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing.</p>",
      "<p>Implementing a managed solution using Cloud Healthcare API for data storage and processing, Cloud Dataflow for real-time analytics, and Cloud Identity Access Management (IAM) for secure data sharing.</p>",
      "<p>Implementing a hybrid solution using Cloud SQL for data storage, Compute Engine for data processing, and BigQuery for real-time analytics.</p>"
    ],
    "explanation": "<p>Implementing a managed solution using Cloud Healthcare API for data storage and processing, Cloud Dataflow for real-time analytics, and Cloud Identity Access Management (IAM) for secure data sharing. -&gt;&nbsp;Correct. The Cloud Healthcare API is designed specifically for healthcare data storage and processing, providing secure and compliant handling of sensitive patient data. Cloud Dataflow enables real-time analytics and data processing, allowing for efficient clinical decision-making. Cloud IAM ensures secure data sharing with authorized partners while maintaining data privacy and security. This managed solution offers a comprehensive set of services that meet the requirements of the healthcare organization.</p><p><br></p><p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling. -&gt; Incorrect. While these services are capable of handling aspects of the requirements, managing a custom-built solution can introduce additional complexity, maintenance overhead, and potential security risks. It may not be the most efficient and cost-effective approach.</p><p><br></p><p>Implementing a serverless solution using Cloud Functions for data processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing. -&gt; Incorrect. While serverless services offer scalability and ease of use, Cloud Firestore may not be the most suitable choice for secure and efficient storage of sensitive patient data. Additionally, a comprehensive solution that addresses all requirements may require additional services and configurations.</p><p><br></p><p>Implementing a hybrid solution using Cloud SQL for data storage, Compute Engine for data processing, and BigQuery for real-time analytics. -&gt; Incorrect. While each service individually has its strengths, this combination may not provide the most efficient and integrated solution for healthcare data storage, processing, and real-time analytics. It may introduce complexities and potential limitations in meeting all the specified requirements.</p><p><br></p><p>https://cloud.google.com/healthcare-api</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146174,
    "question_plain": "A large logistics company is looking to build a secure and scalable platform for tracking shipments and managing delivery schedules. The platform must meet the following requirements:support real-time tracking of shipments with high accuracyensure secure storage and processing of sensitive shipment informationenable efficient coordination of delivery schedules across multiple locationsminimize downtime during maintenance and upgradesminimize costs while still providing high performanceWhich solution would you recommend to meet these requirements?",
    "answers": [
      "<p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling.</p>",
      "<p>Implementing a serverless solution using Cloud Functions for data processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing.</p>",
      "<p>Implementing a managed solution using Google Maps Platform for real-time tracking, Cloud Datastore for data storage, and Cloud Identity and Access Management (IAM) for secure access control.</p>",
      "<p>Implementing a hybrid solution using Cloud SQL for data storage, Compute Engine for data processing, and BigQuery for real-time analytics.</p>"
    ],
    "explanation": "<p>Implementing a managed solution using Google Maps Platform for real-time tracking, Cloud Datastore for data storage, and Cloud Identity and Access Management (IAM) for secure access control. -&gt; Correct. Google Maps Platform provides real-time location tracking capabilities that can be used to track shipments. Cloud Datastore is a highly scalable, managed NoSQL database that can be used to store sensitive shipment information securely. And, Cloud Identity and Access Management (IAM) can be used to control access to the platform, ensuring that sensitive information is only accessible to authorized individuals. This solution would meet all the requirements while still providing high performance and minimizing costs.</p><p><br></p><p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling. -&gt; Incorrect. Using a custom-built solution introduces additional complexity and potential security risks. Bigtable may not be the most suitable choice for structured data storage and retrieval of sensitive shipment information. This option does not explicitly address the requirement for real-time tracking or secure access control.</p><p><br></p><p>Implementing a serverless solution using Cloud Functions for data processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing. -&gt;&nbsp;Incorrect. While serverless solutions can provide scalability and ease of deployment, Cloud Functions may not be the most efficient choice for real-time data processing and tracking of shipments. Cloud Firestore, a NoSQL document database, may not be the optimal option for structured data storage and secure processing of sensitive shipment information. This option does not explicitly address the requirement for efficient coordination of delivery schedules across multiple locations.</p><p><br></p><p>Implementing a hybrid solution using Cloud SQL for data storage, Compute Engine for data processing, and BigQuery for real-time analytics. -&gt;&nbsp;Incorrect. While Cloud SQL can handle structured data storage and Compute Engine allows for data processing, this hybrid solution may not provide the same level of scalability, reliability, and cost-effectiveness as the managed solutions. BigQuery is a powerful analytics platform but may not be the most efficient choice for real-time analytics or real-time tracking of shipments. This option does not explicitly address the requirement for secure access control.</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146176,
    "question_plain": "You are designing a cloud-based architecture for a company that requires a secure and scalable solution for its database. The database must be able to handle high volumes of transactions, and the data must be encrypted at rest and in transit. Which of the following options provides the best solution?",
    "answers": [
      "<p>Use a managed database service with automatic encryption and scale-up capability.</p>",
      "<p>Use a self-managed database service with manual encryption and scale-out capability.</p>",
      "<p>Use a NoSQL database service with built-in encryption and automatic scaling.</p>",
      "<p>Use a hybrid approach with a self-managed database service and a third-party encryption tool.</p>"
    ],
    "explanation": "<p>Use a managed database service with automatic encryption and scale-up capability. -&gt; Correct. Using a managed database service with automatic encryption and scale-up capability provides the best solution for the company's requirements. Managed database services such as Amazon RDS, Google Cloud SQL, and Azure Database offer automatic encryption of data at rest and in transit, and they provide built-in scalability features that can automatically scale up or down the database depending on the workload. This solution also reduces the operational burden of managing a database and ensures that the database is always up to date with the latest security patches.</p><p><br></p><p>Use a self-managed database service with manual encryption and scale-out capability. -&gt; Incorrect. Using a self-managed database service with manual encryption and scale-out capability may provide scalability, but it requires manual setup and management of encryption, which can be error-prone and time-consuming.</p><p><br></p><p>Use a NoSQL database service with built-in encryption and automatic scaling. -&gt; Incorrect. Using a NoSQL database service with built-in encryption and automatic scaling may provide automatic encryption and scalability, but it may not be suitable for a company that requires a traditional SQL database.</p><p><br></p><p>Use a hybrid approach with a self-managed database service and a third-party encryption tool. -&gt; Incorrect. Using a hybrid approach with a self-managed database service and a third-party encryption tool may provide encryption, but it adds complexity to the architecture and requires more manual management.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146178,
    "question_plain": "As a cloud architect, you are working with a media company to develop a web application for live streaming events. The application needs to handle high incoming traffic during popular events. It's also important for the company to keep costs as low as possible when there are no live events (low traffic). Which environment of App Engine should you choose for this scenario?",
    "answers": [
      "<p>App Engine Standard environment, because it can scale to zero instances when there's no traffic.</p>",
      "<p>App Engine Flexible environment, because it can scale to zero instances when there's no traffic.</p>",
      "<p>Both environments would suit this application equally well.</p>",
      "<p>App Engine Flexible environment, because it supports third-party software.</p>"
    ],
    "explanation": "<p>App Engine Standard environment, because it can scale to zero instances when there's no traffic. -&gt;&nbsp;Correct. It automatically scales instances up and down, even down to zero when there's no traffic. This feature will help the company to keep costs low when there are no live events.</p><p><br></p><p>App Engine Flexible environment, because it can scale to zero instances when there's no traffic. -&gt; Incorrect. The App Engine Flexible environment doesn't scale down to zero instances. This means it will continue to accrue costs even when there's no traffic, which is not suitable for the scenario.</p><p><br></p><p>Both environments would suit this application equally well. -&gt; Incorrect. Both environments do not suit this application equally well due to the specific scaling and cost requirements of the scenario, making this option incorrect.</p><p><br></p><p>App Engine Flexible environment, because it supports third-party software. -&gt; Incorrect. Although the App Engine Flexible environment supports third-party software, it doesn't scale down to zero instances, which is a crucial requirement for the scenario.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 82146180,
    "question_plain": "Your organization uses a BigQuery data warehouse for analytics. You expect the volume of data and the complexity of queries to increase significantly over the next year. What could be a strategy to ensure query performance remains high?",
    "answers": [
      "<p>Use the BigQuery Reservation model to purchase dedicated query processing capacity.</p>",
      "<p>Increase the amount of storage available to BigQuery.</p>",
      "<p>Migrate data from BigQuery to Cloud Spanner for increased performance.</p>",
      "<p>Split your data into multiple BigQuery datasets to balance the load.</p>"
    ],
    "explanation": "<p>Use the BigQuery Reservation model to purchase dedicated query processing capacity. -&gt;&nbsp;Correct. The BigQuery Reservation model allows you to purchase dedicated query processing capacity (slots). This can ensure consistent performance even as query load increases.</p><p><br></p><p>Increase the amount of storage available to BigQuery. -&gt;&nbsp;Incorrect. BigQuery is serverless, so increasing storage doesn't directly affect query performance. BigQuery automatically manages resources like storage.</p><p><br></p><p>Migrate data from BigQuery to Cloud Spanner for increased performance. -&gt;&nbsp;Incorrect. Cloud Spanner is a relational database service designed for high transaction rates, and might not be as suitable for analytics as BigQuery.</p><p><br></p><p>Split your data into multiple BigQuery datasets to balance the load. -&gt;&nbsp;Incorrect. BigQuery's performance doesn't depend on the number of datasets. Therefore, splitting data into multiple datasets wouldn't necessarily improve query performance.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681520,
    "question_plain": "As a cloud architect, you are working with a rapidly expanding startup that uses Google Cloud. They have been manually creating individual Google user accounts for each employee, but now they are scaling and need a more efficient and secure way to manage access to Google Cloud resources. Which of the following options should you recommend?",
    "answers": [
      "<p>Set up a Cloud Identity or G Suite domain, then use Google Cloud Directory Sync to synchronize the users to Google Cloud.</p>",
      "<p>Continue creating individual user accounts for each employee manually.</p>",
      "<p>Use the Google Cloud IAM service to create a custom IAM role for each employee.</p>",
      "<p>Create service accounts for each employee and distribute the private keys.</p>"
    ],
    "explanation": "<p>Set up a Cloud Identity or G Suite domain, then use Google Cloud Directory Sync to synchronize the users to Google Cloud. -&gt; Correct. Setting up a Cloud Identity or G Suite domain allows the organization to manage user accounts more efficiently. Google Cloud Directory Sync can then be used to synchronize these users to Google Cloud, and IAM policies can be applied as needed.</p><p><br></p><p>Continue creating individual user accounts for each employee manually. -&gt; Incorrect. Continuing to manually create individual user accounts is inefficient and does not scale well.</p><p><br></p><p>Use the Google Cloud IAM service to create a custom IAM role for each employee. -&gt; Incorrect. Creating a custom IAM role for each employee can be useful for fine-grained access control, but it doesn't solve the problem of efficiently managing multiple user accounts.</p><p><br></p><p>Create service accounts for each employee and distribute the private keys. -&gt; Incorrect. Service accounts are designed for authenticating applications and services, not individual users. Distributing private keys is also a security risk.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681522,
    "question_plain": "As a cloud architect, you are working with a client that needs an automated solution to back up their Compute Engine workloads. They want to ensure minimal downtime during the backup process and be able to restore their workloads to any point in the last 30 days. Which of the following solutions would be the most effective for this scenario?",
    "answers": [
      "<p>Use Persistent Disk Snapshots scheduled at regular intervals.</p>",
      "<p>Use Cloud Storage to store periodic snapshots of the Compute Engine instances.</p>",
      "<p>Use Cloud Datastore to keep the state of the Compute Engine instances.</p>",
      "<p>Use Google Cloud's operations suite to monitor and backup Compute Engine instances.</p>"
    ],
    "explanation": "<p>Use Persistent Disk Snapshots scheduled at regular intervals. -&gt; Correct. Persistent Disk Snapshots can be scheduled to run at regular intervals, providing an automated solution for backing up Compute Engine workloads.</p><p><br></p><p>Use Cloud Storage to store periodic snapshots of the Compute Engine instances. -&gt; Incorrect. While Cloud Storage is a versatile and durable storage solution, it doesn't provide an automated mechanism for creating instance snapshots.</p><p><br></p><p>Use Cloud Datastore to keep the state of the Compute Engine instances. -&gt; Incorrect. Cloud Datastore is a NoSQL database and isn't designed for backing up Compute Engine instances.</p><p><br></p><p>Use Google Cloud's operations suite to monitor and backup Compute Engine instances. -&gt;&nbsp;Incorrect. Google Cloud's operations suite provides monitoring, troubleshooting, and application performance management, but it doesn't offer automated backup of Compute Engine instances.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681524,
    "question_plain": "You run a small startup and want to optimize costs in GCP. As a cloud architect, you need to research resource consumption charges and provide a summary of your expenses. You want to do it in the most efficient way. What should you do?",
    "answers": [
      "<p>You should attach labels to resources to reflect the purpose. Than, export Cloud Billing data into BigQuery, and analyze it with Data Studio.</p>",
      "<p>You should rename resources to reflect the purpose. Write a Python script to analyze resource consumption.</p>",
      "<p>You should assign tags to resources to reflect the purpose. Export Cloud Billing data into Cloud SQL, and analyze it with Data Studio.</p>",
      "<p>You should use Google Cloud Recommender to see if expenses can be reduced.</p>"
    ],
    "explanation": "<p>You should attach labels to resources to reflect the purpose. Than, export Cloud Billing data into BigQuery, and analyze it with Data Studio. -&gt; Correct. Attaching labels to resources to reflect their purpose allows you to track and categorize resource usage. By exporting Cloud Billing data into BigQuery, you can store and analyze the billing data in a structured format. Data Studio can then be used to create visualizations and reports based on the analyzed billing data. This approach provides an efficient and effective way to understand resource consumption charges and optimize costs.</p><p><br></p><p>You should rename resources to reflect the purpose. Write a Python script to analyze resource consumption. -&gt;&nbsp;Incorrect. Renaming resources to reflect their purpose may help with identification but does not directly address the task of analyzing resource consumption charges and optimizing costs. Writing a Python script to analyze resource consumption would require significant development effort and may not be as efficient as using tools specifically designed for this purpose.</p><p><br></p><p>You should assign tags to resources to reflect the purpose. Export Cloud Billing data into Cloud SQL, and analyze it with Data Studio. -&gt;&nbsp;Incorrect. Assigning tags to resources to reflect their purpose is similar to attaching labels and can be helpful for organizing and categorizing resources. Exporting Cloud Billing data into Cloud SQL is not a recommended approach.</p><p><br></p><p>You should use Google Cloud Recommender to see if expenses can be reduced. -&gt;&nbsp;Incorrect. It does not directly address the task of researching resource consumption charges and providing a summary of expenses. Google Cloud Recommender focuses on providing recommendations for improving performance, security, and cost-efficiency based on analyzing the current state of resources and configurations.</p><p><br></p><p>https://cloud.google.com/compute/docs/labeling-resources</p><p>https://cloud.google.com/blog/topics/cost-management/use-labels-to-gain-visibility-into-gcp-resource-usage-and-spending</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681526,
    "question_plain": "Your organization is planning to create a custom VPC network on Google Cloud for deploying a multi-tier application. The application includes frontend servers, backend servers, and database servers, each of which requires a separate subnet. What considerations should you keep in mind while creating this VPC?",
    "answers": [
      "<p>Assign one large CIDR block to the VPC and divide it into smaller CIDR blocks for each subnet.</p>",
      "<p>Use Shared VPC to host all the tiers of the application.</p>",
      "<p>Use the same CIDR block for all the subnets.</p>",
      "<p>Use Cloud NAT for the frontend servers to connect with the internet.</p>"
    ],
    "explanation": "<p>Assign one large CIDR block to the VPC and divide it into smaller CIDR blocks for each subnet. -&gt;&nbsp;Correct. By assigning a large CIDR block to the VPC, you can then create smaller subnets for each tier of your application, giving you greater control over IP address ranges.</p><p><br></p><p>Use Shared VPC to host all the tiers of the application. -&gt; Incorrect. Shared VPC allows resources from multiple projects to communicate with each other securely, but it may not be necessary if all tiers of your application are within the same project.</p><p><br></p><p>Use the same CIDR block for all the subnets. -&gt;&nbsp;Incorrect. If you use the same CIDR block for all subnets, it would result in overlapping IP addresses which is not allowed in GCP.</p><p><br></p><p>Use Cloud NAT for the frontend servers to connect with the internet. -&gt;&nbsp;Incorrect. The need for Cloud NAT depends on whether the frontend servers need to initiate communication with the internet without having public IP addresses. It's not mandatory for all cases.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681528,
    "question_plain": "When deploying your application to App Engine, your development team aims to scale the number of instances in response to the request rate. It is necessary to maintain a minimum of five idle instances at all times. Which scaling method should they employ?",
    "answers": [
      "<p>Automatic Scaling with <code>min_idle_instances</code> set to 5.</p>",
      "<p>Basic Scaling with <code>min_instances</code> set to 5.</p>",
      "<p>Manual Scaling with 5 instances.</p>",
      "<p>Basic Scaling with <code>max_instances</code> set to 5.</p>"
    ],
    "explanation": "<p>Automatic Scaling with <code>min_idle_instances</code> set to 5. -&gt; Correct. Automatic Scaling is an App Engine scaling type that automatically adjusts the number of instances based on request rate, and scales down to zero when there are no incoming requests. The <code>min_idle_instances</code> setting controls the minimum number of idle instances that the service should maintain at all times, which in this case is 5. This ensures that the service has enough instances to handle incoming requests without any delays.</p><p><br></p><p>Basic Scaling with <code>min_instances</code> set to 5. -&gt; Incorrect. Basic Scaling with <code>min_instances</code> set to 5 would not work in this scenario because Basic Scaling only scales up to a fixed number of instances based on incoming request traffic, and does not scale down to zero.</p><p><br></p><p>Manual Scaling with 5 instances. -&gt; Incorrect. Manual Scaling with 5 instances would require the development team to manually manage the number of instances, which can be time-consuming and prone to errors.</p><p><br></p><p>Basic Scaling with <code>max_instances</code> set to 5. -&gt; Incorrect. Basic Scaling with <code>max_instances</code> set to 5 would not work in this scenario because it does not ensure that there are at least 5 instances available at all times. Additionally, the number of instances would not automatically scale up based on incoming request traffic.</p><p><br></p><p>https://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed#apps_with_automatic_scaling</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681530,
    "question_plain": "You are creating a VPC network to host a set of compute resources for a multi-tier web application in Google Cloud. How should you configure the network to optimize security and manageability?",
    "answers": [
      "<p>Create separate subnets for each tier of the application.</p>",
      "<p>Use automatic mode to create the VPC network.</p>",
      "<p>Assign public IP addresses to all compute instances to facilitate direct access.</p>",
      "<p>Use Shared VPC to host all the compute resources.</p>"
    ],
    "explanation": "<p>Create separate subnets for each tier of the application. -&gt; Correct. By creating separate subnets for each tier of the application, you can use firewall rules to control traffic between different tiers, enhancing security.</p><p><br></p><p>Use automatic mode to create the VPC network. -&gt; Incorrect. Automatic mode creates a subnet in each region and sets up the appropriate IP ranges. This may not provide the granularity of control you need for a multi-tier web application.</p><p><br></p><p>Assign public IP addresses to all compute instances to facilitate direct access. -&gt; Incorrect. Assigning public IP addresses to all instances can expose them to potential threats. It's better to limit public IP addresses and use Cloud NAT or bastion hosts for access.</p><p><br></p><p>Use Shared VPC to host all the compute resources. -&gt; Incorrect. Shared VPC allows resources from multiple projects to communicate with each other securely, but it may not be necessary if all your compute resources are within the same project.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681532,
    "question_plain": "As a cloud architect, you are working with a cost-conscious client who regularly runs large, complex queries on BigQuery. They want to understand how they can estimate the cost of running a BigQuery query before they execute it to avoid unexpected expenses. Which of the following is the most suitable solution for this?",
    "answers": [
      "<p>Preview the query in the BigQuery web UI, which displays an estimate of the amount of data the query will process.</p>",
      "<p>Use Google Cloud Pricing Calculator by entering the estimated amount of data processed by the query.</p>",
      "<p>Use the BigQuery command-line tool's <code>bq show</code> command to inspect the query.</p>",
      "<p>Use Cloud Monitoring to estimate the cost based on previous query execution times.</p>"
    ],
    "explanation": "<p>Preview the query in the BigQuery web UI, which displays an estimate of the amount of data the query will process. -&gt; Correct. When you paste a query into the BigQuery web UI and click on the 'Compose Query' button without running the query, it will show an estimate of the amount of data that the query will process. You can then use this data to calculate the cost based on BigQuery's pricing.</p><p><br></p><p>Use Google Cloud Pricing Calculator by entering the estimated amount of data processed by the query. -&gt; Incorrect. The Google Cloud Pricing Calculator can help estimate costs for cloud services based on usage predictions, but it is not designed to predict the cost of individual BigQuery queries.</p><p><br></p><p>Use the BigQuery command-line tool's <code>bq show</code> command to inspect the query. -&gt; Incorrect. The <code>bq show</code> command in BigQuery command-line tool is used to display information about dataset, table, or view, not to estimate the cost of a query.</p><p><br></p><p>Use Cloud Monitoring to estimate the cost based on previous query execution times. -&gt; Incorrect. Cloud Monitoring is for monitoring resource usage and performance, not for estimating the cost of individual BigQuery queries.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681534,
    "question_plain": "As a cloud architect, you are working with an e-commerce client who expects a significant increase in traffic to their application hosted on Google Kubernetes Engine (GKE) during an upcoming sales event. To manage the increased load, the client wishes to enable autoscaling for their application. However, they want to ensure that any newly created pods during autoscaling are fully ready to handle traffic before being included in the service. Which of the following solutions should you implement?",
    "answers": [
      "<p>Configure a readiness probe in your pod specification.</p>",
      "<p>Enable Cluster Autoscaler on your GKE cluster.</p>",
      "<p>Configure a startup script in the pod specification to delay service registration.</p>",
      "<p>Configure a liveness probe in your pod specification.</p>"
    ],
    "explanation": "<p>Configure a readiness probe in your pod specification. -&gt; Correct. A readiness probe is used in Kubernetes to determine when a pod is ready to accept traffic. The load balancer only sends traffic to pods that pass their readiness probe, ensuring that new pods are fully ready before they start receiving traffic.</p><p><br></p><p>Enable Cluster Autoscaler on your GKE cluster. -&gt; Incorrect. Enabling Cluster Autoscaler on your GKE cluster would indeed help manage the number of nodes based on the traffic, but it would not ensure that new pods are ready before they receive traffic.</p><p><br></p><p>Configure a startup script in the pod specification to delay service registration. -&gt; Incorrect. It would not be an effective solution, as it would delay all pods, not just the ones that are not ready. It would also require manual intervention to manage the delay.</p><p><br></p><p>Configure a liveness probe in your pod specification. -&gt; Incorrect. Configuring a liveness probe in your pod specification would be used to know when to restart a pod, but it does not determine when a pod is ready to accept traffic.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681536,
    "question_plain": "In the europe-central2-a zone, your team has an application server running on Compute Engine. What should you do to ensure high availability and replicate this server to europe-central2-b zone in as few steps as possible?",
    "answers": [
      "<p>You should create a snapshot from the disk and then create a disk from this snapshot in the <code>europe-central2-b</code> zone. Finally, create a new virtual machine with that disk.</p>",
      "<p>You should create a snapshot from the disk and then create a disk from this snapshot in the <code>europe-central2-a</code> zone, move this disk to <code>europe-central2-b</code> and finally create a new virtual machine with that disk.</p>",
      "<p>You should use <code>gcloud</code> tool to copy the drive to the <code>europe-central2-b</code> zone and then create a new virtual machine with this disk.</p>",
      "<p>You should use <code>gcloud</code> tool to copy the drive to the <code>europe-central2-a</code> zone, move this disk to <code>europe-central2-b</code> and finally create a new virtual machine with this disk.</p>"
    ],
    "explanation": "<p>You should create a snapshot from the disk and then create a disk from this snapshot in the <code>europe-central2-b</code> zone. Finally, create a new virtual machine with that disk. -&gt; Correct. Creating a snapshot of the disk is the quickest and easiest way to replicate an application server running on Compute Engine from one zone to another. A snapshot is a point-in-time copy of a disk that can be used to create a new disk in another zone or region. By creating a snapshot of the disk and then creating a new disk in the <code>europe-central2-b</code> zone, you can ensure that the data is replicated in as few steps as possible.</p><p><br></p><p>You should create a snapshot from the disk and then create a disk from this snapshot in the <code>europe-central2-a</code> zone, move this disk to <code>europe-central2-b</code> and finally create a new virtual machine with that disk. -&gt; Incorrect. It suggests creating a disk from the snapshot in the same zone, moving the disk to the <code>europe-central2-b</code> zone and then creating a new virtual machine with that disk. This option adds an extra step of moving the disk to the new zone, which is not necessary since a disk can be created directly in the desired zone from the snapshot.</p><p><br></p><p>You should use <code>gcloud</code> tool to copy the drive to the <code>europe-central2-b</code> zone and then create a new virtual machine with this disk. -&gt; Incorrect. It suggests using the <code>gcloud</code> tool to copy the drive to the <code>europe-central2-b</code> zone and then creating a new virtual machine with this disk. This option is not as efficient as creating a snapshot because it requires copying the entire drive, which may take longer and consume more resources.</p><p><br></p><p>You should use <code>gcloud</code> tool to copy the drive to the <code>europe-central2-a</code> zone, move this disk to <code>europe-central2-b</code> and finally create a new virtual machine with this disk. -&gt; Incorrect. It suggests using the <code>gcloud</code> tool to copy the drive to the <code>europe-central2-a</code> zone, moving the disk to <code>europe-central2-b</code> and then creating a new virtual machine with that disk. This option is also not as efficient as creating a snapshot since it involves an additional step of moving the disk between zones.</p><p><br></p><p>https://cloud.google.com/compute/docs/disks/create-snapshots</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681538,
    "question_plain": "As a cloud architect, you're helping a large organization deploy an update to their mission-critical application on App Engine. The application is used 24/7 by users worldwide, and it is crucial that the update does not affect application availability or user experience. Which of the following deployment strategies should you recommend?",
    "answers": [
      "<p>Canary Deployment: Create a new version of the application and gradually migrate user traffic from the old version to the new version.</p>",
      "<p>Blue-Green Deployment: Create a new version of the application, migrate all traffic to the new version, and delete the old version.</p>",
      "<p>Rolling Update: Update the application code in the existing version and restart the application.</p>",
      "<p>Big Bang Deployment: Replace the existing application with the new version in a single operation.</p>"
    ],
    "explanation": "<p>Canary Deployment: Create a new version of the application and gradually migrate user traffic from the old version to the new version. -&gt; Correct. This strategy allows you to gradually shift traffic from the old version to the new version, allowing for real-time monitoring and the ability to roll back if issues are detected.</p><p><br></p><p>Blue-Green Deployment: Create a new version of the application, migrate all traffic to the new version, and delete the old version. -&gt; Incorrect. Blue-Green Deployment minimizes downtime but doesn't allow for gradual roll-out or testing of the new version with a subset of users before all traffic is moved.</p><p><br></p><p>Rolling Update: Update the application code in the existing version and restart the application. -&gt; Incorrect. Rolling Update would likely cause downtime and wouldn't allow for testing the new version with a subset of users before all users are moved to the new version.</p><p><br></p><p>Big Bang Deployment: Replace the existing application with the new version in a single operation. -&gt; Incorrect. Big Bang Deployment has the highest risk because it doesn't allow for testing the new version with a subset of users and has the potential for major downtime if issues arise.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681540,
    "question_plain": "You are a cloud architect tasked with architecting a data storage solution for a media company. The company has the following data storage requirements:store large media files that are regularly accessed for the first montharchive media files that have not been accessed for over a yearensure redundancy and high availabilitykeep costs optimized based on the frequency of data accessWhich combination of storage classes should be used for Google Cloud Storage to meet these requirements?",
    "answers": [
      "<p>Use Standard Storage for the large media files and enable Object Lifecycle Management to change the storage class to Archive for files not accessed in over a year.</p>",
      "<p>Use Nearline Storage for all media files and enable Object Lifecycle Management to change the storage class to Coldline for files not accessed in over a year.</p>",
      "<p>Use Standard Storage for all media files and enable Object Lifecycle Management to change the storage class to Nearline for files not accessed in over a year.</p>",
      "<p>Use Multi-Regional Storage for the large media files and Coldline Storage for archiving files not accessed in over a year.</p>"
    ],
    "explanation": "<p>Use Standard Storage for the large media files and enable Object Lifecycle Management to change the storage class to Archive for files not accessed in over a year. -&gt; Correct. Standard Storage is appropriate for frequently accessed data and offers high performance. By enabling Object Lifecycle Management to automatically change the storage class to Archive for files not accessed in over a year, the company can optimize costs for data that is essentially archival. Archive Storage is the most cost-effective storage class for long-term archiving where data is not expected to be accessed for over a year.</p><p><br></p><p>Use Nearline Storage for all media files and enable Object Lifecycle Management to change the storage class to Coldline for files not accessed in over a year. -&gt; Incorrect. Nearline Storage is not the best choice for frequently accessed data, as it is meant for data that is accessed about once a month. Additionally, Coldline Storage is meant for colder data but is not as cost-effective as Archive Storage for data not accessed in over a year.</p><p><br></p><p>Use Standard Storage for all media files and enable Object Lifecycle Management to change the storage class to Nearline for files not accessed in over a year. -&gt; Incorrect. Nearline Storage is meant for data that is accessed less frequently (about once a month), and using it for files not accessed in over a year would not be the most cost-effective solution.</p><p><br></p><p>Use Multi-Regional Storage for the large media files and Coldline Storage for archiving files not accessed in over a year. -&gt; Incorrect. Multi-Regional Storage is primarily used for serving content to geographically distributed users and would likely be overkill for this scenario. Coldline Storage is used for archiving, but this option doesn't mention the usage of Object Lifecycle Management for automation.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681542,
    "question_plain": "Your organization uses Google Kubernetes Engine (GKE) for its microservices-based application. The number of services is expected to grow significantly over the next two years. What would be the best approach to ensure manageability and operational efficiency as the number of services increases?",
    "answers": [
      "<p>Implement a Service Mesh using Istio for fine-grained control and observability.</p>",
      "<p>Migrate to Compute Engine instances to reduce the complexity of managing Kubernetes.</p>",
      "<p>Increase the size of the GKE cluster nodes to accommodate the growing number of services.</p>",
      "<p>Implement Cloud Logging for better log management.</p>"
    ],
    "explanation": "<p>Implement a Service Mesh using Istio for fine-grained control and observability. -&gt;&nbsp;Correct. Istio is a service mesh that provides traffic management, policy enforcement, and telemetry collection. It would allow for improved control and observability of the growing number of services.</p><p><br></p><p>Migrate to Compute Engine instances to reduce the complexity of managing Kubernetes. -&gt;&nbsp;Incorrect. While Compute Engine instances might be simpler to manage than a Kubernetes cluster, they wouldn't provide the benefits of container orchestration for a growing microservices architecture.</p><p><br></p><p>Increase the size of the GKE cluster nodes to accommodate the growing number of services. -&gt;&nbsp;Incorrect. While having larger nodes could potentially accommodate more pods, it doesn't address the complexity of managing a growing number of services.</p><p><br></p><p>Implement Cloud Logging for better log management. -&gt;&nbsp;Incorrect. While proper log management is crucial, it's only one aspect of managing a growing microservices-based application and wouldn't address other aspects like traffic management or policy enforcement.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681544,
    "question_plain": "You are designing a Google Cloud architecture for an organization that operates in a heavily regulated industry. It needs to meet strict compliance requirements regarding data storage and access. Which approach should you use?",
    "answers": [
      "<p>Use customer-managed encryption keys (CMEK) and set up access controls for the keys.</p>",
      "<p>Encrypt all data at rest using Google-managed encryption keys.</p>",
      "<p>Enable VPC Service Controls to limit data exfiltration.</p>",
      "<p>Use Cloud Audit Logs to monitor and audit data access.</p>"
    ],
    "explanation": "<p>Use customer-managed encryption keys (CMEK) and set up access controls for the keys. -&gt;&nbsp;Correct. Using CMEK provides greater control over the encryption keys. Combined with proper access controls, it can help meet strict compliance requirements regarding data storage and access.</p><p><br></p><p>Encrypt all data at rest using Google-managed encryption keys. -&gt;&nbsp;Incorrect. While this provides some level of security, it might not meet the strict requirements of heavily regulated industries that often require more control over the encryption keys.</p><p><br></p><p>Enable VPC Service Controls to limit data exfiltration. -&gt;&nbsp;Incorrect. While VPC Service Controls can prevent data exfiltration, they don't control who has access to the data within the secured perimeter.</p><p><br></p><p>Use Cloud Audit Logs to monitor and audit data access. -&gt;&nbsp;Incorrect. Cloud Audit Logs can help track who did what, where, and when, but it's a reactive approach and doesn't prevent unauthorized access from happening in the first place.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681546,
    "question_plain": "As a cloud architect, you are consulting for a manufacturing company implementing an IoT solution. They plan to collect real-time sensor data from their machinery across multiple plants, with high frequency. This data will be used to monitor machine health, and it's crucial to have low-latency access to the last hour of data. Which of the following would be the most effective way to use Google Cloud Bigtable in this scenario?",
    "answers": [
      "<p>Use Cloud Bigtable to store all sensor data, and use row keys that include the machine ID and timestamp to enable fast access to recent data.</p>",
      "<p>Use Cloud Bigtable to store all sensor data, and use row keys that include the sensor type and machine ID to enable fast access to recent data.</p>",
      "<p>Use Cloud Bigtable to store only the last hour of sensor data, and use a separate system for long-term storage.</p>",
      "<p>Use Cloud Bigtable to store all sensor data, but partition the data by machine ID and timestamp.</p>"
    ],
    "explanation": "<p>Use Cloud Bigtable to store all sensor data, and use row keys that include the machine ID and timestamp to enable fast access to recent data. -&gt; Correct. Cloud Bigtable is designed for handling large quantities of high-frequency, single-keyed data like IoT sensor readings. By using row keys that include the machine ID and a reverse timestamp, you can ensure fast access to recent data for each machine.</p><p><br></p><p>Use Cloud Bigtable to store all sensor data, and use row keys that include the sensor type and machine ID to enable fast access to recent data. -&gt; Incorrect. Using row keys that include sensor type and machine ID might not allow for efficient retrieval of recent data, since it would not take into account the timestamp.</p><p><br></p><p>Use Cloud Bigtable to store only the last hour of sensor data, and use a separate system for long-term storage. -&gt; Incorrect. While it's feasible to use Cloud Bigtable for recent data and another system for long-term storage, this choice doesn't address how to structure the row keys for efficient access to recent data.</p><p><br></p><p>Use Cloud Bigtable to store all sensor data, but partition the data by machine ID and timestamp. -&gt; Incorrect. Cloud Bigtable doesn't support partitioning data in the same way as a relational database. It primarily relies on row key design for efficient data access.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681548,
    "question_plain": "As a cloud architect, you are working with a company to deploy their multi-tier application on Google Kubernetes Engine (GKE). The application includes a front-end service that needs to maintain high availability. The company has requested that at least five replicas of the front-end service run at all times to meet their availability requirements. Which field in the Kubernetes deployment manifest should you use to specify this requirement?",
    "answers": [
      "<p><code>spec.replicas</code> </p>",
      "<p><code>metadata.replicas</code> </p>",
      "<p><code>spec.pods</code> </p>",
      "<p><code>metadata.scale</code> </p>"
    ],
    "explanation": "<p><code>spec.replicas</code> -&gt;&nbsp;Correct. In a Kubernetes deployment manifest, the <code>spec.replicas</code> field is used to specify the number of desired pods to run for the deployment. In this scenario, setting <code>spec.replicas</code> to 5 ensures that at least five replicas of the front-end service are always running.</p><p><br></p><p><code>metadata.replicas</code> -&gt; Incorrect. There is no <code>replicas</code> field in the <code>metadata</code> section of a Kubernetes deployment manifest. The metadata field is used to provide data about the deployment such as its name, labels, and namespace.</p><p><br></p><p><code>spec.pods</code> -&gt; Incorrect. There is no <code>pods</code> field in the <code>spec</code> section of a Kubernetes deployment manifest. The <code>spec</code> section describes the desired state of the objects, but the number of pods is controlled with the <code>replicas</code> field.</p><p><br></p><p><code>metadata.scale</code> -&gt; Incorrect. There is no <code>scale</code> field in the <code>metadata</code> section of a Kubernetes deployment manifest. The <code>metadata</code> field provides data about the deployment itself, not about its scaling configuration.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681550,
    "question_plain": "As a cloud architect, you are working with a media company that runs a complex application on a Kubernetes cluster in Google Kubernetes Engine (GKE). After evaluating the application's resource usage over the past few months, you've determined that the cluster is over-provisioned and you need to resize it to save costs. The application is stateless, and there are no user sessions to maintain. The application is also globally distributed and must maintain high availability during the resizing process. What is the best approach to resize the cluster?",
    "answers": [
      "<p>Use the <code>gcloud container clusters resize</code> command to reduce the number of nodes in the cluster.</p>",
      "<p>Delete the existing cluster and create a new one with fewer nodes.</p>",
      "<p>Manually remove nodes from the existing cluster until you reach the desired size.</p>",
      "<p>Adjust the number of vCPUs and memory allocated to each node in the cluster.</p>"
    ],
    "explanation": "<p>Use the <code>gcloud container clusters resize</code> command to reduce the number of nodes in the cluster. -&gt; Correct. This command allows you to safely reduce the size of the cluster. Google Kubernetes Engine takes care of gracefully draining nodes and ensuring that pods are properly rescheduled onto other nodes.</p><p><br></p><p>Delete the existing cluster and create a new one with fewer nodes. -&gt; Incorrect. Deleting the existing cluster and creating a new one would cause downtime for the application, which is not acceptable for a high-availability requirement.</p><p><br></p><p>Manually remove nodes from the existing cluster until you reach the desired size. -&gt; Incorrect. Manually removing nodes from the cluster might lead to errors and inconsistencies. It is not recommended.</p><p><br></p><p>Adjust the number of vCPUs and memory allocated to each node in the cluster. -&gt;&nbsp;Incorrect. Adjusting the number of vCPUs and memory allocated to each node would change the capacity of each node, but not the number of nodes in the cluster. This might also lead to over-provisioning or under-provisioning for certain workloads.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681552,
    "question_plain": "A marketing company creates a lot of landing pages (static websites). As a cloud architect, which storage service would you recommend in this case to minimize costs?",
    "answers": [
      "<p>Cloud Storage</p>",
      "<p>Cloud SDK</p>",
      "<p>Cloud Endpoints</p>",
      "<p>Cloud Datastore</p>",
      "<p>Compute Engine + Persistent Disk</p>"
    ],
    "explanation": "<p>Cloud Storage -&gt; Correct. It is a Google Cloud service designed for storing and accessing unstructured data such as images, videos, and static web content. It offers a simple and cost-effective solution for storing large amounts of data and making it accessible via the internet. In the given scenario, since the marketing company creates a lot of landing pages (static websites), the content is typically unstructured and can be stored in a simple file-based system. Cloud Storage is an ideal solution for storing and serving static web content as it allows you to host static websites directly from a Cloud Storage bucket. With this approach, you can minimize costs and reduce the complexity of managing web servers.</p><p><br></p><p>Cloud SDK -&gt; Incorrect. It is a command-line interface tool for managing Google Cloud services and resources. It is not a storage service and cannot be used to store or serve static web content.</p><p><br></p><p>Cloud Endpoints -&gt; Incorrect. It is a Google Cloud service designed for creating, deploying, and managing APIs. It is not a storage service and cannot be used to store or serve static web content.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. It is a NoSQL document database designed for storing non-relational data. It is not optimized for storing and serving static web content and is more suited for applications that require complex data structures.</p><p><br></p><p>Compute Engine + Persistent Disk -&gt; Incorrect. It is a more complex and expensive solution for serving static web content compared to Cloud Storage. Compute Engine is designed for running virtual machines in the cloud and is more suitable for running complex applications and workloads that require a higher level of customization and control.</p><p><br></p><p>https://cloud.google.com/storage/docs/hosting-static-website</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681554,
    "question_plain": "You have a web application operating on a Managed Instance Group, which receives a high volume of requests every minute. Your objective is to apply patches to the application without reducing the number of instances in the MIG. What action should you take?",
    "answers": [
      "<p>You should perform a rolling-action start-update with <code>max-unavailable</code> set to 0 and <code>max-surge</code> set to 1.</p>",
      "<p>You should perform a rolling-action start-update with <code>max-unavailable</code> set to 1 and <code>max-surge</code> set to 0.</p>",
      "<p>You should deploy the update in a new Managed Instance Group and add it as a backend service to the existing production Load Balancer. Then remove the old Managed Instance Group from the Load Balancer backend and remove the group.</p>",
      "<p>You should update the existing Managed Instance Group to point to a new instance template containing the updated version. Terminate all existing instances in this group and wait until they are all replaced by new instances created from the new template.</p>"
    ],
    "explanation": "<p>You should perform a rolling-action start-update with <code>max-unavailable</code> set to 0 and <code>max-surge</code> set to 1. -&gt; Correct. Performing a rolling-action start-update with <code>max-unavailable</code> set to 0 and <code>max-surge</code> set to 1 allows you to apply patches to the application without reducing the number of instances in the Managed Instance Group (MIG). The rolling update process will replace instances one by one while maintaining the desired number of instances. With <code>max-unavailable</code> set to 0, no instances will be unavailable during the update process, ensuring continuous availability.</p><p><br></p><p>You should perform a rolling-action start-update with <code>max-unavailable</code> set to 1 and <code>max-surge</code> set to 0. -&gt;&nbsp;Incorrect. Performing a rolling-action start-update with <code>max-unavailable</code> set to 1 and <code>max-surge</code> set to 0 means that during the update process, one instance will be unavailable at a time, potentially reducing the number of instances in the MIG. This contradicts the objective of applying patches without reducing the number of instances.</p><p><br></p><p>You should deploy the update in a new Managed Instance Group and add it as a backend service to the existing production Load Balancer. Then remove the old Managed Instance Group from the Load Balancer backend and remove the group. -&gt;&nbsp;Incorrect. It involves creating a new group and Load Balancer configuration, which is not necessary if the goal is to apply patches without reducing the number of instances in the current MIG.</p><p><br></p><p>You should update the existing Managed Instance Group to point to a new instance template containing the updated version. Terminate all existing instances in this group and wait until they are all replaced by new instances created from the new template. -&gt;&nbsp;Incorrect. This approach does not align with the objective of applying patches without reducing the number of instances.</p><p><br></p><p>https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681556,
    "question_plain": "A mobile game developer wants to launch a new mobile game that will be available to users around the world. The game requires RDBMS for storing player profiles. As a cloud architect, which storage service should&nbsp;you recommend so they can scale to a global audience with minimal configuration updates?",
    "answers": [
      "<p>Cloud Spanner</p>",
      "<p>Cloud SQL</p>",
      "<p>Cloud Datastore</p>",
      "<p>Cloud Firestore</p>"
    ],
    "explanation": "<p>Cloud Spanner -&gt; Correct. It is a fully managed, globally distributed, and horizontally scalable relational database service, specifically designed to scale horizontally across multiple regions and zones while maintaining strong consistency and high availability. It is suitable for applications that require a relational data model and ACID transactions. On the other hand, Cloud SQL, Cloud Datastore, and Cloud Firestore are also storage services provided by Google Cloud Platform (GCP), but they are designed for different use cases.</p><p><br></p><p>Cloud SQL -&gt; Incorrect. It is a fully managed relational database service that allows developers to choose the database engine that best fits their application requirements (e.g., MySQL, PostgreSQL, SQL Server). It is suitable for small to medium-sized workloads that require a traditional relational database.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. It is a NoSQL document database service that provides a schemaless data model and eventually consistent reads, making it suitable for storing non-relational data such as user-generated content, metadata, and logs.</p><p><br></p><p>Cloud Firestore -&gt; Incorrect. It is a NoSQL document database service that provides real-time updates, offline support, and mobile sync capabilities. It is designed for mobile and web application developers who need to store and synchronize data across multiple devices and platforms.</p><p><br></p><p>https://cloud.google.com/spanner/docs/quickstart-console</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681558,
    "question_plain": "As a cloud architect, you are working with a global gaming company that needs a database for their new real-time, multi-player game. They require low latency, high transaction throughput, and the ability to scale to millions of users worldwide. Which of the following approaches would be the most effective way to use Google Cloud Spanner to meet these requirements?",
    "answers": [
      "<p>Use Cloud Spanner's multi-region configurations and configure the game to direct traffic based on user location.</p>",
      "<p>Create a single-region Cloud Spanner instance for the lowest possible latency and manually partition the database by user location.</p>",
      "<p>Create a single Cloud Spanner instance and rely on Google's global network for low latency.</p>",
      "<p>Set up multiple Cloud Spanner instances in each region where the game has users, and manually replicate data between instances.</p>"
    ],
    "explanation": "<p>Use Cloud Spanner's multi-region configurations and configure the game to direct traffic based on user location. -&gt; Correct. Cloud Spanner's multi-region configurations allow for low latency, high availability, and high throughput. Directing traffic based on user location helps to reduce latency.</p><p><br></p><p>Create a single-region Cloud Spanner instance for the lowest possible latency and manually partition the database by user location. -&gt; Incorrect. Creating a single-region Cloud Spanner instance could introduce higher latencies for users who are far from the region, and manually partitioning the database by user location can be complex and difficult to manage.</p><p><br></p><p>Create a single Cloud Spanner instance and rely on Google's global network for low latency. -&gt; Incorrect. While Cloud Spanner provides high scalability and Google's global network can deliver low latencies, for a global user base a multi-region configuration would be better suited to provide consistently low latencies across the world.</p><p><br></p><p>Set up multiple Cloud Spanner instances in each region where the game has users, and manually replicate data between instances. -&gt; Incorrect. Setting up multiple Cloud Spanner instances and manually replicating data between them would be complex and likely more expensive. It doesn't leverage Cloud Spanner's built-in ability to replicate data across regions.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681560,
    "question_plain": "As a cloud architect, you need to establish connection between your on-premises network and Google Cloud. Your company will need 5 Gbps of bandwidth in total between the on-premises data center and Google Cloud. The traffic may be split between multiple connections. How many VPN endpoints will you need?",
    "answers": ["<p>2</p>", "<p>1</p>", "<p>3</p>", "<p>4</p>"],
    "explanation": "<p>2 -&gt;&nbsp;Correct. For Google Cloud VPN, each VPN tunnel can provide up to 3 Gbps of throughput, which means that you will need at least two VPN endpoints to achieve the required 5 Gbps of bandwidth.</p><p><br></p><p>https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview#network-bandwidth</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681562,
    "question_plain": "When planning your migration, you find out that some members of the network management team will need to be able to manage all network components, but other team members will only need read access. What mechanism should you use to control this?",
    "answers": [
      "<p>IAM&nbsp;roles</p>",
      "<p>Firewall rules</p>",
      "<p>Virtual Private Clouds</p>",
      "<p>Virtual Private Networks</p>"
    ],
    "explanation": "<p>IAM&nbsp;roles -&gt; Correct. IAM (Identity and Access Management) roles are a mechanism in cloud computing that allow you to manage permissions for different users or groups of users within your network. With IAM roles, you can grant specific access privileges to different members of your team based on their role, such as read-only access or full access to manage all network components.</p><p><br></p><p>Firewall rules -&gt; Incorrect. Firewall rules are used to control incoming and outgoing network traffic, but they do not provide access control for individual users or groups of users.</p><p><br></p><p>Virtual Private Clouds -&gt; Incorrect. Virtual Private Clouds (VPCs) is a network-related technology, but it is not directly related to access control for individual users or groups of users. VPCs are used to create isolated virtual networks within a cloud computing environment.</p><p><br></p><p>Virtual Private Networks -&gt; Incorrect. Virtual Private Networks (VPNs) is a network-related technology, but it is not directly related to access control for individual users or groups of users. VPNs are used to establish secure connections between different networks or devices.</p><p><br></p><p>https://cloud.google.com/iam/docs/understanding-roles</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681564,
    "question_plain": "Your company's data science team anticipates a significant increase in the number and complexity of machine learning (ML) models they plan to train over the next year. Currently, the team uses Compute Engine instances for training. What would be the best approach to accommodate this increase?",
    "answers": [
      "<p>Use AI Platform Training to offload and scale model training tasks.</p>",
      "<p>Upgrade the Compute Engine instances to have more CPUs and more memory.</p>",
      "<p>Use Cloud Functions to train individual models in response to events.</p>",
      "<p>Increase the disk size for the Compute Engine instances to store more training data.</p>"
    ],
    "explanation": "<p>Use AI Platform Training to offload and scale model training tasks. -&gt;&nbsp;Correct. AI Platform Training is a managed service that allows data scientists to run ML training jobs on Google Cloud at scale and without the need to manage infrastructure.</p><p><br></p><p>Upgrade the Compute Engine instances to have more CPUs and more memory. -&gt;&nbsp;Incorrect. This solution, also known as vertical scaling, could accommodate some growth, but it has limitations and can lead to overprovisioning and higher costs. Also, it doesn't fully utilize the benefits of distributed ML training.</p><p><br></p><p>Use Cloud Functions to train individual models in response to events. -&gt;&nbsp;Incorrect. Cloud Functions is primarily designed to handle lightweight, event-driven serverless compute tasks and wouldn't be appropriate for resource-intensive ML training jobs.</p><p><br></p><p>Increase the disk size for the Compute Engine instances to store more training data. -&gt;&nbsp;Incorrect. Increasing disk size may allow more data to be stored locally but does not address the computational requirements of training more complex ML models.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681566,
    "question_plain": "Your company has a robust e-commerce application running on Compute Engine instances and using Cloud Storage. As the business expands globally, you notice latency issues affecting customers in different regions. What approach should you consider to improve your solution?",
    "answers": [
      "<p>Implement a Content Delivery Network (CDN) using Cloud CDN to cache static content closer to users.</p>",
      "<p>Increase the size of the Compute Engine instances to enhance the application's performance.</p>",
      "<p>Move the application to App Engine to leverage its automatic scaling features.</p>",
      "<p>Migrate from Cloud Storage to Persistent Disk for faster I/O operations.</p>"
    ],
    "explanation": "<p>Implement a Content Delivery Network (CDN) using Cloud CDN to cache static content closer to users. -&gt;&nbsp;Correct. Implementing a CDN would cache static content closer to the users, significantly reducing the latency experienced by users in different geographical locations.</p><p><br></p><p>Increase the size of the Compute Engine instances to enhance the application's performance. -&gt;&nbsp;Incorrect. While this could marginally improve performance, it would not significantly impact the latency issues experienced by global customers.</p><p><br></p><p>Move the application to App Engine to leverage its automatic scaling features. -&gt;&nbsp;Incorrect. Although App Engine provides automatic scaling, this doesn't directly address the latency issue for global users.</p><p><br></p><p>Migrate from Cloud Storage to Persistent Disk for faster I/O operations. -&gt;&nbsp;Incorrect. Persistent Disks may provide faster I/O for specific use cases, but they wouldn't solve the latency problem for users in different geographical locations.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681568,
    "question_plain": "Your company has a multi-tier web application running on Google Cloud. You've been tasked with planning for future improvements. User feedback indicates that the application occasionally experiences latency issues, which you suspect is due to increasing database load. What would be the best approach to address this issue and improve the application?",
    "answers": [
      "<p>Migrate the database from Cloud SQL to Cloud Spanner to take advantage of horizontal scaling.</p>",
      "<p>Increase the size of the Cloud SQL instance to accommodate the higher load.</p>",
      "<p>Implement a caching layer using Cloud Memorystore to reduce database load.</p>",
      "<p>Use Cloud Dataflow to preprocess and aggregate data before storing it in Cloud SQL.</p>"
    ],
    "explanation": "<p>Migrate the database from Cloud SQL to Cloud Spanner to take advantage of horizontal scaling. -&gt;&nbsp;Correct. Cloud Spanner is Google's horizontally scalable, globally-distributed database service, which is designed to handle high transaction loads with low latency.</p><p><br></p><p>Increase the size of the Cloud SQL instance to accommodate the higher load. -&gt;&nbsp;Incorrect. This is an example of vertical scaling. While it could provide some temporary relief, it's not a long-term solution as it has limitations and can lead to overprovisioning and higher costs.</p><p><br></p><p>Implement a caching layer using Cloud Memorystore to reduce database load. -&gt;&nbsp;Incorrect. While adding a caching layer could help improve performance for read-heavy workloads, it may not solve issues with write-heavy loads or complex queries, and it adds complexity to the application.</p><p><br></p><p>Use Cloud Dataflow to preprocess and aggregate data before storing it in Cloud SQL. -&gt;&nbsp;Incorrect. While this could potentially reduce the amount of data written to Cloud SQL, it might not solve the problem if the database load is due to a large number of queries or complex joins, and it adds complexity to the data processing pipeline.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681570,
    "question_plain": "As a cloud architect, you are planning to run stateful applications in Kubernetes Engine. What should you use to support stateful applications?",
    "answers": [
      "<p>StatefulSets</p>",
      "<p>Pods</p>",
      "<p>StatefulPods</p>",
      "<p>DeamonSet</p>",
      "<p>ReplicaSet</p>"
    ],
    "explanation": "<p>StatefulSets -&gt; Correct. StatefulSets is a Kubernetes feature that provides guarantees about the ordering and uniqueness of pods when deploying stateful applications. It ensures that each pod in the StatefulSet gets a stable network identity, which is necessary for stateful applications such as databases. This means that the StatefulSet creates a unique hostname and persistent storage for each pod in the set, and ensures that the pods are created and deleted in a predictable order.</p><p><br></p><p>Pods -&gt; Incorrect. Pods, which are the smallest deployable units in Kubernetes, are not suitable for stateful applications because they do not provide guarantees about the ordering and uniqueness of pods. Pods are ephemeral, meaning they are created and destroyed frequently in Kubernetes, which can lead to data loss or inconsistencies for stateful applications.</p><p><br></p><p>StatefulPods -&gt; Incorrect. StatefulPods is not a valid Kubernetes feature or resource, so it is not a correct answer to the question.</p><p><br></p><p>DeamonSet -&gt; Incorrect. DeamonSet is a Kubernetes feature that ensures that all (or some) nodes run a copy of a pod. It is useful for running background or monitoring tasks on each node in a cluster, but it is not specifically designed for stateful applications.</p><p><br></p><p>ReplicaSet -&gt; Incorrect. ReplicaSet is a Kubernetes feature that ensures a specified number of replicas of a pod are running at any given time. While it can be used to ensure high availability and scalability of stateful applications, it does not provide guarantees about the ordering and uniqueness of pods, which are necessary for stateful applications</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/concepts/statefulset</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681572,
    "question_plain": "As a cloud architect, you are working with a client who wants to migrate a large number of files from their on-premises data center to a Cloud Storage bucket. The client's on-premises data center is located in a region with a slow and unstable network connection. They would like the transfer to happen as quickly as possible, while also ensuring that the transfer can continue from where it left off in case of network interruptions. Which gsutil command option should you recommend for this scenario?",
    "answers": [
      "<p><code>-m</code> </p>",
      "<p><code>-c</code> </p>",
      "<p><code>-r</code> </p>",
      "<p><code>-o</code> </p>"
    ],
    "explanation": "<p><code>-m</code> -&gt; Correct. It stands for \"multi-threading\". This option causes gsutil to perform operations (like cp and rsync) in parallel, which can significantly speed up the upload process. It also helps in case of network interruptions, as the already transferred files are not re-transferred.</p><p><br></p><p><code>-c</code> -&gt; Incorrect. It is used for continuing a transfer after a process has been interrupted. While this is helpful, it does not speed up the process, and it can be combined with -m for better performance in this scenario.</p><p><br></p><p><code>-r</code> -&gt; Incorrect. It is used for copying directories recursively, but it does not provide the benefits needed in this scenario.</p><p><br></p><p><code>-o</code> -&gt; Incorrect. This option is used to specify options for gsutil, but it doesn't help to speed up the process or handle network interruptions.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681574,
    "question_plain": "Your compliance team is concerned about using a public cloud service because other companies will be running their systems in the same cloud. You assure them that your company's resources will be isolated and inaccessible to others. Which resource is used for this purpose?",
    "answers": [
      "<p>Virtual Private Clouds - VPCs</p>",
      "<p>CIDR&nbsp;blocks</p>",
      "<p>Cloud Interconnect</p>",
      "<p>Cloud VPN</p>"
    ],
    "explanation": "<p>Virtual Private Clouds - VPCs -&gt; Correct. Virtual Private Clouds (VPCs) are a mechanism in cloud computing that allow you to create isolated virtual networks within a public cloud environment. Each VPC is logically separated from other VPCs and the public internet, providing a secure and isolated environment for your company's resources. With a VPC, you can define your own IP address range, create subnets, and configure routing tables, security groups, and network access control lists (ACLs) to control access to your resources. You can also use VPC peering to connect VPCs within the same cloud provider, or use a VPN connection to establish a secure connection between your VPC and your on-premises network.</p><p><br></p><p>CIDR&nbsp;blocks -&gt; Incorrect. CIDR blocks are a method for allocating and addressing IP networks, but they do not provide isolation or security for your resources within a public cloud environment.</p><p><br></p><p>Cloud Interconnect -&gt; Incorrect. Cloud Interconnect is a method for establishing secure connection between your on-premises network and a public cloud environment, but it does not provide isolation or security for your resources within the cloud environment itself.</p><p><br></p><p>Cloud VPN -&gt; Incorrect. Cloud VPN is a method for establishing secure connection between your on-premises network and a public cloud environment, but it does not provide isolation or security for your resources within the cloud environment itself.</p><p><br></p><p>https://cloud.google.com/vpc/docs/vpc</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681576,
    "question_plain": "A SaaS solution for enterprise customers needs to be updated. Many components of the service are stateful and the system wasn't designed to allow incremental rollout of new code. The entire environment has to be running the same version of the deployed code. What implementation strategy would you recommend?",
    "answers": [
      "<p>Blue/Green deployment strategy</p>",
      "<p>Canary deployment strategy</p>",
      "<p>Rolling deployment strategy</p>",
      "<p>A/B deployment strategy</p>"
    ],
    "explanation": "<p>Blue/Green deployment strategy -&gt; Correct. In a Blue/Green deployment, two environments (the \"blue\" and the \"green\") are maintained. One of them is live (let's say blue) and serving users while the other (green) is used for deploying and testing the new version of the software. Once the new version has been fully tested and is ready to go live, the router is switched to redirect all traffic to the green environment. Therefore, all instances are running the same version of the software at all times, which suits the stated requirement.</p><p><br></p><p>Canary deployment strategy -&gt;&nbsp;Incorrect. This strategy involves releasing the new version of software to a small subset of users before rolling it out to the entire user base. This approach wouldn't work in this scenario since the system needs to run the same version of the code across the entire environment, and canary deployment inherently involves running multiple versions at the same time.</p><p><br></p><p>Rolling deployment strategy -&gt;&nbsp;Incorrect. In a rolling deployment, the new version of software is gradually rolled out to all instances, replacing the old version. But during the rollout process, different instances will be running different versions of the software, which contradicts the system requirements described.</p><p><br></p><p>A/B deployment strategy -&gt;&nbsp;Incorrect. Similar to the Canary strategy, A/B testing involves running two (or more) versions of the software simultaneously to compare their performance. This also doesn't fit the requirement of running the same code version across all instances.</p><p><br></p><p>https://cloud.google.com/architecture/implementing-deployment-and-testing-strategies-on-gke#perform_a_bluegreen_deployment</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681578,
    "question_plain": "Your company offers online services that collect data about users and operates in North America. You want to start a business in Europe. What regulations must your company meet?",
    "answers": [
      "<p>GDPR</p>",
      "<p>COPPA</p>",
      "<p>HIPAA/HITECH</p>",
      "<p>SOX</p>"
    ],
    "explanation": "<p>GDPR -&gt; Correct. It is a regulation in EU law on data protection and privacy for all individuals within the European Union (EU) and the European Economic Area (EEA). It also addresses the export of personal data outside the EU and EEA. Any company that collects data from individuals in the EU, regardless of where the company is based, must comply with GDPR.</p><p><br></p><p>COPPA -&gt;&nbsp;Incorrect. (Children's Online Privacy Protection Act) is a U.S. law that applies to websites or online services that collect personal information from children under 13. </p><p><br></p><p>HIPAA/HITECH -&gt;&nbsp;Incorrect. (Health Insurance Portability and Accountability Act/Health Information Technology for Economic and Clinical Health Act) are U.S. laws that govern the use and disclosure of protected health information (PHI). </p><p><br></p><p>SOX -&gt; Incorrect. (Sarbanes-Oxley Act) is a U.S. law that governs financial reporting and accounting practices for publicly traded companies.</p><p><br></p><p>https://cloud.google.com/privacy/gdpr</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681580,
    "question_plain": "In your role as a cloud architect, you're preparing to transition your on-site data warehouse to Google Cloud, utilizing BigQuery. You have to create a presentation for the management team to explain the cost structure of BigQuery. There are two primary elements to consider when it comes to the pricing of BigQuery. Select the costs you incur when using BigQuery.",
    "answers": [
      "<p>The cost to process queries&nbsp;(analysis pricing).</p>",
      "<p>The cost to store data that you load into BigQuery (storage pricing).</p>",
      "<p>The cost of Identity and Access Management (IAM pricing).</p>",
      "<p>The cost of viewing table schemas (schema pricing).</p>"
    ],
    "explanation": "<p>The cost to process queries&nbsp;(analysis pricing). -&gt;&nbsp;Correct. BigQuery charges based on the amount of data processed by each query. This is often referred to as \"analysis\" or \"query\" pricing. BigQuery does not charge for queries that access cached results or metadata operations.</p><p><br></p><p>The cost to store data that you load into BigQuery (storage pricing). -&gt;&nbsp;Correct. BigQuery charges for storing data. The costs are incurred monthly and vary depending on the amount of data stored.</p><p><br></p><p>The cost of Identity and Access Management (IAM pricing). -&gt;&nbsp;Incorrect. Google Cloud Platform does not charge for IAM. IAM is a feature provided by Google Cloud to manage access to resources, and there's no direct cost associated with it.</p><p><br></p><p>The cost of viewing table schemas (schema pricing). -&gt;&nbsp;Incorrect. There is no cost associated with viewing table schemas. BigQuery does not charge for metadata operations, which includes viewing table schemas.</p><p><br></p><p>https://cloud.google.com/bigquery/pricing</p>",
    "correct_response": ["a", "b"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 71681582,
    "question_plain": "As a cloud architect, you are responsible for preparing migration strategy. Your company needs to mount a shared filesystem to Compute Engine instances. Which GCP&nbsp;service should you use?",
    "answers": [
      "<p>Cloud Filestore</p>",
      "<p>Cloud Firestore</p>",
      "<p>Local SSD</p>",
      "<p>Cloud Storage</p>"
    ],
    "explanation": "<p>Cloud Filestore -&gt; Correct. It is the best GCP service to use when mounting a shared file system to Compute Engine instances. Cloud Filestore is a fully-managed file storage service that provides a high-performance, NFSv3-compatible file system. It is ideal for enterprise applications that require high-performance file storage for applications that need low-latency access to a shared file system.</p><p><br></p><p>Cloud Firestore -&gt; Incorrect. It is a NoSQL document database that is optimized for mobile and web application development. It is not designed for file storage.</p><p><br></p><p>Local SSD -&gt; Incorrect. It is a type of temporary, high-performance storage that is directly attached to a virtual machine instance. It is not suitable for shared file systems that need to be accessed by multiple instances.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. It is a fully-managed object storage service that is designed for storing and retrieving large unstructured data objects. While it can be used for file storage, it is not optimized for high-performance file sharing among multiple instances.</p><p><br></p><p>https://cloud.google.com/filestore/docs/concepts</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681584,
    "question_plain": "As a cloud architect, you are working with a client who operates a popular web service. Their traffic patterns are highly variable, and they need a deployment strategy that will scale in response to the load on their system. The client wants to deploy a Managed Instance Group (MIG) with a minimum of 5 instances and a maximum of 50 instances. Which of the following strategies should you use to meet these requirements?",
    "answers": [
      "<p>Deploy the MIG and configure it with an Autoscaler, setting the target utilization level to match the desired load.</p>",
      "<p>Deploy the MIG without Autoscaler, manually adding instances as needed.</p>",
      "<p>Deploy the MIG with a minimum of 5 instances, but do not specify a maximum number of instances.</p>",
      "<p>Deploy the MIG with Autoscaler, and use Instance Templates to specify the maximum number of instances.</p>"
    ],
    "explanation": "<p>Deploy the MIG and configure it with an Autoscaler, setting the target utilization level to match the desired load. -&gt; Correct. The Autoscaler in a Managed Instance Group automatically adds or removes instances based on the load. By setting a target utilization level, the Autoscaler will maintain the number of instances between the minimum and maximum number of instances defined.</p><p><br></p><p>Deploy the MIG without Autoscaler, manually adding instances as needed. -&gt; Incorrect. This approach does not satisfy the requirement for automatic scaling. The number of instances would need to be manually adjusted based on the load, which is not practical or efficient.</p><p><br></p><p>Deploy the MIG with a minimum of 5 instances, but do not specify a maximum number of instances. -&gt; Incorrect. While you can deploy a MIG with a specified minimum number of instances, without defining a maximum number of instances and using an Autoscaler, the number of instances could scale beyond what is required or affordable for the client.</p><p><br></p><p>Deploy the MIG with Autoscaler, and use Instance Templates to specify the maximum number of instances. -&gt; Incorrect. Instance Templates define the properties of the instances that will be created in a MIG, but they do not control the number of instances. The Autoscaler uses the Instance Template to create new instances, but the maximum number of instances must be defined in the Autoscaler configuration, not in the Instance Template.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681586,
    "question_plain": "As a cloud architect, you need to plan your enterprise network in Google Cloud. Why should enterprises use custom VPC networks rather than the default network? (select 2)",
    "answers": [
      "<p>Custom VPC networks integrate better with existing IP address management schemes. The default networks use the same set of internal IP ranges. IP ranges might overlap when connected with your on-premises corporate networks.</p>",
      "<p>Because you cannot connect two auto mode VPC networks to each other using VPC Network Peering because their subnets use identical primary IP ranges.</p>",
      "<p>Because in custom mode you cannot choose unique, descriptive names for custom mode subnets.</p>",
      "<p>Because in custom mode you automatically get some pre-populated firewall rules.</p>"
    ],
    "explanation": "<p>Custom VPC networks integrate better with existing IP address management schemes. The default networks use the same set of internal IP ranges. IP ranges might overlap when connected with your on-premises corporate networks. -&gt; Correct. Custom VPC networks can be designed to fit an organization's specific IP address management scheme, which may help avoid IP range overlaps when connecting with on-premises networks. The default network uses a predefined set of internal IP ranges that might conflict with the organization's IP addresses.</p><p><br></p><p>Because you cannot connect two auto mode VPC networks to each other using VPC Network Peering because their subnets use identical primary IP ranges. -&gt;&nbsp;Correct. Auto mode VPC networks use a specific range of IP addresses that are the same across all networks. This makes it impossible to peer two auto mode networks with identical primary IP ranges, which limits network connectivity options.</p><p><br></p><p>Because in custom mode you cannot choose unique, descriptive names for custom mode subnets. -&gt; Incorrect. In custom mode, subnets can have unique, descriptive names.</p><p><br></p><p>Because in custom mode you automatically get some pre-populated firewall rules. -&gt; Incorrect. In custom mode, users must manually create all firewall rules.</p><p><br></p><p>https://cloud.google.com/architecture/best-practices-vpc-design#custom-mode</p>",
    "correct_response": ["a", "b"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 71681588,
    "question_plain": "Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdfAs the Data Compliance Officer for Mountkirk Games, your responsibility is to safeguard customers' personally identifiable information (PII). The company seeks to generate anonymized usage reports for its new game and implement a data deletion policy for PII after a designated timeframe. Your role is to ensure compliance while considering the business and technical requirements with minimal cost implications. What course of action would you recommend?",
    "answers": [
      "<p>You should archive audit logs in BigQuery, and generate reports using Google Data Studio.</p>",
      "<p>You should archive audit logs in Cloud Storage, and manually generate reports.</p>",
      "<p>You should write a Cloud Logging filter to export specific date ranges to Pub/Sub.</p>",
      "<p>You should archive user logs on a locally attached persistent disk, and cat them to a text file for auditing.</p>"
    ],
    "explanation": "<p>You should archive audit logs in BigQuery, and generate reports using Google Data Studio. -&gt;&nbsp;Correct. BigQuery allows easy querying for report generation, with low storage costs.</p><p><br></p><p>You should archive audit logs in Cloud Storage, and manually generate reports. -&gt; Incorrect. Cloud Storage is an object store with no query language access for report generation.</p><p><br></p><p>You should write a Cloud Logging filter to export specific date ranges to Pub/Sub. -&gt; Incorrect. It does not address log storage for data retention.</p><p><br></p><p>You should archive user logs on a locally attached persistent disk, and cat them to a text file for auditing. -&gt; Incorrect. Long term storage in persistent disks is expensive.</p><p><br></p><p>https://cloud.google.com/logging/docs/audit</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681590,
    "question_plain": "Refer to the Helicopter Racing League (HRL) case study for this question: https://services.google.com/fh/files/blogs/master_case_study_helicopter_racing_league.pdfThe Helicopter Racing League (HRL) is seeking your assistance in expanding the reach of their existing recorded video content to attract new fans in emerging regions. In light of the business and technical requirements of HRL, what actions do you need to take?",
    "answers": [
      "<p>You should use Cloud CDN to cache the video content from HRL’s existing public cloud provider.</p>",
      "<p>You should serve the video content directly from a multi-region Cloud Storage bucket.</p>",
      "<p>You should use Apigee Edge to cache the video content from HRL’s existing public cloud provider.</p>",
      "<p>You should replicate the video content in Google Kubernetes Engine clusters in regions close to the fans.</p>"
    ],
    "explanation": "<p>You should use Cloud CDN to cache the video content from HRL’s existing public cloud provider. -&gt; Correct. Cloud CDN can be used to cache data hosted on other cloud providers and supports large objects such as video.</p><p><br></p><p>You should serve the video content directly from a multi-region Cloud Storage bucket. -&gt;&nbsp;Incorrect. A multi-region bucket does not serve all global areas with similar latency.</p><p><br></p><p>You should use Apigee Edge to cache the video content from HRL’s existing public cloud provider. -&gt; Incorrect. Apigee Edge is not designed to cache data larger than 512 KB.</p><p><br></p><p>You should replicate the video content in Google Kubernetes Engine clusters in regions close to the fans. -&gt; Incorrect. Replicating the video content introduces unnecessary complexity.</p><p><br></p><p>https://cloud.google.com/storage/docs/locations#considerations</p><p>https://cloud.google.com/cdn/docs/caching#maximum-size</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681592,
    "question_plain": "As a cloud architect, you are assisting a client with an e-commerce application hosted on Google Cloud Platform (GCP). The client's application is expected to receive an increased volume of traffic during an upcoming sale. They want to distribute incoming traffic across multiple Compute Engine instances using Cloud Load Balancer. Additionally, they want to ensure that if any instance is unresponsive or unable to handle requests, it is automatically removed from the pool of available instances. Which of the following configurations would best achieve this?",
    "answers": [
      "<p>Configure an HTTP(S) Load Balancer with global backend services and set up an HTTP health check.</p>",
      "<p>Configure a Network Load Balancer and set up an HTTPS health check.</p>",
      "<p>Configure a TCP/SSL Proxy Load Balancer and set up a TCP health check.</p>",
      "<p>Configure a Network Load Balancer and set up a TCP health check.</p>"
    ],
    "explanation": "<p>Configure an HTTP(S) Load Balancer with global backend services and set up an HTTP health check. -&gt; Correct. An HTTP(S) Load Balancer with global backend services allows traffic to be distributed across instances in multiple regions. Setting up an HTTP health check will ensure that only responsive instances receive traffic.</p><p><br></p><p>Configure a Network Load Balancer and set up an HTTPS health check. -&gt; Incorrect. Network Load Balancer is a good option for distributing traffic, but it is not designed for global traffic distribution and does not support HTTP(S) health checks.</p><p><br></p><p>Configure a TCP/SSL Proxy Load Balancer and set up a TCP health check. -&gt; Incorrect. TCP/SSL Proxy Load Balancer can be used to distribute TCP traffic, but in this case, the application is likely to be HTTP(S) based as it is an e-commerce site, making HTTP(S) Load Balancer a more appropriate choice.</p><p><br></p><p>Configure a Network Load Balancer and set up a TCP health check. -&gt;&nbsp;Incorrect. Network Load Balancer would be beneficial for distributing TCP and UDP traffic. However, for an e-commerce application which is HTTP(S) based, HTTP(S) Load Balancer with HTTP health checks would be more appropriate.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681594,
    "question_plain": "Refer to the TerramEarth case study for this question: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdfUpon evaluating TerramEarth's business requirements to minimize downtime, it was determined that a substantial amount of time-saving could be achieved by decreasing customers' wait time for parts. Consequently, the decision has been made to concentrate efforts on reducing the aggregate reporting time of three weeks. What changes to the company's processes would you suggest?",
    "answers": [
      "<p>Increase fleet cellular connectivity to 80%, migrate from FTP to streaming transport, and develop machine learning analysis of metrics.</p>",
      "<p>Migrate from CSV to binary format, migrate from FTP to SFTP transport, and develop machine learning analysis of metrics.</p>",
      "<p>Migrate from FTP to streaming transport, migrate from CSV to binary format, and develop machine learning analysis of metrics.</p>",
      "<p>Migrate from FTP to SFTP transport, develop machine learning analysis of metrics, and increase dealer local inventory by a fixed factor.</p>"
    ],
    "explanation": "<p>Increase fleet cellular connectivity to 80%, migrate from FTP to streaming transport, and develop machine learning analysis of metrics. -&gt; Correct. Using cellular connectivity will greatly improve the freshness of data used for analysis from where it is now, collected when the machines are in for maintenance. Streaming transport instead of periodic FTP will tighten the feedback loop even more. Machine learning is ideal for predictive maintenance workloads.</p><p><br></p><p>Migrate from CSV to binary format, migrate from FTP to SFTP transport, and develop machine learning analysis of metrics. -&gt;&nbsp;Incorrect. Machine learning analysis is a good means toward the end of reducing downtime, but shuffling formats and transport doesn't directly help at all.</p><p><br></p><p>Migrate from FTP to streaming transport, migrate from CSV to binary format, and develop machine learning analysis of metrics. -&gt; Incorrect. Machine learning analysis is a good means toward the end of reducing downtime, and moving to streaming can improve the freshness of the information in that analysis, but changing the format doesn't directly help at all.</p><p><br></p><p>Migrate from FTP to SFTP transport, develop machine learning analysis of metrics, and increase dealer local inventory by a fixed factor. -&gt; Incorrect. Machine learning analysis is a good means toward the end of reducing downtime, but the rest of these changes don't directly help at all.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681596,
    "question_plain": "As a cloud architect, you're helping a large organization configure IAM roles for their security team. The team needs to audit corporate applications deployed on GCP, identify security risks, and recommend improvements, but they should not be allowed to modify resources. Which of the following is the most suitable IAM role to assign to this team?",
    "answers": [
      "<p>Security Auditor role</p>",
      "<p>Project Editor role</p>",
      "<p>Project Viewer role</p>",
      "<p>Security Admin role</p>",
      "<p>Security Reviewer role</p>"
    ],
    "explanation": "<p>Security Auditor role -&gt; Correct. The Security Auditor role is designed for exactly this scenario - it provides permissions to view security configuration and state for all resources, making it perfect for auditing, identifying risks and suggesting improvements, but does not allow modification of resources.</p><p><br></p><p>Project Editor role -&gt;&nbsp;Incorrect. The Project Editor role would allow the team to modify resources in the project, which is not desired in this scenario.</p><p><br></p><p>Project Viewer role -&gt;&nbsp;Incorrect. The Project Viewer role would allow the team to view all resources in a project, but it might not provide the detailed security insights that a specific security-focused role like Security Auditor would provide.</p><p><br></p><p>Security Admin role -&gt;&nbsp;Incorrect. The Security Admin role provides broad control over security features in the project and would allow modifications, which is not desired in this case.</p><p><br></p><p>Security Reviewer role -&gt;&nbsp;Incorrect. No such role as \"Security Reviewer\" exists in Google Cloud IAM.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681598,
    "question_plain": "As a cloud architect, you are helping an organization set up a secure, low-latency network connection between their Compute Engine instances in the us-central1 region and their local data center. They want to ensure the connection is private and the data isn't exposed over the public internet. Which of the following options should you recommend?",
    "answers": [
      "<p>Set up Dedicated Interconnect at a colocation facility near their data center to connect directly to Google's network.</p>",
      "<p>Set up VPN Gateway on Google Cloud and enable IPsec for secure, encrypted communication.</p>",
      "<p>Use a NAT gateway to enable communication between the Compute Engine instances and the local data center.</p>",
      "<p>Set up a VPC Network Peering between their Google Cloud VPC and their local data center.</p>"
    ],
    "explanation": "<p>Set up Dedicated Interconnect at a colocation facility near their data center to connect directly to Google's network. -&gt; Correct. Dedicated Interconnect provides a direct, private connection from the organization's on-premises network to Google's network, which does not involve the public internet.</p><p><br></p><p>Set up VPN Gateway on Google Cloud and enable IPsec for secure, encrypted communication. -&gt; Incorrect. VPN Gateway and IPsec would indeed encrypt the traffic, but it would still go over the public internet which does not fulfil the requirement for a private connection.</p><p><br></p><p>Use a NAT gateway to enable communication between the Compute Engine instances and the local data center. -&gt; Incorrect. NAT gateways are used for allowing instances without public IPs to connect to the internet, not for connecting a local data center to Google Cloud.</p><p><br></p><p>Set up a VPC Network Peering between their Google Cloud VPC and their local data center. -&gt; Incorrect. VPC Network Peering is used to connect two VPC networks, not an on-premises data center and a Google Cloud VPC.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681600,
    "question_plain": "As a cloud architect, you are working with an organization that wants to have granular control over its Google Cloud resources. The organization has multiple projects, each managed by different teams and belonging to different departments. The company wants to enforce the principle of least privilege and ensure that the resources are only accessible to the team that needs them, while also keeping the management simple and efficient. Which of the following approaches should you recommend?",
    "answers": [
      "<p>Create Google Groups for each team, assign roles to the groups at the project level, and add the team members to the appropriate groups.</p>",
      "<p>Assign individual roles to each team member at the project level.</p>",
      "<p>Assign roles to the departments at the organization level and let the departments manage their individual teams' access.</p>",
      "<p>Assign roles to the teams at the organization level.</p>"
    ],
    "explanation": "<p>Create Google Groups for each team, assign roles to the groups at the project level, and add the team members to the appropriate groups. -&gt; Correct. By creating Google Groups for each team and assigning roles to these groups at the project level, you can ensure that only the teams that need access to a project get it. This approach is also efficient because when a team member joins or leaves, you only need to update the Google Group membership, not the IAM policy.</p><p><br></p><p>Assign individual roles to each team member at the project level. -&gt; Incorrect. Assigning individual roles to each team member at the project level is not efficient, as it would require a lot of manual effort to manage the permissions and does not scale well with the growth of the organization.</p><p><br></p><p>Assign roles to the departments at the organization level and let the departments manage their individual teams' access. -&gt; Incorrect. Assigning roles to the departments at the organization level would not provide the desired granularity. The resources of all projects under the organization would be accessible to all teams under a department, which does not adhere to the principle of least privilege.</p><p><br></p><p>Assign roles to the teams at the organization level. -&gt; Incorrect. Assigning roles to the teams at the organization level would not ensure that a team only has access to the projects it needs. This approach also doesn't adhere to the principle of least privilege.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681602,
    "question_plain": "As a cloud architect, you are working with an organization to structure their BigQuery permissions. The company has three teams: data analysts, data scientists, and data engineers. The data analysts should be able to run SQL queries, the data scientists need to create and run machine learning models in BigQuery, and the data engineers should have full control over BigQuery resources. What is the most appropriate IAM role assignment for these teams?",
    "answers": [
      "<p>Assign the roles/bigquery.user role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.admin role to the data engineers.</p>",
      "<p>Assign the roles/bigquery.dataViewer role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.dataOwner role to the data engineers.</p>",
      "<p>Assign the roles/bigquery.dataEditor role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.admin role to the data engineers.</p>",
      "<p>Assign the roles/bigquery.jobUser role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.dataEditor role to the data engineers.</p>"
    ],
    "explanation": "<p>Assign the roles/bigquery.user role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.admin role to the data engineers. -&gt;&nbsp;Correct. The roles/bigquery.user role allows data analysts to run queries and read dataset metadata, roles/bigquery.mlUser allows data scientists to create and run ML models, and roles/bigquery.admin gives data engineers full control over BigQuery resources.</p><p><br></p><p>Assign the roles/bigquery.dataViewer role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.dataOwner role to the data engineers. -&gt; Incorrect. The roles/bigquery.dataViewer role would not allow data analysts to run queries, it only provides read access to datasets. Also, roles/bigquery.dataOwner would not provide full control to data engineers over BigQuery resources.</p><p><br></p><p>Assign the roles/bigquery.dataEditor role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.admin role to the data engineers. -&gt; Incorrect. The roles/bigquery.dataEditor role allows users to edit (but not run) queries and provides read/write access to datasets, which is more permission than data analysts typically need.</p><p><br></p><p>Assign the roles/bigquery.jobUser role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.dataEditor role to the data engineers. -&gt;&nbsp;Incorrect. The roles/bigquery.jobUser role would not allow data analysts to run queries, it only provides the ability to run jobs. Also, roles/bigquery.dataEditor does not provide full control to data engineers over BigQuery resources.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681604,
    "question_plain": "As a cloud architect, you're working with an organization that operates a complex microservices application on Google Kubernetes Engine (GKE). They want to utilize Service Mesh visualization in the Google Cloud Console to gain insights into their services' performance and interactions. However, after setting up their environment with Istio, they are unable to see any traffic flow between services. Which of the following could be a possible reason and the appropriate fix?",
    "answers": [
      "<p>The required Envoy proxy sidecar containers might not have been injected into each relevant Kubernetes pod. They should ensure automatic or manual sidecar injection is configured.</p>",
      "<p>The Service Mesh visualization doesn't support GKE. They should use Stackdriver for visualizing service interactions.</p>",
      "<p>Service Mesh visualization does not support microservices. They should refactor their application into a monolith.</p>",
      "<p>Istio is not installed correctly. They should reinstall it and restart their services.</p>"
    ],
    "explanation": "<p>The required Envoy proxy sidecar containers might not have been injected into each relevant Kubernetes pod. They should ensure automatic or manual sidecar injection is configured. -&gt;&nbsp;Correct. Istio relies on the Envoy proxy sidecars to manage traffic between services in a Service Mesh. If these are not injected into the Kubernetes pods, service traffic will not be correctly managed and visualized.</p><p><br></p><p>The Service Mesh visualization doesn't support GKE. They should use Stackdriver for visualizing service interactions. -&gt; Incorrect. Service Mesh visualization does support GKE.</p><p><br></p><p>Service Mesh visualization does not support microservices. They should refactor their application into a monolith. -&gt; Incorrect. Service Mesh is designed specifically to help manage and understand microservices architectures.</p><p><br></p><p>Istio is not installed correctly. They should reinstall it and restart their services. -&gt; Incorrect. Reinstalling Istio might be necessary in some cases, but the problem could also be due to the missing Envoy proxy sidecars, which are responsible for routing and collecting telemetry for traffic between services.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681606,
    "question_plain": "As a cloud architect, you are designing a secure architecture for a client who needs to limit the use of external IP addresses on their Compute Engine instances. The client wants to ensure that only specific, approved instances can be assigned external IP addresses. What method would be the most appropriate for enforcing this requirement?",
    "answers": [
      "<p>Use an organization policy to restrict external IP addresses and apply it to the project.</p>",
      "<p>Use Shared VPC to restrict instances from obtaining external IP addresses.</p>",
      "<p>Use firewall rules to block outbound traffic from instances without approved external IPs.</p>",
      "<p>Use VPC Service Controls to restrict instances from obtaining external IP addresses.</p>"
    ],
    "explanation": "<p>Use an organization policy to restrict external IP addresses and apply it to the project. -&gt; Correct. An organization policy allows you to define fine-grained, location-specific policies for your Google Cloud resources. You can use the compute.vmExternalIpAccess policy constraint to restrict instances from obtaining external IP addresses, and apply exceptions to approved instances.</p><p><br></p><p>Use Shared VPC to restrict instances from obtaining external IP addresses. -&gt; Incorrect. While Shared VPC can be used to share network resources across multiple projects, it does not provide a direct way to restrict instances from obtaining external IP addresses.</p><p><br></p><p>Use firewall rules to block outbound traffic from instances without approved external IPs. -&gt; Incorrect. Firewall rules control network traffic, but they can't prevent instances from being assigned external IP addresses. They could only block outbound traffic, but the instances would still have external IPs.</p><p><br></p><p>Use VPC Service Controls to restrict instances from obtaining external IP addresses. -&gt; Incorrect. VPC Service Controls are mainly used to define a security perimeter around Google Cloud resources and services to mitigate data exfiltration risks. They do not limit instances from obtaining external IP addresses.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681608,
    "question_plain": "As a cloud architect, you are working with a large enterprise customer who is storing millions of daily logs in a Cloud Storage bucket. To manage costs and ensure data is retained appropriately, they want to automatically move files older than 30 days to Nearline storage and delete any files older than 365 days. Which of the following is the best approach to meet these requirements?",
    "answers": [
      "<p>Use the lifecycle management feature of Cloud Storage to move older files to Nearline and delete files older than 365 days.</p>",
      "<p>Use gsutil to move older files to Nearline storage and delete files older than 365 days manually.</p>",
      "<p>Use Cloud Storage Transfer service to move files to Nearline storage and lifecycle management to delete files older than 365 days.</p>",
      "<p>Use gsutil to periodically move all files to Nearline storage and use lifecycle management to delete files older than 365 days.</p>"
    ],
    "explanation": "<p>Use the lifecycle management feature of Cloud Storage to move older files to Nearline and delete files older than 365 days. -&gt;&nbsp;Correct. The lifecycle management feature of Cloud Storage is designed to handle this exact scenario, allowing you to set rules to automatically change the storage class of objects and delete them based on their age.</p><p><br></p><p>Use gsutil to move older files to Nearline storage and delete files older than 365 days manually. -&gt; Incorrect. This approach requires manual intervention and would not be practical or efficient for managing a large number of files.</p><p><br></p><p>Use Cloud Storage Transfer service to move files to Nearline storage and lifecycle management to delete files older than 365 days. -&gt; Incorrect. The Cloud Storage Transfer service is designed to move data into Cloud Storage from online and on-premises sources, not to manage the lifecycle of objects already in Cloud Storage.</p><p><br></p><p>Use gsutil to periodically move all files to Nearline storage and use lifecycle management to delete files older than 365 days. -&gt; Incorrect. Moving all files to Nearline storage with gsutil doesn't make sense if the files are being frequently accessed. Also, you'd still need to manage the deletion of older files.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681610,
    "question_plain": "Your company offers a downloadable rendering software through its website, serving customers globally. To ensure optimal customer experience, you aim to minimize delays for all users while adhering to Google's recommended practices. What is the recommended approach for storing the files?",
    "answers": [
      "<p>Save the files in multiple Multi-Regional Cloud Storage buckets, one bucket per multi-region.</p>",
      "<p>Save the files in multiple Regional Cloud Storage buckets, one bucket per zone per region.</p>",
      "<p>Save the files in multiple Regional Cloud Storage buckets, one bucket per region.</p>",
      "<p>Save the files in a Multi-Regional Cloud Storage bucket.</p>"
    ],
    "explanation": "<p>Save the files in multiple Multi-Regional Cloud Storage buckets, one bucket per multi-region. -&gt;&nbsp;Correct. Multi-Regional Cloud Storage buckets are designed for storing data that needs to be accessed frequently from anywhere in the world with low latency. By storing the files in multiple Multi-Regional Cloud Storage buckets, one bucket per multi-region, you can ensure that your customers will experience low latency when accessing your software files from anywhere in the world.</p><p><br></p><p>Save the files in multiple Regional Cloud Storage buckets, one bucket per zone per region. -&gt; Incorrect. It is not the best answer because Regional Cloud Storage buckets are designed for storing data that needs to be accessed frequently from a specific region. Therefore, using multiple Regional Cloud Storage buckets would not be optimal for a company that has customers all over the world.</p><p><br></p><p>Save the files in multiple Regional Cloud Storage buckets, one bucket per region. -&gt; Incorrect. It suggests storing files in multiple Regional Cloud Storage buckets, one per region, which is still not optimal for a company that has customers all over the world.</p><p><br></p><p>Save the files in a Multi-Regional Cloud Storage bucket. -&gt; Incorrect. It is also not the best answer because Multi-Regional Cloud Storage buckets are designed for storing data that needs to be accessed frequently from anywhere in the world, but it's recommended to use multiple buckets in different multi-regions to ensure low latency access for users in different regions.</p><p><br></p><p>https://cloud.google.com/storage/docs/locations</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681612,
    "question_plain": "As a cloud architect, you are working with an organization that frequently provisions and de-provisions a set of Google Cloud resources for temporary projects. They are looking for an automated and reproducible way to manage this. Which of the following approaches best utilizes Google Cloud Deployment Manager for this purpose?",
    "answers": [
      "<p>Create a Deployment Manager configuration file for the required resources, and use Deployment Manager to create and delete these resources as needed.</p>",
      "<p>Use the Deployment Manager API to manually create and delete resources as needed.</p>",
      "<p>Use Deployment Manager to periodically check for unused resources and delete them.</p>",
      "<p>Use Deployment Manager to automate the process of manually creating and deleting resources in the Google Cloud Console.</p>"
    ],
    "explanation": "<p>Create a Deployment Manager configuration file for the required resources, and use Deployment Manager to create and delete these resources as needed. -&gt; Correct. Google Cloud Deployment Manager allows you to specify all the resources needed for an application in a declarative format using yaml. You can create a configuration file for your resources and use it to provision and de-provision these resources as needed.</p><p><br></p><p>Use the Deployment Manager API to manually create and delete resources as needed. -&gt; Incorrect. Using the Deployment Manager API directly would not be as simple or maintainable as using a configuration file.</p><p><br></p><p>Use Deployment Manager to periodically check for unused resources and delete them. -&gt; Incorrect. Deployment Manager does not have built-in functionality to check for unused resources. It is used to create and manage resources based on configuration files.</p><p><br></p><p>Use Deployment Manager to automate the process of manually creating and deleting resources in the Google Cloud Console. -&gt; Incorrect. Deployment Manager does not automate manual actions in the Google Cloud Console. It uses configuration files to create and manage resources.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681614,
    "question_plain": "Your customer is planning to run a data processing pipeline that ingests, processes, and stores large amounts of data in Google Cloud. The customer has the following requirements:the pipeline must be easily scalable and able to handle a large volume of datathe pipeline must be able to handle failures and recover gracefullythe pipeline must be easily integratable with other Google Cloud servicesthe pipeline must be cost-effectiveWhich Google Cloud service should you recommend to meet these requirements?",
    "answers": [
      "<p>Cloud Dataflow</p>",
      "<p>Cloud Dataproc</p>",
      "<p>Cloud BigQuery</p>",
      "<p>Cloud Pub/Sub</p>"
    ],
    "explanation": "<p>Cloud Dataflow -&gt; Correct. Cloud Dataflow is a fully-managed service for transforming and enriching data in stream (real-time) and batch (historical) modes with equal reliability and expressiveness. It provides a simple and cost-effective way to build and manage data processing pipelines that automatically scales up and down as needed. Cloud Dataflow also has built-in fault tolerance and recovery mechanisms, making it able to handle failures and recover gracefully. It can easily integrate with other Google Cloud services like Cloud Storage, BigQuery, and Pub/Sub.</p><p><br></p><p>Cloud Dataproc -&gt;&nbsp;Incorrect. While it can handle data processing tasks, it is more suitable for batch and interactive big data processing rather than building scalable data processing pipelines. It may require additional development and management efforts to achieve the desired pipeline functionality.</p><p><br></p><p>Cloud BigQuery -&gt;&nbsp;Incorrect. Cloud BigQuery is a fully managed data warehouse and analytics platform. It is excellent for querying and analyzing large datasets, but it is not designed specifically for building data processing pipelines. It does not provide the same level of pipeline orchestration and data transformation capabilities as Cloud Dataflow.</p><p><br></p><p>Cloud Pub/Sub -&gt;&nbsp;Incorrect. While it can be used as part of a data processing pipeline for message-based communication, it does not provide the full range of capabilities required for ingesting, processing, and storing large amounts of data in a scalable and cost-effective manner.</p><p><br></p><p>https://cloud.google.com/dataflow/docs</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681616,
    "question_plain": "Your customer is planning to run a containerized workload in Google Cloud. The customer has the following requirements:the workload must be easily deployable and manageablethe workload must be easily scalable and able to handle a large volume of datathe workload must be able to handle failures and recover gracefullythe workload must be easily integratable with other Google Cloud servicesWhich Google Cloud service should you recommend to meet these requirements?",
    "answers": [
      "<p>Amazon EC2</p>",
      "<p>Google Kubernetes Engine</p>",
      "<p>Cloud Functions</p>",
      "<p>Compute Engine</p>"
    ],
    "explanation": "<p>Google Kubernetes Engine -&gt; Correct. Google Kubernetes Engine (GKE) is a managed container orchestration service that allows customers to deploy, manage, and scale containerized applications in Google Cloud. GKE is designed to meet the requirements mentioned in the question. Firstly, GKE provides a simple and easy-to-use interface for deploying and managing containerized workloads. It supports various container images and provides integrated support for monitoring, logging, and debugging. Secondly, GKE provides horizontal scaling capabilities that allow customers to handle large volumes of data and traffic by adding or removing nodes to their cluster based on demand. Thirdly, GKE provides automated failover and recovery capabilities that ensure high availability of the applications. It automatically replaces failed nodes and reschedules containers in the event of a node failure. Finally, GKE is easily integratable with other Google Cloud services, such as Cloud Storage, Cloud SQL, and Cloud Pub/Sub. This allows customers to build scalable and reliable applications that can leverage other Google Cloud services.</p><p><br></p><p>Amazon EC2 -&gt; Incorrect. It is an Amazon Web Services (AWS) service for running virtual machines in the cloud. It is not a container orchestration service and cannot meet the requirements mentioned in the question.</p><p><br></p><p>Cloud Functions -&gt; Incorrect. It is a serverless compute service that allows customers to run code in response to events. It is not designed for managing containerized workloads and does not provide container orchestration capabilities.</p><p><br></p><p>Compute Engine -&gt; Incorrect. It is a Google Cloud service for running virtual machines in the cloud. While it is possible to run containers on Compute Engine, it does not provide container orchestration capabilities and cannot meet the requirements mentioned in the question.</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681618,
    "question_plain": "Your customer is planning to run a highly available and scalable web application in Google Cloud. The customer has the following requirements:the application must be easily deployable and manageablethe application must be highly available and recover from failures automaticallythe application must be able to handle incoming traffic spikes and scale dynamicallythe application must be secure and protect against common web attacksWhich Google Cloud service should you recommend to secure and protect the application against common web attacks?",
    "answers": [
      "<p>App Engine</p>",
      "<p>Google Kubernetes Engine</p>",
      "<p>Cloud Functions</p>",
      "<p>Cloud Armor</p>"
    ],
    "explanation": "<p>Cloud Armor -&gt; Correct. Cloud Armor is a distributed denial-of-service (DDoS) and web application firewall (WAF) service that protects against external threats such as DDoS attacks, application-layer attacks, and SQL injection attacks. It provides a set of security policies that can be customized to allow or deny traffic based on IP address, geo-location, URL, or other attributes. Cloud Armor integrates with other Google Cloud services, such as HTTP(S) Load Balancing and Cloud CDN, to provide a comprehensive security solution for web applications.</p><p><br></p><p>App Engine -&gt; Incorrect. App Engine is a fully managed platform for building and deploying web applications and APIs. It provides automatic scaling, load balancing, and high availability out of the box. However, it does not provide native DDoS protection or WAF capabilities.</p><p><br></p><p>Google Kubernetes Engine -&gt; Incorrect. Google Kubernetes Engine (GKE) is a managed Kubernetes service that allows developers to deploy and manage containerized applications at scale. GKE provides a flexible and portable platform for running microservices-based applications. However, it does not provide native DDoS protection or WAF capabilities.</p><p><br></p><p>Cloud Functions -&gt; Incorrect. Cloud Functions is a serverless compute platform that allows developers to run code in response to events without managing any infrastructure. It is suitable for small to medium-sized workloads that require a short-lived, event-driven architecture. However, it does not provide native DDoS protection or WAF capabilities.</p><p><br></p><p>https://cloud.google.com/armor/docs/cloud-armor-overview</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681620,
    "question_plain": "When migrating a legacy application to Google Cloud Platform (GCP), which of the following strategies is most likely to result in the quickest and least disruptive migration, while also taking into account future scalability needs?",
    "answers": [
      "<p>Re-architect the entire application to use Google Cloud Platform services and design patterns.</p>",
      "<p>Use the Virtual Private Cloud (VPC) to create a dedicated network for the application and its components, and deploy the components to Compute Engine instances.</p>",
      "<p>Use App Engine to host the application and store data in Cloud Datastore or Cloud Firestore.</p>",
      "<p>Lift and shift the application to Compute Engine Virtual Machines (VMs) and use Cloud Storage for data storage.</p>"
    ],
    "explanation": "<p>Lift and shift the application to Compute Engine Virtual Machines (VMs) and use Cloud Storage for data storage. -&gt; Correct. Lifting and shifting the application to Compute Engine VMs and using Cloud Storage for data storage provides a relatively quick and least disruptive migration strategy while considering future scalability needs. It minimizes the need for significant code changes and allows for a straightforward migration process.</p><p><br></p><p>Re-architect the entire application to use Google Cloud Platform services and design patterns. -&gt;&nbsp;Incorrect. Re-architecting the entire application to use Google Cloud Platform (GCP) services and design patterns can be a valid approach for long-term scalability and optimization. However, it typically requires significant time, effort, and resources to re-engineer the application to take full advantage of GCP services. This option may not result in the quickest and least disruptive migration, as it involves substantial changes to the application architecture.</p><p><br></p><p>Use the Virtual Private Cloud (VPC) to create a dedicated network for the application and its components, and deploy the components to Compute Engine instances. -&gt;&nbsp;Incorrect. Using Virtual Private Cloud (VPC) to create a dedicated network for the application and deploying the components to Compute Engine instances can provide network isolation and control. However, this approach still requires managing and maintaining the application components on virtual machines, which may not be the quickest and least disruptive migration strategy. It also doesn't address scalability needs in an efficient manner.</p><p><br></p><p>Use App Engine to host the application and store data in Cloud Datastore or Cloud Firestore. -&gt;&nbsp;Incorrect. Migrating a legacy application to App Engine often requires making significant changes to the codebase and architecture to fit the PaaS model. This option may not be the quickest and least disruptive migration strategy, especially if the legacy application is not well-suited for the App Engine environment.</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681622,
    "question_plain": "A financial services company wants to implement a secure and scalable solution for processing large volumes of financial transactions. The solution should also provide real-time visibility into transaction status, and enable employees to resolve issues and perform auditing tasks. The solution should be fully managed, and minimize the need for additional hardware and software investments. Which of the following options would be the most effective approach to meet these requirements?",
    "answers": [
      "<p>Use Cloud Bigtable for storing transaction data and Cloud Functions for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data.</p>",
      "<p>Use Cloud Datastore for storing transaction data and Cloud Functions for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data.</p>",
      "<p>Use Cloud SQL for storing transaction data and Cloud Functions for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data.</p>",
      "<p>Use Cloud Spanner for storing transaction data and Cloud Dataflow for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data.</p>"
    ],
    "explanation": "<p>Use Cloud Spanner for storing transaction data and Cloud Dataflow for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data. -&gt; Correct. Cloud Spanner is a globally distributed, scalable, and fully managed relational database that provides strong consistency and horizontal scaling. It is an ideal solution for processing large volumes of financial transactions while providing real-time visibility into transaction status. Cloud Dataflow is a fully managed data processing service that can be used to process transactions in real-time. Cloud Pub/Sub can be used for real-time updates on transaction status, and Cloud IAM can be used to control access to the data. Cloud Spanner eliminates the need for additional hardware and software investments, making it a cost-effective solution for the financial services company.</p><p><br></p><p>Use Cloud Bigtable for storing transaction data and Cloud Functions for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data. -&gt;&nbsp;Incorrect. Cloud Bigtable is a NoSQL database that does not provide strong consistency and is not ideal for processing financial transactions that require strong consistency. Cloud Functions can be used to process transactions in real-time, Cloud Pub/Sub can be used for real-time updates on transaction status, and Cloud IAM can be used to control access to the data.</p><p><br></p><p>Use Cloud Datastore for storing transaction data and Cloud Functions for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data. -&gt;&nbsp;Incorrect. Cloud Datastore is a NoSQL database that does not provide strong consistency and is not ideal for processing financial transactions that require strong consistency. Cloud Functions can be used to process transactions in real-time, Cloud Pub/Sub can be used for real-time updates on transaction status, and Cloud IAM can be used to control access to the data.</p><p><br></p><p>Use Cloud SQL for storing transaction data and Cloud Functions for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data. -&gt; Incorrect. Cloud SQL is a relational database that is not globally distributed and does not provide strong consistency. It is not ideal for processing financial transactions that require strong consistency. Cloud Functions can be used to process transactions in real-time, Cloud Pub/Sub can be used for real-time updates on transaction status, and Cloud IAM can be used to control access to the data.</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681624,
    "question_plain": "A large retail company wants to launch a new personalized shopping experience for customers using an e-commerce platform. The platform should allow customers to create a profile, save their preferences and purchase history, and receive personalized product recommendations in real-time. The platform should also provide real-time analytics and reporting on customer behavior and purchasing patterns. Which of the following solutions would be the most effective for meeting these requirements?",
    "answers": [
      "<p>Use Cloud Datastore for storing customer data, Cloud Functions for processing customer preferences and purchase history, and Cloud Machine Learning Engine for generating product recommendations. Use Cloud Data Studio for analytics and reporting.</p>",
      "<p>Use BigQuery for storing customer data, Cloud Dataflow for processing customer preferences and purchase history, and Cloud Machine Learning Engine for generating product recommendations. Use Cloud Dataproc for analytics and reporting.</p>",
      "<p>Use Cloud SQL for storing customer data, Cloud Functions for processing customer preferences and purchase history, and Cloud Machine Learning Engine for generating product recommendations. Use Cloud Data Studio for analytics and reporting.</p>",
      "<p>Use Cloud Firestore for storing customer data, Cloud Functions for processing customer preferences and purchase history, and Cloud AI Platform for generating product recommendations. Use Cloud Data Studio for analytics and reporting.</p>"
    ],
    "explanation": "<p>Use BigQuery for storing customer data, Cloud Dataflow for processing customer preferences and purchase history, and Cloud Machine Learning Engine for generating product recommendations. Use Cloud Dataproc for analytics and reporting. -&gt;&nbsp;Correct. BigQuery is designed for handling and analyzing large datasets, perfect for storing customer data. Cloud Dataflow allows for real-time data processing, which meets the requirement of processing customer preferences and purchase history. Cloud Machine Learning Engine can indeed handle personalized product recommendations, and Cloud Dataproc offers more powerful processing for analytics and reporting compared to Data Studio.</p><p><br></p><p>Use Cloud Datastore for storing customer data, Cloud Functions for processing customer preferences and purchase history, and Cloud Machine Learning Engine for generating product recommendations. Use Cloud Data Studio for analytics and reporting. -&gt;&nbsp;Incorrect. Cloud Datastore is a NoSQL database, which is not the best option for handling complex queries or data analysis, and Cloud Functions might not provide the real-time processing needed. Cloud Data Studio is primarily a visualizing tool and may lack the processing power for complex real-time analytics.</p><p><br></p><p>Use Cloud SQL for storing customer data, Cloud Functions for processing customer preferences and purchase history, and Cloud Machine Learning Engine for generating product recommendations. Use Cloud Data Studio for analytics and reporting. -&gt;&nbsp;Incorrect. Although Cloud SQL can handle customer data, it's not designed for large-scale data analytics. Also, Cloud Functions might not provide the real-time processing required. Cloud Data Studio may not handle the analytical requirements.</p><p><br></p><p>Use Cloud Firestore for storing customer data, Cloud Functions for processing customer preferences and purchase history, and Cloud AI Platform for generating product recommendations. Use Cloud Data Studio for analytics and reporting. -&gt;&nbsp;Incorrect. While Cloud Firestore and Cloud Functions can handle the respective tasks, Cloud AI Platform is more oriented towards model training and deployment, not specifically towards generating product recommendations. Again, Data Studio may not be able to handle complex real-time analytics.</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681626,
    "question_plain": "A global media company wants to implement a video streaming platform that can handle high traffic and deliver high-quality video to users. The platform should also be able to provide real-time analytics on video viewing trends and user engagement. The company wants to use the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?",
    "answers": [
      "<p>Use Cloud Pub/Sub to ingest video data, BigQuery to store video data, and Cloud Dataflow to process video analytics.</p>",
      "<p>Use Cloud Storage to store video data, Cloud Functions to process video data, and Cloud SQL to store analytics data.</p>",
      "<p>Use Cloud CDN to deliver high-quality video to users, Cloud Storage to store video data, and Cloud Bigtable to store video analytics data.</p>",
      "<p>Use Cloud Video Intelligence API to extract insights from video data, Cloud Pub/Sub to ingest video data, and Cloud Dataflow to process video analytics.</p>"
    ],
    "explanation": "<p>Use Cloud CDN to deliver high-quality video to users, Cloud Storage to store video data, and Cloud Bigtable to store video analytics data. -&gt; Correct. Cloud CDN (Content Delivery Network) can help deliver high-quality video to users by caching and delivering content from a location closest to the user, reducing latency and improving user experience. Cloud Storage is a highly scalable and durable storage solution that can store video data. Cloud Bigtable is a NoSQL database that can handle high read and write throughput and can be used to store analytics data for real-time analysis.</p><p><br></p><p>Use Cloud Pub/Sub to ingest video data, BigQuery to store video data, and Cloud Dataflow to process video analytics. -&gt; Incorrect. It is not the best solution because BigQuery is not suitable for storing large video files.</p><p><br></p><p>Use Cloud Storage to store video data, Cloud Functions to process video data, and Cloud SQL to store analytics data. -&gt; Incorrect. It is not the best solution because Cloud Functions may not be able to handle the processing requirements for video data and Cloud SQL may not be able to handle the high write throughput for analytics data.</p><p><br></p><p>Use Cloud Video Intelligence API to extract insights from video data, Cloud Pub/Sub to ingest video data, and Cloud Dataflow to process video analytics. -&gt; Incorrect. It is not the best solution because it relies on the Cloud Video Intelligence API for video analysis, which may not be sufficient for real-time analytics on high traffic video streaming platform.</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681628,
    "question_plain": "A multinational retail company wants to implement a new e-commerce platform that can handle high traffic and provide real-time recommendations to users. The platform should also be able to track user behavior and provide insights into customer preferences. The company wants to use the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?",
    "answers": [
      "<p>Use Cloud Storage to store customer data, Cloud Functions to process customer data, and Cloud Datastore to store analytics data.</p>",
      "<p>Use Cloud SQL to store customer data, Cloud Pub/Sub to ingest customer data, and Cloud Dataflow to process customer analytics.</p>",
      "<p>Use Cloud Bigtable to store customer data, Cloud Pub/Sub to ingest customer data, and Cloud Functions to process customer analytics.</p>",
      "<p>Use Cloud Firestore to store customer data, Cloud Functions to process customer data, and Cloud Datalab to store customer analytics.</p>"
    ],
    "explanation": "<p>Use Cloud SQL to store customer data, Cloud Pub/Sub to ingest customer data, and Cloud Dataflow to process customer analytics. -&gt; Correct. Cloud SQL is a fully-managed relational database service, good for storing customer data. Cloud Pub/Sub allows you to quickly ingest large amounts of data which can be especially useful for high traffic situations. Cloud Dataflow is a fully-managed service for stream and batch processing, making it ideal for processing customer analytics in real-time.</p><p><br></p><p>Use Cloud Storage to store customer data, Cloud Functions to process customer data, and Cloud Datastore to store analytics data. -&gt;&nbsp;Incorrect. Cloud Storage is not the best for storing structured customer data as it is an object storage service primarily used for unstructured data. Cloud Functions are serverless and event-driven, not best suited for high traffic real-time analytics. Cloud Datastore is a NoSQL document database, which can handle analytics data but not the best for real-time operations or complex querying.</p><p><br></p><p>Use Cloud Bigtable to store customer data, Cloud Pub/Sub to ingest customer data, and Cloud Functions to process customer analytics. -&gt;&nbsp;Incorrect. Cloud Bigtable is a NoSQL database service designed for large operational and analytical workloads, however, it may not be the best choice for storing structured customer data due to cost and complexity. Cloud Pub/Sub is good for ingesting data, but Cloud Functions may not provide the throughput and consistent performance needed for high traffic real-time analytics.</p><p><br></p><p>Use Cloud Firestore to store customer data, Cloud Functions to process customer data, and Cloud Datalab to store customer analytics. -&gt;&nbsp;Incorrect. Cloud Firestore is a NoSQL document database which may not be the best choice for real-time analytics or complex querying of customer data. Cloud Datalab is an interactive tool for exploration, transformation, analysis, and visualization of data but it's not ideal for storing analytics data.</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681630,
    "question_plain": "A large financial services company wants to build a new trading platform that can process millions of transactions per second. The platform should be able to handle high levels of volatility, and provide low latency data access for real-time decision making. Which of the following options would be the best solution for this requirement?",
    "answers": [
      "<p>Use BigQuery for data storage and processing.</p>",
      "<p>Use Cloud Pub/Sub for data streaming and BigTable for data storage.</p>",
      "<p>Use Cloud Spanner for data storage and Cloud Dataflow for data processing.</p>",
      "<p>Use Cloud Datastore for data storage and Cloud Functions for data processing.</p>"
    ],
    "explanation": "<p>Use Cloud Pub/Sub for data streaming and BigTable for data storage. -&gt; Correct. Using Cloud Pub/Sub for data streaming and BigTable for data storage is the recommended solution for this requirement. Cloud Pub/Sub provides a highly scalable and reliable messaging system for streaming large volumes of data in real-time. It is well-suited for handling high levels of volatility and processing millions of transactions per second. BigTable is a NoSQL database designed for low-latency, high-throughput applications. It can handle the high transactional workload and provide fast data access for real-time decision making.</p><p><br></p><p>Use BigQuery for data storage and processing. -&gt;&nbsp;Incorrect. Using BigQuery for data storage and processing is not the most suitable option for a trading platform that requires low latency and real-time decision making. While BigQuery is excellent for large-scale data analytics and querying, it may not provide the necessary speed and responsiveness required for processing millions of transactions per second in real-time.</p><p><br></p><p>Use Cloud Spanner for data storage and Cloud Dataflow for data processing. -&gt;&nbsp;Incorrect. Using Cloud Spanner for data storage and Cloud Dataflow for data processing is a valid option for building scalable and reliable applications. However, it may introduce additional complexity and overhead, and it may not provide the same level of low latency and high throughput as Cloud Pub/Sub and BigTable for a high-frequency trading platform.</p><p><br></p><p>Use Cloud Datastore for data storage and Cloud Functions for data processing. -&gt;&nbsp;Incorrect. Using Cloud Datastore for data storage and Cloud Functions for data processing may not be the most optimal solution for a high-frequency trading platform. Cloud Datastore, while providing scalability, may not deliver the necessary performance and latency required for processing millions of transactions per second. Cloud Functions are event-driven functions and may not provide the real-time processing capabilities needed for real-time decision making.</p><p><br></p><p>https://cloud.google.com/pubsub/docs</p><p>https://cloud.google.com/bigtable/docs</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681632,
    "question_plain": "A large media company is looking to build a secure and scalable platform for hosting and streaming video content. The platform must meet the following requirements:support high volumes of concurrent video streams with low latencyensure secure storage and processing of sensitive video contentenable real-time analytics and reporting on video usage and performanceminimize downtime during maintenance and upgradesminimize costs while still providing high performanceWhich solution would you recommend to meet these requirements?",
    "answers": [
      "<p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling.</p>",
      "<p>Implementing a serverless solution using Cloud Functions for data processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing.</p>",
      "<p>Implementing a managed solution using Video Intelligence API for video analysis, Cloud Storage for video storage, and Cloud Load Balancing for high-availability video streaming.</p>",
      "<p>Implementing a hybrid solution using Compute Engine for video processing, Cloud Storage for video storage, and BigQuery for real-time analytics.</p>"
    ],
    "explanation": "<p>Implementing a managed solution using Video Intelligence API for video analysis, Cloud Storage for video storage, and Cloud Load Balancing for high-availability video streaming. -&gt;&nbsp;Correct. It is the recommended approach. Video Intelligence API provides specialized capabilities for video analysis, enabling real-time analytics and reporting on video usage and performance. Cloud Storage offers scalable and secure storage for video content. Cloud Load Balancing ensures high availability and low latency for video streaming, providing a seamless experience to users. This combination of services offers a managed and efficient solution specifically designed for hosting and streaming video content.</p><p><br></p><p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling. -&gt;&nbsp;Incorrect. It may not be the most efficient and cost-effective approach for hosting and streaming video content. These services are not specifically designed for video processing and streaming, and building a custom solution for such requirements can be complex and time-consuming.</p><p><br></p><p>Implementing a serverless solution using Cloud Functions for data processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing. -&gt;&nbsp;Incorrect. It may not be the most suitable option for hosting and streaming video content. Cloud Functions are primarily designed for lightweight, event-driven functions and may not provide the necessary scalability and performance for video streaming. Cloud Firestore may not be the most suitable data storage solution for large volumes of video content. It may not provide the low latency and high performance required for video streaming.</p><p><br></p><p>Implementing a hybrid solution using Compute Engine for video processing, Cloud Storage for video storage, and BigQuery for real-time analytics. -&gt;&nbsp;Incorrect. iI introduces additional complexity and management overhead. Compute Engine requires managing virtual machines for video processing, which may not be the most scalable and cost-effective solution for streaming high volumes of video content. BigQuery, while powerful for analytics, may not be the most optimal choice for real-time analytics and reporting on video usage and performance.</p><p><br></p><p>https://cloud.google.com/video-intelligence/docs</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681634,
    "question_plain": "A large healthcare organization is looking to build a secure and scalable platform for storing and analyzing medical records. The platform must meet the following requirements:support high volumes of medical records with low latencyensure secure storage and processing of sensitive medical informationenable real-time data analysis and reporting on patient health and treatment outcomesminimize downtime during maintenance and upgradesminimize costs while still providing high performanceWhich solution would you recommend to meet these requirements?",
    "answers": [
      "<p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling.</p>",
      "<p>Implementing a serverless solution using Cloud Functions for data processing,&nbsp; Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing.</p>",
      "<p>Implementing a managed solution using Cloud Healthcare API for data analysis, Cloud Storage for data storage, and Cloud Load Balancing for high-availability data access.</p>",
      "<p>Implementing a hybrid solution using Compute Engine for data processing, Cloud SQL for data storage, and BigQuery for real-time analytics.</p>"
    ],
    "explanation": "<p>Implementing a managed solution using Cloud Healthcare API for data analysis, Cloud Storage for data storage, and Cloud Load Balancing for high-availability data access. -&gt;&nbsp;Correct. This solution meets the requirements by using Cloud Healthcare API, which is specifically designed for healthcare organizations to store, process, and analyze sensitive medical information securely and efficiently. Cloud Storage provides secure and scalable storage for the medical records. And, Cloud Load Balancing ensures high-availability of the data access, minimizing downtime during maintenance and upgrades.</p><p><br></p><p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling. -&gt;&nbsp;Incorrect. While this option provides flexibility and control over the solution components, it also requires significant development and maintenance effort. It may not be the most cost-effective and scalable option for meeting the specified requirements.</p><p><br></p><p>Implementing a serverless solution using Cloud Functions for data processing,&nbsp; Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing. -&gt;&nbsp;Incorrect. This option offers simplicity and scalability through serverless components. However, Cloud Firestore may not be the most suitable data storage solution for high volumes of medical records. It may not provide the low latency and performance required for real-time data analysis and reporting.</p><p><br></p><p>Implementing a hybrid solution using Compute Engine for data processing, Cloud SQL for data storage, and BigQuery for real-time analytics. -&gt;&nbsp;Incorrect. While this option combines different services for specific purposes, it introduces additional complexity and maintenance overhead. Compute Engine and Cloud SQL require managing virtual machines and databases, which may not be the most scalable and cost-effective approach.</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681636,
    "question_plain": "Your company has a massive amount of data (approx 50 TB) in its on-premises data center, which needs to be transferred to Cloud Storage for further analysis and machine learning processing. Due to security concerns, your company has decided not to use physical data transfer services like Transfer Appliance. Your company has a very high-speed internet connection, but the rate of data generation is also high, leading to a shrinking time window for data transfer. What would be the best way to migrate this data over the internet in a reasonable amount of time?",
    "answers": [
      "<p>Use Storage Transfer Service with Cloud VPN for secure data transfer.</p>",
      "<p>Use gsutil with multi-threaded/multi-processing options to copy data directly into Cloud Storage.</p>",
      "<p>Use Cloud Dataflow to transfer the data.</p>",
      "<p>Use a combination of Google Cloud's Data Transfer Service and gsutil for multi-threaded transfers.</p>"
    ],
    "explanation": "<p>Use Storage Transfer Service with Cloud VPN for secure data transfer. -&gt;&nbsp;Correct. Storage Transfer Service is designed to move large amounts of data from online and on-premises sources to Cloud Storage. When combined with Cloud VPN, this option would provide both secure and efficient data transfer. Cloud VPN provides secure and private connections for transferring the data, which addresses the company's security concerns.</p><p><br></p><p>Use gsutil with multi-threaded/multi-processing options to copy data directly into Cloud Storage. -&gt; Incorrect. gsutil with multi-threaded/multi-processing options is a great way to move data into GCS, but with such a large amount of data (50 TB), it's not the most efficient or fastest option. Plus, it doesn't inherently include a mechanism for ensuring a secure connection.</p><p><br></p><p>Use Cloud Dataflow to transfer the data. -&gt; Incorrect. Cloud Dataflow is primarily used for stream and batch processing of data. While it could theoretically be used for the transfer, it is not its primary use case and would not be as efficient as the Storage Transfer Service for large volumes of data.</p><p><br></p><p>Use a combination of Google Cloud's Data Transfer Service and gsutil for multi-threaded transfers. -&gt;&nbsp;Incorrect. Using a combination of Google Cloud's Data Transfer Service and gsutil for multi-threaded transfers could potentially speed up the data transfer, but it would add unnecessary complexity. Additionally, it doesn't inherently address the security concerns, unlike the Cloud VPN.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681638,
    "question_plain": "Your company is building a multi-tier web application in Google Cloud. The application has a web frontend, an application backend, and a database backend. The database backend is critical to the operation of the application and must be highly available and scalable. How would you design the database backend to meet these requirements?",
    "answers": [
      "<p>Use a single Cloud SQL instance for the database backend.</p>",
      "<p>Use a single Bigtable instance for the database backend.</p>",
      "<p>Use a Cloud SQL read replica for the database backend and configure automatic failover.</p>",
      "<p>Use a Cloud Spanner instance for the database backend.</p>"
    ],
    "explanation": "<p>Use a Cloud Spanner instance for the database backend. -&gt;&nbsp;Correct. Using a Cloud Spanner instance for the database backend is the recommended approach for several reasons. Cloud Spanner is a globally distributed, horizontally scalable database service that offers strong consistency, high availability, and automatic scaling. It provides ACID transactions and allows for distributed data storage across multiple regions, ensuring data resilience and low latency access. Cloud Spanner is designed to handle critical workloads and can meet the requirements of a highly available and scalable database backend for a multi-tier web application.</p><p><br></p><p>Use a single Cloud SQL instance for the database backend. -&gt;&nbsp;Incorrect. Cloud SQL offers managed database services, a single instance can become a single point of failure, and scaling can be limited compared to other options.</p><p><br></p><p>Use a single Bigtable instance for the database backend. -&gt;&nbsp;Incorrect. Using a single Bigtable instance for the database backend may not be the most suitable choice for a multi-tier web application. Bigtable is a NoSQL database designed for high throughput and scalability, but it may not provide the same level of consistency and transactional capabilities required for critical database operations.</p><p><br></p><p>Use a Cloud SQL read replica for the database backend and configure automatic failover. -&gt;&nbsp;Incorrect. Using a Cloud SQL read replica for the database backend and configuring automatic failover can provide improved availability by having a standby replica. However, this approach may not provide the same level of scalability and performance as the correct option. Cloud SQL read replicas are primarily used for read scaling and may not be the best fit for high availability and scalability in critical database operations.</p><p><br></p><p>https://cloud.google.com/spanner/docs</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681640,
    "question_plain": "A company wants to build a data processing pipeline that ingests data from various sources, transforms the data, and then stores the processed data in a data warehouse for analysis. The pipeline should be scalable, easy to manage, and able to handle changes in data sources and data processing requirements. How would you design this data processing pipeline using Google Cloud services?",
    "answers": [
      "<p>Use Cloud Functions to ingest data from the various sources, Cloud Dataflow to transform and store the data.</p>",
      "<p>Use Cloud Dataflow to ingest data from the various sources, Cloud Dataproc to transform the data, and BigQuery to store the processed data.</p>",
      "<p>Use Cloud Pub/Sub to ingest data from the various sources, Cloud Dataproc to transform and store the data.</p>",
      "<p>Use Cloud Storage to ingest data from the various sources, Cloud Dataproc to transform the data, and BigQuery to store the processed data.</p>"
    ],
    "explanation": "<p>Use Cloud Dataflow to ingest data from the various sources, Cloud Dataproc to transform the data, and BigQuery to store the processed data. -&gt; Correct. Cloud Dataflow is a fully-managed data processing service that can ingest data from various sources and transform it before storing it in BigQuery, which is a scalable and cost-effective data warehouse solution. Cloud Dataproc can also be used for data transformation but is more suited for heavy compute workloads. </p><p><br></p><p>Use Cloud Functions to ingest data from the various sources, Cloud Dataflow to transform and store the data. -&gt; Incorrect. Using Cloud Functions for ingestion may not be suitable for large-scale data processing.</p><p><br></p><p>Use Cloud Pub/Sub to ingest data from the various sources, Cloud Dataproc to transform and store the data. -&gt; Incorrect. Using Cloud Pub/Sub for ingestion may require additional services for data transformation and storage. </p><p><br></p><p>Use Cloud Storage to ingest data from the various sources, Cloud Dataproc to transform the data, and BigQuery to store the processed data. -&gt; Incorrect. Using Cloud Storage for ingestion may not provide sufficient data processing capabilities.</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681642,
    "question_plain": "Your company is developing a new mobile application that needs to store user data in the cloud. The data must be highly available and durable, and the application must be able to read and write data even when the device is offline. How would you store the user data in Google Cloud to meet these requirements?",
    "answers": [
      "<p>Use Cloud Datastore to store the user data.</p>",
      "<p>Use Cloud Firestore in Native mode to store the user data.</p>",
      "<p>Use Cloud Firestore in Datastore mode to store the user data.</p>",
      "<p>Use Cloud Bigtable to store the user data.</p>"
    ],
    "explanation": "<p>Use Cloud Firestore in native mode to store the user data. -&gt;&nbsp;Correct. Cloud Firestore in Native mode is the most effective solution for storing user data in the cloud with high availability and durability, and allowing the application to read and write data even when the device is offline. Cloud Firestore is a document database that allows for flexible data modeling and querying. In Native mode, Cloud Firestore provides multi-region replication and automatic scaling, making it highly available and durable. It also provides offline data synchronization and conflict resolution, allowing the application to continue working even when the device is offline. This makes it a good fit for mobile applications that require offline functionality.</p><p><br></p><p>Use Cloud Datastore to store the user data. -&gt; Incorrect. Cloud Datastore is an older version of Cloud Firestore and does not provide the same level of offline functionality. </p><p><br></p><p>Use Cloud Firestore in Datastore mode to store the user data. -&gt; Incorrect. Cloud Firestore in Datastore mode is a compatibility mode that provides the same API as Cloud Datastore, but does not include the same features as Cloud Firestore in Native mode.</p><p><br></p><p>Use Cloud Bigtable to store the user data. -&gt; Incorrect. Cloud Bigtable is a NoSQL database designed for high throughput and low latency, but does not provide the same level of offline functionality as Cloud Firestore in Native mode.</p><p><br></p><p>https://cloud.google.com/datastore/docs/firestore-or-datastore</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681644,
    "question_plain": "Your company wants to use Google Cloud to host a static website that will receive high amounts of traffic. The website must be highly available and must load quickly for users around the world. How would you host the website in Google Cloud to meet these requirements?",
    "answers": [
      "<p>Host the website on a single Compute Engine virtual machine and use a global load balancer to distribute traffic.</p>",
      "<p>Host the website on a single App Engine Standard environment instance.</p>",
      "<p>Host the website on multiple Compute Engine virtual machines in different regions and use a regional load balancer to distribute traffic.</p>",
      "<p>Host the website on Cloud Storage and use Cloud CDN to distribute traffic.</p>"
    ],
    "explanation": "<p>Host the website on Cloud Storage and use Cloud CDN to distribute traffic. -&gt;&nbsp;Correct. Cloud CDN (Content Delivery Network) uses Google's globally distributed edge points of presence to cache external HTTP(S) load balanced content close to your users. Caching content at the edges of Google's network provides faster delivery of content to your users while reducing serving costs.</p><p><br></p><p>Host the website on a single Compute Engine virtual machine and use a global load balancer to distribute traffic. -&gt;&nbsp;Incorrect. Hosting the website on a single Compute Engine virtual machine and using a global load balancer to distribute traffic is not the most optimal solution for high availability and quick loading times. If the single virtual machine fails, the website will become inaccessible. Additionally, the response time for users located far away from the virtual machine's location may be slower.</p><p><br></p><p>Host the website on a single App Engine Standard environment instance. -&gt;&nbsp;Incorrect. Hosting the website on a single App Engine Standard environment instance can provide scalability and automatic load balancing, but it may not provide the same level of performance and global availability as other options. App Engine Standard is limited to specific regions and may not have the same level of caching and content delivery optimizations as Cloud CDN.</p><p><br></p><p>Host the website on multiple Compute Engine virtual machines in different regions and use a regional load balancer to distribute traffic. -&gt;&nbsp;Incorrect. Hosting the website on multiple Compute Engine virtual machines in different regions and using a regional load balancer can provide high availability and better performance by distributing traffic across regions. However, managing and synchronizing multiple virtual machines across regions can add complexity and increase costs. It may not be the most cost-effective and straightforward solution for a static website.</p><p><br></p><p>https://cloud.google.com/cdn/docs</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681646,
    "question_plain": "Your company wants to deploy a batch processing workload in Google Cloud using Apache Spark. The workload consists of a large number of compute-intensive tasks that can be executed in parallel. The workload must be scalable and cost-effective. How would you deploy the batch processing workload in Google Cloud to meet these requirements?",
    "answers": [
      "<p>Use Cloud Functions to run the batch processing tasks.</p>",
      "<p>Use a fleet of Compute Engine virtual machines to run the batch processing tasks.</p>",
      "<p>Use Cloud Dataproc to run the batch processing tasks.</p>",
      "<p>Use Cloud Batch to run the batch processing tasks.</p>"
    ],
    "explanation": "<p>Use Cloud Dataproc to run the batch processing tasks. -&gt;&nbsp;Correct. Dataproc is a managed Apache Spark and Apache Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data.</p><p><br></p><p>Use Cloud Functions to run the batch processing tasks. -&gt;&nbsp;Incorrect. Using Cloud Functions to run the batch processing tasks is not the most suitable option for a compute-intensive workload. Cloud Functions are primarily designed for lightweight, event-driven functions and may not provide the necessary scalability and performance for parallel execution of compute-intensive tasks.</p><p><br></p><p>Use a fleet of Compute Engine virtual machines to run the batch processing tasks. -&gt;&nbsp;Incorrect. Using a fleet of Compute Engine virtual machines to run the batch processing tasks can be a viable option for parallel execution. However, it requires manual management of the virtual machines, scaling, and configuration. This approach may not be as cost-effective and easy to manage compared to dedicated managed services designed specifically for batch processing workloads.</p><p><br></p><p>Use Cloud Batch to run the batch processing tasks. -&gt;&nbsp;Incorrect. Batch is a fully managed service that lets you schedule, queue, and execute batch processing workloads on Google Cloud resources, but it does not allow you to use Apache Spark.</p><p><br></p><p>https://cloud.google.com/dataproc/docs</p><p>https://cloud.google.com/dataproc-serverless/docs/quickstarts/spark-batch</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681648,
    "question_plain": "You are designing a cloud-based architecture for a company that requires high availability and fault tolerance for its critical application. Which of the following options provides the best solution?",
    "answers": [
      "<p>Use a single instance virtual machine in one region with an attached disk.</p>",
      "<p>Use multiple instance virtual machines across different regions with a load balancer.</p>",
      "<p>Use a serverless computing approach with AWS Lambda.</p>",
      "<p>Use a container-based approach with Kubernetes.</p>"
    ],
    "explanation": "<p>Use multiple instance virtual machines across different regions with a load balancer. -&gt; Correct. Using multiple instance virtual machines across different regions with a load balancer is the best solution for high availability and fault tolerance, as it ensures that the application is available even if one or more instances fail, and it distributes the load across multiple instances to ensure that the application can handle high traffic. By using different regions, the application is also protected against regional failures.</p><p><br></p><p>Use a single instance virtual machine in one region with an attached disk. -&gt; Incorrect. Using a single instance virtual machine in one region with an attached disk does not provide high availability or fault tolerance, as the application would be vulnerable to single points of failure.</p><p><br></p><p>Use a serverless computing approach with AWS Lambda. -&gt; Incorrect. Using a serverless computing approach with AWS Lambda may provide high availability, but it does not provide fault tolerance, as the application is still dependent on the underlying cloud infrastructure.</p><p><br></p><p>Use a container-based approach with Kubernetes. -&gt; Incorrect. Using a container-based approach with Kubernetes may provide both high availability and fault tolerance, but it requires more complex setup and management.</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681650,
    "question_plain": "As a cloud architect, you are working with a global company that uses Cloud Storage for data storage. The company has a large number of contractors who need to upload data to a specific Cloud Storage bucket, but they should not have any other access rights to the data or the other resources. At the same time, the data engineers should have the ability to manage all Cloud Storage resources. What IAM roles should be assigned to the contractors and the data engineers?",
    "answers": [
      "<p>Assign roles/storage.objectCreator to contractors and roles/storage.admin to data engineers.</p>",
      "<p>Assign roles/storage.objectViewer to contractors and roles/storage.objectAdmin to data engineers.</p>",
      "<p>Assign roles/storage.objectAdmin to contractors and roles/storage.admin to data engineers.</p>",
      "<p>Assign roles/storage.objectCreator to contractors and roles/storage.objectAdmin to data engineers.</p>"
    ],
    "explanation": "<p>Assign roles/storage.objectCreator to contractors and roles/storage.admin to data engineers. -&gt; Correct. The roles/storage.objectCreator role would give contractors the ability to create objects (i.e., upload data) in the Cloud Storage bucket, but would not allow them to access or delete data. The roles/storage.admin role would give data engineers full control over Cloud Storage resources, allowing them to manage buckets and objects.</p><p><br></p><p>Assign roles/storage.objectViewer to contractors and roles/storage.objectAdmin to data engineers. -&gt; Incorrect. The roles/storage.objectViewer role would only give contractors the ability to view objects, not to upload data. The roles/storage.objectAdmin role would give data engineers the ability to manage objects but would not allow them to manage buckets.</p><p><br></p><p>Assign roles/storage.objectAdmin to contractors and roles/storage.admin to data engineers. -&gt; Incorrect. The roles/storage.objectAdmin role would give contractors the ability to read, write, and delete objects, which is more access than they need.</p><p><br></p><p>Assign roles/storage.objectCreator to contractors and roles/storage.objectAdmin to data engineers. -&gt; Incorrect. The roles/storage.objectCreator role would give contractors the ability to create objects, which is correct. However, the roles/storage.objectAdmin role would only give data engineers the ability to manage objects, not buckets.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681652,
    "question_plain": "As a cloud architect, you are assisting a client to transfer a large set of files from their local data center to Cloud Storage. The client has asked to minimize the costs associated with network egress. They have a Cloud Interconnect connection set up and want to transfer files over this connection. What is the most appropriate gsutil command to use for this purpose?",
    "answers": [
      "<p><code>gsutil -m cp -r ./local_directory gs://bucket_name</code> </p>",
      "<p><code>gsutil cp -r ./local_directory gs://bucket_name</code> </p>",
      "<p><code>gsutil -o \"Boto:parallel_composite_upload_threshold=150M\" cp -r ./local_directory gs://bucket_name</code> </p>",
      "<p><code>gsutil -o \"GSUtil:parallel_composite_upload_threshold=150M\" cp -r ./local_directory gs://bucket_name</code> </p>"
    ],
    "explanation": "<p><code>gsutil -m cp -r ./local_directory gs://bucket_name</code> -&gt; Correct. The <code>-m</code> option enables parallel uploads and downloads, which can speed up the transfer process. It doesn't explicitly minimize egress costs, but the faster the files are transferred, the shorter the duration of the data transfer, potentially reducing costs associated with a prolonged transfer.</p><p><br></p><p><code>gsutil cp -r ./local_directory gs://bucket_name</code> -&gt; Incorrect. This command does not utilize parallel uploads and downloads, and therefore it won't be as efficient as using the -m option.</p><p><br></p><p><code>gsutil -o \"Boto:parallel_composite_upload_threshold=150M\" cp -r ./local_directory gs://bucket_name</code> -&gt; Incorrect. The <code>Boto:parallel_composite_upload_threshold=150M</code> option tells gsutil to use composite uploads for files larger than 150M, but it doesn't help with cost reduction directly nor makes use of the Cloud Interconnect setup.</p><p><br></p><p><code>gsutil -o \"GSUtil:parallel_composite_upload_threshold=150M\" cp -r ./local_directory gs://bucket_name</code> -&gt; Incorrect. There is no <code>GSUtil:parallel_composite_upload_threshold=150M</code> option in gsutil. The <code>parallel_composite_upload_threshold</code> parameter is part of the Boto configuration.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681654,
    "question_plain": "Your company is developing a new IoT application that is expected to produce massive amounts of data for real-time analytics and future predictive models. Currently, the IoT devices publish data to a Pub/Sub topic, which then triggers a Cloud Function to store the data in BigQuery. What can be done to enhance this solution as data volume increases?",
    "answers": [
      "<p>Migrate the data processing from Cloud Function to Dataflow to better manage streaming data.</p>",
      "<p>Increase the memory and CPU allocated to the Cloud Function to process more data.</p>",
      "<p>Migrate from BigQuery to Cloud SQL for data storage.</p>",
      "<p>Use Firestore to store IoT data because of its real-time capabilities.</p>"
    ],
    "explanation": "<p>Migrate the data processing from Cloud Function to Dataflow to better manage streaming data. -&gt;&nbsp;Correct. Cloud Dataflow is more suitable for processing large amounts of streaming or batch data compared to Cloud Function, which is designed for lightweight, event-driven computing tasks.</p><p><br></p><p>Increase the memory and CPU allocated to the Cloud Function to process more data. -&gt; Incorrect. Although this could marginally improve performance, it wouldn't significantly affect the system's ability to handle a significant increase in data volume.</p><p><br></p><p>Migrate from BigQuery to Cloud SQL for data storage. -&gt; Incorrect. Cloud SQL isn't designed to handle the same volume of data as BigQuery, and it's not optimized for analytics.</p><p><br></p><p>Use Firestore to store IoT data because of its real-time capabilities. -&gt; Incorrect. Firestore provides real-time updates and can be suitable for certain use cases, but it's not designed for massive-scale analytics like BigQuery.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681656,
    "question_plain": "You're creating a custom VPC in Google Cloud with several subnets. One of your subnets requires communication with an on-premises network via Cloud VPN. What should you consider when designing this scenario?",
    "answers": [
      "<p>Ensure the CIDR range of the VPC subnet does not overlap with the on-premises network.</p>",
      "<p>The CIDR range of the on-premises network and the VPC subnet should be the same for easy routing.</p>",
      "<p>Assign public IP addresses to all instances in the VPC subnet.</p>",
      "<p>Create a Shared VPC instead of a custom VPC.</p>"
    ],
    "explanation": "<p>Ensure the CIDR range of the VPC subnet does not overlap with the on-premises network. -&gt;&nbsp;Correct. To set up Cloud VPN, the CIDR range of the VPC subnet and the on-premises network must not overlap.</p><p><br></p><p>The CIDR range of the on-premises network and the VPC subnet should be the same for easy routing. -&gt; Incorrect. Overlapping IP ranges between your on-premises network and VPC subnets would cause routing issues.</p><p><br></p><p>Assign public IP addresses to all instances in the VPC subnet. -&gt; Incorrect. Assigning public IP addresses to all instances is not necessary for communication with an on-premises network via Cloud VPN, as VPN connections operate at the network level.</p><p><br></p><p>Create a Shared VPC instead of a custom VPC. -&gt; Incorrect. Shared VPC allows resources from multiple projects to communicate with each other securely. However, it's not inherently necessary for a VPC subnet to communicate with an on-premises network.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 71681658,
    "question_plain": "You are configuring a fleet of Compute Engine instances that need to communicate with each other for data processing tasks. These instances need to access a Google Cloud Storage bucket. You want to optimize network throughput and costs. What should you consider?",
    "answers": [
      "<p>Place instances and Cloud Storage bucket in the same region.</p>",
      "<p>Use Compute Engine instances with higher vCPU count and memory to increase network performance.</p>",
      "<p>Enable Cloud CDN on Compute Engine instances.</p>",
      "<p>Use dedicated Interconnect to connect instances and Cloud Storage.</p>"
    ],
    "explanation": "<p>Place instances and Cloud Storage bucket in the same region. -&gt;&nbsp;Correct. Placing your Compute Engine instances and the Cloud Storage bucket in the same region reduces latency and egress costs.</p><p><br></p><p>Use Compute Engine instances with higher vCPU count and memory to increase network performance. -&gt;&nbsp;Incorrect. While instances with more vCPUs have better network performance, using larger instances isn't the only or the most cost-effective way to optimize network throughput.</p><p><br></p><p>Enable Cloud CDN on Compute Engine instances. -&gt;&nbsp;Incorrect. Cloud CDN is beneficial for content delivery to end users and would not optimize network throughput between Compute Engine instances and a Cloud Storage bucket.</p><p><br></p><p>Use dedicated Interconnect to connect instances and Cloud Storage. -&gt;&nbsp;Incorrect. Dedicated Interconnect provides direct physical connections to Google's network and would not be useful in this scenario, as both Compute Engine instances and Cloud Storage are already on Google's network.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297176,
    "question_plain": "As a cloud architect, you want to control expenses and you want to be automatically informed about project expenses so that you can take action when you get close to your limit. What should you do?",
    "answers": [
      "<p>Create a budget alert for the appropriate levels for your total monthly budget (for example: 50%, 90%, 100%).</p>",
      "<p>Set up a credit card with a monthly limit equal to your budget.</p>",
      "<p>Set up a PayPal account with a monthly limit equal to your budget.</p>",
      "<p>You can't automatically control your Google Cloud expenses.</p>"
    ],
    "explanation": "<p>Create a budget alert for the appropriate levels for your total monthly budget (for example: 50%, 90%, 100%). -&gt; Correct. This option is correct because creating a budget alert can help control expenses by monitoring project spending and alerting when certain thresholds are reached. Budget alerts can be set up for total monthly spending or for specific services and can be set up to trigger at different levels to allow for proactive action to be taken to avoid overspending.</p><p><br></p><p>Set up a credit card with a monthly limit equal to your budget. -&gt; Incorrect. While this option can help control expenses, it doesn't provide automatic notification when project expenses are nearing the limit. This option also doesn't provide the same level of granularity as budget alerts and may require manual monitoring to ensure expenses are within the monthly limit.</p><p><br></p><p>Set up a PayPal account with a monthly limit equal to your budget. -&gt; Incorrect. This option doesn't provide automatic notification when project expenses are nearing the limit and doesn't offer the same level of granularity as budget alerts.</p><p><br></p><p>You can't automatically control your Google Cloud expenses. -&gt; Incorrect. This option is incorrect because Google Cloud provides several tools, such as budget alerts and quotas, to help control expenses and be automatically informed about project expenses.</p><p><br></p><p>https://cloud.google.com/billing/docs/how-to/budgets</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297178,
    "question_plain": "As a cloud architect, you advise the development team. They have a new application and want to deploy it to a production environment. You need to estimate the costs of running this application in App Engine. What should you do?",
    "answers": [
      "<p>Use the Google Cloud Price Calculator to accurately estimate expected expenses.</p>",
      "<p>Create a ticket for Cloud Billing Support to get an estimate.</p>",
      "<p>Calculate costs based on the current price list.</p>",
      "<p>Calculate costs based on the expenses incurred during the development stage.</p>"
    ],
    "explanation": "<p>Use the Google Cloud Price Calculator to accurately estimate expected expenses. -&gt; Correct. The Google Cloud Pricing Calculator provides an accurate estimate of the expected expenses of running an application in App Engine. The calculator takes into account factors such as the type and number of instances, storage, network usage, and other usage metrics that can impact cost.</p><p><br></p><p>Create a ticket for Cloud Billing Support to get an estimate. -&gt;&nbsp;Incorrect. Creating a ticket for Cloud Billing Support to get an estimate may not be the most efficient method for estimating costs. While Cloud Billing Support can provide assistance with billing-related inquiries, it may take time to receive a response, which could delay the estimation process.</p><p><br></p><p>Calculate costs based on the current price list. -&gt;&nbsp;Incorrect. Calculating costs based on the current price list can give you a general idea of the pricing structure for App Engine, but it may not provide an accurate estimate tailored to the specific requirements of the application. The price list typically provides the cost of each resource individually, and calculating the total costs based on these rates may not consider the application's specific usage patterns and requirements.</p><p><br></p><p>Calculate costs based on the expenses incurred during the development stage. -&gt;&nbsp;Incorrect. It may not provide an accurate estimate for the production environment. The development stage expenses may not reflect the actual resource usage, traffic, and other factors that could influence the costs in a production environment. It's important to consider the differences between development and production environments when estimating costs.</p><p><br></p><p>https://cloud.google.com/products/calculator</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297180,
    "question_plain": "In a weather forecasting project where a company needs to process large amounts of time-stamped IoT data at high speed, which Google Cloud product should you recommend for efficient data write and change operations? Choose the correct option from the five given answers.",
    "answers": [
      "<p>Cloud Bigtable</p>",
      "<p>Cloud Datastore</p>",
      "<p>Cloud Storage</p>",
      "<p>Cloud Pub/Sub</p>"
    ],
    "explanation": "<p>Cloud Bigtable -&gt; Correct. Cloud Bigtable is the recommended product for efficiently handling high-speed data write and change operations, especially when dealing with large amounts of time-stamped IoT data. It is a NoSQL wide-column database that provides high scalability, low-latency reads and writes, and automatic scaling to handle petabytes of data.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. Cloud Datastore is a NoSQL document database and may not provide the same level of performance and scalability as Cloud Bigtable for high-speed data write and change operations.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. Cloud Storage is a scalable object storage solution but may not be optimized for high-speed data write and change operations, especially for time-stamped IoT data.</p><p><br></p><p>Cloud Pub/Sub -&gt; Incorrect. Cloud Pub/Sub is a messaging service for building event-driven systems and is not specifically designed for high-speed data write and change operations.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297182,
    "question_plain": "In a scenario where an application has a large international user group and runs stateless virtual machines in a Managed Instance Group across multiple Google Cloud locations, which storage solution is recommended for storing and analyzing large volumes of log data generated by the application? Choose the correct option from the five given answers.",
    "answers": [
      "<p>Cloud Logging with Cloud Storage export</p>",
      "<p>Persistent SSD on virtual machine instances</p>",
      "<p>Cloud Memorystore for Redis</p>",
      "<p>Cloud Datastore</p>"
    ],
    "explanation": "<p>Cloud Logging with Cloud Storage export -&gt; Correct. Cloud Logging with Cloud Storage export is the recommended storage solution for storing and analyzing large volumes of log data. Cloud Logging allows you to centralize and aggregate logs generated by the application, while the export feature enables you to export logs to Cloud Storage for long-term storage or further analysis using other tools.</p><p><br></p><p>Persistent SSD on virtual machine instances -&gt; Incorrect. Persistent SSD on virtual machine instances is not suitable for storing and analyzing large volumes of log data. It is more suitable for fast and reliable storage of application-specific data within the virtual machine instances themselves.</p><p><br></p><p>Cloud Memorystore for Redis -&gt; Incorrect. Cloud Memorystore for Redis is an in-memory caching service and is not specifically designed for storing and analyzing log data. It is better suited for caching frequently accessed data to improve application performance.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. Cloud Datastore is a NoSQL document database and is not the ideal solution for storing and analyzing log data at scale.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297184,
    "question_plain": "A development team needs to deploy a web application that will scale based on HTTP traffic. They have an instance template that contains this web application. What should you advise them?",
    "answers": [
      "<p>They should create a Managed Instance Group based on the instance template. Then configure autoscaling based on HTTP traffic and configure the instance group as the backend service of an HTTP load balancer.</p>",
      "<p>They should create a virtual machine from the instance template. Then create an App Engine application in Automatic Scaling mode that forwards all traffic to this virtual machine.</p>",
      "<p>They should create the necessary number of instances required for peak traffic based on the instance template.</p>",
      "<p>They should create a Managed Instance Group based on the instance template. Then configure autoscaling based on CPU utilization.</p>"
    ],
    "explanation": "<p>They should create a Managed Instance Group based on the instance template. Then configure autoscaling based on HTTP traffic and configure the instance group as the backend service of an HTTP load balancer. -&gt; Correct. This option meets the requirements by automatically scaling the number of instances in response to HTTP traffic, and the HTTP load balancer distributes traffic evenly among the instances.</p><p><br></p><p>They should create a virtual machine from the instance template. Then create an App Engine application in Automatic Scaling mode that forwards all traffic to this virtual machine. -&gt; Incorrect. This option is not ideal as it adds unnecessary complexity by involving App Engine, which is designed to work with its own scalable environment.</p><p><br></p><p>They should create the necessary number of instances required for peak traffic based on the instance template. -&gt; Incorrect. This option doesn't meet the requirement for automatic scaling, and the development team would need to manually manage the number of instances, which is not an efficient solution.</p><p><br></p><p>They should create a Managed Instance Group based on the instance template. Then configure autoscaling based on CPU utilization. -&gt; Incorrect. This option doesn't meet the requirement for scaling based on HTTP traffic, which is what the development team needs.</p><p><br></p><p>https://cloud.google.com/compute/docs/autoscaler</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297186,
    "question_plain": "A development team needs to create a Kubernetes Engine cluster to deploy multiple pods and use BigQuery to store all container logs for later analysis. What solution can you advise to follow Google's best practices?",
    "answers": [
      "<p>They should enable Cloud Logging when creating a Kubernetes Engine cluster.</p>",
      "<p>They should enable Cloud Monitoring when creating a Kubernetes Engine cluster.</p>",
      "<p>They should use the Cloud Logging export feature to create a sink to Cloud Storage, than create a Cloud Dataflow job that imports log files from Cloud Storage to BigQuery.</p>",
      "<p>The only solution is to develop a custom add-on that uses the Cloud Logging API and BigQuery API.</p>"
    ],
    "explanation": "<p>They should enable Cloud Logging when creating a Kubernetes Engine cluster. -&gt; Correct. This is the recommended approach as Cloud Logging provides a centralized location for logging across multiple Google Cloud Platform services, including Kubernetes Engine. Cloud Logging automatically collects logs generated by the Kubernetes Engine cluster and stores them in a single location. This makes it easy to search, analyze, and troubleshoot logs from multiple sources.</p><p><br></p><p>They should enable Cloud Monitoring when creating a Kubernetes Engine cluster. -&gt; Incorrect. While Cloud Monitoring can be used to monitor Kubernetes Engine clusters, it doesn't provide a solution for storing all container logs in BigQuery for later analysis.</p><p><br></p><p>They should use the Cloud Logging export feature to create a sink to Cloud Storage, than create a Cloud Dataflow job that imports log files from Cloud Storage to BigQuery. -&gt; Incorrect. This option is not necessary as Cloud Logging can already export logs to BigQuery.</p><p><br></p><p>The only solution is to develop a custom add-on that uses the Cloud Logging API and BigQuery API. -&gt; Incorrect. While this option is technically feasible, it requires additional development effort and maintenance, which is not necessary as there is an existing, recommended solution provided in the correct option.</p><p><br></p><p>https://cloud.google.com/logging/docs/quickstart</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297188,
    "question_plain": "You are a cloud architect responsible for a microservices-based application deployed in Google Kubernetes Engine (GKE). The application consists of several interacting containers. To optimize performance, you want to ensure that these containers are located as close to each other as possible. Which of the following would be the most effective way to achieve this?",
    "answers": [
      "<p>Use the <code>PodAffinity</code> and <code>PodAntiAffinity</code> rules in your Kubernetes pod specification.</p>",
      "<p>Use node taints and tolerations to force certain containers to run on specific nodes.</p>",
      "<p>Use GKE Regional Clusters to deploy the containers across multiple zones in the same region.</p>",
      "<p>Increase the number of replicas for each container to ensure they are distributed across all nodes in the cluster.</p>"
    ],
    "explanation": "<p>Use the <code>PodAffinity</code> and <code>PodAntiAffinity</code> rules in your Kubernetes pod specification. -&gt; Correct. The <code>PodAffinity</code> and <code>PodAntiAffinity</code> rules are designed to control how Kubernetes schedules pods relative to each other. By properly configuring these rules, you can make sure that certain pods (and therefore the containers inside them) are co-located on the same node or within the same zone.</p><p><br></p><p>Use node taints and tolerations to force certain containers to run on specific nodes. -&gt; Incorrect. Node taints and tolerations are used to restrict the kinds of pods that can be scheduled on specific nodes, but they are not the best way to ensure that specific containers are co-located.</p><p><br></p><p>Use GKE Regional Clusters to deploy the containers across multiple zones in the same region. -&gt; Incorrect. GKE Regional Clusters are used to increase the availability of your application by deploying across multiple zones in the same region. This strategy might spread your containers further apart, not bring them closer together.</p><p><br></p><p>Increase the number of replicas for each container to ensure they are distributed across all nodes in the cluster. -&gt;&nbsp;Incorrect. Increasing the number of replicas can improve the availability and performance of your application by distributing the workload across more containers. However, it does not ensure that specific containers are co-located.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297190,
    "question_plain": "In an e-commerce project where a customer wants to balance traffic between backend virtual machines in a multi-tier application, which load balancing option should they choose? Choose the correct option from the five given answers.",
    "answers": [
      "<p>The regional internal load balancer</p>",
      "<p>The global TCP proxy</p>",
      "<p>The global SSL proxy</p>",
      "<p>The network load balancer</p>"
    ],
    "explanation": "<p>The regional internal load balancer -&gt; Correct. The regional internal load balancer is the recommended choice for balancing traffic between backend virtual machines in a multi-tier application within a specific region. It ensures that traffic is distributed evenly across the virtual machines to optimize performance and avoid overloading any specific instance.</p><p><br></p><p>The global TCP proxy -&gt; Incorrect. The global TCP proxy load balancer is designed for distributing TCP traffic globally and is not specifically focused on load balancing traffic between backend virtual machines in a single region.</p><p><br></p><p>The global SSL proxy -&gt; Incorrect. The global SSL proxy load balancer is designed for distributing SSL traffic globally and is not specifically focused on load balancing traffic between backend virtual machines in a single region.</p><p><br></p><p>The network load balancer -&gt; Incorrect. The network load balancer is designed for load balancing traffic at the network level and is not specifically focused on load balancing traffic between backend virtual machines in a multi-tier application.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297192,
    "question_plain": "You don't know how long your data will be stored in the Google Cloud bucket and you are currently using Standard storage class. Which bucket feature you can use to switch storage class for objects when they reach or pass a certain age (for example 30 days)?",
    "answers": [
      "<p>Object lifecycle management rules.</p>",
      "<p>Object versioning setting.</p>",
      "<p>Object permissions.</p>",
      "<p>Object protection.</p>"
    ],
    "explanation": "<p>Object lifecycle management rules. -&gt; Correct. Object lifecycle management rules can be used to switch the storage class for objects in a Google Cloud bucket based on certain conditions, such as their age. With lifecycle management rules, you can define actions to be taken on objects after a specific duration, like transitioning them to a different storage class or deleting them. In this case, you can set a rule to switch the storage class for objects when they reach or pass a certain age, such as 30 days.</p><p><br></p><p>Object versioning setting. -&gt;&nbsp;Incorrect. Object versioning setting in Google Cloud Storage enables the storage and retrieval of multiple versions of an object. However, it does not directly address the need to switch the storage class based on the age of the objects.</p><p><br></p><p>Object permissions. -&gt;&nbsp;Incorrect. Object permissions determine who has access to the objects and what actions they can perform on them. While object permissions are essential for controlling access to the objects, they do not provide a feature for automatically switching the storage class based on the age of the objects.</p><p><br></p><p>Object protection. -&gt;&nbsp;Incorrect. Object protection typically refers to mechanisms for securing objects from unauthorized access or accidental deletion. While object protection is important for maintaining data integrity and security, it does not provide the functionality to switch the storage class based on the age of the objects.</p><p><br></p><p>https://cloud.google.com/storage/docs/lifecycle</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297194,
    "question_plain": "Your company is developing a mobile game application that is expected to have millions of users worldwide. The application needs to read and write user data quickly with low latency, and the data model is fairly simple. As the cloud architect, you have been tasked with choosing the most suitable NoSQL database for this requirement. Which of the following would be the most appropriate choice?",
    "answers": [
      "<p>Use Cloud Firestore in Datastore mode.</p>",
      "<p>Use Cloud Bigtable.</p>",
      "<p>Use Cloud SQL.</p>",
      "<p>Use Cloud Spanner.</p>"
    ],
    "explanation": "<p>Use Cloud Firestore in Datastore mode. -&gt; Correct. Firestore in Datastore mode is optimized for server-side applications that require massive scale and have a fairly simple data model, making it the best fit for this use case.</p><p><br></p><p>Use Cloud Bigtable. -&gt; Incorrect. Cloud Bigtable is designed for large analytical and operational workloads. However, for mobile and web applications where the data model is fairly simple, Firestore in Datastore mode is typically a more suitable choice.</p><p><br></p><p>Use Cloud SQL. -&gt; Incorrect. Cloud SQL is a fully managed relational database service, while the requirement here is for a NoSQL database. Thus, it doesn't fit the requirement.</p><p><br></p><p>Use Cloud Spanner. -&gt; Incorrect. Cloud Spanner is a fully managed relational database that provides strong consistency at a global scale. It is overkill for this scenario where the requirement is a simple NoSQL database.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297196,
    "question_plain": "Your company has implemented a microservices architecture on Google Cloud and needs to streamline their deployment processes. They want to implement a Continuous Integration/Continuous Deployment (CI/CD) pipeline. What is the most appropriate action to take?",
    "answers": [
      "<p>Utilize Google Cloud Source Repositories and set up triggers to automatically deploy code to App Engine.</p>",
      "<p>Use Cloud Functions to automate the deployment of services.</p>",
      "<p>Use Cloud Scheduler to schedule deployments.</p>",
      "<p>Manually deploy code to Compute Engine instances.</p>"
    ],
    "explanation": "<p>Utilize Google Cloud Source Repositories and set up triggers to automatically deploy code to App Engine. -&gt;&nbsp;Correct. This approach makes use of Cloud Source Repositories for version control and deploys the applications to App Engine when changes are made. This is the most suitable option for setting up a CI/CD pipeline.</p><p><br></p><p>Use Cloud Functions to automate the deployment of services. -&gt;&nbsp;Incorrect. While Cloud Functions can be used to automate certain processes, they aren't designed for managing an entire CI/CD pipeline.</p><p><br></p><p>Use Cloud Scheduler to schedule deployments. -&gt;&nbsp;Incorrect. Cloud Scheduler is more for scheduling tasks based on a timer, rather than reacting to events, such as a code push. It wouldn't be the best choice for a CI/CD pipeline.</p><p><br></p><p>Manually deploy code to Compute Engine instances. -&gt;&nbsp;Incorrect. Manual deployments would not allow for continuous integration and continuous deployment, defeating the purpose of a CI/CD pipeline.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297198,
    "question_plain": "You're a cloud architect of a technology company that is transitioning its existing monolithic application to a microservices architecture and intends to deploy these services as containerized applications on Google Cloud. The application must be highly available, scalable, and capable of rolling updates without downtime. Which of the following solutions should you use?",
    "answers": [
      "<p>Google Kubernetes Engine (GKE)</p>",
      "<p>Compute Engine instances with Docker installed</p>",
      "<p>Cloud Functions</p>",
      "<p>Cloud Run</p>"
    ],
    "explanation": "<p>Google Kubernetes Engine (GKE) -&gt; Correct. GKE is a managed, production-ready environment for deploying containerized applications. It offers high availability, scalability, and supports rolling updates without downtime. This makes it a perfect choice for a complex microservices-based application.</p><p><br></p><p>Compute Engine instances with Docker installed -&gt; Incorrect. Deploying containers on Compute Engine instances would not provide the orchestration capabilities such as automatic scaling, self-healing, and rolling updates that Kubernetes provides. Hence, this would not be the best solution in this scenario.</p><p><br></p><p>Cloud Functions -&gt; Incorrect. Cloud Functions is a serverless execution environment for building and connecting cloud services. While it does offer scalability, it does not support containerized applications and is more suitable for smaller, discrete tasks triggered by events.</p><p><br></p><p>Cloud Run -&gt; Incorrect. While Cloud Run is a great option for running containerized applications and does provide automatic scaling and high availability, it doesn't offer the same level of control over the infrastructure and configuration settings that GKE does. For more complex microservice architectures, GKE would be a better fit.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297200,
    "question_plain": "As a cloud architect of a firm, you need to implement a solution that regularly runs a batch job to transfer data from your company's CRM system to a BigQuery dataset. The transfer involves substantial data transformation. Considering cost optimization and the non-urgent, fault-tolerant nature of the task, you have opted to use preemptible VMs for this task. Which of the following would be the best approach?",
    "answers": [
      "<p>Use Cloud Composer to orchestrate the data transfer and transformation job using a preemptible Compute Engine instance.</p>",
      "<p>Create a Compute Engine instance with the necessary transformation scripts, schedule it to run at regular intervals using Cloud Scheduler, and use a regular (non-preemptible) VM instance.</p>",
      "<p>Create a Kubernetes Engine cluster with preemptible VMs and schedule the batch jobs using Kubernetes cron jobs.</p>",
      "<p>Use Dataproc to run the transformation job on a preemptible VM cluster.</p>"
    ],
    "explanation": "<p>Use Cloud Composer to orchestrate the data transfer and transformation job using a preemptible Compute Engine instance. -&gt; Correct. Cloud Composer, which is a managed Apache Airflow service, is designed to orchestrate complex workflows, making it a good fit for this scenario. It can leverage preemptible VMs to control costs.</p><p><br></p><p>Create a Compute Engine instance with the necessary transformation scripts, schedule it to run at regular intervals using Cloud Scheduler, and use a regular (non-preemptible) VM instance. -&gt; Incorrect. This option does not make use of preemptible VMs and may result in higher costs, which contradicts the requirements.</p><p><br></p><p>Create a Kubernetes Engine cluster with preemptible VMs and schedule the batch jobs using Kubernetes cron jobs. -&gt; Incorrect. While GKE with preemptible VMs could technically accomplish the task, the added complexity of managing a Kubernetes cluster might not be necessary if you're just running batch jobs. Cloud Composer would be a simpler and more manageable option.</p><p><br></p><p>Use Dataproc to run the transformation job on a preemptible VM cluster. -&gt; Incorrect. Dataproc is used for running big data tasks like Hadoop and Spark. It would be an overkill for this scenario where we are just transferring and transforming data to BigQuery.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297202,
    "question_plain": "Your company's development team is using a monorepo for their codebase. They need to set up a CI/CD pipeline that is capable of triggering a build only when changes are made to a certain directory in the repo. Which option below will achieve this?",
    "answers": [
      "<p>Utilize Cloud Build and create a trigger that filters on file changes within the desired directory.</p>",
      "<p>Use Google Cloud Functions to monitor the Git repo and manually trigger a build process when changes are detected.</p>",
      "<p>Use Cloud Scheduler to execute the build process at a specific time regardless of the changes.</p>",
      "<p>Use Google Cloud Storage to store the repo and set up object change notifications.</p>"
    ],
    "explanation": "<p>Utilize Cloud Build and create a trigger that filters on file changes within the desired directory. -&gt;&nbsp;Correct. Cloud Build supports using a regular expression to match file paths within your source repository when configuring a build trigger. This way, you can specify which directory changes will trigger the build.</p><p><br></p><p>Use Google Cloud Functions to monitor the Git repo and manually trigger a build process when changes are detected. -&gt;&nbsp;Incorrect. This approach is less efficient and more complex than simply using Cloud Build, and it involves maintaining an extra piece of infrastructure.</p><p><br></p><p>Use Cloud Scheduler to execute the build process at a specific time regardless of the changes. -&gt;&nbsp;Incorrect. While Cloud Scheduler can help execute scheduled tasks, it does not provide a solution for triggering builds based on changes to a specific directory.</p><p><br></p><p>Use Google Cloud Storage to store the repo and set up object change notifications. -&gt;&nbsp;Incorrect. This approach is not practical. Git repositories are better managed using dedicated source control tools, not object storage services.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297204,
    "question_plain": "A mission-critical application is migrated to Google Kubernetes Engine from your on-premises data center and uses e2-standard-4 machine types. How can your development team deploy additional pods on e2-standard-32 machine types without causing application downtime?",
    "answers": [
      "<p>They should create a new cluster with two node pools - one with <code>e2-standard-4</code> machine types and other with <code>e2-standard-32</code> machine types. Then deploy the application on this new cluster and remove the older one.</p>",
      "<p>They should update the existing cluster to add a new node pool with <code>e2-standard-32</code> machine types and deploy the pods.</p>",
      "<p>They should create a new cluster with node pool instances with <code>e2-standard-32</code> machine types. Then deploy the application on the new cluster and remove the older one.</p>",
      "<p>Your development team cannot deploy additional pods on <code>e2-standard-32</code> machine types, as this will cause application downtime.</p>"
    ],
    "explanation": "<p>They should create a new cluster with two node pools - one with <code>e2-standard-4</code> machine types and other with <code>e2-standard-32</code> machine types. Then deploy the application on this new cluster and remove the older one. -&gt; Correct. Creating a new cluster with two node pools, one with <code>e2-standard-4</code> machine types and the other with <code>e2-standard-32</code> machine types, allows the development team to deploy additional pods on the new node pool without causing application downtime. They can deploy the application on the new cluster and then remove the older one.</p><p><br></p><p>They should update the existing cluster to add a new node pool with <code>e2-standard-32</code> machine types and deploy the pods. -&gt; Incorrect. Updating the existing cluster to add a new node pool with <code>e2-standard-32</code> machine types could cause application downtime if the pods need to be rescheduled to the new node pool.</p><p><br></p><p>Your development team cannot deploy additional pods on <code>e2-standard-32</code> machine types, as this will cause application downtime. -&gt; incorrect. It suggests that it is not possible to deploy additional pods on <code>e2-standard-32</code> machine types without causing application downtime, which is not true.</p><p><br></p><p>They should create a new cluster with node pool instances with <code>e2-standard-32</code> machine types. Then deploy the application on the new cluster and remove the older one. -&gt; Incorrect. Creating a new cluster with node pool instances with <code>e2-standard-32</code> machine types requires more effort and would not be the most efficient solution.</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297206,
    "question_plain": "An internal company application is deployed with Compute Engine VMs. This application is used only during regular business hours. Your development team needs to backup the VMs outside the business hours and remove images older than 30 days to reduce expenses. As a cloud architect, what should you advise them?",
    "answers": [
      "<p>They should enable a snapshot schedule for automated creation of daily snapshots and set snapshot retention policy to 30 days.</p>",
      "<p>They should add three metadata tags on the Compute Engine instance&nbsp;(enabling snapshot creation, specifying the snapshot schedule, specifying the retention period = 30 days).</p>",
      "<p>They should use Cloud Scheduler to trigger a Cloud Function that creates snapshots of the disk on a daily basis. Also they should use Cloud Scheduler to trigger another Cloud Function that iterates over the snapshots and removes older than 30 days.</p>",
      "<p>They should use AppEngine Cron service to trigger a custom script that creates snapshots of the disk on a daily basis. Also they should use AppEngine Cron service to trigger another custom script that iterates over the snapshots and removes snapshots older than 30 days.</p>"
    ],
    "explanation": "<p>They should enable a snapshot schedule for automated creation of daily snapshots and set snapshot retention policy to 30 days. -&gt; Correct. Enabling a snapshot schedule for automated creation of daily snapshots and setting the snapshot retention policy to 30 days is the recommended approach. Compute Engine provides built-in snapshot functionality that allows for automated, regular backups of VM disks. By enabling the snapshot schedule and setting the retention policy to 30 days, the development team can ensure that backups are taken outside business hours and older images are automatically removed after the specified period, reducing expenses.</p><p><br></p><p>They should add three metadata tags on the Compute Engine instance&nbsp;(enabling snapshot creation, specifying the snapshot schedule, specifying the retention period = 30 days). -&gt;&nbsp;Incorrect. Compute Engine metadata tags are used for attaching additional information to instances but do not directly provide the backup and retention functionality required in this scenario.</p><p><br></p><p>They should use Cloud Scheduler to trigger a Cloud Function that creates snapshots of the disk on a daily basis. Also they should use Cloud Scheduler to trigger another Cloud Function that iterates over the snapshots and removes older than 30 days. -&gt;&nbsp;Incorrect. It introduces additional complexity compared to the built-in snapshot functionality provided by Compute Engine. It may not be the most straightforward and efficient solution for achieving the desired backup and retention tasks.</p><p><br></p><p>They should use AppEngine Cron service to trigger a custom script that creates snapshots of the disk on a daily basis. Also they should use AppEngine Cron service to trigger another custom script that iterates over the snapshots and removes snapshots older than 30 days. -&gt;&nbsp;Incorrect. App Engine is designed for building and running web applications and may not provide the same level of integration and functionality as Compute Engine for VM backups and retention.</p><p><br></p><p>https://cloud.google.com/compute/docs/disks/scheduled-snapshots</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297208,
    "question_plain": "Within your company, each developer possesses an individual development Google Cloud Platform (GCP) project associated with a central billing account. You have recommended that they establish alerts for any situations where a developer exceeds a monthly expenditure of $500. What actions should they take to implement these alerts?",
    "answers": [
      "<p>They should set up a budget for each development projects. Then, set an alert for each budget when expenses exceed $ 500.</p>",
      "<p>Export billing data from all development projects to a single BigQuery dataset. Use a Data Studio dashboard to plot expenses.</p>",
      "<p>They should set up a single budget for all development projects. Then, set an alert for budget when expenses exceed $ 500.</p>",
      "<p>They should set up a single budget for all development projects. Then, set an alert for budget when expenses exceed $ 500 multiplied by the number of developers.</p>"
    ],
    "explanation": "<p>They should set up a budget for each development projects. Then, set an alert for each budget when expenses exceed $ 500. -&gt; Correct. Setting up a budget for each development project and then setting an alert for each budget when expenses exceed $500 is the recommended approach. By creating separate budgets for each project, developers can track their individual expenses and receive alerts when they exceed the defined threshold. This ensures that each developer is notified when their expenditures reach or surpass the specified limit.</p><p><br></p><p>Export billing data from all development projects to a single BigQuery dataset. Use a Data Studio dashboard to plot expenses. -&gt;&nbsp;Incorrect. It does not provide a straightforward way to set up individual alerts for each developer's project when expenses exceed $500. It focuses more on analyzing expenses collectively rather than monitoring individual project budgets.</p><p><br></p><p>They should set up a single budget for all development projects. Then, set an alert for budget when expenses exceed $ 500. -&gt;&nbsp;Incorrect. It is not the most suitable solution for this scenario. Since each developer has their own individual project associated with a central billing account, it is more appropriate to have separate budgets and alerts for each project to monitor their specific expenditures.</p><p><br></p><p>They should set up a single budget for all development projects. Then, set an alert for budget when expenses exceed $ 500 multiplied by the number of developers. -&gt;&nbsp;Incorrect. It is not necessary or efficient. Each developer's project should be monitored individually, and setting up a separate budget and alert for each project is more appropriate than multiplying the budget threshold by the number of developers.</p><p><br></p><p>https://cloud.google.com/billing/docs/how-to/budgets</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297210,
    "question_plain": "There are three projects in your organization, for development, testing and production. Your manager wants to monitor resource utilization (RAM, disk, network, CPU) for all applications in these three projects. What should you do?",
    "answers": [
      "<p>You should create a Cloud Monitoring workspace in the production project and add development and testing projects to it.</p>",
      "<p>In Cloud Monitoring, share charts from development, testing and production projects.</p>",
      "<p>You should use the default Cloud Monitoring dashboards in all the projects.</p>",
      "<p>You cannot combine metrics from different projects.</p>"
    ],
    "explanation": "<p>You should create a Cloud Monitoring workspace in the production project and add development and testing projects to it. -&gt; Correct. It is the recommended approach. By creating a centralized workspace in the production project, you can monitor resource utilization for all applications across the three projects. This allows for a unified monitoring experience and provides a centralized view of resource metrics for all projects.</p><p><br></p><p>In Cloud Monitoring, share charts from development, testing and production projects. -&gt;&nbsp;Incorrect. It may not provide a comprehensive and centralized monitoring solution for monitoring resource utilization across all projects.</p><p><br></p><p>You should use the default Cloud Monitoring dashboards in all the projects. -&gt;&nbsp;Incorrect. It may not allow for a consolidated view or unified monitoring across all projects. The default dashboards are specific to individual projects and may not provide the desired visibility across all applications.</p><p><br></p><p>You cannot combine metrics from different projects. -&gt;&nbsp;Incorrect. With the appropriate setup, you can aggregate and combine metrics from different projects in Cloud Monitoring by creating a centralized workspace. This allows for monitoring resource utilization across multiple projects.</p><p><br></p><p>https://cloud.google.com/monitoring/docs</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297212,
    "question_plain": "In a multi-regional Cloud Storage bucket, your company stores Personally Identifiable Information (PII) of customers. Your compliance department has asked you to record all operations/requests on this bucket. What should you do?",
    "answers": [
      "<p>You should turn on data access audit logging in Cloud Storage to record this information.</p>",
      "<p>You should use the Identity-Aware Proxy API to record this information.</p>",
      "<p>You should use the Data Loss Prevention API to record this information.</p>",
      "<p>You should enable the default Cloud Storage service account exclusive access to read all operations and record them.</p>"
    ],
    "explanation": "<p>You should turn on data access audit logging in Cloud Storage to record this information. -&gt;&nbsp;Correct. To record all operations/requests on a multi-regional Cloud Storage bucket that stores Personally Identifiable Information (PII) of customers, data access audit logging in Cloud Storage should be turned on. Data access audit logs can provide information about who accessed the data, what actions were taken on the data, and when those actions were taken. This information can be useful for audit, compliance, and forensic purposes.</p><p><br></p><p>You should use the Identity-Aware Proxy API to record this information. -&gt; Incorrect. It is incorrect because the Identity-Aware Proxy API is used to authenticate and authorize users before they access an application or resource, not to record audit logs.</p><p><br></p><p>You should use the Data Loss Prevention API to record this information. -&gt; Incorrect. It is also incorrect because the Data Loss Prevention API is used to scan and analyze data to identify and prevent the accidental or intentional disclosure of sensitive data, not to record audit logs.</p><p><br></p><p>You should enable the default Cloud Storage service account exclusive access to read all operations and record them. -&gt; Incorrect. It is also incorrect because the default Cloud Storage service account is used to perform operations on objects in a bucket and does not have the capability to record audit logs.</p><p><br></p><p>https://cloud.google.com/logging/docs/audit</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297214,
    "question_plain": "Your company has a service that needs to run on a fleet of identical instances and scales according to traffic patterns. You are tasked with setting up a Managed Instance Group (MIG) on Google Cloud Platform. The instances are created from an instance template and a startup script, which fetches the latest version of the application code from a Cloud Storage bucket every time an instance starts. However, your company wants to avoid any potential downtime when deploying updates to the application. Which strategy should you recommend?",
    "answers": [
      "<p>Use a rolling update to gradually replace instances in the MIG.</p>",
      "<p>Increase the number of instances in the MIG before deploying an update to the application.</p>",
      "<p>Manually replace instances in the MIG after updating the application.</p>",
      "<p>Modify the application code to poll the Cloud Storage bucket for updates periodically.</p>"
    ],
    "explanation": "<p>Use a rolling update to gradually replace instances in the MIG. -&gt; Correct. Rolling updates gradually replace instances in the MIG, which helps to maintain service availability during updates. When an instance is replaced, the startup script will fetch the latest version of the application code from the Cloud Storage bucket.</p><p><br></p><p>Increase the number of instances in the MIG before deploying an update to the application. -&gt; Incorrect. Increasing the number of instances before an update would not ensure that the new instances fetch the updated application code. It would also increase costs.</p><p><br></p><p>Manually replace instances in the MIG after updating the application. -&gt; Incorrect. Manually replacing instances would not be an efficient use of resources, especially in a large MIG. It could also cause service interruptions.</p><p><br></p><p>Modify the application code to poll the Cloud Storage bucket for updates periodically. -&gt; Incorrect. Modifying the application to poll for updates would complicate the application logic and might not work if instances need to be replaced (e.g., for maintenance or if an instance becomes unhealthy). It also doesn't take advantage of the built-in capabilities of MIGs to handle updates.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297216,
    "question_plain": "A web application is running on App Engine. You created an update for this application and want to deploy this update without impacting users. If this update fails, you want to be able to roll back as quickly as possible. What should you do?",
    "answers": [
      "<p>You should deploy the update as a new version, then migrate traffic from the current version to the new version. If it fails, migrate the traffic back to your older version.</p>",
      "<p>You should deploy the update as the same version that is currently running. If the update fails, redeploy your older version using the same version identifier.</p>",
      "<p>You should deploy the update as the same version that is currently running because you are sure it won't fail.</p>",
      "<p>You should notify your users of an upcoming maintenance window and ask them not to use your application during that window. Then, deploy the update in that maintenance window.</p>"
    ],
    "explanation": "<p>You should deploy the update as a new version, then migrate traffic from the current version to the new version. If it fails, migrate the traffic back to your older version. -&gt; Correct. Deploying the update as a new version and then migrating traffic from the current version to the new version is the recommended approach for deploying updates without impacting users and allowing for a quick rollback. By creating a new version, you ensure that the existing version remains active and accessible to users. If the update fails, you can easily roll back by migrating traffic back to the older version, minimizing the impact on users.</p><p><br></p><p>You should deploy the update as the same version that is currently running. If the update fails, redeploy your older version using the same version identifier. -&gt;&nbsp;Incorrect. It is not an effective approach for a quick rollback. Redeploying the older version using the same version identifier may cause confusion and potential issues with caching and consistency. It may not provide a straightforward way to roll back to the previous version.</p><p><br></p><p>You should deploy the update as the same version that is currently running because you are sure it won't fail. -&gt;&nbsp;Incorrect. Deploying the update as the same version that is currently running assumes that the update won't fail. However, software updates can have unexpected issues or bugs that may cause failures. Not having a separate version for the update can lead to difficulties in rolling back if the update does fail, as there won't be a previous version to switch back to.</p><p><br></p><p>You should notify your users of an upcoming maintenance window and ask them not to use your application during that window. Then, deploy the update in that maintenance window. -&gt;&nbsp;Incorrect. Notifying users of an upcoming maintenance window and asking them not to use the application during that window is not the most user-friendly approach and may result in a negative user experience. It is also not necessary if you can deploy the update without impacting users and provide a quick rollback option if needed.</p><p><br></p><p>https://cloud.google.com/appengine/docs</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297218,
    "question_plain": "As a new cloud architect, you need to manage your first GCP project. The project will involve product owners, developers and testers. You need to make sure that only specific members of the development team have access to sensitive information (PII data). To do this, you want to assign the appropriate IAM roles. What should you do?",
    "answers": [
      "<p>You should create groups. Assign an IAM Predefined role to each group as required, including those who should have access to sensitive data. Than, assign users to groups.</p>",
      "<p>You should create groups. Assign a Custom role to each group, including those who should have access to sensitive data. Then, assign users to groups.</p>",
      "<p>You should create groups. Assign a basic role to each group, and then assign users to groups.</p>",
      "<p>You should assign a basic role to each user.</p>"
    ],
    "explanation": "<p>You should create groups. Assign an IAM Predefined role to each group as required, including those who should have access to sensitive data. Than, assign users to groups. -&gt; Correct. In this scenario, the cloud architect needs to manage a GCP project that involves product owners, developers, and testers. The goal is to restrict access to sensitive information (PII data) to specific members of the development team. To accomplish this, the cloud architect should first create groups and assign IAM predefined roles to each group as required, including those who should have access to sensitive data. The cloud architect can then assign users to the groups. Using groups is important because it simplifies the management of permissions. If the cloud architect assigned roles to individual users instead of groups, the process would be more difficult to manage and would require updating each user's permissions individually.</p><p><br></p><p>You should create groups. Assign a Custom role to each group, including those who should have access to sensitive data. Then, assign users to groups. -&gt; Incorrect. Custom roles can be created, but it is not necessary in this scenario since IAM predefined roles already exist that can grant the necessary permissions.</p><p><br></p><p>You should assign a basic role to each user. -&gt; Incorrect. Assigning basic roles to each user is not sufficient to restrict access to sensitive information.</p><p><br></p><p>You should create groups. Assign a basic role to each group, and then assign users to groups. -&gt; Incorrect. Assigning basic roles to each group is not sufficient to restrict access to sensitive information.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297220,
    "question_plain": "As a cloud architect, you need to design an IoT application that requires data storage up to 30 petabytes. Your application must support fast reads and writes. Your data schema is rather simple and you want to use the most economical solution for this. What should you do?",
    "answers": [
      "<p>You should store the data in Cloud Bigtable.</p>",
      "<p>You should use BigQuery, and implement the business logic in SQL.</p>",
      "<p>You should store the data in Cloud Storage.</p>",
      "<p>You should store the data in Cloud Spanner, and add an in-memory cache for speed.</p>"
    ],
    "explanation": "<p>You should store the data in Cloud Bigtable. -&gt; Correct. Cloud Bigtable is a highly scalable NoSQL database service that is designed to handle massive amounts of data with low latency. It is ideal for IoT applications that require fast reads and writes and can store up to hundreds of petabytes of data. In addition, Cloud Bigtable is an economical solution for storing large amounts of data as it is charged based on usage and is highly optimized for cost-effective storage. </p><p><br></p><p>You should use BigQuery, and implement the business logic in SQL. -&gt; Incorrect. BigQuery is a data warehousing solution and may not be the most efficient or economical solution for storing data with simple schema. </p><p><br></p><p>You should store the data in Cloud Storage. -&gt; Incorrect. Cloud Storage is a good option for storing unstructured data, but may not be ideal for IoT data that requires fast reads and writes. </p><p><br></p><p>You should store the data in Cloud Spanner, and add an in-memory cache for speed. -&gt; Incorrect. Cloud Spanner is a fully-managed, scalable, relational database service, which is a good option for transactions across large datasets, but may not be the most economical solution for storing 30 petabytes of data.</p><p><br></p><p>https://cloud.google.com/bigtable/docs</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297222,
    "question_plain": "You are a cloud architect for a large-scale company that is transitioning its monolithic applications to a microservices-based architecture on Google Cloud. You have been tasked with recommending an optimal strategy for deploying, managing, and scaling these microservices. Which of the following approaches would be most effective?",
    "answers": [
      "<p>Deploy the microservices on Google Kubernetes Engine (GKE), utilizing the orchestration capabilities of Kubernetes to manage and scale services.</p>",
      "<p>Use Compute Engine to individually manage each microservice, utilizing autoscaling groups to handle scaling.</p>",
      "<p>Deploy each microservice as a separate App Engine application, utilizing App Engine's automatic scaling and load balancing capabilities.</p>",
      "<p>Use Cloud Functions for each microservice, utilizing the event-driven nature of Cloud Functions to manage and scale services.</p>"
    ],
    "explanation": "<p>Deploy the microservices on Google Kubernetes Engine (GKE), utilizing the orchestration capabilities of Kubernetes to manage and scale services. -&gt;&nbsp;Correct. GKE provides a powerful platform for managing microservices through Kubernetes, which provides features like service discovery, load balancing, and automated scaling that are ideal for a microservices architecture.</p><p><br></p><p>Use Compute Engine to individually manage each microservice, utilizing autoscaling groups to handle scaling. -&gt; Incorrect. Compute Engine is more suitable for traditional and monolithic applications. It can be used for microservices, but it would involve significant manual effort in managing and coordinating between services, which is not ideal.</p><p><br></p><p>Deploy each microservice as a separate App Engine application, utilizing App Engine's automatic scaling and load balancing capabilities. -&gt; Incorrect. App Engine is a platform as a service (PaaS) and is designed for web applications rather than microservices. While it can support microservices, it would not be as effective for managing a large-scale microservices architecture as other options.</p><p><br></p><p>Use Cloud Functions for each microservice, utilizing the event-driven nature of Cloud Functions to manage and scale services. -&gt; Incorrect. Cloud Functions are designed for event-driven, single-purpose applications and would not be as effective for large-scale, multi-functional microservices. Also, they can become expensive and complex to manage at scale.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297224,
    "question_plain": "Your organization is preparing to build a complex data science solution on Google Cloud Platform. The solution involves various stages, including data collection, cleaning, analysis, machine learning model training, and deploying models for real-time predictions. The data volume is significant, and the solution will require multiple services on GCP for various stages. As the cloud architect, which of the following architectures will you recommend for managing this data science solution effectively?",
    "answers": [
      "<p>Use Cloud Pub/Sub for data collection, Dataflow for cleaning and analysis, AutoML for machine learning model training, and Cloud Run for deploying models.</p>",
      "<p>Use Cloud Storage for data collection, Dataflow for cleaning and analysis, Cloud Dataprep for machine learning model training, and App Engine for deploying models.</p>",
      "<p>Use Pub/Sub for data collection, Dataflow for cleaning and analysis, BigQuery ML for machine learning model training, and Cloud Functions for deploying models.</p>",
      "<p>Use Cloud Storage for data collection, Cloud Dataproc for cleaning and analysis, Cloud ML Engine for machine learning model training, and Cloud Endpoints for deploying models.</p>"
    ],
    "explanation": "<p>Use Cloud Pub/Sub for data collection, Dataflow for cleaning and analysis, AutoML for machine learning model training, and Cloud Run for deploying models. -&gt; Correct. Cloud Pub/Sub is designed for reliable messaging in applications. Dataflow can effectively handle the cleaning and analysis stages. AutoML is ideal for training machine learning models without requiring machine learning expertise. Cloud Run is designed to run containers and would be an excellent choice for deploying machine learning models packaged as containers.</p><p><br></p><p>Use Cloud Storage for data collection, Dataflow for cleaning and analysis, Cloud Dataprep for machine learning model training, and App Engine for deploying models. -&gt; Incorrect. Cloud Dataprep is a data service for visually exploring, cleaning, and preparing data for analysis. However, it is not designed for machine learning model training.</p><p><br></p><p>Use Pub/Sub for data collection, Dataflow for cleaning and analysis, BigQuery ML for machine learning model training, and Cloud Functions for deploying models. -&gt; Incorrect. BigQuery ML enables users to create and execute machine learning models in BigQuery using SQL queries. However, it may not handle the complex machine learning requirements of large data science solutions as effectively as AutoML or Cloud ML Engine.</p><p><br></p><p>Use Cloud Storage for data collection, Cloud Dataproc for cleaning and analysis, Cloud ML Engine for machine learning model training, and Cloud Endpoints for deploying models. -&gt; Incorrect. This approach is viable for managing a data science solution, but Cloud Endpoints is generally used for deploying APIs, not machine learning models.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297226,
    "question_plain": "An insurance company uses several third-party enterprise applications that require special licenses. These licenses are not transferrable to the cloud. The third-party software vendor offers an option to pay a licensing fee based on how long you use the application in the cloud. What is this approach called?",
    "answers": [
      "<p>Pay-as-you-go license</p>",
      "<p>Bringing your own licenses</p>",
      "<p>On-demand pricing</p>",
      "<p>Flat-rate pricing</p>"
    ],
    "explanation": "<p>Pay-as-you-go license -&gt; Correct. The approach of paying a licensing fee based on how long you use the application in the cloud is commonly known as a \"pay-as-you-go\" license. This approach allows organizations to pay for software licenses only when they are needed, rather than paying for a fixed number of licenses upfront, regardless of how much they are used. This can help reduce costs and increase flexibility in the cloud.</p><p><br></p><p>Bringing your own licenses -&gt;&nbsp;Incorrect. Bringing your own licenses (BYOL) refers to a licensing model where the company brings their existing licenses from on-premises or other environments to use in the cloud. This option is not applicable in this scenario as the third-party licenses are not transferrable to the cloud.</p><p><br></p><p>On-demand pricing -&gt;&nbsp;Incorrect. On-demand pricing typically refers to a pricing model where services are billed based on usage, usually with per-hour or per-minute rates. While it is a common pricing model for cloud services, it does not specifically address the licensing aspect mentioned in the question.</p><p><br></p><p>Flat-rate pricing -&gt;&nbsp;Incorrect. Flat-rate pricing refers to a fixed pricing model where a set fee is charged regardless of usage. This option does not align with the scenario described, as the licensing fee in this case is based on how long the application is used in the cloud, rather than a fixed flat rate.</p><p><br></p><p>https://cloud.google.com/blog/products/compute/compute-engine-licensing-explained</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297228,
    "question_plain": "As a cloud architect, you need to establish connection between your on-premises network and Google Cloud. Your company will need 2 Gbps of bandwidth in total between the on-premises data center and Google Cloud. How many VPN endpoints will you need?",
    "answers": ["<p>1</p>", "<p>2</p>", "<p>3</p>", "<p>6</p>"],
    "explanation": "<p>1 -&gt; Correct. A VPN endpoint is a virtual device that is used to establish a secure connection between your on-premises network and Google Cloud. A single VPN gateway can support up to 3 Gbps of throughput, so in this scenario, a single VPN endpoint is sufficient to support the required bandwidth of 2 Gbps.</p><p><br></p><p>https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview#network-bandwidth</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297232,
    "question_plain": "As you plan your migration, you find out that traffic to the subnet containing databases must be restricted. As a cloud architect, what mechanism should you use to control this?",
    "answers": [
      "<p>Firewall rules</p>",
      "<p>Virtual Private Networks</p>",
      "<p>Virtual Private Clouds</p>",
      "<p>IAM&nbsp;roles</p>"
    ],
    "explanation": "<p>Firewall rules -&gt; Correct. Firewall rules can be used to restrict traffic to a specific subnet in Google Cloud. Firewall rules allow you to define what traffic is allowed to enter or leave a particular subnet or VM instance based on its protocol, port, and IP address. By creating firewall rules that only allow traffic from authorized sources, you can effectively restrict traffic to a subnet containing databases. </p><p><br></p><p>Virtual Private Networks -&gt; Incorrect. Virtual Private Networks is not used to restrict traffic to a subnet, but rather to provide secure connectivity between networks or to isolate resources within a network. </p><p><br></p><p>Virtual Private Clouds -&gt; Incorrect. Virtual Private Clouds is not used to restrict traffic to a subnet, but rather to provide secure connectivity between networks or to isolate resources within a network. </p><p><br></p><p>IAM&nbsp;roles -&gt; Incorrect. IAM roles control who can access resources in a project, but do not restrict traffic to a specific subnet.</p><p><br></p><p>https://cloud.google.com/vpc/docs/firewalls</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297234,
    "question_plain": "A manufacturing company uses IoT devices and collects data from millions of devices around the world. The IoT data is streamed from each device every 5 seconds - 5 KB per message. This company wants to use a managed service from Google Cloud. What would you recommend?",
    "answers": [
      "<p>Cloud Bigtable</p>",
      "<p>BigQuery</p>",
      "<p>Cloud SQL</p>",
      "<p>Cloud Spanner</p>",
      "<p>Dataproc</p>"
    ],
    "explanation": "<p>Cloud Bigtable -&gt; Correct. Cloud Bigtable is a highly scalable NoSQL database that can handle high volume and high-speed data ingestion, storage, and retrieval. It is designed for applications that require very high throughput and low-latency data access, making it a good choice for storing and analyzing large amounts of IoT data that are generated frequently. </p><p><br></p><p>BigQuery -&gt;&nbsp;Incorrect. BigQuery is a fully managed, serverless data warehouse that excels at analyzing large datasets using SQL queries. While it is a powerful tool for analytics and querying, it may not be the best fit for real-time ingestion and processing of streaming data from IoT devices.</p><p><br></p><p>Cloud SQL -&gt;&nbsp;Incorrect. Cloud SQL is a managed relational database service that provides a traditional SQL database environment. While it can handle structured data, it may not be the most efficient or scalable solution for managing and analyzing high volumes of IoT data.</p><p><br></p><p>Cloud Spanner -&gt;&nbsp;Incorrect. Cloud Spanner is a globally distributed, horizontally scalable relational database service. While it can handle high-volume transactions and provides strong consistency, it may not be the most cost-effective solution for storing and analyzing IoT data that is constantly streamed from millions of devices.</p><p><br></p><p>Dataproc -&gt;&nbsp;Incorrect. Dataproc is a managed Apache Hadoop and Spark service. While it can be used for processing and analyzing data, it may not be the most suitable choice for real-time analytics and querying of streaming IoT data.</p><p><br></p><p>https://cloud.google.com/bigtable/docs/overview</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297236,
    "question_plain": "Your organization is operating several Compute Engine instances on Google Cloud Platform. You've been asked to ensure that logs from these instances are centralized and accessible from the Cloud Logging. Given the different operational needs and data sensitivities across instances, which strategy would ensure appropriate setup of the Cloud Logging agent?",
    "answers": [
      "<p>Install the Cloud Logging agent on each instance and have it log data to individual Cloud Logging projects based on each instance's operational needs and data sensitivity.</p>",
      "<p>Install the Cloud Logging agent on each instance and log data to a single Cloud Logging project to consolidate all logs.</p>",
      "<p>Install the Cloud Logging agent on a single, dedicated instance and use that to log data from all other instances.</p>",
      "<p>Install the Cloud Logging agent on each instance and configure the agent to log data to Cloud Storage instead of Cloud Logging to save costs.</p>"
    ],
    "explanation": "<p>Install the Cloud Logging agent on each instance and have it log data to individual Cloud Logging projects based on each instance's operational needs and data sensitivity. -&gt; Correct. By installing the Cloud Logging agent on each instance and configuring each agent to log data to the appropriate Cloud Logging project, you can separate data according to its sensitivity and operational needs.</p><p><br></p><p>Install the Cloud Logging agent on each instance and log data to a single Cloud Logging project to consolidate all logs. -&gt; Incorrect. This strategy may create problems in separating concerns between different types of data or different sensitivity levels, even though it centralizes all the logs.</p><p><br></p><p>Install the Cloud Logging agent on a single, dedicated instance and use that to log data from all other instances. -&gt; Incorrect. Having a single instance log data for all other instances would not be efficient or scalable and could lead to a single point of failure.</p><p><br></p><p>Install the Cloud Logging agent on each instance and configure the agent to log data to Cloud Storage instead of Cloud Logging to save costs. -&gt; Incorrect. While storing logs in Cloud Storage may have cost advantages in some cases, it doesn't provide the real-time analysis, metrics, and log-based alerting that Cloud Logging offers. This strategy might not meet all operational needs.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297238,
    "question_plain": "As a cloud architect, you work for a courier company that delivers packages all over the world. You are responsible for preparing a system that will track the location of packages. This solution must be scalable and ensure high consistency. Your data storage solution must also support SQL queries. Which GCP&nbsp;service should you recommend?",
    "answers": [
      "<p>Cloud Spanner</p>",
      "<p>Cloud SQL</p>",
      "<p>BigQuery</p>",
      "<p>Dataproc</p>"
    ],
    "explanation": "<p>Cloud Spanner -&gt;&nbsp;Correct. It is a fully managed, horizontally scalable, and strongly consistent relational database service. It is designed to scale globally and provide strong consistency across regions. It also supports SQL queries, making it suitable for the scenario described in the question, where a scalable and highly consistent solution is required to track package locations. </p><p><br></p><p>Cloud SQL -&gt; Incorrect. It is also a relational database service, but it does not provide the same level of scalability and consistency as Cloud Spanner. </p><p><br></p><p>BigQuery -&gt;&nbsp;Incorrect. It is a data warehouse service that supports SQL queries, but it is not a relational database and does not provide the same level of consistency. </p><p><br></p><p>Dataproc -&gt; Incorrect. It is a managed service for running Apache Hadoop and Apache Spark, which is not a suitable choice for this scenario.</p><p><br></p><p>https://cloud.google.com/spanner/docs/quickstart-console</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297240,
    "question_plain": "As part of the software development life cycle, you are designing a testing strategy for a new application being developed on Google Cloud. The application is expected to have high demand and needs to be highly reliable. Which approach would be the most appropriate?",
    "answers": [
      "<p>Use blue-green deployments to perform end-to-end testing.</p>",
      "<p>Implement unit tests only to speed up the development process.</p>",
      "<p>Use a canary release strategy for testing.</p>",
      "<p>Test in the production environment to ensure accurate results.</p>"
    ],
    "explanation": "<p>Use blue-green deployments to perform end-to-end testing. -&gt;&nbsp;Correct. Blue-green deployments involve running two environments, \"blue\" and \"green,\" where one serves live traffic (blue) while the other (green) is used for testing. Once testing is done, traffic is switched to the green environment. This ensures high availability and allows for comprehensive testing.</p><p><br></p><p>Implement unit tests only to speed up the development process. -&gt;&nbsp;Incorrect. While unit tests are important, they only cover individual components of your application. To ensure reliability, more comprehensive testing strategies are needed.</p><p><br></p><p>Use a canary release strategy for testing. -&gt;&nbsp;Incorrect. A canary release is a technique to reduce the risk of introducing a new software version in production by gradually rolling out the change to a small subset of users. While it can be useful, it may not provide the comprehensive testing required for high reliability.</p><p><br></p><p>Test in the production environment to ensure accurate results. -&gt;&nbsp;Incorrect. Testing in production might expose users to bugs and can lead to a poor user experience. It's better to use strategies like blue-green deployments where testing is done in a separate environment.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297242,
    "question_plain": "As a cloud architect, you plan to migrate your on-premises data warehouse to Google Cloud using BigQuery. You need to make a presentation to management of what the costs look like in BigQuery. Select all true statements about the BigQuery pricing model. (select 2)",
    "answers": [
      "<p>BigQuery offers a choice of two pricing models for running queries: On-demand pricing and Flat-rate pricing.</p>",
      "<p>BigQuery charges for certain operations, such as streaming inserts and using the BigQuery Storage API.</p>",
      "<p>BigQuery has no free usage tier.</p>",
      "<p>By default, queries are billed using the Flat-rate pricing model.</p>"
    ],
    "explanation": "<p>BigQuery offers a choice of two pricing models for running queries: On-demand pricing and Flat-rate pricing. -&gt; Correct. BigQuery offers a choice of two pricing models for running queries: On-demand pricing and Flat-rate pricing. On-demand pricing charges users based on the amount of data processed by the query, while flat-rate pricing provides a predictable monthly cost for a set amount of query processing capacity.</p><p><br></p><p>BigQuery charges for certain operations, such as streaming inserts and using the BigQuery Storage API. -&gt; Correct. BigQuery charges for certain operations, such as streaming inserts and using the BigQuery Storage API. These operations are charged separately from query processing.</p><p><br></p><p>BigQuery has no free usage tier. -&gt; Incorrect. BigQuery actually does offer a free tier, which allows for 1 TB of data processed per month at no cost.</p><p><br></p><p>By default, queries are billed using the Flat-rate pricing model. -&gt; Incorrect. The default pricing model for queries in BigQuery is actually On-demand pricing, not Flat-rate pricing. Flat-rate pricing must be explicitly enabled and configured.</p><p><br></p><p>https://cloud.google.com/bigquery/pricing</p>",
    "correct_response": ["a", "b"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 81297244,
    "question_plain": "In GCP, your ingestion services cannot keep up with the rate that new data is received. What can you do to ensure that data is not lost if ingestion services cannot keep up with the rate at which new data is received?",
    "answers": [
      "<p>You can use Cloud&nbsp;Pub/Sub queue. Add data to a queue. Than, you can use Cloud Function to remove the data from the queue, transform and write it to another storage system.</p>",
      "<p>You should change the capacity of your storage system.</p>",
      "<p>You can use BigQuery dataset. Add data to a temporary table. Than, use Dataflow to process the data and writes it to another storage system.</p>",
      "<p>You cannot prevent data loss.</p>"
    ],
    "explanation": "<p>You can use Cloud&nbsp;Pub/Sub queue. Add data to a queue. Than, you can use Cloud Function to remove the data from the queue, transform and write it to another storage system. -&gt; Correct. Using Cloud Pub/Sub queue allows you to decouple the ingestion services from the processing services. In this approach, the ingestion services add data to a Pub/Sub queue, which acts as a buffer. Cloud Functions can then be used to consume messages from the queue, perform any necessary transformations, and write the data to another storage system. This ensures that data is not lost even if the ingestion services cannot keep up with the incoming data rate.</p><p><br></p><p>You should change the capacity of your storage system. -&gt;&nbsp;Incorrect. Changing the capacity of your storage system may help in some cases, but it does not directly address the issue of data loss when the ingestion services cannot keep up with the data rate. Simply increasing the storage capacity will not guarantee that the incoming data is processed and stored in a timely manner.</p><p><br></p><p>You can use BigQuery dataset. Add data to a temporary table. Than, use Dataflow to process the data and writes it to another storage system. -&gt;&nbsp;Incorrect. Using a BigQuery dataset and a temporary table can be a valid approach, but it does not directly address the issue of data loss when the ingestion services are overwhelmed. While Dataflow can be used to process the data and write it to another storage system, the temporary table in BigQuery may not be sufficient to handle the high influx of incoming data if the ingestion services are unable to keep up.</p><p><br></p><p>You cannot prevent data loss. -&gt;&nbsp;Incorrect. Saying that you cannot prevent data loss is not entirely accurate. While it may not be possible to completely eliminate the risk of data loss, using appropriate buffering mechanisms like Cloud Pub/Sub queues can help mitigate the risk and ensure that data is not lost if the ingestion services are unable to keep up.</p><p><br></p><p>https://cloud.google.com/pubsub/docs/overview</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297246,
    "question_plain": "As a cloud architect, you need to deploy MySQL database using Google Cloud Platform. Select all possible options for deploying MySQL database to Google Cloud.",
    "answers": [
      "<p>You can use Cloud SQL to host MySQL database. This option reduces administrative duties.</p>",
      "<p>You can use Cloud Marketplace with click-to-deploy interface to install MySQL onto a Compute Engine instance.</p>",
      "<p>You can manually install and customize MySQL on your Compute Engine instance.</p>",
      "<p>You can use Cloud Marketplace with click-to-deploy interface to install MySQL using Google Kubernetes Engine.</p>"
    ],
    "explanation": "<p>You can use Cloud SQL to host MySQL database. This option reduces administrative duties. -&gt;&nbsp;Correct. Cloud SQL is a fully managed relational database service that supports MySQL. It provides automated backups, replication, scaling, and patching, reducing administrative duties for managing MySQL databases.</p><p><br></p><p>You can use Cloud Marketplace with click-to-deploy interface to install MySQL onto a Compute Engine instance. -&gt; Correct. Cloud Marketplace offers click-to-deploy interfaces for deploying various applications, including MySQL, on Compute Engine instances. This option provides easy and fast deployment of MySQL databases.</p><p><br></p><p>You can manually install and customize MySQL on your Compute Engine instance. -&gt; Correct. You can manually install and customize MySQL on your Compute Engine instance. This option provides full control and customization over the MySQL installation.</p><p><br></p><p>You can use Cloud Marketplace with click-to-deploy interface to install MySQL using Google Kubernetes Engine. -&gt; Incorrect. It is not a valid answer since it mentions using Google Kubernetes Engine (GKE) to deploy MySQL. GKE is a container orchestration service and is not recommended for hosting databases directly.</p><p><br></p><p>https://cloud.google.com/architecture/setup-mysql?hl=en</p>",
    "correct_response": ["a", "b", "c"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 81297248,
    "question_plain": "Your company has two Google Cloud projects - Project A and Project B. The goal is to move data from a Cloud Storage bucket in Project A to another bucket in Project B on a regular schedule. Which of the following is the best way to achieve this?",
    "answers": [
      "<p>Use the Storage Transfer Service to schedule and manage the data transfer between the source and destination buckets.</p>",
      "<p>Use the <code>gsutil cp</code> command to manually copy the objects from the source bucket to the destination bucket.</p>",
      "<p>Use Cloud Functions to trigger a Cloud Storage event whenever data is added to the source bucket, which then copies the data to the destination bucket.</p>",
      "<p>Use Cloud Dataflow to create a pipeline that reads from the source bucket and writes to the destination bucket.</p>"
    ],
    "explanation": "<p>Use the Storage Transfer Service to schedule and manage the data transfer between the source and destination buckets. -&gt; Correct. Storage Transfer Service is designed specifically for this use case. It provides a simple, robust solution for transferring large amounts of data between Cloud Storage buckets, even across different projects or regions. It also supports scheduling, so it is the correct answer.</p><p><br></p><p>Use the <code>gsutil cp</code> command to manually copy the objects from the source bucket to the destination bucket. -&gt; Incorrect. While <code>gsutil cp</code> could be used to copy objects between buckets, it's a manual process and is not suitable for regular, automated data transfers.</p><p><br></p><p>Use Cloud Functions to trigger a Cloud Storage event whenever data is added to the source bucket, which then copies the data to the destination bucket. -&gt; Incorrect. Cloud Functions can react to changes in a Cloud Storage bucket, but they are more suited to processing individual objects rather than transferring large amounts of data on a regular schedule.</p><p><br></p><p>Use Cloud Dataflow to create a pipeline that reads from the source bucket and writes to the destination bucket. -&gt; Incorrect. Cloud Dataflow is a powerful service for processing and transforming large data streams, but it's overkill for this scenario and it doesn't support scheduling natively. It would also require more management and configuration than the Storage Transfer Service.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297250,
    "question_plain": "Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdfAs a cloud architect for Mountkirk Games, your responsibility is to ensure that their new gaming platform adheres to Google's best practices. Your objective is to validate the implementation of Google's recommended security practices while also providing the necessary metrics to support your operations teams. What steps should you take to achieve this goal? (select 2)",
    "answers": [
      "<p>Ensure that you are not running privileged containers.</p>",
      "<p>Ensure that you are using the native logging mechanisms.</p>",
      "<p>Ensure that you are using obfuscated Tags on workloads.</p>",
      "<p>Ensure that workloads are not using securityContext to run as a group.</p>",
      "<p>Ensure that each cluster is running GKE metering so each team can be charged for their usage.</p>"
    ],
    "explanation": "<p>Ensure that you are not running privileged containers. -&gt;&nbsp;Correct. This is High Priority according to Google best practices.</p><p><br></p><p>Ensure that you are using the native logging mechanisms. -&gt; Correct. This is High Priority according to Google best practices.</p><p><br></p><p>Ensure that you are using obfuscated Tags on workloads. -&gt;&nbsp;Incorrect. Tags should be readable and useful to the operations teams when they are working on the clusters.</p><p><br></p><p>Ensure that workloads are not using securityContext to run as a group. -&gt;&nbsp;Incorrect. This may be required for some workloads.</p><p><br></p><p>Ensure that each cluster is running GKE metering so each team can be charged for their usage. -&gt; Incorrect. Although from a business process this may be useful it won’t impact the operations or security of the cluster.</p><p><br></p><p>https://cloud.google.com/architecture/best-practices-for-operating-containers</p>",
    "correct_response": ["a", "b"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 81297252,
    "question_plain": "Refer to the TerramEarth case study for this question: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdfAs the Data Compliance Officer for TerramEarth, your primary responsibility is to safeguard customers' personally identifiable information (PII), including sensitive data like credit card information. TerramEarth aims to offer personalized product recommendations to its extensive base of industrial customers. It is crucial to prioritize data privacy while designing an appropriate solution. What steps would you propose to address this challenge?",
    "answers": [
      "<p>You should use the Cloud Data Loss Prevention (DLP) API to provide data to the recommendation service.</p>",
      "<p>You should use AutoML to provide data to the recommendation service.</p>",
      "<p>You should process PII data on-premises to keep the private information more secure.</p>",
      "<p>You should manually build, train, and test machine learning models to provide product recommendations anonymously.</p>"
    ],
    "explanation": "<p>You should use the Cloud Data Loss Prevention (DLP) API to provide data to the recommendation service. -&gt; Correct. Cloud DLP was specifically designed for this use case.</p><p><br></p><p>You should use AutoML to provide data to the recommendation service. -&gt; Incorrect. AutoML does not inherently provide data de-identification.</p><p><br></p><p>You should process PII data on-premises to keep the private information more secure. -&gt; Incorrect. TerramEarth's requirements are to go into the cloud, not stay on-premises.</p><p><br></p><p>You should manually build, train, and test machine learning models to provide product recommendations anonymously. -&gt;&nbsp;Incorrect. Developing machine learning models is an excessive way to de-identify data.</p><p><br></p><p>https://cloud.google.com/dlp/docs/deidentify-sensitive-data</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297254,
    "question_plain": "You are responsible for architecting a three-tier web application on Google Cloud Platform. This application includes a web server tier, an application server tier, and a database tier, each tier hosted on separate Compute Engine instances. You need to control network traffic between these tiers and from the public internet. To achieve this, you decide to use tags and firewall rules. Which of the following options would be the most effective way to apply these tags and set up firewall rules?",
    "answers": [
      "<p>Assign each tier a unique tag and create individual firewall rules to control traffic between tags and from the public internet.</p>",
      "<p>Add a single tag to all instances irrespective of the tier and create one firewall rule to allow all inbound traffic.</p>",
      "<p>Assign the same tag to the web server and application server tier, and a different tag to the database tier. Then, create a firewall rule to allow all traffic within the same tag and limited traffic to the database tag.</p>",
      "<p>Add a tag only to the database tier and create a firewall rule that only allows traffic from the IP addresses of the other two tiers.</p>"
    ],
    "explanation": "<p>Assign each tier a unique tag and create individual firewall rules to control traffic between tags and from the public internet. -&gt; Correct. By tagging each tier separately and creating individual firewall rules for each, you can finely control the allowed traffic between tiers and from the public internet, enhancing security.</p><p><br></p><p>Add a single tag to all instances irrespective of the tier and create one firewall rule to allow all inbound traffic. -&gt; Incorrect. Using a single tag for all instances would not allow for differentiated access control between tiers. One firewall rule for all inbound traffic would not provide the necessary security controls.</p><p><br></p><p>Assign the same tag to the web server and application server tier, and a different tag to the database tier. Then, create a firewall rule to allow all traffic within the same tag and limited traffic to the database tag. -&gt; Incorrect. This approach does not provide a differentiated control between the web server and application server tiers. Also, while the database tier should indeed have restricted access, this option doesn't provide sufficient granularity in its controls.</p><p><br></p><p>Add a tag only to the database tier and create a firewall rule that only allows traffic from the IP addresses of the other two tiers. -&gt; Incorrect. Adding a tag only to the database tier would not allow for differentiated access control for the other two tiers.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297256,
    "question_plain": "Your organization has a suite of applications that have been containerized. You've been tasked to design a deployment strategy that leverages the power of Google Cloud's managed services, allows for auto-scaling based on demand, provides an ability to deploy updates with zero-downtime, and supports granular IAM roles and policies for your DevOps team. What would be your recommended approach?",
    "answers": [
      "<p>Utilize Google Kubernetes Engine (GKE) for deploying and managing the containerized applications, utilizing Kubernetes' native support for autoscaling and rolling updates.</p>",
      "<p>Deploy the containers directly onto Compute Engine VM instances and manually manage scaling and updates.</p>",
      "<p>Use Cloud Run to deploy the containerized applications and benefit from its automatic scaling and deployment features.</p>",
      "<p>Deploy the containers on App Engine standard environment and let App Engine handle scaling and updates.</p>"
    ],
    "explanation": "<p>Utilize Google Kubernetes Engine (GKE) for deploying and managing the containerized applications, utilizing Kubernetes' native support for autoscaling and rolling updates. -&gt; Correct. GKE is a managed Kubernetes service that provides a powerful platform for managing containerized applications. It allows for automatic scaling, zero-downtime deployments, and supports granular IAM roles and policies.</p><p><br></p><p>Deploy the containers directly onto Compute Engine VM instances and manually manage scaling and updates. -&gt; Incorrect. Compute Engine provides the necessary infrastructure to run the containers, but it would require manual management for scaling and updates, which is not efficient or recommended for containerized applications.</p><p><br></p><p>Use Cloud Run to deploy the containerized applications and benefit from its automatic scaling and deployment features. -&gt; Incorrect. Cloud Run can be a great option for stateless containerized applications as it provides automatic scaling and easy deployments. However, it lacks the granular control over the environment that Kubernetes offers.</p><p><br></p><p>Deploy the containers on App Engine standard environment and let App Engine handle scaling and updates. -&gt; Incorrect. App Engine standard environment does not support containers. App Engine flexible environment does, but it does not offer the same level of control and orchestration that GKE provides.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297258,
    "question_plain": "You are a cloud architect working on a web application that requires horizontal scaling based on traffic load. The application runs on stateless virtual machines, and you plan to use Managed Instance Groups (MIGs) for this purpose. You also want to ensure that any updates to the instance templates do not disrupt the service. Which of the following deployment strategies would you recommend?",
    "answers": [
      "<p>Use Rolling update strategy with Proactive update mode and Automatic scaling.</p>",
      "<p>Use Canary update strategy with Proactive update mode and Manual scaling.</p>",
      "<p>Use Rolling update strategy with Opportunistic update mode and Manual scaling.</p>",
      "<p>Use Opportunistic update mode with Maximum surge policy and Automatic scaling.</p>"
    ],
    "explanation": "<p>Use Rolling update strategy with Proactive update mode and Automatic scaling. -&gt; Correct. The Rolling update strategy gradually replaces instances in the group with instances based on the new template, which ensures minimal disruption. The Proactive update mode starts the update as soon as the group is stable, and Automatic scaling adjusts the number of instances based on the load, which is ideal for a web application that requires horizontal scaling based on traffic.</p><p><br></p><p>Use Canary update strategy with Proactive update mode and Manual scaling. -&gt; Incorrect. While the Canary update strategy allows you to roll out updates to a subset of instances before updating the rest, Manual scaling would not provide the necessary flexibility for a load-based scaling scenario.</p><p><br></p><p>Use Rolling update strategy with Opportunistic update mode and Manual scaling. -&gt; Incorrect. The Opportunistic update mode updates instances when they are recreated for other reasons, such as auto-healing, which does not guarantee timely updates. Manual scaling would not provide the necessary flexibility for a load-based scaling scenario.</p><p><br></p><p>Use Opportunistic update mode with Maximum surge policy and Automatic scaling. -&gt;&nbsp;Incorrect. The Opportunistic update mode and Maximum surge policy are not the ideal combination for this scenario. The Opportunistic update mode may delay updates, and the Maximum surge policy, which defines the number of additional instances that can be created, does not have any direct impact on the update strategy.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297260,
    "question_plain": "As a cloud architect, you are designing a hybrid cloud setup where you need to connect on-premises infrastructure with Google Cloud. The on-premises network uses the IP range 192.168.0.0/16. You need to ensure that the IP range used on Google Cloud does not overlap with the on-premises range to avoid IP conflicts. Which of the following strategies should you adopt?",
    "answers": [
      "<p>Choose an IP range of 10.0.0.0/16 for the Google Cloud network.</p>",
      "<p>Choose an IP range of 192.168.0.0/24 for the Google Cloud network.</p>",
      "<p>Choose an IP range of 192.168.1.0/24 for the Google Cloud network.</p>",
      "<p>Choose an IP range of 192.168.0.0/16 for the Google Cloud network.</p>"
    ],
    "explanation": "<p>Choose an IP range of 10.0.0.0/16 for the Google Cloud network. -&gt; Correct. The IP range 10.0.0.0/16 is within the private IP range defined by RFC1918 and does not overlap with the on-premises IP range.</p><p><br></p><p>Choose an IP range of 192.168.0.0/24 for the Google Cloud network. -&gt; Incorrect. This IP range overlaps with the on-premises IP range because it is a subset of 192.168.0.0/16.</p><p><br></p><p>Choose an IP range of 192.168.1.0/24 for the Google Cloud network. -&gt; Incorrect. This IP range overlaps with the on-premises IP range because it is a subset of 192.168.0.0/16.</p><p><br></p><p>Choose an IP range of 192.168.0.0/16 for the Google Cloud network. -&gt; Incorrect. This IP range is exactly the same as the on-premises IP range, which would lead to IP address conflicts.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297262,
    "question_plain": "You're architecting a web application on Google Cloud that is expected to store and process personal data from users in the European Union (EU), thus it must meet the General Data Protection Regulation (GDPR) requirements. Which of the following strategies would you adopt?",
    "answers": [
      "<p>Store all personal data in a regional storage bucket in an EU country, leverage Google Cloud Armor for data protection, and use Cloud Audit Logs for auditing.</p>",
      "<p>Store all personal data in a regional storage bucket in the United States, and leverage Google's built-in data protection features.</p>",
      "<p>Store all personal data in a multi-regional storage bucket, and leverage Google Cloud Data Loss Prevention (DLP) to discover, classify, and redact sensitive data.</p>",
      "<p>Implement user authentication and authorization using Firebase Authentication, and store all personal data in a multi-regional storage bucket.</p>"
    ],
    "explanation": "<p>Store all personal data in a regional storage bucket in an EU country, leverage Google Cloud Armor for data protection, and use Cloud Audit Logs for auditing. -&gt; Correct. This strategy covers data storage in the EU, data protection, and auditing, all of which are crucial for GDPR compliance. Google Cloud Armor helps protect against Distributed Denial of Service (DDoS) attacks, while Cloud Audit Logs provides visibility into how your cloud resources are being used.</p><p><br></p><p>Store all personal data in a regional storage bucket in the United States, and leverage Google's built-in data protection features. -&gt; Incorrect. GDPR has strict requirements about transferring personal data outside the EU. Therefore, storing personal data in a regional storage bucket in the United States could potentially violate GDPR, even if Google's built-in data protection features are used.</p><p><br></p><p>Store all personal data in a multi-regional storage bucket, and leverage Google Cloud Data Loss Prevention (DLP) to discover, classify, and redact sensitive data. -&gt; Incorrect. Google Cloud DLP is a powerful tool to discover, classify, and redact sensitive data. However, using a multi-regional storage bucket could potentially violate GDPR if personal data of EU users is stored outside the EU.</p><p><br></p><p>Implement user authentication and authorization using Firebase Authentication, and store all personal data in a multi-regional storage bucket. -&gt; Incorrect. Even though Firebase Authentication can help implement user authentication and authorization, storing personal data in a multi-regional storage bucket could potentially violate GDPR as personal data of EU users may be stored outside the EU.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297264,
    "question_plain": "You are designing a solution to deploy a stateful workload on Google Cloud, and one of your requirements is to provide the same POSIX filesystem to all the nodes. Which of the following strategies should you adopt?",
    "answers": [
      "<p>Deploy the application on a Google Kubernetes Engine cluster and use Cloud Filestore as the storage backend.</p>",
      "<p>Deploy the application on Compute Engine instances and use local SSDs for storage.</p>",
      "<p>Deploy the application on a Google Kubernetes Engine cluster and use Persistent Disk for storage.</p>",
      "<p>Deploy the application on Compute Engine instances and use Cloud Storage for shared filesystem.</p>"
    ],
    "explanation": "<p>Deploy the application on a Google Kubernetes Engine cluster and use Cloud Filestore as the storage backend. -&gt; Correct. Cloud Filestore provides fully managed NFS file servers on Google Cloud for applications that require a filesystem interface and a shared filesystem for data. It can be used as a storage backend for a stateful application running on a GKE cluster, meeting the requirement.</p><p><br></p><p>Deploy the application on Compute Engine instances and use local SSDs for storage. -&gt; Incorrect. Local SSDs provide high-performance storage but do not provide a shared POSIX filesystem. Each Compute Engine instance will have its own separate filesystem.</p><p><br></p><p>Deploy the application on a Google Kubernetes Engine cluster and use Persistent Disk for storage. -&gt; Incorrect. While Persistent Disk provides durable and high-performance block storage for Google Cloud instances, it does not provide a shared filesystem that can be accessed simultaneously by multiple nodes of the application.</p><p><br></p><p>Deploy the application on Compute Engine instances and use Cloud Storage for shared filesystem. -&gt; Incorrect. Cloud Storage provides object storage and does not provide a shared POSIX filesystem. Therefore, it cannot be used as a shared filesystem for Compute Engine instances.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297266,
    "question_plain": "You're a cloud architect who is developing a system that needs to trigger a Cloud Function on a regular basis. You've decided to use Cloud Scheduler to achieve this task. The Cloud Function you've developed is designed to automatically update the inventory in a Cloud Firestore database every day at midnight based on information pulled from an external API. Given this scenario, which of the following approaches is the most suitable way to accomplish this task?",
    "answers": [
      "<p>Create a Pub/Sub topic and use Cloud Scheduler to publish a message to that topic at midnight every day. Set up the Cloud Function to trigger on this topic.</p>",
      "<p>Set up the Cloud Scheduler to trigger a Compute Engine instance that runs a script to call the Cloud Function at midnight every day.</p>",
      "<p>Set up the Cloud Function to trigger at midnight every day using its built-in scheduling functionality.</p>",
      "<p>Use Cloud Scheduler to create a cron job that runs on a Kubernetes Engine cluster to call the Cloud Function at midnight every day.</p>"
    ],
    "explanation": "<p>Create a Pub/Sub topic and use Cloud Scheduler to publish a message to that topic at midnight every day. Set up the Cloud Function to trigger on this topic. -&gt; Correct. Cloud Scheduler can trigger a Cloud Function indirectly by publishing a message to a Pub/Sub topic. This is a common and recommended way to schedule function execution in Google Cloud.</p><p><br></p><p>Set up the Cloud Scheduler to trigger a Compute Engine instance that runs a script to call the Cloud Function at midnight every day. -&gt; Incorrect. This method introduces unnecessary complexity and potential failure points by incorporating Compute Engine, and it's not the recommended way to schedule a Cloud Function.</p><p><br></p><p>Set up the Cloud Function to trigger at midnight every day using its built-in scheduling functionality. -&gt; Incorrect. Cloud Functions do not have built-in scheduling functionality. You would need to use an external service like Cloud Scheduler to trigger the function.</p><p><br></p><p>Use Cloud Scheduler to create a cron job that runs on a Kubernetes Engine cluster to call the Cloud Function at midnight every day. -&gt; Incorrect. This method introduces unnecessary complexity by incorporating a Kubernetes Engine cluster, and it's not the recommended way to schedule a Cloud Function. It would be more efficient and cost-effective to use Pub/Sub with Cloud Scheduler to trigger the function.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297268,
    "question_plain": "You are a cloud architect working on an application that makes HTTP requests to a third-party API. To make your application more resilient, you decide to implement retry logic using a truncated exponential backoff strategy. Which of the following approaches would be the most effective way to implement this in your application?",
    "answers": [
      "<p>Use the <code>Retry</code> class from the Google API client library, and configure it to use exponential backoff.</p>",
      "<p>Implement a static wait time between retries, irrespective of the number of attempts made.</p>",
      "<p>Implement a linear backoff strategy, increasing the wait time by a fixed amount after each failed attempt.</p>",
      "<p>Use the <code>Retry</code> class from the Google API client library, and configure it to use a constant backoff strategy.</p>",
      "<p>Immediately retry the request upon each failure without any wait time.</p>"
    ],
    "explanation": "<p>Use the <code>Retry</code> class from the Google API client library, and configure it to use exponential backoff. -&gt; Correct. The <code><strong>Retry</strong></code> class from the Google API client library provides functionality for retrying requests with exponential backoff, which fits the requirement of implementing a truncated exponential backoff strategy.</p><p><br></p><p>Implement a static wait time between retries, irrespective of the number of attempts made. -&gt; Incorrect. Implementing a static wait time between retries does not follow the exponential backoff strategy. The idea behind exponential backoff is to gradually increase the wait time after each failed attempt, thus reducing the potential for contention.</p><p><br></p><p>Implement a linear backoff strategy, increasing the wait time by a fixed amount after each failed attempt. -&gt; Incorrect. A linear backoff strategy increases the wait time by a fixed amount after each failed attempt, which is not as efficient as the exponential backoff strategy in terms of reducing contention and system load.</p><p><br></p><p>Use the <code>Retry</code> class from the Google API client library, and configure it to use a constant backoff strategy. -&gt; Incorrect. A constant backoff strategy, where the wait time between retries remains constant, is not as effective as the exponential backoff strategy in terms of reducing contention and system load.</p><p><br></p><p>Immediately retry the request upon each failure without any wait time. -&gt; Incorrect. Immediately retrying the request upon each failure without any wait time can increase contention and system load, and can potentially lead to a failure spiral if the external system is already under high load or experiencing temporary issues. It is not a recommended approach to handle failures.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297270,
    "question_plain": "As a cloud architect, you have been tasked with setting up a robust and flexible content delivery system. The requirement is to have different Compute Engine instances serve content based on the URL path. For example, requests to www.example.com/audio/* should be served by one set of instances, and requests to www.example.com/video/* should be served by another set of instances. Which of the following would be the most appropriate approach to achieve this?",
    "answers": [
      "<p>Create an HTTPS load balancer and define URL maps to route traffic to the appropriate set of instances based on the path.</p>",
      "<p>Create two separate HTTPS load balancers, one for each path (<code><strong>/audio</strong></code> and <code><strong>/video</strong></code>), each pointing to a different set of instances.</p>",
      "<p>Create an HTTPS load balancer with a single backend service. Use instance tagging to route traffic to the correct set of instances.</p>",
      "<p>Create a single instance group and use instance templates to define the content to be served based on the URL path.</p>"
    ],
    "explanation": "<p>Create an HTTPS load balancer and define URL maps to route traffic to the appropriate set of instances based on the path. -&gt; Correct. With Google Cloud HTTPS load balancer, you can create URL maps to route requests to specified backend services based on the URL paths.</p><p><br></p><p>Create two separate HTTPS load balancers, one for each path (<code><strong>/audio</strong></code> and <code><strong>/video</strong></code>), each pointing to a different set of instances. -&gt; Incorrect. Creating two separate HTTPS load balancers would not be cost-effective or efficient. A single load balancer with URL maps can handle different URL paths appropriately.</p><p><br></p><p>Create an HTTPS load balancer with a single backend service. Use instance tagging to route traffic to the correct set of instances. -&gt; Incorrect. Instance tagging is primarily used for applying metadata to instances and for network firewall rules. It cannot route traffic based on the URL path.</p><p><br></p><p>Create a single instance group and use instance templates to define the content to be served based on the URL path. -&gt;&nbsp;Incorrect. Instance templates define the machine type, boot disk image, and other instance properties for instances in a managed instance group. They cannot route traffic based on the URL path.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297272,
    "question_plain": "Your organization has deployed a series of containerized applications on Google Kubernetes Engine (GKE). Given the unpredictable demand for these applications, you've been asked to ensure they scale efficiently to handle increased load without over-provisioning resources. Specifically, you've been asked to configure the system so that it automatically adds or removes pods based on the CPU utilization of existing ones. Which approach should you use?",
    "answers": [
      "<p>Configure the Horizontal Pod Autoscaler for your deployments, setting an appropriate target CPU utilization.</p>",
      "<p>Implement a custom solution using Compute Engine instances that manually scales the number of pods based on CPU utilization.</p>",
      "<p>Use the Vertical Pod Autoscaler to automatically adjust the CPU requests for pods, effectively scaling the pod's resources up and down based on utilization.</p>",
      "<p>Implement a custom scaling solution using Cloud Functions to monitor CPU utilization and add or remove pods as needed.</p>"
    ],
    "explanation": "<p>Configure the Horizontal Pod Autoscaler for your deployments, setting an appropriate target CPU utilization. -&gt; Correct. The Horizontal Pod Autoscaler automatically scales the number of pods in a replication controller, deployment, replica set, or stateful set based on observed CPU utilization.</p><p><br></p><p>Implement a custom solution using Compute Engine instances that manually scales the number of pods based on CPU utilization. -&gt; Incorrect. While it's possible to implement a custom solution using Compute Engine, it would be labor-intensive and would not take full advantage of the automated scaling capabilities available within the Kubernetes platform.</p><p><br></p><p>Use the Vertical Pod Autoscaler to automatically adjust the CPU requests for pods, effectively scaling the pod's resources up and down based on utilization. -&gt; Incorrect. The Vertical Pod Autoscaler adjusts the CPU requests and limits of the pods, but does not adjust the number of pods. This approach would help optimize resource use per pod but would not help directly with scaling the number of pods to meet demand.</p><p><br></p><p>Implement a custom scaling solution using Cloud Functions to monitor CPU utilization and add or remove pods as needed. -&gt; Incorrect. While technically possible, implementing a custom scaling solution with Cloud Functions would likely be more complex, less efficient, and less reliable than utilizing the built-in Horizontal Pod Autoscaler.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297274,
    "question_plain": "Your company wants to implement a scalable and highly available solution for a new web-based service. The service needs to be able to handle a large number of concurrent users, and must have the ability to automatically recover from individual machine failures. Which Google Cloud Platform service would you recommend to meet these requirements?",
    "answers": [
      "<p>Compute Engine instances in an auto-scaling group behind a load balancer.</p>",
      "<p>App Engine Standard environment.</p>",
      "<p>Google Kubernetes Engine cluster.</p>",
      "<p>Cloud Functions with a managed instance group.</p>"
    ],
    "explanation": "<p>Compute Engine instances in an auto-scaling group behind a load balancer. -&gt; Correct. It is the best option because it provides scalability, high availability, and automatic recovery from machine failures. Compute Engine instances can be set up in an auto-scaling group to automatically add or remove instances based on the current demand. By using a load balancer, traffic can be distributed across the instances, ensuring that the service remains available even if some of the instances fail.</p><p><br></p><p>App Engine Standard environment. -&gt; Incorrect. It is also a good option for scalability and automatic recovery, but it has some limitations in terms of customizability and control. It may not be suitable for all types of applications.</p><p><br></p><p>Google Kubernetes Engine cluster. -&gt; Incorrect. It requires more setup and management than Compute Engine instances in an auto-scaling group.</p><p><br></p><p>Cloud Functions with a managed instance group. -&gt; Incorrect. It is not a suitable option for this requirement, as Cloud Functions are event-driven and not designed for handling a large number of concurrent users.</p><p><br></p><p>https://cloud.google.com/compute/docs/autoscaler</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297276,
    "question_plain": "Your company is rapidly expanding its operations globally and the amount of data you need to store and analyze is increasing at an exponential rate. The data is highly variable, comprising structured and unstructured data, and comes from various sources such as logs, user-generated content, and IoT devices. As a cloud architect, you are tasked with devising a strategy to store and analyze this data in a cost-effective manner, while ensuring performance, scalability, and data accessibility. Which of the following options would be most suitable?",
    "answers": [
      "<p>Store all data in Google Cloud Storage (GCS) and use Google BigQuery for analysis.</p>",
      "<p>Use Cloud Bigtable for both storing and analyzing the data.</p>",
      "<p>Store structured data in Cloud SQL and unstructured data in Cloud Storage, using BigQuery for analysis.</p>",
      "<p>Store all data in Cloud Spanner and use BigQuery for analysis.</p>"
    ],
    "explanation": "<p>Store all data in Google Cloud Storage (GCS) and use Google BigQuery for analysis. -&gt;&nbsp;Correct. Cloud Storage is cost-effective for storing large amounts of data, and it can handle structured and unstructured data. BigQuery is a serverless, highly scalable, and cost-effective multi-cloud data warehouse designed for business agility, which can be used to analyze the data stored in Cloud Storage.</p><p><br></p><p>Use Cloud Bigtable for both storing and analyzing the data. -&gt; Incorrect. Cloud Bigtable is designed for storing very large amounts of single-keyed data with low latency. It's excellent for time-series data like IoT, but it might not be as cost-effective for diverse data types or for complex analytical queries, compared to BigQuery.</p><p><br></p><p>Store structured data in Cloud SQL and unstructured data in Cloud Storage, using BigQuery for analysis. -&gt; Incorrect. While this approach might work for some use-cases, it requires managing two different storage services and may not be as cost-effective as option A, considering the overhead of managing Cloud SQL instances.</p><p><br></p><p>Store all data in Cloud Spanner and use BigQuery for analysis. -&gt; Incorrect. loud Spanner is a fully managed, mission-critical, relational database service that offers transactional consistency at global scale, schemas, SQL querying, and automatic, synchronous replication for high availability but it could be an overkill and not cost-effective for this scenario. It's better suited for application data which requires strong transactional consistency. Using BigQuery for analysis is right but storing data in Cloud Spanner might not be cost-effective.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297278,
    "question_plain": "When designing a disaster recovery solution for a multi-tier application on Google Cloud Platform (GCP), which of the following options should be considered in order to ensure a recovery time objective (RTO) of less than 1 hour?",
    "answers": [
      "<p>Use Cloud Storage for data backup and replication to a secondary region.</p>",
      "<p>Use Cloud Load Balancer with instance groups in multiple regions for high availability.</p>",
      "<p>Use Cloud SQL with automatic failover to a secondary zone in the same region.</p>",
      "<p>Use&nbsp; Cloud Spanner for database management with multi-region replication.</p>"
    ],
    "explanation": "<p>Use Cloud Spanner for database management with multi-region replication. -&gt;&nbsp;Correct. Cloud Spanner is a horizontally scalable, globally-distributed database service that features multi-region replication. It can automatically handle replication and failover without manual intervention, making it ideal to meet a short RTO like 1 hour. It ensures that your data is available and consistent even in the event of a disaster.</p><p><br></p><p>Use Cloud Storage for data backup and replication to a secondary region. -&gt;&nbsp;Incorrect. While Cloud Storage is suitable for data backup and replication, this strategy alone may not guarantee a recovery time objective (RTO) of less than 1 hour due to potential time required for restoring the system state and data from the backup.</p><p><br></p><p>Use Cloud Load Balancer with instance groups in multiple regions for high availability. -&gt;&nbsp;Incorrect. Cloud Load Balancer with instance groups in multiple regions indeed improves availability, but in the context of a disaster recovery, it doesn't cover database or storage failover. Hence, it may not be able to ensure an RTO of less than 1 hour.</p><p><br></p><p>Use Cloud SQL with automatic failover to a secondary zone in the same region. -&gt;&nbsp;Incorrect. Cloud SQL with automatic failover to a secondary zone improves high availability but it's limited to the same region. In a disaster that affects an entire region, this solution may not meet the RTO requirement of less than 1 hour.</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297280,
    "question_plain": "In order to achieve compliance with data privacy regulations, a company must encrypt sensitive data in transit and at rest. In Google Cloud Platform (GCP), which of the following options would meet this requirement for data stored in Google Cloud Storage?",
    "answers": [
      "<p>Use Google Cloud Storage Transfer Service to encrypt data in transit and enable server-side encryption in Google Cloud Storage.</p>",
      "<p>Use SSL/TLS encryption to secure data in transit and enable Google Cloud Key Management Service (KMS) encryption for data at rest.</p>",
      "<p>Use customer-managed encryption keys for data in transit and at rest using Google Cloud Storage Bucket Policy.</p>",
      "<p>Enable Google-managed encryption for data in transit and at rest using Google Cloud Storage Bucket Policy.</p>"
    ],
    "explanation": "<p>Use SSL/TLS encryption to secure data in transit and enable Google Cloud Key Management Service (KMS) encryption for data at rest. -&gt;&nbsp;Correct. In Google Cloud Platform,&nbsp; SSL/TLS encryption should be used to secure data in transit, and Google Cloud Key Management Service (KMS) encryption should be enabled for data at rest. This meets the requirement for data privacy regulations to encrypt sensitive data in transit and at rest. </p><p><br></p><p>Use Google Cloud Storage Transfer Service to encrypt data in transit and enable server-side encryption in Google Cloud Storage. -&gt; Incorrect. It is incorrect because it only provides server-side encryption for data at rest but does not cover data in transit. </p><p><br></p><p>Use customer-managed encryption keys for data in transit and at rest using Google Cloud Storage Bucket Policy. -&gt; Incorrect. It is incorrect because customer-managed encryption keys only cover data at rest and not data in transit. </p><p><br></p><p>Enable Google-managed encryption for data in transit and at rest using Google Cloud Storage Bucket Policy. -&gt; Incorrect. It is incorrect because Google-managed encryption keys also only cover data at rest and not data in transit.</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297282,
    "question_plain": "A company is planning to deploy a new web application on Google Cloud Platform (GCP) that will handle sensitive customer data. In order to meet security and compliance requirements, which of the following strategies should be considered when designing the application's architecture?",
    "answers": [
      "<p>Store sensitive data in Cloud SQL and implement role-based access controls to restrict access.</p>",
      "<p>Store sensitive data in Cloud Bigtable and use Identity and Access Management (IAM) to manage access.</p>",
      "<p>Store sensitive data in Cloud Storage and use customer-managed encryption keys to secure data at rest.</p>",
      "<p>Store sensitive data in Cloud Datastore and use Cloud Data Loss Prevention (DLP) to classify and redact sensitive data.</p>"
    ],
    "explanation": "<p>Store sensitive data in Cloud Datastore and use Cloud Data Loss Prevention (DLP) to classify and redact sensitive data. -&gt;&nbsp;Correct. Storing sensitive data in Cloud Datastore and using Cloud Data Loss Prevention (DLP) provides a comprehensive solution for securing and managing sensitive data. It includes features for data classification and redaction, ensuring compliance with security and privacy requirements.</p><p><br></p><p>Store sensitive data in Cloud SQL and implement role-based access controls to restrict access. -&gt; Incorrect. It may not cover all aspects of security and compliance requirements. Additional measures, such as encryption of data at rest and in transit, may be necessary to ensure the confidentiality and integrity of sensitive data.</p><p><br></p><p>Store sensitive data in Cloud Bigtable and use Identity and Access Management (IAM) to manage access. -&gt; Incorrect. It may not provide the same level of data security and compliance as the correct choice. Bigtable is a NoSQL database and may require additional security measures to ensure data confidentiality and integrity.</p><p><br></p><p>Store sensitive data in Cloud Storage and use customer-managed encryption keys to secure data at rest. -&gt; Incorrect. It may not directly address the need for sensitive data handling and compliance requirements, such as data classification and redaction.</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297284,
    "question_plain": "A company is planning to migrate a large number of virtual machines (VMs) from an on-premises data center to Google Cloud Platform (GCP). The VMs are running a mix of Linux and Windows operating systems and have varying CPU, memory, and storage requirements. Which of the following options would be the most effective approach to automate the migration process and minimize downtime?",
    "answers": [
      "<p>Use Google Cloud Storage Transfer Service to transfer data to GCP and then manually create and configure new instances in Google Compute Engine.</p>",
      "<p>Use the Google Cloud Deployment Manager to automate the creation and configuration of new instances in Google Compute Engine, and then manually transfer data to the new instances.</p>",
      "<p>Use the Google Cloud Migrate for Compute Engine to automate the discovery, assessment, and migration of the VMs to Google Compute Engine.</p>",
      "<p>Use the Google Cloud Dataproc to automate the creation and configuration of new instances in Google Compute Engine, and then use the Hadoop Distributed File System (HDFS) to transfer data to the new instances.</p>"
    ],
    "explanation": "<p>Use the Google Cloud Migrate for Compute Engine to automate the discovery, assessment, and migration of the VMs to Google Compute Engine. -&gt; Correct. Google Cloud Migrate for Compute Engine is a service designed to simplify the migration of virtual machines from on-premises data centers or other cloud platforms to Google Compute Engine. It automates the discovery, assessment, and migration of virtual machines, making the migration process more efficient and reducing the risk of downtime.</p><p><br></p><p>Use Google Cloud Storage Transfer Service to transfer data to GCP and then manually create and configure new instances in Google Compute Engine. -&gt; Incorrect. This option requires manual creation and configuration of new instances, which can be&nbsp; time-consuming and increase the risk of errors.</p><p><br></p><p>Use the Google Cloud Deployment Manager to automate the creation and configuration of new instances in Google Compute Engine, and then manually transfer data to the new instances. -&gt; Incorrect. This option requires manual creation and configuration of new instances, which can be&nbsp; time-consuming and increase the risk of errors.</p><p><br></p><p>Use the Google Cloud Dataproc to automate the creation and configuration of new instances in Google Compute Engine, and then use the Hadoop Distributed File System (HDFS) to transfer data to the new instances. -&gt; Incorrect. It is designed for running big data workloads, and is not the best option for VM migration.</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297286,
    "question_plain": "A company is planning to deploy a new e-commerce application on Google Cloud Platform (GCP) that will handle high volumes of traffic during peak periods. The application will be deployed in multiple regions to provide low latency and high availability to customers globally. Which of the following options would be the most effective approach to handle traffic spikes and ensure that the application can scale dynamically to meet demand?",
    "answers": [
      "<p>Use Cloud Load Balancer with instance groups in each region to distribute traffic and handle spikes.</p>",
      "<p>Use Cloud CDN to cache content closer to the end-user and handle spikes in traffic.</p>",
      "<p>Use Cloud AutoML to build and deploy custom machine learning models that can automatically adjust the number of instances in each region based on traffic patterns.</p>",
      "<p>Use Google Cloud Kubernetes Engine to deploy and manage containers for the application and handle spikes in traffic.</p>"
    ],
    "explanation": "<p>Use Cloud Kubernetes Engine to deploy and manage containers for the application and handle spikes in traffic. -&gt;&nbsp;Correct. Using Cloud Kubernetes Engine to deploy and manage containers provides a scalable and flexible solution for handling traffic spikes and ensuring that the application can dynamically scale to meet demand. It offers efficient container management, auto-scaling capabilities, and high availability, making it suitable for an e-commerce application that needs to handle high volumes of traffic during peak periods.</p><p><br></p><p>Use Cloud Load Balancer with instance groups in each region to distribute traffic and handle spikes. -&gt;&nbsp;Incorrect. It may not provide the same level of scalability and flexibility as container-based solutions. Load balancers can distribute traffic, but managing and scaling individual instances may be more cumbersome compared to container-based deployments.</p><p><br></p><p>Use Cloud CDN to cache content closer to the end-user and handle spikes in traffic. -&gt;&nbsp;Incorrect. While it can help with offloading static content and reducing the load on the application servers, it may not be the most effective approach for handling dynamic scaling of the application itself.</p><p><br></p><p>Use Cloud AutoML to build and deploy custom machine learning models that can automatically adjust the number of instances in each region based on traffic patterns. -&gt;&nbsp;Incorrect. It may introduce unnecessary complexity for handling traffic spikes. AutoML is typically used for building machine learning models and making predictions rather than directly managing application scalability.</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297288,
    "question_plain": "A multinational corporation with offices in multiple regions is looking to deploy a disaster recovery solution to ensure business continuity in the event of a regional disaster. They have the following requirements:minimize downtime in the event of a disasterautomatically failover to a secondary site in the event of a disasterensure data consistency between the primary and secondary sitescost-effective solutionWhich of the following Google Cloud solutions would best meet these requirements?",
    "answers": [
      "<p>Cloud Load Balancer with auto-scaling groups.</p>",
      "<p>Cloud Datastore with multi-region replication.</p>",
      "<p>Cloud SQL with read replicas in multiple regions.</p>",
      "<p>Cloud Storage with object versioning and multi-region bucket replication.</p>"
    ],
    "explanation": "<p>Cloud Storage with object versioning and multi-region bucket replication. -&gt;&nbsp;Correct. It is the recommended solution for disaster recovery in this scenario. Object versioning allows for the preservation of previous versions of objects, ensuring data consistency and recoverability. Multi-region bucket replication automatically replicates data to a secondary location, providing automatic failover capabilities and minimizing downtime in the event of a disaster. This solution is cost-effective and meets all the specified requirements.</p><p><br></p><p>Cloud Load Balancer with auto-scaling groups. -&gt;&nbsp;Incorrect. While it can help with minimizing downtime, it does not directly address the requirement of automatically failing over to a secondary site in the event of a disaster or ensuring data consistency.</p><p><br></p><p>Cloud Datastore with multi-region replication. -&gt;&nbsp;Incorrect. It may not be the most suitable solution for disaster recovery as it does not offer automatic failover capabilities or provide the same level of data consistency between primary and secondary sites as other options.</p><p><br></p><p>Cloud SQL with read replicas in multiple regions. -&gt;&nbsp;Incorrect. While it can help with data consistency and scalability, it does not provide automatic failover capabilities or the same level of disaster recovery features as other options.</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297290,
    "question_plain": "A large financial services company is looking to migrate its legacy data warehousing solution to the cloud to reduce costs and improve performance. The data warehousing solution must handle the following requirements:store and process petabytes of financial datasupport real-time data ingestion and analysisensure data security and compliance with industry regulationsprovide a flexible and scalable architecture for future growthWhich of the following Google Cloud solutions would best meet these requirements?",
    "answers": [
      "<p>BigQuery with Cloud Dataflow and Cloud Pub/Sub</p>",
      "<p>Cloud SQL with Cloud Storage and Cloud Data Fusion</p>",
      "<p>Bigtable with Cloud Storage and Cloud Functions</p>",
      "<p>Cloud Dataproc with Cloud Storage and Cloud Datastore</p>"
    ],
    "explanation": "<p>BigQuery with Cloud Dataflow and Cloud Pub/Sub -&gt;&nbsp;Correct. BigQuery is designed to handle petabyte-scale data and supports real-time data analysis, meeting the primary requirements. It also provides robust security measures and supports compliance with regulations. Cloud Dataflow can manage real-time data ingestion and processing, and Cloud Pub/Sub can handle real-time messaging, which all together provide a flexible and scalable solution for the company's needs.</p><p><br></p><p>Cloud SQL with Cloud Storage and Cloud Data Fusion -&gt;&nbsp;Incorrect. Cloud SQL is a relational database service and might not be the best fit for handling petabytes of data or real-time analysis. Cloud Storage, although durable and scalable, is not ideal for real-time data ingestion and processing. Cloud Data Fusion is an integrated data pipeline solution, but does not fully support real-time operations.</p><p><br></p><p>Bigtable with Cloud Storage and Cloud Functions -&gt;&nbsp;Incorrect. While Bigtable is designed for large operational and analytical workloads, it might not be cost-effective for petabytes of financial data. Cloud Functions is a serverless execution environment and may not handle real-time data ingestion and processing at the required scale.</p><p><br></p><p>Cloud Dataproc with Cloud Storage and Cloud Datastore -&gt;&nbsp;Incorrect. Cloud Dataproc is a managed Hadoop and Spark service, which is not designed for real-time data ingestion and analysis. Cloud Datastore is a NoSQL document database which is not ideal for large-scale, complex analytical workloads.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297292,
    "question_plain": "You are designing a public-facing web application on Google Cloud. The application is expected to handle sensitive user data. Which of the following should you consider implementing to enhance security?",
    "answers": [
      "<p>Use IAM roles to restrict access to resources.</p>",
      "<p>Enable HTTP(S) Load Balancing to distribute traffic.</p>",
      "<p>Use Cloud CDN to cache and deliver content.</p>",
      "<p>Enable Cloud Trace to monitor application performance.</p>"
    ],
    "explanation": "<p>Use IAM roles to restrict access to resources. -&gt; Correct. IAM roles are critical for controlling who has what kind of access to your resources. By giving the least privileges required to perform a task, you can greatly enhance security.</p><p><br></p><p>Enable HTTP(S) Load Balancing to distribute traffic. -&gt;&nbsp;Incorrect. While HTTP(S) Load Balancing can distribute traffic, it doesn't directly enhance security.</p><p><br></p><p>Use Cloud CDN to cache and deliver content. -&gt;&nbsp;Incorrect. While Cloud CDN can improve the speed of content delivery, it doesn't directly enhance security.</p><p><br></p><p>Enable Cloud Trace to monitor application performance. -&gt;&nbsp;Incorrect. Cloud Trace helps you analyze the latency of your application, but it doesn't directly enhance security.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297294,
    "question_plain": "A company has recently adopted Google Cloud Platform (GCP) for its infrastructure and wants to ensure that its virtual machine (VM) instances are automatically restarted if they fail. Which of the following options should be used to meet this requirement?",
    "answers": [
      "<p>Enable automatic restart for individual instances in the Cloud Console.</p>",
      "<p>Use a startup script to automatically restart instances.</p>",
      "<p>Implement a managed instance group and enable automatic restart.</p>",
      "<p>Use a custom health check to determine instance failure and trigger an automatic restart.</p>"
    ],
    "explanation": "<p>Implement a managed instance group and enable automatic restart. -&gt; Correct. A managed instance group (MIG) is a collection of homogeneous VM instances that are created from a common instance template. By using a MIG, you can configure automatic restart for failed instances. The MIG will automatically recreate any instance that has failed due to a software or hardware failure. You can also specify a health check for the instances in the MIG, so that the MIG can determine when an instance is unhealthy and replace it with a new instance.</p><p><br></p><p>Enable automatic restart for individual instances in the Cloud Console. -&gt; Incorrect. It is not scalable and requires manual intervention. </p><p><br></p><p>Use a startup script to automatically restart instances. -&gt; Incorrect. It can be used to automate the restart process, but does not provide the benefits of a MIG, such as scalability and automatic replacement of unhealthy instances. </p><p><br></p><p>Use a custom health check to determine instance failure and trigger an automatic restart. -&gt; Incorrect. It is not as efficient as using a MIG because it requires manual intervention to trigger the restart process.</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297296,
    "question_plain": "A company is planning to migrate its on-premises data center to Google Cloud Platform (GCP). The company has large amounts of data and wants to minimize downtime during the migration process and ensure that its data is secure during the transfer. Which of the following options should be used to meet these requirements?",
    "answers": [
      "<p>Use <code>gsutil</code> to transfer data from on-premises to GCP and encrypt data in transit using SSL.</p>",
      "<p>Use <code>rsync</code> to transfer data from on-premises to GCP and encrypt data at rest using customer-supplied encryption keys.</p>",
      "<p>Use <code>gcloud compute scp</code> to transfer data from on-premises to GCP and encrypt data in transit using SSH.</p>",
      "<p>Use Google Transfer Appliance to physically transfer data from on-premises to GCP and encrypt data at rest using Google-managed encryption keys.</p>"
    ],
    "explanation": "<p>Use Google Transfer Appliance to physically transfer data from on-premises to GCP and encrypt data at rest using Google-managed encryption keys. -&gt;&nbsp;Correct. It is the recommended approach. Google Transfer Appliance allows for offline, high-speed data transfer by physically shipping the appliance to the company's premises. It provides a secure and efficient method to migrate large amounts of data while ensuring data security at rest through encryption with Google-managed encryption keys. This option helps minimize downtime during the migration process.</p><p><br></p><p>Use <code>gsutil</code> to transfer data from on-premises to GCP and encrypt data in transit using SSL. -&gt;&nbsp;Incorrect. It does not specifically address the requirement of minimizing downtime during the migration process. Additionally, encrypting data in transit ensures data security during transfer but does not address data security at rest.</p><p><br></p><p>Use <code>rsync</code> to transfer data from on-premises to GCP and encrypt data at rest using customer-supplied encryption keys. -&gt;&nbsp;Incorrect. It does not address the need to minimize downtime during the migration process. <code>rsync</code> is a file synchronization tool and may not be the most efficient option for transferring large amounts of data during migration.</p><p><br></p><p>Use <code>gcloud compute scp</code> to transfer data from on-premises to GCP and encrypt data in transit using SSH. -&gt;&nbsp;Incorrect. It does not specifically address the requirement of minimizing downtime during the migration process. While encrypting data in transit provides security during transfer, it does not cover data security at rest.</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297298,
    "question_plain": "A company is using Cloud Storage as its primary object storage solution and wants to ensure that all data stored in the storage is available even in the event of a regional outage. Which of the following options should be used to meet this requirement?",
    "answers": [
      "<p>Enable multi-region bucket replication for the storage.</p>",
      "<p>Enable cross-region bucket transfer for the storage.</p>",
      "<p>Enable cross-project bucket transfer for the storage.</p>",
      "<p>Enable multi-project bucket replication for the storage.</p>"
    ],
    "explanation": "<p>Enable multi-region bucket replication for the storage. -&gt;&nbsp;Correct. Enabling multi-region bucket replication for the storage is the recommended approach to ensure data availability even in the event of a regional outage. With multi-region bucket replication, the data stored in Cloud Storage is automatically replicated to multiple regions, providing redundancy and allowing for continued access to the data even if one region becomes unavailable.</p><p><br></p><p>Enable cross-region bucket transfer for the storage. -&gt; Incorrect. Enabling cross-region bucket transfer for the storage is not the most suitable option for ensuring data availability in the event of a regional outage. Cross-region bucket transfer is typically used for copying or migrating data between different regions, but it does not provide the same level of redundancy and continuous availability as multi-region bucket replication.</p><p><br></p><p>Enable cross-project bucket transfer for the storage. -&gt; Incorrect. Enabling cross-project bucket transfer for the storage allows for transferring data between different projects but does not directly address the requirement of ensuring data availability in the event of a regional outage. Cross-project bucket transfer focuses on sharing or moving data between projects rather than providing redundancy for high availability.</p><p><br></p><p>Enable multi-project bucket replication for the storage. -&gt; Incorrect. Enabling multi-project bucket replication for the storage is not a valid option as there is no built-in feature in Cloud Storage specifically called \"multi-project bucket replication.\"</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297300,
    "question_plain": "Your client is planning to deploy a multi-tier application in Google Cloud that involves storing and processing highly sensitive user data. What is the most secure method for managing secrets such as database credentials and API keys within this environment?",
    "answers": [
      "<p>Implement secrets using Google Cloud Secret Manager and enforce access control with IAM policies.</p>",
      "<p>Hard-code secrets into the application source code and rely on source code management for security.</p>",
      "<p>Store secrets in environment variables of the cloud VM instances.</p>",
      "<p>Utilize Cloud Security Command Center to automatically manage and rotate secrets.</p>"
    ],
    "explanation": "<p>Implement secrets using Google Cloud Secret Manager and enforce access control with IAM policies. -&gt; Correct. Google Cloud Secret Manager provides a centralized and secure service for managing, accessing, and auditing secrets across Google Cloud, with IAM providing fine-grained access control.</p><p><br></p><p>Hard-code secrets into the application source code and rely on source code management for security. -&gt; Incorrect. Hard-coding secrets in source code exposes sensitive information to unnecessary risks and is not recommended as it violates basic security best practices.</p><p><br></p><p>Store secrets in environment variables of the cloud VM instances. -&gt; Incorrect. Although environment variables are a common method to handle secrets, they can be accessed by any process in the VM and are not secure enough for highly sensitive data.</p><p><br></p><p>Utilize Cloud Security Command Center to automatically manage and rotate secrets. -&gt; Incorrect. Cloud Security Command Center is primarily used for security monitoring and compliance and does not manage secrets.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297302,
    "question_plain": "A company is using Google Cloud Platform (GCP) for hosting its mission-critical applications and wants to ensure that the data stored in Google Cloud Storage is accessible from its on-premises data center. Which of the following options should be used to meet this requirement?",
    "answers": [
      "<p>Enable Direct Peering between the company's on-premises data center and GCP.</p>",
      "<p>Use Cloud Interconnect to connect the company's on-premises data center to GCP.</p>",
      "<p>Mount Cloud Storage as a network file system using GCSFuse.</p>",
      "<p>Use Transfer Appliance to physically transfer data from GCP to the company's on-premises data center.</p>"
    ],
    "explanation": "<p>Use Cloud Interconnect to connect the company's on-premises data center to GCP. -&gt;&nbsp;Correct. If a company wants to ensure that the data stored in Google Cloud Storage is accessible from its on-premises data center, Cloud Interconnect should be used to connect the two. Cloud Interconnect is a service that allows customers to connect their on-premises data centers to Google Cloud Platform (GCP) over a dedicated, highly available, low-latency connection.</p><p><br></p><p>Enable Direct Peering between the company's on-premises data center and GCP. -&gt; Incorrect. It is incorrect because Direct Peering is used to connect a company's network to Google's network at one of Google's edge locations, not to connect on-premises data centers to GCP.</p><p><br></p><p>Mount Cloud Storage as a network file system using GCSFuse. -&gt; Incorrect. It is also incorrect because GCSFuse is a tool that allows Cloud Storage buckets to be mounted as file systems on virtual machines and does not provide connectivity between on-premises data centers and GCP.</p><p><br></p><p>Use Transfer Appliance to physically transfer data from GCP to the company's on-premises data center. -&gt; Incorrect. It is also incorrect because Transfer Appliance is used to securely transfer large amounts of data to Google Cloud, but it is not a solution for ongoing data access between on-premises data centers and GCP.</p><p><br></p><p>https://cloud.google.com/network-connectivity/docs/interconnect</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297304,
    "question_plain": "In a gaming project where a company is planning to deploy a high-performance transactional database on Google Cloud Platform (GCP) and prioritize high availability and scalability, which GCP service should be used to meet these requirements? Choose the correct option from the five given answers.",
    "answers": [
      "<p>Cloud Spanner</p>",
      "<p>Cloud Datastore</p>",
      "<p>Cloud Pub/Sub</p>",
      "<p>Cloud Storage</p>"
    ],
    "explanation": "<p>Cloud Spanner -&gt; Correct. Cloud Spanner is a globally distributed and highly available relational database service on GCP. It provides strong consistency guarantees, automatic scaling, and built-in high availability features. It is an ideal choice for a high-performance database in a gaming project that requires both scalability and high availability.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. Cloud Datastore is a NoSQL document database and may not provide the same level of performance and scalability as Cloud Spanner in this gaming project scenario.</p><p><br></p><p>Cloud Pub/Sub -&gt; Incorrect. Cloud Pub/Sub is a messaging service for building event-driven systems and does not focus on high-performance databases or addressing high availability and sudden traffic spikes.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. Cloud Storage is a scalable object storage solution, but it is not specifically designed for hosting high-performance databases or addressing high availability and sudden traffic spikes in a gaming project.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297306,
    "question_plain": "Your organization has a web application running on a single virtual machine (VM) in Google Compute Engine. The application is experiencing high traffic and the VM is struggling to handle the load. You want to scale the application to multiple VMs to increase performance and availability. However, the application requires access to a shared file system for storing user data. What is the most efficient and cost-effective way to scale the application while still maintaining access to the shared file system?",
    "answers": [
      "<p>Use a managed file storage service, such as Google Cloud Filestore, and configure each VM to mount the shared file system.</p>",
      "<p>Use Google Cloud Storage to store user data and modify the application to access the data through the Cloud Storage API.</p>",
      "<p>Create a dedicated VM to host the shared file system and configure each VM in the auto-scaling group to mount the file system.</p>",
      "<p>Modify the application to store user data in a distributed database, such as Cloud Spanner, and configure each VM to access the database.</p>"
    ],
    "explanation": "<p>Use a managed file storage service, such as Google Cloud Filestore, and configure each VM to mount the shared file system. -&gt; Correct. This is the correct answer because it is the most efficient and cost-effective solution. Google Cloud Filestore is a fully managed service that provides a high-performance file system with low latency access to data. It can be easily mounted by each VM in the auto-scaling group, allowing the application to scale seamlessly while still maintaining access to the shared file system.</p><p><br></p><p>Use Google Cloud Storage to store user data and modify the application to access the data through the Cloud Storage API. -&gt; Incorrect. While this is a viable solution, it would require significant changes to the application code and could potentially result in performance issues due to the latency involved in accessing data over the network.</p><p><br></p><p>Create a dedicated VM to host the shared file system and configure each VM in the auto-scaling group to mount the file system. -&gt; Incorrect. While this would work, it would be less efficient and more expensive than using a managed file storage service like Google Cloud Filestore.</p><p><br></p><p>Modify the application to store user data in a distributed database, such as Cloud Spanner, and configure each VM to access the database. -&gt; Incorrect. While this is a valid solution, it would require significant changes to the application code and would be more expensive than using a managed file storage service like Google Cloud Filestore.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297308,
    "question_plain": "You're a cloud architect assigned to manage access to resources in a Google Cloud project for a team. The team consists of a Data Scientist who needs to run BigQuery jobs, a Cloud Engineer who deploys applications on App Engine, a Network Engineer who manages VPCs, and an Auditor who checks IAM permissions and resource usage. Which of the following sets of predefined roles should you assign to each team member to grant them the least privilege they need to do their job effectively?",
    "answers": [
      "<p>BigQuery User to the Data Scientist, App Engine Admin to the Cloud Engineer, Compute Network Admin to the Network Engineer, and Security Reviewer to the Auditor.</p>",
      "<p>BigQuery User to the Data Scientist, App Engine Viewer to the Cloud Engineer, Compute Network Admin to the Network Engineer, and Viewer to the Auditor.</p>",
      "<p>BigQuery User to the Data Scientist, App Engine Admin to the Cloud Engineer, Compute Network User to the Network Engineer, and Security Reviewer to the Auditor.</p>",
      "<p>BigQuery Data Editor to the Data Scientist, App Engine Service Admin to the Cloud Engineer, Compute Network Viewer to the Network Engineer, and Security Reviewer to the Auditor.</p>"
    ],
    "explanation": "<p>BigQuery User to the Data Scientist, App Engine Admin to the Cloud Engineer, Compute Network Admin to the Network Engineer, and Security Reviewer to the Auditor. -&gt; Correct. The BigQuery User role allows the Data Scientist to run BigQuery jobs. The App Engine Admin role allows the Cloud Engineer to deploy applications. The Compute Network Admin role allows the Network Engineer to manage VPCs. The Security Reviewer role allows the Auditor to view IAM permissions and resource usage.</p><p><br></p><p>BigQuery User to the Data Scientist, App Engine Viewer to the Cloud Engineer, Compute Network Admin to the Network Engineer, and Viewer to the Auditor. -&gt; Incorrect. In this set, the Cloud Engineer and Network Engineer don't have the necessary roles to carry out their duties effectively. App Engine Viewer and Compute Network Admin do not provide enough permissions for application deployment and network management, respectively.</p><p><br></p><p>BigQuery User to the Data Scientist, App Engine Admin to the Cloud Engineer, Compute Network User to the Network Engineer, and Security Reviewer to the Auditor. -&gt;&nbsp;Incorrect. In this set, the Network Engineer has the Compute Network User role, which doesn't allow managing VPCs.</p><p><br></p><p>BigQuery Data Editor to the Data Scientist, App Engine Service Admin to the Cloud Engineer, Compute Network Viewer to the Network Engineer, and Security Reviewer to the Auditor. -&gt; Incorrect. In this set, the Data Scientist and Network Engineer roles are inadequate. BigQuery Data Editor doesn't provide permission for job execution, and Compute Network Viewer doesn't provide enough permissions for network management.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297310,
    "question_plain": "You're a cloud architect working for a company that heavily uses BigQuery for data analysis. Recently, the company has experienced a surge in costs from BigQuery operations, and you've been tasked with finding a solution to reduce these costs without affecting the data analysis process significantly. Which of the following strategies will effectively achieve this goal?",
    "answers": [
      "<p>Implement query caching to avoid running redundant queries and paying for them.</p>",
      "<p>Migrate all data from BigQuery to Cloud SQL to reduce storage costs.</p>",
      "<p>Schedule queries during off-peak hours to benefit from lower demand pricing.</p>",
      "<p>Increase the number of slots in your BigQuery reservations to improve query performance and reduce cost.</p>"
    ],
    "explanation": "<p>Implement query caching to avoid running redundant queries and paying for them. -&gt; Correct. BigQuery's built-in query caching can store results of a query for up to 24 hours, and re-running the same query within that timeframe can retrieve results from cache instead of re-running the entire query, thus reducing costs.</p><p><br></p><p>Migrate all data from BigQuery to Cloud SQL to reduce storage costs. -&gt; Incorrect. Migrating all data from BigQuery to Cloud SQL could increase costs due to the overhead of maintaining a fully managed relational database, and it would not be the right solution for big data analysis that BigQuery is designed for.</p><p><br></p><p>Schedule queries during off-peak hours to benefit from lower demand pricing. -&gt; Incorrect. BigQuery does not have demand-based pricing, so running queries during off-peak hours would not reduce costs.</p><p><br></p><p>Increase the number of slots in your BigQuery reservations to improve query performance and reduce cost. -&gt; Incorrect. Increasing the number of slots in your BigQuery reservations would increase costs since you pay for slots on an hourly basis whether they are used or not.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297312,
    "question_plain": "Your company has made the decision to adopt Google Cloud Platform to host its sensitive application data. As a security measure, it is imperative that the Virtual Machines (VMs) hosting this application are safeguarded from threats like rootkits and boot malware. Which of the following would be the most suitable approach?",
    "answers": [
      "<p>Leverage the use of Shielded VMs.</p>",
      "<p>Deploy the application on Preemptible VMs with an additional layer of network security controls.</p>",
      "<p>Use Compute Engine VMs with encrypted disk storage.</p>",
      "<p>Run the application on Google Kubernetes Engine with Workload Identity enabled.</p>"
    ],
    "explanation": "<p>Leverage the use of Shielded VMs. -&gt; Correct. Shielded VMs are designed to offer superior security for running applications on Google Cloud Platform. They come with features like secure boot, vTPM enabled, and integrity monitoring, making them the best fit for hosting sensitive applications.</p><p><br></p><p>Deploy the application on Preemptible VMs with an additional layer of network security controls. -&gt; Incorrect. Preemptible VMs are not suitable for sensitive applications as they are short-lived and can be terminated by Google Cloud at any time.</p><p><br></p><p>Use Compute Engine VMs with encrypted disk storage. -&gt; Incorrect. While using encrypted disk storage with Compute Engine VMs adds an extra layer of security, it does not protect against boot malware or rootkits as Shielded VMs do.</p><p><br></p><p>Run the application on Google Kubernetes Engine with Workload Identity enabled. -&gt; Incorrect. Google Kubernetes Engine (GKE) with Workload Identity does provide an identity for each service in the cluster, but it doesn't provide protection against boot malware or rootkits like Shielded VMs do.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297314,
    "question_plain": "Your organization is moving towards a DevOps culture and wants to leverage Google Cloud for managing the software development life cycle. They are looking to standardize the development and deployment process to minimize errors and improve efficiency. Which approach should you suggest?",
    "answers": [
      "<p>Use Cloud Source Repositories for version control, Cloud Build for building and testing, and Google Kubernetes Engine (GKE) for deployment.</p>",
      "<p>Use Compute Engine instances for deployment and ask developers to manually upload their code.</p>",
      "<p>Develop on local machines and then use Cloud Storage for deployment.</p>",
      "<p>Rely heavily on manual testing to catch all potential errors before deployment.</p>"
    ],
    "explanation": "<p>Use Cloud Source Repositories for version control, Cloud Build for building and testing, and Google Kubernetes Engine (GKE) for deployment. -&gt;&nbsp;Correct. This answer integrates several Google Cloud services to manage the SDLC in a streamlined, efficient manner, embodying the principles of a DevOps culture.</p><p><br></p><p>Use Compute Engine instances for deployment and ask developers to manually upload their code. -&gt;&nbsp;Incorrect. This approach lacks the automated, streamlined process that DevOps emphasizes. Manual operations can lead to human error and inconsistencies.</p><p><br></p><p>Develop on local machines and then use Cloud Storage for deployment. -&gt;&nbsp;Incorrect. While Cloud Storage is effective for storing static content, it is not designed to be a destination for application deployments.</p><p><br></p><p>Rely heavily on manual testing to catch all potential errors before deployment. -&gt;&nbsp;Incorrect. While testing is a crucial part of SDLC, solely relying on manual testing goes against the DevOps principle of automation and does not scale effectively.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297316,
    "question_plain": "You are working on an application deployed on Google Kubernetes Engine (GKE). The application has a large number of microservices and has been experiencing intermittent failures. You've been asked to establish a testing and validation process to identify the issues. What is the best approach to take?",
    "answers": [
      "<p>Leverage Cloud Logging and Cloud Monitoring to identify issues, followed by integration tests for microservices communication.</p>",
      "<p>Perform extensive manual testing for each microservice.</p>",
      "<p>Use Cloud Scheduler to schedule regular restarts of the services to avoid failure.</p>",
      "<p>Implement unit tests for each service and ignore integration testing.</p>"
    ],
    "explanation": "<p>Leverage Cloud Logging and Cloud Monitoring to identify issues, followed by integration tests for microservices communication. -&gt;&nbsp;Correct. Cloud Logging and Cloud Monitoring can help identify where failures are occurring. After identifying potential issues, performing integration tests ensures that the services can effectively communicate with each other, which is key in a microservices architecture.</p><p><br></p><p>Perform extensive manual testing for each microservice. -&gt; Incorrect. While manual testing might be helpful in certain contexts, it is not efficient or effective for a large number of microservices that could have interaction issues. Automation would be a more effective approach.</p><p><br></p><p>Use Cloud Scheduler to schedule regular restarts of the services to avoid failure. -&gt; Incorrect. Regularly restarting services doesn't address the root cause of the failures. It's more of a workaround than a solution, and it can lead to additional problems such as downtime during the restarts.</p><p><br></p><p>Implement unit tests for each service and ignore integration testing. -&gt; Incorrect. While unit tests are necessary to validate individual service functionality, ignoring integration testing in a microservices architecture can miss the issues occurring in the interaction between services.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297600,
    "question_plain": "As a cloud architect, you set up billing for your project and want to prevent excessive consumption of resources due to an error or malicious attack. What should you recommend to do?",
    "answers": [
      "<p>You should set up quotas for the resources that your project will be using.</p>",
      "<p>You should set up budgets and alerts in your project.</p>",
      "<p>You should set up a spending limit on the credit card used in your billing account.</p>",
      "<p>You should label all resources, regularly export the billing reports, and analyze them with BigQuery.</p>"
    ],
    "explanation": "<p>You should set up quotas for the resources that your project will be using. -&gt; By setting up quotas for the resources used in your project, you can limit the maximum amount of resources that can be used in a given period of time. This can help prevent excessive consumption of resources due to an error or malicious attack. </p><p><br></p><p>You should set up budgets and alerts in your project. -&gt; Incorrect. Budgets and alerts can help you track your spending and alert you when your spending exceeds a certain threshold, but they do not prevent excessive resource consumption. </p><p><br></p><p>You should set up a spending limit on the credit card used in your billing account. -&gt; Incorrect. It can prevent unexpected charges but may not prevent excessive resource consumption. </p><p><br></p><p>You should label all resources, regularly export the billing reports, and analyze them with BigQuery. -&gt; Incorrect. Labeling resources and analyzing billing reports with BigQuery can provide insights into resource usage patterns, but it does not prevent excessive resource consumption.</p><p><br></p><p>https://cloud.google.com/compute/quotas</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297602,
    "question_plain": "You are a cloud architect for a multinational corporation that has a wide array of legacy applications hosted on-premises. You have been tasked to devise a strategy to migrate these applications to Google Cloud. Considering the least disruption, the nature of the applications, and the organization's business objectives, which migration strategy should you recommend?",
    "answers": [
      "<p>Use the Strangler pattern where a new system slowly replaces the old one over time.</p>",
      "<p>Lift-and-Shift strategy for all applications regardless of their complexity.</p>",
      "<p>Prioritize a hybrid approach, maintaining a mix of on-premises and cloud-hosted applications.</p>",
      "<p>Migrate all applications to serverless compute options, such as Google Cloud Functions or App Engine.</p>"
    ],
    "explanation": "<p>Use the Strangler pattern where a new system slowly replaces the old one over time. -&gt; Correct. The Strangler pattern involves building a new system around the edges of the old, gradually replacing it. This method reduces risk, allows feedback to influence the process, and enables gradual skill, process, and even cultural changes.</p><p><br></p><p>Lift-and-Shift strategy for all applications regardless of their complexity. -&gt; Incorrect. The Lift-and-Shift approach, although the fastest, might not be suitable for all applications due to their complexity, differences in infrastructure, and possible compatibility issues with the cloud environment.</p><p><br></p><p>Prioritize a hybrid approach, maintaining a mix of on-premises and cloud-hosted applications. -&gt; Incorrect. A hybrid approach could be beneficial in certain cases. However, this approach requires maintaining two environments (on-premises and cloud), which can be complicated and might not fully realize the benefits of moving to the cloud.</p><p><br></p><p>Migrate all applications to serverless compute options, such as Google Cloud Functions or App Engine. -&gt; Incorrect. While serverless options provide scalability and operational benefits, migrating all applications to serverless might not be feasible or cost-effective, especially if applications have specific requirements that are not well-suited for a serverless architecture.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297604,
    "question_plain": "Your company plans to deploy a new mission-critical web application on Google Cloud Platform (GCP) and it must be highly available and scalable. As a cloud architect, you've decided to use managed instance groups (MIGs). Which features should you include in your deployment to ensure high availability and scalability?",
    "answers": [
      "<p>Multi-zone deployment, autohealing, and autoscaling.</p>",
      "<p>Single-zone deployment, autohealing, and autoscaling.</p>",
      "<p>Multi-zone deployment, no autohealing, and autoscaling.</p>",
      "<p>Multi-zone deployment, autohealing, and no autoscaling.</p>"
    ],
    "explanation": "<p>Multi-zone deployment, autohealing, and autoscaling. -&gt;&nbsp;Correct. A multi-zone deployment helps to ensure high availability by spreading instances across multiple zones within a region. Autohealing helps maintain high availability by automatically recreating instances that become unresponsive due to health-check failures. Autoscaling helps to handle increased load by automatically adding instances to the group, and to save costs by removing unneeded instances. </p><p><br></p><p>Single-zone deployment, autohealing, and autoscaling. -&gt;&nbsp;Incorrect. While autohealing and autoscaling ensure high availability and scalability respectively, deploying instances in a single zone does not provide high availability. If the zone experiences an outage, the application becomes unavailable.</p><p><br></p><p>Multi-zone deployment, no autohealing, and autoscaling. -&gt;&nbsp;Incorrect. Although multi-zone deployment and autoscaling are used, the absence of autohealing means that if an instance becomes unresponsive, it won't be automatically recreated, affecting the application's availability.</p><p><br></p><p>Multi-zone deployment, autohealing, and no autoscaling. -&gt;&nbsp;Incorrect. While multi-zone deployment and autohealing ensure high availability, not using autoscaling would mean the application may not handle increased load efficiently.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297606,
    "question_plain": "As a cloud architect, you need to do a presentation on how to interact with the Google Cloud. Select all possible ways to interact with GCP.",
    "answers": [
      "<p>Cloud Platform Console</p>",
      "<p>Cloud Shell and Cloud SDK</p>",
      "<p>Cloud Console Mobile App</p>",
      "<p>REST-based API</p>",
      "<p>CloudFormation</p>"
    ],
    "explanation": "<p>Cloud Platform Console -&gt; Correct. Cloud Platform Console is the web-based interface for interacting with GCP. It provides a graphical user interface (GUI) for managing resources and services. </p><p><br></p><p>Cloud Shell and Cloud SDK -&gt; Correct. Cloud Shell and Cloud SDK provide a command-line interface (CLI) for interacting with GCP. Cloud Shell is a web-based terminal that includes the Cloud SDK pre-installed, while Cloud SDK is a set of tools that allows developers to interact with GCP programmatically. </p><p><br></p><p>Cloud Console Mobile App -&gt; Correct. Cloud Console Mobile App is a mobile application that allows users to manage their GCP resources and services from their mobile devices. </p><p><br></p><p>REST-based API -&gt; Correct. REST-based API allows developers to interact with GCP programmatically using HTTP requests. This allows developers to build custom applications that integrate with GCP. </p><p><br></p><p>CloudFormation -&gt; Incorrect. CloudFormation is not a valid option as it is an AWS service and not a GCP service.</p><p><br></p><p>https://cloud.google.com/docs/overview#ways_to_interact_with_the_services</p>",
    "correct_response": ["a", "b", "c", "d"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 81297608,
    "question_plain": "Users of your application complaints about long wait times while loading application pages with images. As a cloud architect, you want to reduce latency. Which of the following options would you use? (select 2)",
    "answers": [
      "<p>Multi-Regional Storage</p>",
      "<p>Cloud CDN</p>",
      "<p>Coldline Storage</p>",
      "<p>Cloud Pub/Sub</p>",
      "<p>Cloud VPN</p>"
    ],
    "explanation": "<p>Multi-Regional Storage -&gt; Correct. Multi-Regional Storage is a good option because it enables the application to serve images from a storage location that is geographically closer to the user, reducing the latency of image loading.</p><p><br></p><p>Cloud CDN -&gt; Correct. Cloud CDN is also a good option because it caches images at edge locations closer to the user, reducing the latency and the number of requests to the application's origin server.</p><p><br></p><p>Coldline Storage -&gt; Incorrect. Coldline Storage is not a good option for reducing latency in this scenario because it is designed for infrequently accessed data, and accessing it may take longer due to retrieval times.</p><p><br></p><p>Cloud Pub/Sub -&gt; Incorrect. Cloud Pub/Sub is not a good option for reducing latency in this scenario because it is a messaging service for asynchronous communication between services and does not help with image loading times.</p><p><br></p><p>Cloud VPN -&gt; Incorrect. Cloud VPN is not a good option for reducing latency in this scenario because it is a service that provides secure connectivity between on-premises networks and Google Cloud resources and does not help with image loading times.</p><p><br></p><p>https://cloud.google.com/storage/docs/storage-classes#legacy</p><p>https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview</p>",
    "correct_response": ["a", "b"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 81297610,
    "question_plain": "You are working as a cloud architect for a large organization. The organization is structured into various departments and you have been tasked with assigning the correct Identity and Access Management (IAM) roles to ensure proper access control. The sales team in your organization has requested access to view and download sales data stored in Cloud Storage but they should not be allowed to delete or modify any data. What IAM role should you assign to the sales team?",
    "answers": [
      "<p>Storage Object Viewer</p>",
      "<p>Storage Admin</p>",
      "<p>Storage Object Creator</p>",
      "<p>Storage Object Admin</p>"
    ],
    "explanation": "<p>Storage Object Viewer -&gt;&nbsp;Correct. The Storage Object Viewer role allows for read-only access to objects in Google Cloud Storage. This fits the requirements of the sales team to view and download data, without the ability to modify or delete it.</p><p><br></p><p>Storage Admin -&gt;&nbsp;Incorrect. The Storage Admin role grants full control over objects and buckets, including the ability to delete and modify data. This would provide more access than is required.</p><p><br></p><p>Storage Object Creator -&gt;&nbsp;Incorrect. The Storage Object Creator role allows for the creation of new objects but does not provide read or download permissions on existing objects. This would not meet the needs of the sales team.</p><p><br></p><p>Storage Object Admin -&gt;&nbsp;Incorrect. The Storage Object Admin role grants full control over objects, including the ability to delete and modify data. This would provide more access than is required.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297612,
    "question_plain": "You are a Google Professional Cloud Architect working with a large e-commerce company. The company wants to optimize its data processing workflows by implementing Google Cloud Dataflow. They aim to leverage the power of Dataflow to process large volumes of streaming and batch data efficiently and reliably. In the context of this complex scenario, which of the following statements about Google Cloud Dataflow is correct?",
    "answers": [
      "<p>Dataflow pipelines are written using Apache Beam, an open-source unified programming model for batch and stream processing.</p>",
      "<p>Cloud Dataflow is a managed service that supports only batch processing of data.</p>",
      "<p>Dataflow pipelines can only process data stored in Cloud Storage and Cloud Bigtable.</p>",
      "<p>Dataflow is primarily designed for small-scale data processing and may not handle high-volume data efficiently.</p>"
    ],
    "explanation": "<p>Dataflow pipelines are written using Apache Beam, an open-source unified programming model for batch and stream processing. -&gt; Correct. Dataflow pipelines are written using Apache Beam, an open-source unified programming model. Apache Beam provides a consistent API for developing data processing pipelines, enabling developers to write code once and run it on multiple processing engines, including Cloud Dataflow.</p><p><br></p><p>Cloud Dataflow is a managed service that supports only batch processing of data. -&gt; Incorrect. Cloud Dataflow is a fully managed service provided by Google Cloud that supports both batch and streaming data processing. It offers flexibility in handling different data processing requirements.</p><p><br></p><p>Dataflow pipelines can only process data stored in Cloud Storage and Cloud Bigtable. -&gt; Incorrect. Dataflow pipelines can process data from various sources, including Cloud Storage, Cloud Pub/Sub, Cloud BigQuery, and external systems using connectors. It is not limited to Cloud Storage and Cloud Bigtable alone.</p><p><br></p><p>Dataflow is primarily designed for small-scale data processing and may not handle high-volume data efficiently. -&gt; Incorrect. Dataflow is designed to handle both small-scale and large-scale data processing. It can scale automatically to process high-volume data efficiently and reliably.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297614,
    "question_plain": "As a cloud architect, you work with Kubernetes in your new cloud project. Select progression of abstraction from the lowest to the highest level in Kubernetes.",
    "answers": [
      "<p>Pods -&gt; Deployments -&gt;&nbsp;Services</p>",
      "<p>Deployments -&gt;&nbsp;Pods -&gt;&nbsp;Services</p>",
      "<p>Deployments -&gt;&nbsp;Services -&gt;&nbsp;Pods</p>",
      "<p>Pods -&gt;&nbsp;Services -&gt; Deployments</p>"
    ],
    "explanation": "<p>Pods -&gt; Deployments -&gt;&nbsp;Services -&gt; Correct. In Kubernetes, a Pod is the smallest and simplest unit in the object model and it represents a single instance of a running process in a cluster. A Deployment is a higher-level abstraction that manages Pods and provides declarative updates to their configurations, allowing them to be scaled up or down. Services define a logical set of Pods and enable network access to them, allowing the communication between different components in a cluster. So the correct progression of abstraction is Pods -&gt; Deployments -&gt; Services.</p><p><br></p><p>https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/</p><p>https://kubernetes.io/docs/concepts/overview/components/</p><p>https://cloud.google.com/kubernetes-engine/docs/quickstart</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297616,
    "question_plain": "You are working on a project for a large multinational company that has numerous APIs with different processing requirements. The architecture team decided to host each API on a separate set of instances and use a single Global HTTP(S) Load Balancer to route requests to the appropriate backend. To ensure the requests reach the right backend service, what should you do?",
    "answers": [
      "<p>Use separate backend services for each API path and configure URL maps to route requests.</p>",
      "<p>Assign different static external IP addresses to each API backend.</p>",
      "<p>Create separate subnetworks for each API backend and route requests based on the subnetwork IP range.</p>",
      "<p>Create a separate VPC network for each API path and configure the Load Balancer to route based on VPC.</p>"
    ],
    "explanation": "<p>Use separate backend services for each API path and configure URL maps to route requests. -&gt; Correct. URL maps in Google Cloud's HTTP(S) Load Balancer allows traffic to be routed based on the URL of the request. Each host and path rule directs a set of URLs to one backend service.</p><p><br></p><p>Assign different static external IP addresses to each API backend. -&gt; Incorrect. Using different static external IP addresses for each API backend would not make sense as we're trying to have a single point of entry (load balancer) to manage all API paths.</p><p><br></p><p>Create separate subnetworks for each API backend and route requests based on the subnetwork IP range. -&gt; Incorrect. Routing requests based on the subnetwork IP range is not applicable here. Load balancer configuration should handle the request routing based on the request content (URL), not the backend network configuration.</p><p><br></p><p>Create a separate VPC network for each API path and configure the Load Balancer to route based on VPC. -&gt; Incorrect. Creating a separate VPC for each API path is an unnecessary over-complication. Routing in Load Balancer is not based on VPCs but on HTTP request parameters such as host and path.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297618,
    "question_plain": "Your objective is to decrease the frequency of unscheduled rollbacks for flawed production deployments within your company's web hosting platform. By enhancing QA/Test processes, you were able to achieve a substantial 80% reduction in rollbacks. Now, what are two additional approaches you can adopt to further minimize the occurrence of rollbacks?",
    "answers": [
      "<p>Implement a green-blue deployment strategy.</p>",
      "<p>Decompose the monolithic platform into microservices.</p>",
      "<p>Replace the QA environment with canary releases.</p>",
      "<p>Reduce the platform's dependency on relational database systems.</p>",
      "<p>Replace the platform's relational database systems with a NoSQL database.</p>"
    ],
    "explanation": "<p>Implement a green-blue deployment strategy. -&gt; Correct. Green-blue deployment is a technique that reduces downtime by creating two identical production environments. While one environment is active and serving requests, the other is on standby, waiting for the next deployment. Once the new deployment is tested and verified, traffic is routed to the new environment, and the old one is decommissioned. This method can significantly reduce the chances of unscheduled rollbacks.</p><p><br></p><p>Decompose the monolithic platform into microservices. -&gt; Correct. Microservices architecture breaks down large applications into smaller, independent components, each with its own development, deployment, and scaling process. This approach can help to isolate issues and prevent them from affecting the entire platform, making it easier to identify and resolve issues before they become catastrophic. It also makes it easier to roll back a single component without affecting the entire platform.</p><p><br></p><p>Replace the QA environment with canary releases. -&gt; Incorrect. Canary releases involve rolling out new features or changes to a small subset of users or servers, rather than the entire user base. This allows you to test the new features in a real-world environment without risking a full-scale rollout. However, this approach may not be effective in reducing rollbacks because it still involves some level of risk.</p><p><br></p><p>Reduce the platform's dependency on relational database systems. -&gt; Incorrect. While this may improve the platform's performance, it may not necessarily reduce unscheduled rollbacks.</p><p><br></p><p>Replace the platform's relational database systems with a NoSQL database. -&gt; Incorrect. This may improve the platform's performance, but it may not necessarily reduce unscheduled rollbacks.</p><p><br></p><p>You can introduce a green-blue deployment model and fragment the monolithic platform into microservices.</p>",
    "correct_response": ["a", "b"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 81297620,
    "question_plain": "You are a cloud architect at a multinational company and have been asked to set up an HTTP(S) load balancer to route traffic to backends in multiple regions. However, the company wants to ensure that user requests are always routed to the closest healthy backend to minimize latency. Additionally, there should be a fallback mechanism if the closest backend is unhealthy. Which of the following strategies would you implement to fulfill these requirements?",
    "answers": [
      "<p>Configure HTTP(S) Load Balancer with multiple backend services in each region and implement Global Load Balancing.</p>",
      "<p>Configure HTTP(S) Load Balancer with a single backend service and health checks.</p>",
      "<p>Configure HTTP(S) Load Balancer with multiple backend services in each region and utilize Cross-Region Load Balancing.</p>",
      "<p>Configure HTTP(S) Load Balancer with a single global backend service and use Global Load Balancing.</p>"
    ],
    "explanation": "<p>Configure HTTP(S) Load Balancer with multiple backend services in each region and implement Global Load Balancing. -&gt; Correct. Creating multiple backend services in each region and implementing Global Load Balancing allows traffic to be directed to the closest healthy backend. If a backend within the closest region is unhealthy, Global Load Balancing would direct traffic to the next closest healthy backend, thereby providing a fallback mechanism.</p><p><br></p><p>Configure HTTP(S) Load Balancer with a single backend service and health checks. -&gt; Incorrect. While health checks are important, a single backend service doesn't support the multi-region requirement.</p><p><br></p><p>Configure HTTP(S) Load Balancer with multiple backend services in each region and utilize Cross-Region Load Balancing. -&gt; Incorrect. Cross-Region Load Balancing enables serving content from the geographically closest backend. However, it does not inherently provide a fallback mechanism.</p><p><br></p><p>Configure HTTP(S) Load Balancer with a single global backend service and use Global Load Balancing. -&gt; Incorrect. While Global Load Balancing does provide a fallback mechanism, a single global backend service does not meet the requirement of routing requests to the closest healthy backend.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297622,
    "question_plain": "You are working for an ad tech company that requires both real-time and batch processing of ad click data. The data needs to be analyzed in real-time for near instant reporting to advertisers, but also needs to be processed in a batch for daily and monthly reporting. Which combination of GCP services would be the most efficient for this use case?",
    "answers": [
      "<p>Use Dataflow with both batch and streaming pipelines for processing data.</p>",
      "<p>Use Pub/Sub for real-time processing and Cloud Functions for batch processing.</p>",
      "<p>Use BigQuery for real-time processing and Cloud Storage for batch processing.</p>",
      "<p>Use Firestore for real-time processing and Cloud Spanner for batch processing.</p>"
    ],
    "explanation": "<p>Use Dataflow with both batch and streaming pipelines for processing data. -&gt; Correct. Cloud Dataflow is designed for both batch and stream data processing paradigms and can handle these use cases efficiently.</p><p><br></p><p>Use Pub/Sub for real-time processing and Cloud Functions for batch processing. -&gt; Incorrect. Pub/Sub is well-suited for real-time processing but Cloud Functions is event-driven and may not be best suited for batch processing.</p><p><br></p><p>Use BigQuery for real-time processing and Cloud Storage for batch processing. -&gt; Incorrect. BigQuery is an analytic data warehouse and while it can handle streaming data, it's not designed for real-time processing. Cloud Storage is an object storage service and doesn't inherently provide batch processing capabilities.</p><p><br></p><p>Use Firestore for real-time processing and Cloud Spanner for batch processing. -&gt; Incorrect. Firestore is a NoSQL database for building web, mobile, and server applications. It is not designed for real-time data processing. Cloud Spanner is a fully managed relational database service and doesn't inherently provide batch processing capabilities.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297624,
    "question_plain": "As a cloud architect, your client has informed you that their recently updated App Engine application is experiencing prolonged loading times of around 30 seconds for certain users. This issue was not present prior to the update. What approach should you adopt to address this problem effectively?",
    "answers": [
      "<p>You should roll back to an earlier known good release initially, then use Cloud Trace and Logging to diagnose the problem in a development/test/staging environment.</p>",
      "<p>You should work with your Internet Service Provider (ISP) to diagnose the problem.</p>",
      "<p>You should open a support ticket to ask for network capture and flow data to diagnose the problem, then roll back your application.</p>",
      "<p>You should roll back to an earlier known good release, then push the release again at a quieter period to investigate. Then use Cloud Trace and Logging to diagnose the problem.</p>"
    ],
    "explanation": "<p>You should roll back to an earlier known good release initially, then use Cloud Trace and Logging to diagnose the problem in a development/test/staging environment. -&gt; Correct. Rolling back to an earlier known good release is a recommended approach to address the issue of prolonged loading times. By reverting to a previous version of the application that did not have this problem, you can quickly restore the desired performance. Afterward, using Cloud Trace and Logging in a development/test/staging environment allows you to diagnose the problem and identify the specific cause of the issue.</p><p><br></p><p>You should work with your Internet Service Provider (ISP) to diagnose the problem. -&gt;&nbsp;Incorrect. It may be a consideration if the issue is related to network connectivity or external factors. However, given that the problem started after the application update, it is more likely that the root cause lies within the application itself.</p><p><br></p><p>You should open a support ticket to ask for network capture and flow data to diagnose the problem, then roll back your application. -&gt;&nbsp;Incorrect. Opening a support ticket to request network capture and flow data may be an option to investigate network-related issues. However, it is important to first roll back to a previous version of the application to restore performance. Additionally, analyzing network data alone may not provide sufficient insights into the specific application-level issue causing the prolonged loading times.</p><p><br></p><p>You should roll back to an earlier known good release, then push the release again at a quieter period to investigate. Then use Cloud Trace and Logging to diagnose the problem. -&gt;&nbsp;Incorrect. Rolling back to an earlier known good release is a recommended approach to address the immediate performance issue. Pushing the release again at a quieter period to investigate allows you to observe the application behavior under different conditions. Using Cloud Trace and Logging can then help diagnose the problem. However, it is advisable to first roll back and restore performance before proceeding with further investigation.</p><p><br></p><p>https://cloud.google.com/logging/</p><p>https://cloud.google.com/trace/docs</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297626,
    "question_plain": "You are a cloud architect of a global online retail company that uses a production database hosted on Cloud SQL. You received an alert that the database is about to run out of storage space. What is the best approach to ensure that the production database does not run out of storage and there is minimal impact on the performance of the application?",
    "answers": [
      "<p>Enable automatic storage increases for the database instance.</p>",
      "<p>Increase the size of the database instance manually.</p>",
      "<p>Export the data to BigQuery and delete the data from the production database.</p>",
      "<p>Create a snapshot of the database, delete some data from the production database, and then restore the data from the snapshot if needed.</p>"
    ],
    "explanation": "<p>Enable automatic storage increases for the database instance. -&gt;&nbsp;Correct. Enabling automatic storage increases for the database instance is the best approach as it allows the system to automatically add storage capacity when the available storage falls below a certain threshold. This allows the application to continue functioning without interruption.</p><p><br></p><p>Increase the size of the database instance manually. -&gt; Incorrect. Increasing the size of the database instance manually could result in downtime and would require constant monitoring of the storage usage. </p><p><br></p><p>Export the data to BigQuery and delete the data from the production database. -&gt; Incorrect. Exporting data to BigQuery and deleting it from the production database is not recommended as it could lead to data loss and inconsistency between the production database and the backup in BigQuery.</p><p><br></p><p>Create a snapshot of the database, delete some data from the production database, and then restore the data from the snapshot if needed. -&gt; Incorrect. Creating a snapshot and deleting some data from the production database is risky as it could lead to data loss. In addition, this approach does not provide a long-term solution to the problem of storage space running out.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297628,
    "question_plain": "As a cloud architect for a rapidly growing e-commerce company, you are tasked with handling Payment Card Industry Data Security Standard (PCI DSS) compliance for the storage and processing of payment card data. The company uses Compute Engine for their application servers, Cloud SQL for their transaction databases, and Cloud Storage for long-term data retention. Which of the following strategies is the most suitable to address this requirement?",
    "answers": [
      "<p>Utilize Google Cloud's Data Loss Prevention (DLP) API to discover, classify, and de-identify sensitive data.</p>",
      "<p>Implement a third-party key management system and utilize customer-supplied encryption keys for all storage systems.</p>",
      "<p>Store all payment card data in Cloud Storage buckets configured with uniform bucket-level access.</p>",
      "<p>Restrict network access to all services using Firewall Rules and Cloud Armor.</p>"
    ],
    "explanation": "<p>Utilize Google Cloud's Data Loss Prevention (DLP) API to discover, classify, and de-identify sensitive data. -&gt;&nbsp;Correct. The Data Loss Prevention (DLP) API is designed to help discover, classify, and de-identify sensitive data, which are key aspects of PCI DSS compliance. This is the best approach to handle sensitive payment card data, ensuring that this data is properly identified and managed wherever it might exist within the Google Cloud environment.</p><p><br></p><p>Implement a third-party key management system and utilize customer-supplied encryption keys for all storage systems. -&gt; Incorrect. Using a third-party key management system and customer-supplied encryption keys can provide an additional layer of control and security. However, this alone doesn't satisfy the broad requirements of PCI DSS compliance, which includes protecting data in transit and use, maintaining a vulnerability management program, and implementing strong access control measures.</p><p><br></p><p>Store all payment card data in Cloud Storage buckets configured with uniform bucket-level access. -&gt; Incorrect. Storing payment card data in Cloud Storage buckets with uniform bucket-level access can simplify permission management. However, this doesn't address many other requirements of PCI DSS compliance, such as protecting data in transit and use, vulnerability management, and regular security assessments.</p><p><br></p><p>Restrict network access to all services using Firewall Rules and Cloud Armor. -&gt; Incorrect. While restricting network access is an important security measure, it is just one aspect of PCI DSS compliance. It doesn't address the need for secure handling, storage, and transmission of payment card data, among other requirements.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297630,
    "question_plain": "You are a cloud architect working for a large e-commerce company. The company is generating massive amounts of clickstream data from its online platform and wants to implement an efficient and scalable solution for storing and analyzing this data. The primary goal is to gain insights into user behavior and improve the overall user experience. Which of the following approaches would be the most suitable for storing and analyzing the clickstream data in this complex scenario?",
    "answers": [
      "<p>Use Cloud Storage for storing the raw clickstream data and BigQuery for performing real-time analytics.</p>",
      "<p>Store the raw clickstream data in Cloud Spanner and use Cloud Dataflow for batch processing and analysis.</p>",
      "<p>Implement a serverless architecture using Cloud Functions to directly process and store the clickstream data in Cloud Firestore.</p>",
      "<p>Set up a self-managed Apache Hadoop cluster on Compute Engine to handle the storage and analysis of the clickstream data.</p>"
    ],
    "explanation": "<p>Use Cloud Storage for storing the raw clickstream data and BigQuery for performing real-time analytics. -&gt; Correct. It is a suitable choice as it leverages Cloud Storage for storing the raw clickstream data, which provides durability and scalability. BigQuery is a powerful analytics tool that can handle real-time queries on large datasets, making it an ideal choice for analyzing the clickstream data.</p><p><br></p><p>Store the raw clickstream data in Cloud Spanner and use Cloud Dataflow for batch processing and analysis. -&gt; Incorrect. It is not the best choice for this scenario. Cloud Spanner is a distributed relational database, which may not be the most efficient solution for storing unstructured clickstream data. Cloud Dataflow is suitable for data processing but may not be the best fit for analyzing clickstream data.</p><p><br></p><p>Implement a serverless architecture using Cloud Functions to directly process and store the clickstream data in Cloud Firestore. -&gt;&nbsp;Incorrect. It is not the optimal approach for this scenario. While Cloud Functions can be used to process and store the clickstream data, Cloud Firestore may not be the most suitable database for complex analysis and querying of the data.</p><p><br></p><p>Set up a self-managed Apache Hadoop cluster on Compute Engine to handle the storage and analysis of the clickstream data. -&gt; Incorrect. It is not the recommended choice. Setting up a self-managed Apache Hadoop cluster on Compute Engine would require significant manual management and may not provide the scalability and ease of use offered by Google Cloud's fully managed services.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297632,
    "question_plain": "Your company is considering an Infrastructure as Code (IaC) approach for deploying resources on Google Cloud Platform (GCP) and has a requirement for creating reusable templates for resource provisioning. As a cloud architect, which tool would you recommend that best aligns with their requirement?",
    "answers": [
      "<p>Google Cloud Deployment Manager</p>",
      "<p>Google Cloud Console</p>",
      "<p>Google Cloud SDK</p>",
      "<p>Cloud Build</p>"
    ],
    "explanation": "<p>Google Cloud Deployment Manager -&gt;&nbsp;Correct. Google Cloud Deployment Manager allows you to specify all the resources needed for your application in a declarative format using yaml. You can also create reusable templates using Python or Jinja2.</p><p><br></p><p>Google Cloud Console -&gt;&nbsp;Incorrect. Google Cloud Console is an interface to manage Google Cloud resources. While it can be used to create and manage resources, it does not support the creation of reusable templates or the IaC approach.</p><p><br></p><p>Google Cloud SDK -&gt;&nbsp;Incorrect. While you can script resource creation with the gcloud command-line tool, it does not natively support the creation of reusable templates, making it less optimal for IaC practices.</p><p><br></p><p>Cloud Build -&gt;&nbsp;Incorrect. Cloud Build is a service that executes your builds on Google Cloud. While it can be used in conjunction with other services to implement IaC, it doesn't natively provide reusable template functionality.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297634,
    "question_plain": "Your company's website experiences high traffic during business hours and almost no traffic during the night. The website is hosted on the Google Cloud Platform, and you have been asked to optimize the configuration for cost efficiency without sacrificing availability during peak hours. Which of the following actions would be the best practice to achieve this?",
    "answers": [
      "<p>Deploy a Managed Instance Group (MIG) and configure autoscaling based on the CPU utilization.</p>",
      "<p>Use Compute Engine instances with maximum possible machine type for higher capacity.</p>",
      "<p>Keep all the Compute Engine instances running 24/7 to avoid startup delays.</p>",
      "<p>Use Cloud Storage to serve the website content instead of Compute Engine instances.</p>"
    ],
    "explanation": "<p>Deploy a Managed Instance Group (MIG) and configure autoscaling based on the CPU utilization. -&gt; Correct. Deploying a Managed Instance Group and configuring autoscaling based on CPU utilization is an efficient way to handle variable traffic. The number of instances will increase during peak hours to handle the load, and decrease during periods of low traffic to save costs.</p><p><br></p><p>Use Compute Engine instances with maximum possible machine type for higher capacity. -&gt; Incorrect. Using instances with the maximum machine type may lead to overprovisioning and unnecessary costs during periods of low traffic. It is generally more cost-effective to scale out (add more instances) than to scale up (use bigger instances).</p><p><br></p><p>Keep all the Compute Engine instances running 24/7 to avoid startup delays. -&gt; Incorrect. Keeping all Compute Engine instances running 24/7 will lead to unnecessary costs, especially during periods of low traffic. It's more cost-effective to turn off or delete unneeded instances, especially if they are not covered by committed use contracts.</p><p><br></p><p>Use Cloud Storage to serve the website content instead of Compute Engine instances. -&gt;&nbsp;Incorrect. Using Cloud Storage to serve the website might be a good choice for static websites, but it is not suitable for dynamic websites that require server-side processing. Therefore, this option is not generally applicable.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297636,
    "question_plain": "Your company is planning to shift their operations to Google Cloud Platform. As a cloud architect, you are tasked with setting up the environment following the best practices. Which of the following is the best strategy to implement?",
    "answers": [
      "<p>Implement IAM policies at the organizational level and fine-tune them at the project level.</p>",
      "<p>Grant all users the 'Owner' role for simplicity and ease of access.</p>",
      "<p>Create a single Virtual Private Cloud (VPC) network for all the company's projects and applications.</p>",
      "<p>Employ a flat project structure where all resources are deployed in a single project for centralized control.</p>"
    ],
    "explanation": "<p>Implement IAM policies at the organizational level and fine-tune them at the project level. -&gt; Correct. IAM policies are inheritable and hierarchical, so it's a good practice to set broad policies at the organization level and then fine-tune them at the project level. This aligns with the principle of least privilege, ensuring that users only have the permissions necessary to perform their tasks.</p><p><br></p><p>Grant all users the 'Owner' role for simplicity and ease of access. -&gt; Incorrect. This is not a good practice as it contradicts the principle of least privilege. Granting everyone the 'Owner' role can lead to accidental deletions or modifications, and it opens the possibility for a major security breach.</p><p><br></p><p>Create a single Virtual Private Cloud (VPC) network for all the company's projects and applications. -&gt; Incorrect. Having a single VPC for all projects and applications may not be ideal due to potential conflicts with firewall rules, IP ranges, and it could create an unnecessary single point of failure.</p><p><br></p><p>Employ a flat project structure where all resources are deployed in a single project for centralized control. -&gt; Incorrect. A flat project structure is not considered a best practice in GCP. It's better to have multiple projects, segregating resources based on their purpose, team, or environment (production, staging, development, etc.), providing better control, security, and billing tracking.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297638,
    "question_plain": "Your organization has decided to run its LAMP (Linux, Apache, MySQL, PHP) stack on Google Cloud Platform. They need a solution that is scalable, easy to manage, and highly available. Which of the following options would you recommend?",
    "answers": [
      "<p>Use Kubernetes Engine to manage the Apache and PHP components in separate containers, with Cloud SQL for MySQL.</p>",
      "<p>Deploy each component of the LAMP stack on separate Compute Engine instances.</p>",
      "<p>Use Cloud Functions to host the PHP application and Cloud SQL for MySQL, with Apache running on a Compute Engine instance.</p>",
      "<p>Use Cloud Run to deploy the Apache and PHP components, with Firestore for MySQL.</p>"
    ],
    "explanation": "<p>Use Kubernetes Engine to manage the Apache and PHP components in separate containers, with Cloud SQL for MySQL. -&gt;&nbsp;Correct. Using Google Kubernetes Engine for managing Apache and PHP components in separate containers, with Cloud SQL for MySQL database, is a robust and scalable solution. It provides high availability, easy management, and the necessary scalability.</p><p><br></p><p>Deploy each component of the LAMP stack on separate Compute Engine instances. -&gt; Incorrect. Deploying each component of the LAMP stack on separate Compute Engine instances would increase management overhead and might not provide the desired level of scalability and high availability.</p><p><br></p><p>Use Cloud Functions to host the PHP application and Cloud SQL for MySQL, with Apache running on a Compute Engine instance. -&gt; Incorrect. While Cloud Functions and Cloud SQL could be used for hosting PHP applications and MySQL databases respectively, Apache cannot be effectively run on Compute Engine for scalable and resilient web serving needs in a LAMP stack scenario.</p><p><br></p><p>Use Cloud Run to deploy the Apache and PHP components, with Firestore for MySQL. -&gt; Incorrect. Cloud Run is a managed compute platform that enables you to run stateless containers, and Firestore is a NoSQL document database, both are not suitable for running a traditional LAMP stack.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81298112,
    "question_plain": "Your company is planning to deploy a new web application on Google Cloud Platform (GCP). The application is expected to have fluctuating usage patterns and significant spikes in traffic. The development team is committed to following best practices for continuous integration and continuous deployment (CI/CD) to enhance the application’s scalability and manageability. As a cloud architect, you are tasked with recommending a CI/CD strategy that ensures the application is always available, scalable, and up to date. Which of the following CI/CD strategies is most appropriate for this scenario?",
    "answers": [
      "<p>Configure a blue-green deployment model using Kubernetes Engine to allow testing in a live environment before full rollout.</p>",
      "<p>Use Cloud Build to automate deployments, employing Cloud Functions for lightweight processing tasks that scale automatically.</p>",
      "<p>Manually deploy updated versions to App Engine standard environment during off-peak hours to minimize disruption.</p>",
      "<p>Implement a rolling update strategy using Compute Engine managed instance groups to ensure zero downtime during deployments.</p>"
    ],
    "explanation": "<p>Configure a blue-green deployment model using Kubernetes Engine to allow testing in a live environment before full rollout. -&gt;&nbsp;Correct. A blue-green deployment model using Kubernetes Engine effectively supports CI/CD by enabling two identical environments where one (green) can serve as a live testing environment before switching traffic to it (turning it blue). This approach allows for immediate rollbacks if issues arise and supports high availability and scalability.</p><p><br></p><p>Use Cloud Build to automate deployments, employing Cloud Functions for lightweight processing tasks that scale automatically. -&gt; Incorrect. While Cloud Build and Cloud Functions are useful tools, this strategy may not fully address the application's need for managing significant traffic spikes, as Cloud Functions are primarily used for event-driven and asynchronous tasks.</p><p><br></p><p>Manually deploy updated versions to App Engine standard environment during off-peak hours to minimize disruption. -&gt; Incorrect. Manual deployments during off-peak hours do not leverage the benefits of CI/CD for continuous updates and automation, making this approach less suitable for dynamic scaling and frequent updates.</p><p><br></p><p>Implement a rolling update strategy using Compute Engine managed instance groups to ensure zero downtime during deployments. -&gt; Incorrect. Rolling updates with Compute Engine managed instance groups provide seamless updates but might not be the most efficient way to handle unpredictable, high-traffic spikes as it's primarily VM-based and not as agile as containerized solutions.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297642,
    "question_plain": "You are a cloud architect at a software company that has an application deployed on Google Cloud. The application has been experiencing performance issues both in the testing and production environments. The DevOps team is unsure if the problem is due to the application's code or the underlying infrastructure. You've been asked to identify an efficient way to isolate and diagnose these performance issues. Which approach would you suggest?",
    "answers": [
      "<p>Use Cloud Profiler to identify performance bottlenecks in the code, while also leveraging Cloud Monitoring and Logging to analyze infrastructure performance.</p>",
      "<p>Create a detailed log for every function call in the application code to identify any bottlenecks.</p>",
      "<p>Upgrade the machine types of all Compute Engine instances in the project to increase performance.</p>",
      "<p>Use only Cloud Monitoring to analyze both the code and the infrastructure performance.</p>"
    ],
    "explanation": "<p>Use Cloud Profiler to identify performance bottlenecks in the code, while also leveraging Cloud Monitoring and Logging to analyze infrastructure performance. -&gt;&nbsp;Correct. Cloud Profiler allows you to analyze how your application's code runs in production and can help you identify performance bottlenecks in the code. Cloud Monitoring and Logging can be used in parallel to analyze infrastructure metrics and logs, allowing you to determine if the infrastructure is affecting performance. This combination provides a comprehensive solution for diagnosing performance issues.</p><p><br></p><p>Create a detailed log for every function call in the application code to identify any bottlenecks. -&gt; Incorrect. While logging function calls can be useful for debugging, it is not efficient for identifying performance issues as it may significantly impact the application's performance and create a large volume of logs.</p><p><br></p><p>Upgrade the machine types of all Compute Engine instances in the project to increase performance. -&gt; Incorrect. Upgrading machine types without first identifying the cause of the performance issues may result in unnecessary costs and may not solve the problem if it is not related to the infrastructure.</p><p><br></p><p>Use only Cloud Monitoring to analyze both the code and the infrastructure performance. -&gt; Incorrect. While Cloud Monitoring is a powerful tool for analyzing infrastructure performance, it does not provide insights into how the application's code runs. It is therefore not sufficient on its own for diagnosing the reported performance issues.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297644,
    "question_plain": "As a cloud architect, you have observed that a few API requests in your microservices application experience significant delays. You are aware that each API request may pass through multiple services. To identify the specific service causing the longest delays in such cases, what course of action should you pursue?",
    "answers": [
      "<p>Instrument your application with Cloud Trace in order to break down the request latencies at each microservice.</p>",
      "<p>Use Cloud Monitoring to look for insights that show when your API latencies are high.</p>",
      "<p>Send custom metrics for each of your requests to Cloud Monitoring.</p>",
      "<p>Set timeouts on your application so that you can fail requests faster.</p>"
    ],
    "explanation": "<p>Instrument your application with Cloud Trace in order to break down the request latencies at each microservice. -&gt; Correct. Instrumenting the application with Cloud Trace enables the tracking of the entire request journey, including each service involved in processing the request. Cloud Trace provides detailed information about request latency and service performance to help isolate the bottleneck and identify the service(s) causing the issue. </p><p><br></p><p>Use Cloud Monitoring to look for insights that show when your API latencies are high. -&gt; Incorrect. Cloud Monitoring could help to monitor the overall performance of the application and track specific metrics, but they may not provide the same level of detail as Cloud Trace. </p><p><br></p><p>Send custom metrics for each of your requests to Cloud Monitoring. -&gt; Incorrect. Cloud Monitoring and custom metrics could help to monitor the overall performance of the application and track specific metrics, but they may not provide the same level of detail as Cloud Trace. </p><p><br></p><p>Set timeouts on your application so that you can fail requests faster. -&gt; Incorrect. Setting timeouts on the application may help to fail requests faster, but it doesn't help to identify the root cause of the latency issue.</p><p><br></p><p>https://cloud.google.com/trace/docs/overview</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297646,
    "question_plain": "As a cloud architect, you are tasked with designing a solution to backup an on-premises PostgreSQL database to Google Cloud Platform. The objective is to create a replica of the on-premises database on GCP for backup purposes, so the data is easily recoverable and accessible in case of an on-premises failure. Which method would be the most efficient way to accomplish this?",
    "answers": [
      "<p>Use Cloud SQL for PostgreSQL and setup Cloud SQL external server replication</p>",
      "<p>Use Cloud Dataflow to stream data from PostgreSQL to BigQuery</p>",
      "<p>Use Cloud Spanner to replicate the PostgreSQL database</p>",
      "<p>Use Google Cloud Storage to store PostgreSQL dump files</p>"
    ],
    "explanation": "<p>Use Cloud SQL for PostgreSQL and setup Cloud SQL external server replication -&gt;&nbsp;Correct. Cloud SQL for PostgreSQL supports the replication of an external PostgreSQL server. This allows you to create a replica of an on-premises PostgreSQL database on Google Cloud, meeting the requirement of this scenario.</p><p><br></p><p>Use Cloud Dataflow to stream data from PostgreSQL to BigQuery -&gt; Incorrect. Cloud Dataflow is primarily used for processing and transforming large data sets, not for creating a replica of a PostgreSQL database. Streaming data to BigQuery wouldn't maintain the relational structure of the PostgreSQL database and isn't suitable for real-time backup.</p><p><br></p><p>Use Cloud Spanner to replicate the PostgreSQL database -&gt;&nbsp;Incorrect. Cloud Spanner is a highly scalable, multi-region database service, but it doesn't support direct replication from an on-premises PostgreSQL database.</p><p><br></p><p>Use Google Cloud Storage to store PostgreSQL dump files -&gt;&nbsp;Incorrect. Storing PostgreSQL dump files in Google Cloud Storage could serve as a form of backup, but it wouldn't create a readily accessible and live replica of the database.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297648,
    "question_plain": "After creating multiple preemptible Linux VM instances through Google Compute Engine, your objective is to ensure the appropriate shutdown of the application prior to the VMs being preempted. What actions are recommended in this situation?",
    "answers": [
      "<p>You should create a shutdown script and use it as the value for a new metadata entry with the key shutdown-script in the Cloud Platform Console when you create the new virtual machine instance.</p>",
      "<p>You should create a shutdown script, registered as a xinetd service in Linux, and use the gcloud compute instances add-metadata command to specify the service URL as the value for a new metadata entry with the key shutdown-script-url.</p>",
      "<p>Create a shutdown script registered as a xinetd service in Linux and configure an endpoint check to call the service.</p>",
      "<p>Create a shutdown script in the <code>/etc/rc.6.d/</code> directory.</p>"
    ],
    "explanation": "<p>You should create a shutdown script and use it as the value for a new metadata entry with the key shutdown-script in the Cloud Platform Console when you create the new virtual machine instance. -&gt; Correct. This option is the best fit for this scenario, as it allows you to specify a script that will be executed before the VM is terminated. You can also use this option to ensure that the script is executed in a timely manner and to make sure that it is executed consistently across all instances.</p><p><br></p><p>You should create a shutdown script, registered as a xinetd service in Linux, and use the gcloud compute instances add-metadata command to specify the service URL as the value for a new metadata entry with the key shutdown-script-url. -&gt; Incorrect. This option may not be the best fit for shutting down the application before the VMs are preempted as it may not be reliable enough to guarantee that the script will be executed before the VM is terminated.</p><p><br></p><p>Create a shutdown script registered as a xinetd service in Linux and configure an endpoint check to call the service. -&gt; Incorrect. While this option may work in some scenarios, it can be complex to set up and may not be the most reliable solution for shutting down the application before the VMs are preempted.</p><p><br></p><p>Create a shutdown script in the <code>/etc/rc.6.d/</code> directory. -&gt; Incorrect. While this option may work, it can be unreliable as the script may not be executed in time before the VM is terminated.</p><p><br></p><p>https://cloud.google.com/compute/docs/instances/startup-scripts</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297652,
    "question_plain": "As a cloud architect, you've been tasked with setting up an e-commerce application on Google Cloud Platform for a multinational company. The application will handle credit card transactions and customer data, so security is of utmost importance. The application consists of various components running on Compute Engine, Cloud Storage, and Cloud SQL. What should be your primary focus in terms of security?",
    "answers": [
      "<p>Use VPC Service Controls to establish a security perimeter around sensitive resources.</p>",
      "<p>Utilize Cloud Armor to protect the application against DDoS attacks.</p>",
      "<p>Apply IAM roles and policies at the organization level to manage resource access.</p>",
      "<p>Enable Secure Boot on all Compute Engine instances to ensure the integrity of the boot process.</p>"
    ],
    "explanation": "<p>Use VPC Service Controls to establish a security perimeter around sensitive resources. -&gt;&nbsp;Correct. VPC Service Controls allows you to define a security perimeter around Google Cloud resources to mitigate data exfiltration risks, which is extremely important when handling sensitive data such as credit card transactions and customer data.</p><p><br></p><p>Utilize Cloud Armor to protect the application against DDoS attacks. -&gt; Incorrect. While using Cloud Armor can protect the application against DDoS attacks, it's only a part of a comprehensive security strategy and it's not the primary focus for securing an application that handles sensitive data across various GCP services.</p><p><br></p><p>Apply IAM roles and policies at the organization level to manage resource access. -&gt; Incorrect. Applying IAM roles and policies at the organization level is good practice, but it does not provide a security perimeter around sensitive data, which is critical when handling credit card transactions and customer data.</p><p><br></p><p>Enable Secure Boot on all Compute Engine instances to ensure the integrity of the boot process. -&gt; Incorrect. Secure Boot is a feature that ensures the integrity of the boot process to protect against threats such as rootkits and bootkits. Although it's a good security practice, it is not the primary focus in this scenario where securing sensitive data across various GCP services is the key.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297654,
    "question_plain": "When considering strong security during the operation of fully autonomous vehicles within your agricultural division, there are two key architecture characteristics that should be taken into account. Which two characteristics should you prioritize in your architecture design?",
    "answers": [
      "<p>Treat every microservice call between modules on the vehicle as untrusted.</p>",
      "<p>Use a Trusted Platform Module (TPM) and verify firmware and binaries on boot.</p>",
      "<p>Use multiple connectivity subsystems for redundancy.</p>",
      "<p>Require IPv6 for connectivity to ensure a secure address space.</p>",
      "<p>Use a functional programming language to isolate code execution cycles.</p>"
    ],
    "explanation": "<p>Treat every microservice call between modules on the vehicle as untrusted. -&gt; Correct. This improves system security by making it more resistant to hacking, especially through man-in-the-middle attacks between modules.</p><p><br></p><p>Use a Trusted Platform Module (TPM) and verify firmware and binaries on boot. -&gt; Correct. This improves system security by making it more resistant to hacking, especially rootkits or other kinds of corruption by malicious actors.</p><p><br></p><p>Use multiple connectivity subsystems for redundancy. -&gt; Incorrect. This improves system durability, but it doesn't have any impact on the security during vehicle operation.</p><p><br></p><p>Require IPv6 for connectivity to ensure a secure address space. -&gt; Incorrect. IPv6 doesn't have any impact on the security during vehicle operation, although it improves system scalability and simplicity.</p><p><br></p><p>Use a functional programming language to isolate code execution cycles. -&gt;&nbsp;Incorrect. Merely using a functional programming language doesn't guarantee a more secure level of execution isolation. Any impact on security from this decision would be incidental at best.</p>",
    "correct_response": ["a", "b"],
    "assessment_type": "multi-select",
    "related_lectures": []
  },
  {
    "id": 81297656,
    "question_plain": "Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdfMountkirk Games is seeking to develop a real-time analytics platform for their upcoming game while ensuring that all their technical requirements are met. Which combination of Google technologies would satisfy these requirements?",
    "answers": [
      "<p>Cloud Dataflow, Cloud Storage, Cloud Pub/Sub, and BigQuery</p>",
      "<p>Kubernetes Engine, Cloud Pub/Sub, and Cloud SQL</p>",
      "<p>Cloud SQL, Cloud Storage, Cloud Pub/Sub, and Cloud Dataflow</p>",
      "<p>Cloud Pub/Sub, Compute Engine, Cloud Storage, and Cloud Dataproc</p>"
    ],
    "explanation": "<p>Cloud Dataflow, Cloud Storage, Cloud Pub/Sub, and BigQuery -&gt; Correct. Cloud Dataflow dynamically scales up or down, can process data in real time, and is ideal for processing data that arrives late using Beam windows and triggers. Cloud Storage can be the landing space for files that are regularly uploaded by users’ mobile devices. Cloud Pub/Sub can ingest the streaming data from the mobile users. BigQuery can query more than 10 TB of historical data.</p><p><br></p><p>Kubernetes Engine, Cloud Pub/Sub, and Cloud SQL -&gt; Incorrect. Cloud SQL is the only storage listed, is limited to 10 TB of storage, and is better suited for transactional workloads. Mountkirk Games needs queries to access at least 10 TB of historical data for analytic purposes.</p><p><br></p><p>Cloud SQL, Cloud Storage, Cloud Pub/Sub, and Cloud Dataflow -&gt; Incorrect. Cloud SQL is the only storage listed, is limited to 10TB of storage, and is better suited for transactional workloads. Mountkirk Games needs queries to access at least 10 TB of historical data for analytic purposes.</p><p><br></p><p>Cloud Pub/Sub, Compute Engine, Cloud Storage, and Cloud Dataproc -&gt; Incorrect. Mountkirk Games needs the ability to query historical data. While this might be possible using workarounds, such as BigQuery federated queries for Cloud Storage or Hive queries for Cloud Dataproc, these approaches are more complex. BigQuery is a simpler and more flexible product that fulfills those requirements.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297658,
    "question_plain": "As a cloud architect, you are responsible for setting up a continuous deployment pipeline for a project hosted in a Git source repository. Your objective is to guarantee that code modifications can be validated prior to being deployed to the production environment. What steps should you take to achieve this?",
    "answers": [
      "<p>Use Jenkins to monitor tags in the repository. Deploy staging tags to a staging environment for testing. After testing, tag the repository for production and deploy that to the production environment.</p>",
      "<p>Use Spinnaker to deploy builds to production using the red/black deployment strategy so that changes can easily be rolled back.</p>",
      "<p>Use Spinnaker to deploy builds to production and run tests on production deployments.</p>",
      "<p>Use Jenkins to build the staging branches and the master branch. Build and deploy changes to production for 10% of users before doing a complete rollout.</p>"
    ],
    "explanation": "<p>Use Jenkins to monitor tags in the repository. Deploy staging tags to a staging environment for testing. After testing, tag the repository for production and deploy that to the production environment. -&gt; Correct. It suggests using Jenkins to monitor tags in the Git repository. When code changes are made, the developer creates a tag and pushes it to the Git repository. Jenkins will monitor these tags and deploy the staging tags to a staging environment for testing. Once the code has been tested and verified, the developer will tag the repository for production and deploy it to the production environment. This process ensures that code changes are verified before being deployed to production, reducing the risk of errors.</p><p><br></p><p>Use Spinnaker to deploy builds to production using the red/black deployment strategy so that changes can easily be rolled back. -&gt; Incorrect. While this strategy is useful for rolling back changes, it does not address the need for testing code changes before deploying them to production.</p><p><br></p><p>Use Spinnaker to deploy builds to production and run tests on production deployments. -&gt;&nbsp;Incorrect. This approach is risky because any issues that arise during testing could affect production users.</p><p><br></p><p>Use Jenkins to build the staging branches and the master branch. Build and deploy changes to production for 10% of users before doing a complete rollout. -&gt; Incorrect. While this approach is useful for gradually rolling out changes, it does not address the need for testing code changes before deploying them to production.</p><p><br></p><p>https://cloud.google.com/architecture/continuous-delivery-jenkins-kubernetes-engine</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297660,
    "question_plain": "Your organization's application is deployed on App Engine. Users have started reporting that some transactions are failing, and the rate of failure seems to be increasing. You need to identify the root cause and mitigate the problem. Which of the following steps is the most efficient way to proceed?",
    "answers": [
      "<p>Use Cloud Monitoring and Logging to identify the source of the problem.</p>",
      "<p>Change the runtime environment of the application and observe if the issue persists.</p>",
      "<p>Migrate the application to Google Kubernetes Engine (GKE) to better handle the load.</p>",
      "<p>Utilize Cloud Pub/Sub to decouple and distribute the transactions across various services.</p>"
    ],
    "explanation": "<p>Use Cloud Monitoring and Logging to identify the source of the problem. -&gt; Correct. Cloud Monitoring and Logging can help you identify issues with your application by monitoring its metrics and logs. This approach allows you to understand what is happening with your application and troubleshoot effectively.</p><p><br></p><p>Change the runtime environment of the application and observe if the issue persists. -&gt;&nbsp;Incorrect. Changing the runtime environment is not a recommended immediate action for troubleshooting. It could lead to additional complications without necessarily addressing the root cause of the problem.</p><p><br></p><p>Migrate the application to Google Kubernetes Engine (GKE) to better handle the load. -&gt; Incorrect. Migrating to GKE is a significant operation and should not be considered as an immediate action for troubleshooting an application issue. Also, moving to GKE doesn't ensure the problem will be resolved, especially if the problem lies within the application's code or configuration.</p><p><br></p><p>Utilize Cloud Pub/Sub to decouple and distribute the transactions across various services. -&gt; Incorrect. Cloud Pub/Sub might help distribute workload across different services, but it doesn't directly address the issue at hand. Furthermore, integrating Pub/Sub into the application requires significant changes to the application's code and is not suitable as an immediate troubleshooting step.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297662,
    "question_plain": "Your company has chosen to adopt Kubernetes for managing its containerized application deployments. As a cloud architect, you have been tasked with creating a Kubernetes cluster using gcloud command-line tool in a way that allows you to perform rolling updates without downtime, supports automatic scaling, and ensures data persistency for stateful apps. Which of the following gcloud commands would be most appropriate for this task?",
    "answers": [
      "<p><code>gcloud container clusters create my-cluster --zone=us-central1-a --num-nodes=3 --enable-autoscaling --min-nodes=1 --max-nodes=5 --scopes=gke-default</code> </p>",
      "<p><code>gcloud container clusters create my-cluster --region=us-central1 --num-nodes=3 --scopes=cloud-platform</code> </p>",
      "<p><code>gcloud container clusters create my-cluster --region=us-central1 --num-nodes=3 --scopes=cloud-platform --enable-autoscaling --min-nodes=1 --max-nodes=5</code> </p>",
      "<p><code>gcloud container clusters create my-cluster --zone=us-central1-a --num-nodes=3 --scopes=gke-default --enable-autoscaling --min-nodes=1 --max-nodes=5 --enable-cloud-logging</code> </p>"
    ],
    "explanation": "<p><code>gcloud container clusters create my-cluster --zone=us-central1-a --num-nodes=3 --enable-autoscaling --min-nodes=1 --max-nodes=5 --scopes=gke-default</code> -&gt; Correct. It creates a cluster, enables autoscaling and assigns the correct scope for a GKE cluster. Autoscaling allows the cluster to automatically adjust the number of nodes as workload demand changes.</p><p><br></p><p><code>gcloud container clusters create my-cluster --region=us-central1 --num-nodes=3 --scopes=cloud-platform</code>&nbsp; -&gt; Incorrect. This command creates a cluster but does not enable autoscaling which is a requirement. The scopes flag is too broad, granting unnecessary permissions.</p><p><br></p><p><code>gcloud container clusters create my-cluster --region=us-central1 --num-nodes=3 --scopes=cloud-platform --enable-autoscaling --min-nodes=1 --max-nodes=5</code>&nbsp; -&gt; Incorrect. While this command enables autoscaling, it uses a broader scope than necessary and also creates the cluster in a region instead of a zone which might not be optimal for the requirements.</p><p><br></p><p><code>gcloud container clusters create my-cluster --zone=us-central1-a --num-nodes=3 --scopes=gke-default --enable-autoscaling --min-nodes=1 --max-nodes=5 --enable-cloud-logging</code>&nbsp; -&gt; Incorrect. While this command enables autoscaling and assigns the correct scope, it enables cloud logging which is not a requirement and may unnecessarily increase costs.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297664,
    "question_plain": "Your team is developing a high-performance computing application that specifically needs to run on a Debian Linux environment. The application is compute-intensive and processes a large amount of data. Given the need for compute resources to be scaled up and down in response to the changing volume of data, as a cloud architect, what deployment strategy would you suggest on Google Cloud?",
    "answers": [
      "<p>Use Compute Engine with Debian Linux images and configure them in a Managed Instance Group.</p>",
      "<p>Deploy the application on App Engine standard environment.</p>",
      "<p>Deploy the application on Google Kubernetes Engine with Debian containers.</p>",
      "<p>Use Cloud Functions with a custom runtime to mimic the Debian Linux environment.</p>"
    ],
    "explanation": "<p>Use Compute Engine with Debian Linux images and configure them in a Managed Instance Group. -&gt; Correct. Compute Engine allows you to create instances with custom images including Debian Linux. By configuring the instances in a Managed Instance Group, the compute resources can be automatically scaled up and down in response to load, making this the best choice for this scenario.</p><p><br></p><p>Deploy the application on App Engine standard environment. -&gt; Incorrect. App Engine standard environment only supports specific runtime environments and Debian Linux is not one of them. Hence, this is not a suitable choice.</p><p><br></p><p>Deploy the application on Google Kubernetes Engine with Debian containers. -&gt; Incorrect. While Google Kubernetes Engine (GKE) allows deploying applications in containers, setting up a Kubernetes cluster for a single application can be overkill and would add unnecessary complexity, especially if the application doesn’t specifically require containerization or Kubernetes features.</p><p><br></p><p>Use Cloud Functions with a custom runtime to mimic the Debian Linux environment. -&gt; Incorrect. Cloud Functions are designed for lightweight, single-purpose functions and are not suitable for high-performance computing applications. Furthermore, while you can specify a custom runtime, mimicking a full Debian Linux environment in a function is not feasible.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297666,
    "question_plain": "As a cloud architect, your company has asked you to architect and deploy a highly scalable web application using Google App Engine. The application will be used globally and should be able to handle large spikes in traffic. The application also needs to be updated frequently with zero downtime. Moreover, the company is very cost-conscious and wants to ensure that they are only billed for the compute resources they actually use. Which of the following App Engine environment and scaling type combinations would you recommend for this situation?",
    "answers": [
      "<p>App Engine Standard Environment with Automatic Scaling.</p>",
      "<p>App Engine Flexible Environment with Basic Scaling.</p>",
      "<p>App Engine Standard Environment with Manual Scaling.</p>",
      "<p>App Engine Flexible Environment with Automatic Scaling.</p>"
    ],
    "explanation": "<p>App Engine Standard Environment with Automatic Scaling. -&gt;&nbsp;Correct. The App Engine Standard Environment is designed to scale instances out when traffic increases, and to scale instances back when traffic decreases, thereby only charging for the actual resources used. It also allows seamless deployments with zero downtime, which makes it the best option in this scenario.</p><p><br></p><p>App Engine Flexible Environment with Basic Scaling. -&gt; Incorrect. The App Engine Flexible Environment is more appropriate for applications that require more customization and have specific runtime requirements. However, Basic Scaling may not handle large spikes in traffic efficiently and could lead to higher costs.</p><p><br></p><p>App Engine Standard Environment with Manual Scaling. -&gt; Incorrect. While the App Engine Standard Environment is a good choice for scalability and cost-efficiency, Manual Scaling doesn't meet the requirement of automatic scalability to handle large traffic spikes.</p><p><br></p><p>App Engine Flexible Environment with Automatic Scaling. -&gt; Incorrect. The App Engine Flexible Environment with Automatic Scaling might handle traffic spikes, but it could lead to higher costs as it's more appropriate for applications with specific runtime requirements and the resources aren't scaled down as efficiently when not in use.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297668,
    "question_plain": "You are leading the team responsible for managing an application hosted on Cloud Run. You are planning to release a new version of the application. To ensure minimal disruption and maintain high availability, you need to define a strategy for deploying the new version. Which of the following would be the most appropriate strategy for this scenario?",
    "answers": [
      "<p>Perform a canary deployment, gradually directing a small percentage of traffic to the new version while monitoring for issues.</p>",
      "<p>Perform a blue-green deployment, keeping the current version (blue) running while the new version (green) is fully deployed and tested. Once the new version is validated, traffic is redirected to it.</p>",
      "<p>Deploy the new version directly to production, testing it in the live environment and rolling it back if issues are found.</p>",
      "<p>Shut down the application, deploy the new version, perform extensive testing, and then bring the application back online.</p>"
    ],
    "explanation": "<p>Perform a canary deployment, gradually directing a small percentage of traffic to the new version while monitoring for issues. -&gt; Correct. Canary deployments are a best practice in software development. They allow a small percentage of traffic to be directed to the new version, while the majority of traffic continues to interact with the stable version. This provides real-world testing of the new version and allows for problems to be detected and corrected without affecting the entire user base.</p><p><br></p><p>Perform a blue-green deployment, keeping the current version (blue) running while the new version (green) is fully deployed and tested. Once the new version is validated, traffic is redirected to it. -&gt; Incorrect. While blue-green deployments can help minimize downtime, Cloud Run does not natively support this type of deployment. However, you can implement it using a combination of services (like Traffic Splitting), but it's more complex and may not be necessary.</p><p><br></p><p>Deploy the new version directly to production, testing it in the live environment and rolling it back if issues are found. -&gt; Incorrect. Directly deploying to production without testing can lead to unforeseen issues affecting all users. This approach should be avoided whenever possible.</p><p><br></p><p>Shut down the application, deploy the new version, perform extensive testing, and then bring the application back online. -&gt; Incorrect. Shutting down the application entirely for an update is not advisable, especially for mission-critical applications. This could result in downtime and poor user experience.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297670,
    "question_plain": "You are a cloud architect working for a global enterprise that runs an e-commerce application. As part of the design, the application layer is hosted on Compute Engine and needs to be scalable to handle potential spikes in traffic during high-usage times. In this scenario, you decided to implement a managed instance group (MIG) for automated scaling. You need to create a process to automate the creation of the managed instance group, ensure it scales based on load, is distributed across multiple zones for high availability, and uses predefined instance templates for uniformity. What should you do?",
    "answers": [
      "<p>Use Terraform to create a regional managed instance group using an instance template. Configure autoscaling based on Cloud Monitoring metrics.</p>",
      "<p>Use Deployment Manager to create and manage a zonal managed instance group. Configure autoscaling based on HTTP load balancing.</p>",
      "<p>Use the Cloud SDK to create a zonal managed instance group with a template, and then manually add instances when needed.</p>",
      "<p>Use Cloud Functions to create an instance template and a zonal managed instance group. Configure autoscaling based on Cloud Monitoring metrics. </p>"
    ],
    "explanation": "<p>Use Terraform to create a regional managed instance group using an instance template. Configure autoscaling based on Cloud Monitoring metrics. -&gt; Correct. Terraform can be used to automate the creation of resources and manage infrastructure as code. A regional managed instance group provides the required high availability across multiple zones. Configuring autoscaling based on Cloud Monitoring metrics will allow the MIG to scale based on application-specific requirements.</p><p><br></p><p>Use Deployment Manager to create and manage a zonal managed instance group. Configure autoscaling based on HTTP load balancing. -&gt; Incorrect. While Deployment Manager can be used to automate the creation of resources, the use of a zonal managed instance group doesn't provide the high availability required across multiple zones.</p><p><br></p><p>Use the Cloud SDK to create a zonal managed instance group with a template, and then manually add instances when needed. -&gt; Incorrect. Manually adding instances when needed contradicts the need for automation and auto-scaling based on load.</p><p><br></p><p>Use Cloud Functions to create an instance template and a zonal managed instance group. Configure autoscaling based on Cloud Monitoring metrics. -&gt; Incorrect. While Cloud Functions could theoretically be used, they are serverless execution environments for building and connecting cloud services, and are not generally used for this kind of infrastructure management task.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297672,
    "question_plain": "As a cloud architect, you have been assigned the task of setting up a Compute Engine application in a single Virtual Private Cloud (VPC) spanning across two regions for a global e-commerce company. The objective is to ensure high availability and seamless connectivity between instances in these two regions, while also keeping latency and cost to a minimum. Which approach would be the most effective to meet these requirements?",
    "answers": [
      "<p>Create a single VPC and deploy two regional subnets with custom dynamic routing.</p>",
      "<p>Create two separate VPCs, one for each region, and connect them using Cloud VPN.</p>",
      "<p>Create a single VPC and deploy two unconnected regional subnets.</p>",
      "<p>Use shared VPC to connect the two regions.</p>"
    ],
    "explanation": "<p>Create a single VPC and deploy two regional subnets with custom dynamic routing. -&gt; Correct. This is the best solution as it would ensure seamless connectivity between instances in different regions within the same VPC. Custom dynamic routing allows traffic to be efficiently routed within the VPC, minimizing latency and cost.</p><p><br></p><p>Create two separate VPCs, one for each region, and connect them using Cloud VPN. -&gt; Incorrect. Cloud VPN allows secure connections between your on-premises network and your VPCs, or between two VPCs, but it introduces additional complexity and cost, which may not be necessary if instances are within the same VPC.</p><p><br></p><p>Create a single VPC and deploy two unconnected regional subnets. -&gt; Incorrect. This would allow the instances to operate in the same VPC, but without proper routing, there wouldn't be seamless connectivity between the two subnets, which doesn't meet the requirements.</p><p><br></p><p>Use shared VPC to connect the two regions. -&gt; Incorrect. Shared VPC can be useful when you want to keep resources isolated in different projects, but it doesn't inherently ensure seamless connectivity or minimize latency and cost across regions.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297650,
    "question_plain": "Your company is structuring their Google Cloud Platform (GCP) resources. The company consists of three distinct departments: Finance, Marketing, and Operations. Each department should only be able to access and manage their own resources. As a cloud architect, what is the most appropriate way to design the GCP resource hierarchy?",
    "answers": [
      "<p>Create one organization, and within it create a folder for each department. In each folder, create projects for different needs of the department.</p>",
      "<p>Create one project for the whole company and use IAM policies to restrict access for each department.</p>",
      "<p>Create three different organizations for each department.</p>",
      "<p>Do not create an organization. Instead, create separate projects for each department.</p>"
    ],
    "explanation": "<p>Create one organization, and within it create a folder for each department. In each folder, create projects for different needs of the department. -&gt;&nbsp;Correct. This approach gives a clear separation of resources, makes it easier to manage IAM roles and policies, and aligns with best practices for designing resource hierarchies in GCP. It provides flexibility and control at the right levels.</p><p><br></p><p>Create one project for the whole company and use IAM policies to restrict access for each department. -&gt; Incorrect. This approach can theoretically work with finely tuned IAM policies, but it can become complex and hard to manage as the organization grows. It's better to segregate resources on the project level to maintain clear separation between departments.</p><p><br></p><p>Create three different organizations for each department. -&gt; Incorrect. Creating separate organizations for each department can result in unnecessary complexity and difficulty in managing company-wide policies and resources. Organizations should typically represent an entire company.</p><p><br></p><p>Do not create an organization. Instead, create separate projects for each department. -&gt; Incorrect. This approach would not take full advantage of GCP's resource hierarchy. An Organization node is the root node in the Google Cloud resource hierarchy and it provides central visibility and control over all of the GCP resources.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297674,
    "question_plain": "Your organization collects telemetry data from thousands of IoT devices. The data needs to be stored in a time-series database on Google Cloud for future analysis and predictions. The data volume is expected to grow exponentially over the next couple of years. Which Google Cloud service should you use to handle this situation?",
    "answers": [
      "<p>Cloud Bigtable</p>",
      "<p>Cloud Storage</p>",
      "<p>Firestore</p>",
      "<p>BigQuery</p>"
    ],
    "explanation": "<p>Cloud Bigtable -&gt;&nbsp;Correct. Cloud Bigtable is a great option for time-series data due to its design, which can handle massive workloads, up to billions of rows and thousands of columns, thus enabling the solution to scale with the growing data.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. Cloud Storage is an object storage service that's great for storing large amounts of unstructured data, but it doesn't provide the database-like querying capabilities typically needed with time-series data.</p><p><br></p><p>Firestore -&gt; Incorrect. Firestore is a NoSQL document database for mobile, web, and server development. It's not optimal for time-series data storage, particularly at a large scale.</p><p><br></p><p>BigQuery -&gt; Incorrect. BigQuery is a great service for analytical workloads, but it is not optimized for time-series data, which could lead to inefficient storage and querying of the growing data.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297676,
    "question_plain": "You are designing a multi-tier web application that is hosted on Google Cloud Platform (GCP). The web application has a high number of users, and you expect that the number of users will increase significantly in the near future. What GCP services would you use to ensure that the web application can scale dynamically to meet the changing demand?",
    "answers": [
      "<p>Google App Engine and Google Compute Engine</p>",
      "<p>Google Compute Engine and Google Cloud Load Balancing</p>",
      "<p>Google App Engine and Google Kubernetes Engine</p>",
      "<p>Google Kubernetes Engine and Google Cloud Load Balancing.</p>"
    ],
    "explanation": "<p>Google Kubernetes Engine and Google Cloud Load Balancing. -&gt;&nbsp;Correct. Google Kubernetes Engine (GKE) and Google Cloud Load Balancing are both designed to dynamically scale with changing demand. GKE allows you to manage and scale containerized applications, while Cloud Load Balancing provides intelligent load balancing and auto-scaling capabilities. This combination enables the web application to automatically scale based on incoming traffic and ensure high availability, even during periods of high demand. </p><p><br></p><p>Google App Engine and Google Compute Engine -&gt; Incorrect. It is incorrect because Google Compute Engine does not have automatic scaling capabilities. </p><p><br></p><p>Google Compute Engine and Google Cloud Load Balancing -&gt; Incorrect. It is incorrect because Cloud Load Balancing alone does not provide the necessary scalability features to automatically adjust resources based on traffic patterns. </p><p><br></p><p>Google App Engine and Google Kubernetes Engine -&gt; Incorrect. It is incorrect because while App Engine can scale dynamically, it may not be the best fit for a multi-tier web application, and Kubernetes provides more advanced scaling and orchestration features.</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview</p><p>https://cloud.google.com/load-balancing/docs</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297678,
    "question_plain": "A company is planning to deploy a multi-tier web application on Google Cloud Platform (GCP) that will be accessed by users globally. The application must be highly available, provide fast and reliable performance, and meet strict security and compliance requirements. Which of the following options would be the most effective approach to meet these requirements while also optimizing cost?",
    "answers": [
      "<p>Use Compute Engine instances in a global load-balanced network to host the web and application tiers, and store data in Cloud SQL.</p>",
      "<p>Use Kubernetes Engine to deploy and manage containers for the web and application tiers, and store data in Cloud Bigtable.</p>",
      "<p>Use App Engine Flexible Environment to host the web and application tiers, and store data in Cloud Datastore.</p>",
      "<p>Use Cloud Functions to host the web and application tiers, and store data in Cloud Firestore.</p>"
    ],
    "explanation": "<p>Use App Engine Flexible Environment to host the web and application tiers, and store data in Cloud Datastore. -&gt; Correct. App Engine Flexible Environment is a fully-managed platform for building scalable and highly available web applications. It automatically scales the resources based on demand and provides automatic load balancing across multiple regions, which makes it highly available. In addition, it meets strict security and compliance requirements as it provides features such as Identity and Access Management (IAM), audit logging, and encryption of data at rest and in transit. Cloud Datastore is a highly-scalable NoSQL document database that can store and serve structured data for the application, making it fast and reliable.</p><p><br></p><p>Use Compute Engine instances in a global load-balanced network to host the web and application tiers, and store data in Cloud SQL. -&gt; Incorrect. While Compute Engine instances in a global load-balanced network can also provide high availability and fast performance, they require more management effort and are generally more expensive than App Engine Flexible Environment. </p><p><br></p><p>Use Kubernetes Engine to deploy and manage containers for the web and application tiers, and store data in Cloud Bigtable. -&gt; Incorrect. Using Kubernetes Engine to deploy and manage containers for the web and application tiers would require more management effort and cost than App Engine Flexible Environment. Cloud Bigtable is also a highly-scalable NoSQL database, but it is better suited for use cases that require high throughput and low latency, such as time-series data processing or real-time analytics. </p><p><br></p><p>Use Cloud Functions to host the web and application tiers, and store data in Cloud Firestore. -&gt; Incorrect. Cloud Functions is a serverless computing platform, and while it can be used for building web applications, it is better suited for event-driven use cases, such as processing data or triggering actions based on certain events.</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297680,
    "question_plain": "A company is planning to deploy a highly scalable and secure cloud infrastructure on Google Cloud Platform (GCP) to support their growing cloud-based product offerings. The infrastructure must be able to handle sudden spikes in demand, provide fast and reliable performance, and meet strict security and compliance requirements. Which of the following options would be the most effective approach to meet these requirements while also optimizing cost?",
    "answers": [
      "<p>Use Compute Engine instances with custom firewall rules and encrypted disks to host the application, and use Cloud VPN to securely connect to on-premises resources.</p>",
      "<p>Use Google Kubernetes Engine with network security policies and encrypted secrets to host the application, and use Cloud Interconnect to securely connect to on-premises resources.</p>",
      "<p>Use App Engine Flexible Environment with custom firewall rules and encrypted environment variables to host the application, and use Cloud Armor to protect against network threats.</p>",
      "<p>Use Cloud Functions with encrypted secrets and environment variables to host the application, and use Cloud Load Balancer with SSL/TLS termination to provide secure access to the application.</p>"
    ],
    "explanation": "<p>Use Google Kubernetes Engine with network security policies and encrypted secrets to host the application, and use Cloud Interconnect to securely connect to on-premises resources. -&gt;&nbsp;Correct. The scenario described in the question involves deploying a highly scalable and secure cloud infrastructure on Google Cloud Platform that can handle sudden spikes in demand, provide fast and reliable performance, and meet strict security and compliance requirements while also optimizing cost. Google Kubernetes Engine (GKE) is a fully managed container orchestration system that provides automatic scaling, self-healing, and rolling updates. It can handle sudden spikes in demand and provide fast and reliable performance. GKE also integrates well with other GCP services, including Cloud Interconnect, which can be used to securely connect to on-premises resources.</p><p><br></p><p>Use Compute Engine instances with custom firewall rules and encrypted disks to host the application, and use Cloud VPN to securely connect to on-premises resources. -&gt; Incorrect. It is a viable approach for hosting applications, but it does not provide the same level of scalability and reliability as GKE. Cloud VPN is also a valid solution for securely connecting to on-premises resources, but Cloud Interconnect provides a more reliable and faster connection.</p><p><br></p><p>Use App Engine Flexible Environment with custom firewall rules and encrypted environment variables to host the application, and use Cloud Armor to protect against network threats. -&gt; Incorrect. It is a good solution for hosting web applications that require automatic scaling and self-healing. However, GKE provides more control over the underlying infrastructure, making it a better fit for highly scalable and secure cloud infrastructures.</p><p><br></p><p>Use Cloud Functions with encrypted secrets and environment variables to host the application, and use Cloud Load Balancer with SSL/TLS termination to provide secure access to the application. -&gt; Incorrect. It is a good solution for deploying event-driven applications that do not require a persistent server. However, it is not suitable for hosting web applications that require a persistent server and automatic scaling. Additionally, Cloud Load Balancer with SSL/TLS termination can provide secure access to the application, but it does not provide the same level of security and compliance as network security policies provided by GKE.</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297682,
    "question_plain": "A company is planning to deploy a real-time data processing solution on Google Cloud Platform (GCP) to process large amounts of incoming sensor data. The solution must be able to scale elastically to accommodate varying levels of incoming data, provide low latency processing, and ensure high data reliability. Additionally, the solution must be able to support complex data transformations and integrations with other data sources. Which of the following options would be the most effective approach to meet these requirements while also optimizing cost?",
    "answers": [
      "<p>Use Cloud Dataflow to process the data in a batch mode and store the results in BigQuery.</p>",
      "<p>Use Cloud Pub/Sub to ingest the data and feed it into Cloud Dataproc clusters for processing and storing the results in BigQuery.</p>",
      "<p>Use Cloud Pub/Sub to ingest the data, Cloud Functions to process the data in real-time, and Cloud Firestore to store the results.</p>",
      "<p>Use Apache Beam on Cloud Dataflow to process the data in real-time and store the results in BigQuery.</p>"
    ],
    "explanation": "<p>Use Apache Beam on Cloud Dataflow to process the data in real-time and store the results in BigQuery. -&gt;&nbsp;Correct. This option suits all requirements. Apache Beam on Cloud Dataflow allows for real-time processing and can automatically scale to accommodate varying data volumes. The processed data is stored in BigQuery, which supports complex queries and ensures high data reliability.</p><p><br></p><p>Use Cloud Dataflow to process the data in a batch mode and store the results in BigQuery. -&gt;&nbsp;Incorrect. This option doesn't satisfy the real-time data processing requirement. Although BigQuery is a good choice for storage and analysis, processing the data in batch mode wouldn't provide the low-latency processing needed.</p><p><br></p><p>Use Cloud Pub/Sub to ingest the data and feed it into Cloud Dataproc clusters for processing and storing the results in BigQuery. -&gt;&nbsp;Incorrect. This approach could potentially work, but managing Cloud Dataproc clusters could be more complex and costlier due to the need for explicit scaling, compared to serverless solutions like Dataflow.</p><p><br></p><p>Use Cloud Pub/Sub to ingest the data, Cloud Functions to process the data in real-time, and Cloud Firestore to store the results. -&gt;&nbsp;Incorrect. While Cloud Pub/Sub can effectively ingest real-time data and Cloud Functions can process it, this architecture may not be the most effective or cost-optimized for large amounts of incoming sensor data because Cloud Functions are priced per invocation and it may become costly with a large amount of data. Cloud Firestore, while a powerful NoSQL database, is not specifically designed for analytics or for handling very large amounts of data, and the costs can add up quickly for high-volume read and write operations.</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297684,
    "question_plain": "A multinational corporation has a large number of remote employees working in different countries. They need to provide secure access to company resources, such as Google Workspace and GCP resources, to these employees. The solution should also meet the following requirements:provide a secure and scalable solution that can handle a large number of remote usersenforce strong authentication and authorization policiesautomatically provide employees with the appropriate level of access to company resources based on their job functionprovide centralized management and auditing of user accessbe cost-effectiveWhich solution would you recommend to meet these requirements and why?",
    "answers": [
      "<p>Implementing Google Workspace Domain-Wide Delegation of Authority and Google Cloud Identity-Aware Proxy (IAP)</p>",
      "<p>Implementing Google Workspace Domain-Wide Delegation of Authority and Google Cloud VPN</p>",
      "<p>Implementing Google Workspace Groups and Google Cloud IAM roles</p>",
      "<p>Implementing Google Workspace Single Sign-On (SSO) and Google Cloud IAM roles</p>"
    ],
    "explanation": "<p>Implementing Google Workspace Domain-Wide Delegation of Authority and Google Cloud Identity-Aware Proxy (IAP) -&gt; Correct. Google Workspace Domain-Wide Delegation of Authority allows service accounts to impersonate users in a Google Workspace domain. This enables the service accounts to perform actions on behalf of users, thus aligning with the requirement of automatic provisioning based on job function. Coupled with Google Cloud Identity-Aware Proxy (IAP), which controls access to cloud applications running on Google Cloud Platform by verifying user identity and the context of the request, this combination provides a robust solution for secure, scalable access, centralized management, and auditing of user access.</p><p><br></p><p>Implementing Google Workspace Domain-Wide Delegation of Authority and Google Cloud VPN -&gt; Incorrect. Google Cloud VPN creates an encrypted IPsec VPN connection for securely connecting remote users to the GCP resources. However, it does not provide fine-grained control over user access to resources based on job function as required, nor does it handle authentication to Google Workspace.</p><p><br></p><p>Implementing Google Workspace Groups and Google Cloud IAM roles -&gt; Incorrect. Google Workspace Groups and Google Cloud IAM roles could be used to manage access to resources, but these tools alone don't provide a complete solution for enforcing strong authentication and authorization policies, and they are not designed to handle a large number of remote users.</p><p><br></p><p>Implementing Google Workspace Single Sign-On (SSO) and Google Cloud IAM roles -&gt; Incorrect. Google Workspace Single Sign-On (SSO) and Google Cloud IAM roles provide a means of access control, but the SSO itself isn't specifically tailored to provide access based on job function or handle the large scale of remote users. Also, IAM roles alone don't handle authentication or context-aware access control.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297686,
    "question_plain": "You are tasked with deploying a global e-commerce application on Google Cloud. To ensure optimal latency and high availability, you decided to use the Global HTTP(S) Load Balancer. The application needs to route incoming requests to the nearest healthy backend that has sufficient capacity to handle the load. How should you configure the load balancer?",
    "answers": [
      "<p>Configure Global HTTP(S) Load Balancer with backend services in multiple regions and use a balancing mode based on the capacity of the backends.</p>",
      "<p>Configure Global HTTP(S) Load Balancer with backend services in a single region and enable Cloud CDN.</p>",
      "<p>Configure Global HTTP(S) Load Balancer with instance groups in different regions and enable session affinity.</p>",
      "<p>Configure Global HTTP(S) Load Balancer with a single instance group and enable Cross-Region load balancing.</p>"
    ],
    "explanation": "<p>Configure Global HTTP(S) Load Balancer with backend services in multiple regions and use a balancing mode based on the capacity of the backends. -&gt;&nbsp;Correct. This is the correct answer as setting up backend services in multiple regions allows for proximity-based routing. A balancing mode based on capacity ensures traffic is sent to backends that are healthy and have sufficient capacity.</p><p><br></p><p>Configure Global HTTP(S) Load Balancer with backend services in a single region and enable Cloud CDN. -&gt; Incorrect. While Cloud CDN can improve latency and reduce load on your backends for cacheable content, it does not fulfill the requirement of routing to the nearest healthy backend with sufficient capacity.</p><p><br></p><p>Configure Global HTTP(S) Load Balancer with instance groups in different regions and enable session affinity. -&gt; Incorrect. Session affinity is typically used to route all requests from a particular client to the same backend for the duration of a session. It does not inherently route to the nearest backend or account for backend capacity.</p><p><br></p><p>Configure Global HTTP(S) Load Balancer with a single instance group and enable Cross-Region load balancing. -&gt; Incorrect. While cross-region load balancing can provide high availability, it does not inherently provide proximity-based routing or account for the capacity of backends.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297688,
    "question_plain": "Your organization is planning to design a cloud solution architecture for a new application that will have significant variations in demand and should be cost-effective. As a cloud architect, what strategy would you propose for this scenario?",
    "answers": [
      "<p>Deploy the application on App Engine standard environment.</p>",
      "<p>Use Compute Engine instances with fixed resources, manually adding or removing instances as demand fluctuates.</p>",
      "<p>Use preemptible VMs for all parts of the application.</p>",
      "<p>Use sole-tenant nodes for hosting the application.</p>"
    ],
    "explanation": "<p>Deploy the application on App Engine standard environment. -&gt;&nbsp;Correct. App Engine standard environment is a fully managed serverless platform that automatically scales your app up and down while balancing the load. This matches the requirements of significant variations in demand and cost-effectiveness.</p><p><br></p><p>Use Compute Engine instances with fixed resources, manually adding or removing instances as demand fluctuates. -&gt;&nbsp;Incorrect. This strategy does not provide automatic scaling in response to demand and would require significant overhead in monitoring and manual intervention.</p><p><br></p><p>Use preemptible VMs for all parts of the application. -&gt;&nbsp;Incorrect. While preemptible VMs can be cost-effective, they are not guaranteed to be available when needed, and they can be terminated at any time, making them unsuitable for a critical application that needs to be highly available.</p><p><br></p><p>Use sole-tenant nodes for hosting the application. -&gt;&nbsp;Incorrect. Sole-tenant nodes are physical Compute Engine servers dedicated to hosting VM instances for your specific project. While they provide isolation, they are not cost-effective for applications with fluctuating demand since you are paying for the entire node regardless of usage.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297690,
    "question_plain": "How would you design a highly available and scalable solution for storing and processing large amounts of time-series data in Google Cloud, taking into consideration the requirement to minimize latency for data retrieval and processing, and cost-effectiveness?",
    "answers": [
      "<p>Use Cloud Datastore as the primary storage, with occasional backups to Cloud Storage for data durability.</p>",
      "<p>Use Bigtable for the primary storage, with Cloud Pub/Sub for real-time data processing and analysis.</p>",
      "<p>Use Cloud Storage as the primary storage, with Cloud Dataflow for batch data processing and analysis.</p>",
      "<p>Use Cloud SQL for the primary storage, with Cloud Functions for real-time data processing and analysis.</p>"
    ],
    "explanation": "<p>Use Bigtable for the primary storage, with Cloud Pub/Sub for real-time data processing and analysis. -&gt;&nbsp;Correct. Bigtable is a highly scalable NoSQL database designed for storing and processing large amounts of time-series data with low latency. It provides automatic sharding and replication, ensuring high availability and durability. Cloud Pub/Sub allows for real-time data processing and analysis. This combination of Bigtable and Pub/Sub provides a highly available and scalable solution for storing and processing time-series data in Google Cloud while also minimizing latency and cost.</p><p><br></p><p>Use Cloud Datastore as the primary storage, with occasional backups to Cloud Storage for data durability. -&gt; Incorrect. It is not the best solution because Cloud Datastore is not optimized for time-series data and may not provide the low-latency required for real-time analysis.</p><p><br></p><p>Use Cloud Storage as the primary storage, with Cloud Dataflow for batch data processing and analysis. -&gt; Incorrect. It may work for batch data processing and analysis, but may not provide the low-latency required for real-time analysis of time-series data.</p><p><br></p><p>Use Cloud SQL for the primary storage, with Cloud Functions for real-time data processing and analysis. -&gt; Incorrect. It is not the best solution because Cloud SQL is not optimized for time-series data and may not provide the scalability required for storing and processing large amounts of time-series data.</p><p><br></p><p>https://cloud.google.com/bigtable/docs</p><p>https://cloud.google.com/bigtable/docs/schema-design-time-series</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297692,
    "question_plain": "How would you design a solution for securely storing and accessing sensitive data in Google Cloud, considering the requirement for encryption at rest, secure data transfer, and fine-grained access control?",
    "answers": [
      "<p>Use Cloud Storage with customer-supplied encryption keys and Cloud VPN for secure data transfer.</p>",
      "<p>Use Cloud Bigtable with customer-supplied encryption keys and Cloud Armor for network security.</p>",
      "<p>Use Cloud SQL with server-side encryption and Cloud Identity-Aware Proxy for fine-grained access control.</p>",
      "<p>Use Cloud Key Management Service for encryption key management, with Cloud Storage for data storage and Cloud IAM for access control.</p>"
    ],
    "explanation": "<p>Use Cloud Key Management Service for encryption key management, with Cloud Storage for data storage and Cloud IAM for access control. -&gt;&nbsp;Correct. It provides encryption at rest through Cloud Storage's default encryption, secure data transfer with Cloud Storage's built-in HTTPS support, and fine-grained access control through Cloud IAM's role-based access control. Additionally, using Cloud Key Management Service for encryption key management allows for better control and management of encryption keys. Overall, this option covers all the requirements for securely storing and accessing sensitive data in Google Cloud.</p><p><br></p><p>Use Cloud Storage with customer-supplied encryption keys and Cloud VPN for secure data transfer. -&gt; Incorrect. While this provides encryption at rest and secure data transfer, it doesn't address the need for fine-grained access control.</p><p><br></p><p>Use Cloud Bigtable with customer-supplied encryption keys and Cloud Armor for network security. -&gt; Incorrect. It provides encryption at rest and network security, but doesn't address the need for fine-grained access control.</p><p><br></p><p>Use Cloud SQL with server-side encryption and Cloud Identity-Aware Proxy for fine-grained access control. -&gt; Incorrect. While this provides encryption at rest and fine-grained access control, it doesn't address the need for secure data transfer.</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297694,
    "question_plain": "How would you design a solution for running a large-scale, highly-available data warehousing platform on Google Cloud, considering the requirement for fast query performance, cost-effectiveness, and the ability to handle petabyte-scale data?",
    "answers": [
      "<p>Use BigQuery for data warehousing and analysis, with Cloud Dataflow for data processing and Cloud Storage for data storage.</p>",
      "<p>Use Apache Hive on Compute Engine for data warehousing, with Apache Hadoop HDFS for data storage and Apache Zeppelin for data analysis.</p>",
      "<p>Use Cloud Dataproc for data processing, with Cloud Bigtable for data storage and Cloud Datalab for data analysis.</p>",
      "<p>Use Apache Impala on Google Kubernetes Engine for data warehousing and analysis, with Apache Kudu for data storage and Apache Superset for data visualization.</p>"
    ],
    "explanation": "<p>Use BigQuery for data warehousing and analysis, with Cloud Dataflow for data processing and Cloud Storage for data storage. -&gt;&nbsp;Correct. BigQuery is a fully-managed, cloud-native data warehousing solution that is optimized for fast query performance and can handle petabyte-scale data. Cloud Dataflow can be used for processing large datasets in a fully-managed way, and Cloud Storage provides a cost-effective and scalable way to store large volumes of data. Overall, this option covers all the requirements for running a large-scale, highly-available data warehousing platform on Google Cloud.</p><p><br></p><p>Use Apache Hive on Compute Engine for data warehousing, with Apache Hadoop HDFS for data storage and Apache Zeppelin for data analysis. -&gt; Incorrect. While this approach may provide some cost savings, it can be complex to manage and maintain at scale. Additionally, Hive may not be the most performant option for querying large datasets.</p><p><br></p><p>Use Cloud Dataproc for data processing, with Cloud Bigtable for data storage and Cloud Datalab for data analysis. -&gt; Incorrect. While Dataproc can be a good option for processing large datasets using Apache Hadoop and Spark, it may not be the most cost-effective solution for data warehousing at petabyte scale. Cloud Bigtable is optimized for handling large volumes of structured data, but may not be the best fit for unstructured data.</p><p><br></p><p>Use Apache Impala on Google Kubernetes Engine for data warehousing and analysis, with Apache Kudu for data storage and Apache Superset for data visualization. -&gt; Incorrect. While Impala can provide fast SQL querying of large datasets, managing the infrastructure for Impala and Kudu can be complex and time-consuming. Additionally, Kubernetes may not be the best fit for managing large-scale data processing workloads.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297696,
    "question_plain": "How would you design a solution for running a large-scale, highly-available, and scalable transactional application on Google Cloud, considering the requirement for fast page load times, and the ability to handle billions of daily transactions?",
    "answers": [
      "<p>Use Cloud Storage for static asset storage, with Cloud CDN for content delivery and Cloud SQL for database management.</p>",
      "<p>Use Cloud Storage for static asset storage, with Cloud Load Balancer for traffic distribution and BigQuery for database management.</p>",
      "<p>Use Cloud CDN for content delivery, with Cloud Functions for serverless computing and Cloud Spanner for database management.</p>",
      "<p>Use Cloud CDN for content delivery, with Cloud Load Balancer for traffic distribution and Cloud Firestore for database management.</p>"
    ],
    "explanation": "<p>Use Cloud CDN for content delivery, with Cloud Functions for serverless computing and Cloud Spanner for database management. -&gt; Correct. Cloud CDN for content delivery ensures fast page load times. Cloud Functions, a serverless compute solution, can scale automatically to handle large loads. Cloud Spanner, a globally-distributed, horizontally scalable relational database service, is designed to handle high transaction rates and ensure high availability, which fits the requirement perfectly.</p><p><br></p><p>Use Cloud Storage for static asset storage, with Cloud CDN for content delivery and Cloud SQL for database management. -&gt;&nbsp;Incorrect. Using Cloud Storage for static asset storage and Cloud CDN for content delivery are good choices for ensuring fast page load times. However, while Cloud SQL is a reliable relational database service, it might struggle to handle billions of daily transactions at scale as compared to a globally distributed, horizontally scalable database service like Cloud Spanner.</p><p><br></p><p>Use Cloud Storage for static asset storage, with Cloud Load Balancer for traffic distribution and BigQuery for database management. -&gt;&nbsp;Incorrect. While Cloud Storage and Cloud Load Balancer are suitable for static asset storage and traffic distribution respectively, BigQuery is not ideal for transactional applications. It's a data warehousing solution designed for analytics and not optimized for transactional workloads.</p><p><br></p><p>Use Cloud CDN for content delivery, with Cloud Load Balancer for traffic distribution and Cloud Firestore for database management. -&gt;&nbsp;Incorrect. Cloud CDN and Cloud Load Balancer are suitable for content delivery and traffic distribution respectively. However, Cloud Firestore, although a powerful NoSQL database, is not optimized for handling billions of daily transactions as efficiently as Cloud Spanner. Firestore is more suited for real-time updates and sync across app clients, not for heavy transactional loads.</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297698,
    "question_plain": "Your company is planning to design a new cloud solution architecture that not only addresses its current needs but is also robust enough to accommodate future improvements and technological advancements. As a Google Professional Cloud Architect, you are tasked with ensuring the solution is scalable, cost-effective, and able to integrate future cloud and technology innovations. Considering Google Cloud Platform's (GCP) capabilities, which of the following approaches would best align with these requirements?",
    "answers": [
      "<p>Using Google Kubernetes Engine (GKE) to containerize and orchestrate microservices.</p>",
      "<p>Implementing a monolithic architecture on Compute Engine for all services.</p>",
      "<p>Relying solely on preemptible VMs to reduce costs, without considering high availability.</p>",
      "<p>Designing the architecture to rely on a single region and availability zone for all services.</p>"
    ],
    "explanation": "<p>Using Google Kubernetes Engine (GKE) to containerize and orchestrate microservices. -&gt;&nbsp;Correct. GKE provides a scalable and flexible environment for managing microservices, making it easier to implement future solution improvements and integrate new technologies.</p><p><br></p><p>Implementing a monolithic architecture on Compute Engine for all services. -&gt;&nbsp;Incorrect. A monolithic architecture may hinder scalability and flexibility, making it challenging to adopt future improvements and technology changes.</p><p><br></p><p>Relying solely on preemptible VMs to reduce costs, without considering high availability. -&gt;&nbsp;Incorrect. Using preemptible VMs can lower costs, but it may compromise high availability and reliability, which are crucial for future-proofing and scalability in response to technology advancements.</p><p><br></p><p>Designing the architecture to rely on a single region and availability zone for all services. -&gt;&nbsp;Incorrect. This approach does not consider the high availability and disaster recovery aspects of cloud architecture. Relying on a single region and availability zone makes the system vulnerable to outages and fails to leverage the global infrastructure of GCP, limiting the ability to adapt to future geographical or technological expansions.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297700,
    "question_plain": "How would you design a solution for running a large-scale, multi-cloud, and secure big data analytics platform on Google Cloud, considering the requirement for real-time data processing, scalability, and the ability to handle petabytes of structured and unstructured data from multiple sources?",
    "answers": [
      "<p>Use Cloud Dataproc for batch processing, with Cloud Dataflow for real-time data processing and Cloud Bigtable for data warehousing.</p>",
      "<p>Use Cloud Tasks for batch processing, with Cloud Pub/Sub for real-time data streaming and Cloud BigQuery for data warehousing.</p>",
      "<p>Use Cloud Dataproc for batch processing, with Cloud Dataflow for real-time data processing and Cloud BigQuery for data warehousing.</p>",
      "<p>Use Cloud Dataproc for batch processing, with Cloud Pub/Sub for real-time data streaming and Cloud Bigtable for data warehousing.</p>"
    ],
    "explanation": "<p>Use Cloud Dataproc for batch processing, with Cloud Dataflow for real-time data processing and Cloud BigQuery for data warehousing. -&gt; Correct. Cloud Dataproc is a managed Hadoop and Spark service designed for batch processing, and Cloud Dataflow is designed for both batch and real-time data processing. Cloud BigQuery is a fully-managed, serverless data warehouse that enables super-fast SQL queries and provides the ability to process petabytes of data, making it suitable for handling structured and unstructured data from multiple sources.</p><p><br></p><p>Use Cloud Dataproc for batch processing, with Cloud Dataflow for real-time data processing and Cloud Bigtable for data warehousing. -&gt;&nbsp;Incorrect. While Cloud Dataproc and Cloud Dataflow are suitable for batch and real-time processing, respectively, Cloud Bigtable, though a high-performance NoSQL database, is not designed for data warehousing needs. It doesn't provide the SQL interface and powerful analytical capabilities offered by BigQuery.</p><p><br></p><p>Use Cloud Tasks for batch processing, with Cloud Pub/Sub for real-time data streaming and Cloud BigQuery for data warehousing. -&gt;&nbsp;Incorrect. Cloud Tasks is designed for asynchronous task execution, which is not a fit for the requirement of batch processing large-scale data. Cloud Pub/Sub is good for real-time data streaming, but it does not offer processing capabilities. BigQuery is an excellent choice for a data warehousing solution, but other components of this option don't meet the requirements.</p><p><br></p><p>Use Cloud Dataproc for batch processing, with Cloud Pub/Sub for real-time data streaming and Cloud Bigtable for data warehousing. -&gt;&nbsp;Incorrect. Although Cloud Dataproc and Cloud Pub/Sub are appropriate for batch processing and real-time data streaming, Cloud Bigtable isn't a suitable choice for a data warehousing solution as it doesn't provide the SQL interface and powerful analytical capabilities offered by BigQuery.</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297702,
    "question_plain": "How would you design a solution for running a large-scale, multi-cloud, and secure disaster recovery plan for a global enterprise, considering the requirement for real-time data replication, low recovery time objective (RTO), and the ability to handle multiple TBs of data from multiple locations?",
    "answers": [
      "<p>Use Cloud Storage Transfer Service for data replication, with Cloud VPN for network connectivity and Cloud Storage for database management.</p>",
      "<p>Use Cloud Functions for data replication, with Cloud Interconnect for network connectivity and Cloud Bigtable for database management.</p>",
      "<p>Use Cloud Storage Transfer Service for data replication, with Cloud Dedicated Interconnect for network connectivity and Cloud Filestore for database management.</p>",
      "<p>Use Cloud Storage Transfer Service for data replication, with Cloud Dedicated Interconnect for network connectivity and Cloud Spanner for database management.</p>"
    ],
    "explanation": "<p>Use Cloud Storage Transfer Service for data replication, with Cloud Dedicated Interconnect for network connectivity and Cloud Spanner for database management. -&gt;&nbsp;Correct. Cloud Storage Transfer Service is suitable for data replication, and Cloud Dedicated Interconnect provides high-bandwidth, low-latency connections for network connectivity. Cloud Spanner is a globally-distributed, horizontally scalable, relational database service that is designed for high transaction rates and can handle multiple TBs of data, ensuring high availability. It can effectively serve as the database management system in a disaster recovery plan.</p><p><br></p><p>Use Cloud Storage Transfer Service for data replication, with Cloud VPN for network connectivity and Cloud Storage for database management. -&gt;&nbsp;Incorrect. Cloud Storage Transfer Service and Cloud VPN are suitable for data replication and network connectivity, respectively. However, Cloud Storage isn't a database management system, so it doesn't offer the transactional and querying capabilities typical databases provide. It's more suitable for storing unstructured data.</p><p><br></p><p>Use Cloud Functions for data replication, with Cloud Interconnect for network connectivity and Cloud Bigtable for database management. -&gt;&nbsp;Incorrect. Cloud Functions is a serverless execution environment and isn't primarily designed for data replication tasks. Cloud Interconnect is suitable for network connectivity, but Cloud Bigtable, while powerful for large-scale, low-latency read/write workloads, is not globally-distributed, which could pose a problem for a global enterprise.</p><p><br></p><p>Use Cloud Storage Transfer Service for data replication, with Cloud Dedicated Interconnect for network connectivity and Cloud Filestore for database management. -&gt;&nbsp;Incorrect. Cloud Storage Transfer Service and Cloud Dedicated Interconnect are appropriate for data replication and network connectivity. However, Cloud Filestore is a managed file storage service for applications that require a filesystem interface and a shared filesystem for data, but it's not designed for handling database workloads.</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297704,
    "question_plain": "Given a scenario where a company wants to migrate their on-premises infrastructure to Google Cloud, which solution is most appropriate to securely transfer sensitive data to Google Cloud while meeting the company's compliance requirements?",
    "answers": [
      "<p>Use <code>gsutil</code> to transfer data directly from the on-premises infrastructure to Cloud Storage buckets.</p>",
      "<p>Use Cloud Storage Transfer Service to transfer data from the on-premises infrastructure to Cloud Storage buckets.</p>",
      "<p>Use Cloud VPN to establish a secure connection from the on-premises infrastructure to Google Cloud, then use <code>gsutil</code> to transfer data to Cloud Storage buckets.</p>",
      "<p>Use <code>gcloud</code> to transfer data directly from the on-premises infrastructure to Cloud Storage buckets.</p>"
    ],
    "explanation": "<p>Use Cloud VPN to establish a secure connection from the on-premises infrastructure to Google Cloud, then use <code>gsutil</code> to transfer data to Cloud Storage buckets. -&gt;&nbsp;Correct. Using Cloud VPN to establish a secure connection from the on-premises infrastructure to Google Cloud ensures that data transfer occurs over an encrypted tunnel. This approach provides a secure and compliant method for transferring sensitive data. After establishing the VPN connection, gsutil can be used to transfer the data to Cloud Storage buckets securely.</p><p><br></p><p>Use <code>gsutil</code> to transfer data directly from the on-premises infrastructure to Cloud Storage buckets. -&gt;&nbsp;Incorrect. It may not provide the necessary security and compliance measures required for transferring sensitive data securely. It lacks the encryption and secure connection provided by other options.</p><p><br></p><p>Use Cloud Storage Transfer Service to transfer data from the on-premises infrastructure to Cloud Storage buckets. -&gt;&nbsp;Incorrect. It may not address the need for secure connection and compliance requirements directly.</p><p><br></p><p>Use <code>gcloud</code> to transfer data directly from the on-premises infrastructure to Cloud Storage buckets. -&gt;&nbsp;Incorrect. It may not provide the necessary security measures required for transferring sensitive data securely. It lacks the encryption and secure connection provided by other options.</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297706,
    "question_plain": "Consider a scenario where a global financial services company wants to move their legacy monolithic application to the cloud. The application is currently hosted on-premise and relies on a combination of custom-built software, third-party software, and hardware appliances. The new cloud-based solution must meet the following requirements:support high availability and disaster recoverymaintain the current level of security and complianceensure data privacy and data residency requirements are met for all regionsoptimize cost while providing scalable and elastic resourcesminimize downtime during migrationWhat is the most appropriate solution to meet the requirements outlined above?",
    "answers": [
      "<p>Migrate the application to Google Cloud using Compute Engine virtual machines and Cloud Storage for data storage. Use Load Balancer for traffic management and Cloud VPN for secure communication between on-premise and cloud resources. Implement a multi-region deployment with active-active replication.</p>",
      "<p>Rebuild the application using App Engine and Google Cloud SQL for data storage. Use Cloud CDN for traffic management and Cloud Interconnect for secure communication between on-premise and cloud resources. Implement a multi-zone deployment with active-standby replication.</p>",
      "<p>Migrate the application to Google Cloud using Compute Engine virtual machines and Cloud Storage for data storage. Use Google Cloud DNS for traffic management and Virtual Private Cloud (VPC) for secure communication between on-premise and cloud resources. Implement a multizone deployment with active-active replication.</p>",
      "<p>Rebuild the application using Cloud Functions and Firestore for data storage. Use Cloud CDN for traffic management and loud VPN for secure communication between on-premise and cloud resources. Implement a multi-region deployment with active-standby replication.</p>"
    ],
    "explanation": "<p>Migrate the application to Google Cloud using Compute Engine virtual machines and Cloud Storage for data storage. Use Load Balancer for traffic management and Cloud VPN for secure communication between on-premise and cloud resources. Implement a multi-region deployment with active-active replication. -&gt;&nbsp;Correct. It is the correct answer for the following reasons:</p><ul><li><p>Compute Engine virtual machines and Cloud Storage can handle legacy monolithic applications with ease and provide high availability and disaster recovery features. They also allow the company to maintain the current level of security and compliance by implementing various security measures.</p></li><li><p>Load Balancer can help in traffic management, and Cloud VPN can provide secure communication between on-premise and cloud resources.</p></li><li><p>Implementing a multi-region deployment with active-active replication ensures data privacy and data residency requirements are met for all regions.</p></li><li><p>The solution provided with A meets the requirement to optimize cost while providing scalable and elastic resources.</p></li><li><p>By migrating the application to Google Cloud using Compute Engine virtual machines and Cloud Storage, the company can minimize downtime during migration.</p></li></ul><p><br></p><p>Rebuild the application using App Engine and Google Cloud SQL for data storage. Use Cloud CDN for traffic management and Cloud Interconnect for secure communication between on-premise and cloud resources. Implement a multi-zone deployment with active-standby replication. -&gt; Incorrect. Rebuilding the application may require a lot of resources, and it may take a considerable amount of time to rebuild the application.</p><p><br></p><p>Migrate the application to Google Cloud using Compute Engine virtual machines and Cloud Storage for data storage. Use Google Cloud DNS for traffic management and Virtual Private Cloud (VPC) for secure communication between on-premise and cloud resources. Implement a multizone deployment with active-active replication. -&gt; Incorrect. Google Cloud DNS does not provide traffic management capabilities like Load Balancer.</p><p><br></p><p>Rebuild the application using Cloud Functions and Firestore for data storage. Use Cloud CDN for traffic management and loud VPN for secure communication between on-premise and cloud resources. Implement a multi-region deployment with active-standby replication. -&gt; Incorrect. Cloud Functions is used for serverless computing and may not be the best fit for a legacy monolithic application. Firestore also may not be suitable for data storage for such applications.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297708,
    "question_plain": "You have been assigned to design a storage system for a multinational company that anticipates substantial growth in data over the next two years. The data will be accessed frequently and changes infrequently. It is crucial to minimize latency and ensure the data's availability globally. Which Cloud Storage class should you choose?",
    "answers": [
      "<p>Multi-Regional Storage</p>",
      "<p>Regional Storage</p>",
      "<p>Archive Storage</p>",
      "<p>Nearline Storage</p>"
    ],
    "explanation": "<p>Multi-Regional Storage -&gt;&nbsp;Correct. Multi-Regional Storage is designed for maximum availability and redundancy, with data geo-replicated to multiple regions. It is ideal for frequently accessed data, such as serving web content, interactive workloads, or data accessed by users around the world.</p><p><br></p><p>Regional Storage -&gt;&nbsp;Incorrect. Regional Storage is for data accessed frequently within a specific geographic region. It does not offer the global availability that is needed in this case.</p><p><br></p><p>Archive Storage -&gt;&nbsp;Incorrect. Archive Storage is for data that will be stored for long periods without the need for access, such as for long-term backups and compliance. It is not suitable for data that needs to be accessed frequently.</p><p><br></p><p>Nearline Storage -&gt;&nbsp;Incorrect. Nearline Storage is for infrequently accessed data, such as backups and long-tail multimedia content. It is not suitable for data that needs to be accessed frequently.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297712,
    "question_plain": "A company wants to migrate their existing on-premise data center to Google Cloud. The company's current data center has 100TB of data, with 1000 servers and 100 network devices. They require a high-availability solution with the lowest possible latency and maximum security. What would be the most cost-effective, scalable and secure solution to meet the company's requirements for data center migration to Google Cloud?",
    "answers": [
      "<p>Use Cloud Storage with Cloud VPN for data transfer and storage. Deploy VMs in a single zone.</p>",
      "<p>Use Cloud Storage with Cloud Interconnect for data transfer and storage. Deploy VMs in multiple zones.</p>",
      "<p>Use Cloud BigQuery with Cloud VPN for data transfer and storage. Deploy VMs in a single zone.</p>",
      "<p>Use Compute Engine for storage. Deploy VMs in multiple regions with load balancing.</p>"
    ],
    "explanation": "<p>Use Cloud Storage with Cloud Interconnect for data transfer and storage. Deploy VMs in multiple zones.. -&gt;&nbsp;Correct. The company wants a high-availability solution with the lowest possible latency and maximum security for their data center migration to Google Cloud. To achieve this, the company can use Cloud Storage with Cloud Interconnect for data transfer and storage, which offers a highly available and low-latency solution with secure connectivity between on-premise data centers and Google Cloud. Deploying VMs in multiple zones will increase the availability and reduce the risk of downtime. This solution is also cost-effective and scalable, as it allows for the company to pay for only what they use and scale up as needed.</p><p><br></p><p>Use Cloud Storage with Cloud VPN for data transfer and storage. Deploy VMs in a single zone. -&gt; Incorrect. Using Cloud Storage with Cloud VPN for data transfer and storage, while still secure, does not offer the same low latency and high availability as Cloud Interconnect. Deploying VMs in a single zone is also not a highly available solution.</p><p><br></p><p>Use Cloud BigQuery with Cloud VPN for data transfer and storage. Deploy VMs in a single zone. -&gt; Incorrect. Using Cloud BigQuery, is not suitable for storing large amounts of unstructured data, as it is designed for analysis of structured data. Deploying VMs in a single zone is also not a highly available solution.</p><p><br></p><p>Use Compute Engine for storage. Deploy VMs in multiple regions with load balancing. -&gt; Incorrect. Using Compute Engine for storage, is not as cost-effective as using Cloud Storage, and deploying VMs in multiple regions with load balancing is not the most secure or low-latency solution.</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297714,
    "question_plain": "You are designing a cloud solution for a multinational retail company with a complex multi-tier architecture. The company has a central web application that serves as a customer-facing e-commerce platform. The web application integrates with multiple backend systems, including a payment gateway, a warehouse management system, and an inventory management system. The company wants to ensure that the platform is highly available and secure, with a disaster recovery strategy in place in case of regional outages. In addition, the company wants to be able to deploy new features and updates to the platform with minimal downtime. Which of the following design patterns would you recommend to achieve this design?",
    "answers": [
      "<p>Implement a regional active-active setup with a regional load balancer and multiple backend systems in different regions.</p>",
      "<p>Implement a regional active-passive setup with a regional load balancer and multiple backend systems in different regions, with real-time replication between regions.</p>",
      "<p>Implement a global active-active setup with a global load balancer and multiple backend systems in different regions, with real-time replication between regions.</p>",
      "<p>Implement a multi-region active-active setup with multiple regional load balancers and backend systems in different regions, with real-time replication between regions.</p>"
    ],
    "explanation": "<p>Implement a multi-region active-active setup with multiple regional load balancers and backend systems in different regions, with real-time replication between regions. -&gt; Correct. It is the most effective way to achieve high availability and disaster recovery for a complex multi-tier architecture like the one described. This approach involves deploying the web application and backend systems across multiple regions and using load balancers to distribute traffic across the different regions. Real-time replication between regions ensures that data is consistent across all regions and that the platform remains highly available even in the event of regional outages.</p><p><br></p><p>Implement a regional active-active setup with a regional load balancer and multiple backend systems in different regions. -&gt; Incorrect. It may provide some degree of redundancy, but it may not be as effective as a multi-region setup in ensuring high availability and disaster recovery.</p><p><br></p><p>Implement a regional active-passive setup with a regional load balancer and multiple backend systems in different regions, with real-time replication between regions. -&gt; Incorrect. It may provide some level of disaster recovery, but it may not be as effective in ensuring high availability as an active-active setup.</p><p><br></p><p>Implement a global active-active setup with a global load balancer and multiple backend systems in different regions, with real-time replication between regions. -&gt; Incorrect. It may be more expensive and complex than a multi-region setup and may not provide significant benefits in terms of high availability and disaster recovery.</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297716,
    "question_plain": "A large multinational corporation has an e-commerce platform with multiple microservices hosted in Google Cloud Platform. Each microservice has its own separate database, but the corporation wants to centralize the authentication process for all of its services. What is the most secure and scalable solution for centralized authentication in this scenario?",
    "answers": [
      "<p>Implementing Google Cloud IAM for each microservice.</p>",
      "<p>Implementing Google Cloud IAP for each microservice.</p>",
      "<p>Implementing a single OAuth 2.0 server using Google Cloud Endpoints.</p>",
      "<p>Implementing a single OAuth 2.0 server using Google Cloud Functions.</p>"
    ],
    "explanation": "<p>Implementing Google Cloud IAP for each microservice. -&gt; Correct. Google Cloud IAP (Identity-Aware Proxy) is a solution that controls access to cloud applications running on Google Cloud Platform. It can secure the application by verifying user identity and the context of the request to determine if a user should be allowed access. It can act as a centralized point for authenticating all requests coming into the microservices, making it the most secure and scalable solution for centralized authentication in this scenario.</p><p><br></p><p>Implementing Google Cloud IAM for each microservice. -&gt;&nbsp;Incorrect. Google Cloud IAM (Identity and Access Management) is used to control who (users or services) has what type of access to which resources. While it's important for controlling access within a cloud environment, it doesn't provide the functionality to centralize authentication across multiple services.</p><p><br></p><p>Implementing a single OAuth 2.0 server using Google Cloud Endpoints. -&gt;&nbsp;Incorrect. An OAuth 2.0 server using Google Cloud Endpoints could handle authentication, but it wouldn't provide the same level of security as Cloud IAP. Also, setting up and maintaining a single OAuth server may not be as scalable or simple as using a managed service like Cloud IAP.</p><p><br></p><p>Implementing a single OAuth 2.0 server using Google Cloud Functions. -&gt;&nbsp;Incorrect. While a single OAuth 2.0 server could centralize authentication, Cloud Functions is more designed to execute code in response to events and isn't specifically tailored for managing secure and scalable authentication across multiple microservices.</p><p><br></p><p>https://cloud.google.com/iap/docs</p>",
    "correct_response": ["b"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297718,
    "question_plain": "A financial company wants to move its existing data warehouse infrastructure to Google Cloud Platform to take advantage of its scalability and cost-effectiveness. The data warehouse processes large amounts of financial data and the company wants to ensure the data is secure and available at all times. Which of the following is the best solution for this requirement in Google Cloud Platform?",
    "answers": [
      "<p>Using BigQuery for data warehousing</p>",
      "<p>Using Cloud SQL for data warehousing</p>",
      "<p>Using Cloud Datastore for data warehousing</p>",
      "<p>Using Cloud Storage for data warehousing</p>"
    ],
    "explanation": "<p>Using BigQuery for data warehousing -&gt; Correct. BigQuery is a fully-managed, highly scalable, and cost-effective data warehouse solution provided by Google Cloud Platform. It is designed to process and analyze large amounts of data in real-time, making it an ideal solution for financial companies that deal with large amounts of financial data. BigQuery provides multiple layers of security to ensure the data is secure, including encryption at rest and in transit, IAM roles and permissions, and audit logging. It also offers high availability, with multi-regional replication and automatic failover, ensuring that the data is available at all times.</p><p><br></p><p>Using Cloud SQL for data warehousing -&gt; Incorrect. Cloud SQL is a managed service for relational databases, but it is not designed for large-scale data warehousing. It is more suitable for smaller-scale databases.</p><p><br></p><p>Using Cloud Datastore for data warehousing -&gt; Incorrect. Cloud Datastore is a NoSQL document database, which is not designed for data warehousing. It is more suitable for applications that require high scalability and low-latency.</p><p><br></p><p>Using Cloud Storage for data warehousing -&gt; Incorrect. Cloud Storage is a highly scalable object storage solution that is suitable for storing unstructured data, but it is not designed for data warehousing. It is more suitable for storing files, backups, and archives.</p><p><br></p><p>https://cloud.google.com/bigquery/docs</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297720,
    "question_plain": "Your company has just acquired another business, and you've been tasked with integrating their existing DNS setup into Google Cloud DNS. The other business already has several registered domains that are critical to its operations, and it's important that the transition is smooth with zero downtime. How would you accomplish this?",
    "answers": [
      "<p>Create a new zone for each domain in Google Cloud DNS, manually recreate the DNS records, then update the name servers at the domain registrar to those of the Google Cloud DNS zones.</p>",
      "<p>Export DNS records from the existing provider, create a new zone for each domain in Google Cloud DNS, and import the records.</p>",
      "<p>Use the <code>gcloud dns record-sets import</code> command to import the records from the existing provider, then update the name servers at the domain registrar.</p>",
      "<p>Contact Google support to have them manually transition the DNS records for you.</p>"
    ],
    "explanation": "<p>Create a new zone for each domain in Google Cloud DNS, manually recreate the DNS records, then update the name servers at the domain registrar to those of the Google Cloud DNS zones. -&gt; Correct. After manually creating the records in Google Cloud DNS, updating the name servers at the domain registrar to point to Google's DNS servers ensures a seamless transition.</p><p><br></p><p>Export DNS records from the existing provider, create a new zone for each domain in Google Cloud DNS, and import the records. -&gt; Incorrect. While it is true that Google Cloud DNS can import and export DNS records, this isn't done automatically and there's no direct way to import from an existing provider, especially given that different DNS providers have different export formats.</p><p><br></p><p>Use the <code>gcloud dns record-sets import</code> command to import the records from the existing provider, then update the name servers at the domain registrar. -&gt; Incorrect. This command is not available and cannot be used to directly import records from another DNS provider.</p><p><br></p><p>Contact Google support to have them manually transition the DNS records for you. -&gt; Incorrect. Google support would not manually transition DNS records for you. It's the responsibility of the Google Cloud user to manage their DNS records.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297722,
    "question_plain": "Your organization operates a large on-premise data warehouse that's currently based on a traditional RDBMS. You have been tasked with migrating this system to the Google Cloud Platform. You're concerned about downtime during the migration and want to minimize it. What would be the best approach?",
    "answers": [
      "<p>Use BigQuery Data Transfer Service to regularly update BigQuery with changes from the on-premise system, followed by a brief downtime to finalize the transfer and switch over.</p>",
      "<p>Use a manual process to extract, transform, and load (ETL) the data from the on-premise system to BigQuery.</p>",
      "<p>Set up a VPN between the on-premise data center and Google Cloud, then use Database Migration Service to move the data.</p>",
      "<p>Use Cloud SQL as the destination for the migration, then transfer the data from Cloud SQL to BigQuery.</p>"
    ],
    "explanation": "<p>Use BigQuery Data Transfer Service to regularly update BigQuery with changes from the on-premise system, followed by a brief downtime to finalize the transfer and switch over. -&gt;&nbsp;Correct. This approach minimizes downtime by keeping BigQuery synchronized with the on-premise system until you're ready to switch over.</p><p><br></p><p>Use a manual process to extract, transform, and load (ETL) the data from the on-premise system to BigQuery. -&gt;&nbsp;Incorrect. This approach is time-consuming and error-prone, and it does not handle ongoing changes to the on-premise system, which may result in extended downtime.</p><p><br></p><p>Set up a VPN between the on-premise data center and Google Cloud, then use Database Migration Service to move the data. -&gt;&nbsp;Incorrect. Database Migration Service can be used to migrate from on-premises or another cloud to Cloud SQL, not BigQuery. It doesn't align with the typical use case of a data warehouse.</p><p><br></p><p>Use Cloud SQL as the destination for the migration, then transfer the data from Cloud SQL to BigQuery. -&gt;&nbsp;Incorrect. Cloud SQL is an operational database service and not designed for analytical workloads typical of data warehouses. This adds an unnecessary step and potential point of failure to the migration process.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297724,
    "question_plain": "A financial services company is looking to store and process large amounts of sensitive customer data in the cloud, while meeting the requirements of strict regulatory compliance. What is the best solution for meeting these requirements using Google Cloud Platform?",
    "answers": [
      "<p>Store the data in Cloud SQL for MySQL and process it using Cloud Functions.</p>",
      "<p>Store the data in Bigtable and process it using Cloud Dataflow.</p>",
      "<p>Store the data in Cloud SQL for PostgreSQL and process it using Cloud Functions.</p>",
      "<p>Store the data in Cloud Storage and process it using Cloud Functions, while ensuring data encryption and access controls using Cloud Key Management Service and Cloud Identity and Access Management.</p>"
    ],
    "explanation": "<p>Store the data in Cloud Storage and process it using Cloud Functions, while ensuring data encryption and access controls using Cloud Key Management Service and Cloud Identity and Access Management. -&gt; Correct. Cloud Storage allows you to store large amounts of data, while Cloud Functions can be used to process it. In addition, Cloud Key Management Service provides tools for managing cryptographic keys for cloud services, which can be used to encrypt sensitive data. Cloud Identity and Access Management ensures proper access control by allowing you to determine who (users or services) has what access to specific resources. This solution addresses not only the data storage and processing requirements but also the data security and compliance requirements, making it the most suitable option for a financial services company dealing with sensitive data.</p><p><br></p><p>Store the data in Cloud SQL for MySQL and process it using Cloud Functions. -&gt;&nbsp;Incorrect. Cloud SQL for MySQL and Cloud Functions can be used for storing and processing data. However, this option doesn't mention any specific services for handling sensitive data or maintaining strict regulatory compliance such as encryption, access control, or key management.</p><p><br></p><p>Store the data in Bigtable and process it using Cloud Dataflow. -&gt;&nbsp;Incorrect. Bigtable and Cloud Dataflow are good for storing and processing large amounts of data, but they don't specify solutions for data security or compliance.</p><p><br></p><p>Store the data in Cloud SQL for PostgreSQL and process it using Cloud Functions. -&gt;&nbsp;Incorrect. While Cloud SQL for PostgreSQL and Cloud Functions can handle data storage and processing, they don't cover additional requirements for sensitive data and regulatory compliance.</p>",
    "correct_response": ["d"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297726,
    "question_plain": "A healthcare organization is looking to implement a cloud-based electronic medical records (EMR) OLTP system that can securely store and process large amounts of sensitive patient data. The solution should also be able to support real-time data access and updates by authorized healthcare providers. Which of the following Google Cloud Platform services would you recommend for this requirement?",
    "answers": [
      "<p>Cloud Storage</p>",
      "<p>Cloud Bigtable</p>",
      "<p>Cloud SQL for PostgreSQL</p>",
      "<p>Cloud Firestore</p>"
    ],
    "explanation": "<p>Cloud SQL for PostgreSQL -&gt;&nbsp;Correct. Cloud SQL for PostgreSQL is a fully managed relational database service that can provide secure storage and processing of large amounts of sensitive patient data. It is designed for OLTP workloads and can provide real-time data access and updates by authorized healthcare providers. </p><p><br></p><p>Cloud Storage -&gt; Incorrect. It is a durable and highly available object storage service but is not a relational database and may not be the best choice for OLTP workloads. </p><p><br></p><p>Cloud Bigtable -&gt; Incorrect. It is a NoSQL database that can be used for high-performance OLTP workloads, but it may require more operations work to maintain compared to Cloud SQL for PostgreSQL. </p><p><br></p><p>Cloud Firestore -&gt; Incorrect. It is a NoSQL document database that is designed for mobile and web application development and may not be the best choice for a healthcare organization's EMR system.</p><p><br></p><p>https://cloud.google.com/sql/docs/postgres</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297728,
    "question_plain": "A company is looking to process real-time data from multiple sources in order to provide real-time analytics to their clients. The company is expecting to process a large volume of data with an estimated peak of up to 1 million requests per second. The solution must be able to scale elastically to handle varying levels of incoming data, ensure low latency processing, and support the processing of data in real-time. Which of the following options would be the most effective approach to meet these requirements?",
    "answers": [
      "<p>Use Cloud Dataproc with Apache Spark Streaming to process the data in real-time.</p>",
      "<p>Use Cloud Dataflow with Apache Beam to process the data in batch mode and store the results in Cloud Bigtable.</p>",
      "<p>Use Cloud Pub/Sub to ingest the data and feed it into Cloud Dataflow for real-time processing.</p>",
      "<p>Use Cloud Dataflow to ingest the data and feed it into Cloud Bigtable for real-time processing.</p>"
    ],
    "explanation": "<p>Use Cloud Pub/Sub to ingest the data and feed it into Cloud Dataflow for real-time processing. -&gt; Correct. It is the most effective approach for this scenario. Cloud Pub/Sub is a fully-managed real-time messaging service that can handle high throughput and automatically scales to meet the incoming data demands. Cloud Dataflow can be used to process the data in real-time and provides low-latency processing capabilities. Additionally, Cloud Dataflow can easily scale up or down to handle varying volumes of incoming data.</p><p><br></p><p>Use Cloud Dataproc with Apache Spark Streaming to process the data in real-time. -&gt; Incorrect. It is a valid option for processing real-time data, but may not be the most effective approach for this particular scenario. While Cloud Dataproc is highly scalable and can support processing of real-time data, the company is expecting to process a large volume of data, which may not be efficiently processed using Apache Spark Streaming in a single cluster.</p><p><br></p><p>Use Cloud Dataflow with Apache Beam to process the data in batch mode and store the results in Cloud Bigtable. -&gt;&nbsp;Incorrect. It is not an effective approach for processing real-time data. Cloud Dataflow is designed for batch processing and cannot provide low-latency processing capabilities.</p><p><br></p><p>Use Cloud Dataflow to ingest the data and feed it into Cloud Bigtable for real-time processing. -&gt; Incorrect. It is not an effective approach for processing real-time data. Cloud Dataflow is designed for batch processing and cannot provide low-latency processing capabilities. Additionally, Cloud Bigtable is a NoSQL database designed for high-throughput and high-scalability of large amounts of data, but may not be an ideal choice for real-time processing of data.</p>",
    "correct_response": ["c"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297730,
    "question_plain": "Your company is developing an application that requires a high volume of read and write operations on the database. The application will be hosted on Google Compute Engine instances. However, the database is expected to grow significantly over time, and the management wants to ensure that the disk performance remains high. As a cloud architect, what type of persistent disk would you recommend for optimal performance?",
    "answers": [
      "<p>SSD persistent disk, because it provides high IOPS and throughput.</p>",
      "<p>Standard persistent disk, because it provides cost-effective storage.</p>",
      "<p>Local SSD, because it provides high IOPS and low latency.</p>",
      "<p>Use Cloud Storage instead of a persistent disk for high performance.</p>"
    ],
    "explanation": "<p>SSD persistent disk, because it provides high IOPS and throughput. -&gt; Correct. SSD persistent disks provide high IOPS and throughput, which makes them ideal for databases with high read/write operations. They also persist data across instance lifecycle events, making them the best choice in this scenario.</p><p><br></p><p>Standard persistent disk, because it provides cost-effective storage. -&gt; Incorrect. While Standard persistent disks are cost-effective and deliver consistent performance, they do not provide the high IOPS (input/output operations per second) and throughput needed for a database with a high volume of read/write operations.</p><p><br></p><p>Local SSD, because it provides high IOPS and low latency. -. Incorrect. Although Local SSDs offer high IOPS and low latency, they are ephemeral. This means that they do not persist data across instance stop, restart, or deletion, making them not ideal for database storage where data persistence is critical.</p><p><br></p><p>Use Cloud Storage instead of a persistent disk for high performance. -&gt; Incorrect. Cloud Storage is an object storage service and it is not designed for high-volume read/write operations as required by most databases.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297732,
    "question_plain": "Your organization has a multi-tier application running on Google Cloud and follows an Agile development process. You are assigned the task of setting up a testing environment that is identical to the production environment. This testing environment should be isolated and must not impact the production environment in any way. It should also be easily reproducible and scalable to facilitate multiple parallel testing efforts. How should you approach this task?",
    "answers": [
      "<p>Use Google Deployment Manager to replicate the infrastructure of the production environment in the testing environment.</p>",
      "<p>Use Google Compute Engine to manually create and configure VMs for the testing environment that match the production environment.</p>",
      "<p>Use Google Kubernetes Engine and set up a separate namespace for testing while using the same cluster as the production environment.</p>",
      "<p>Use Google Cloud Functions and create separate functions for testing.</p>"
    ],
    "explanation": "<p>Use Google Deployment Manager to replicate the infrastructure of the production environment in the testing environment. -&gt; Correct. Using Google Deployment Manager is the most effective approach here. Deployment Manager allows you to define and deploy resources in a repeatable way using configuration files, which makes it easier to recreate identical environments and scale the testing efforts.</p><p><br></p><p>Use Google Compute Engine to manually create and configure VMs for the testing environment that match the production environment. -&gt; Incorrect. While it is possible to use Google Compute Engine to manually create and configure VMs for the testing environment, this process can be labor-intensive and may not be easily reproducible or scalable.</p><p><br></p><p>Use Google Kubernetes Engine and set up a separate namespace for testing while using the same cluster as the production environment. -&gt; Incorrect. While Google Kubernetes Engine could provide an isolated environment by using namespaces, using the same cluster as the production environment could have implications on the cluster resources which could inadvertently affect the production environment.</p><p><br></p><p>Use Google Cloud Functions and create separate functions for testing. -&gt; Incorrect. Cloud Functions is a serverless execution environment for building and connecting cloud services, and it's not designed to recreate complete multi-tier environments like a testing environment.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297734,
    "question_plain": "Your company is using a Google Kubernetes Engine (GKE) to run a mission-critical web application. The application's traffic patterns are inconsistent, with significant spikes in demand at unpredictable intervals. You are tasked with ensuring the application is highly available and responsive, while maintaining cost-effectiveness. Which of the following autoscaling configurations should you use?",
    "answers": [
      "<p>Configure GKE autoscaling with CPU utilization as the primary metric, set at 80%.</p>",
      "<p>Set up a custom autoscaling policy based on network traffic, triggering scaling at 90% of peak traffic.</p>",
      "<p>Configure GKE autoscaling to scale based on memory utilization, set at 70%.</p>",
      "<p>Set up a custom autoscaling policy based on the number of incoming requests, triggering scaling after 1000 requests per second.</p>"
    ],
    "explanation": "<p>Configure GKE autoscaling with CPU utilization as the primary metric, set at 80%. -&gt;&nbsp;Correct. GKE autoscaling with CPU utilization as the primary metric, set at 80%, ensures that new pods are added when the CPU utilization exceeds 80%. This helps in managing sudden increases in traffic, thereby ensuring high availability and responsiveness of the application.</p><p><br></p><p>Set up a custom autoscaling policy based on network traffic, triggering scaling at 90% of peak traffic. -&gt; Incorrect. Autoscaling based on network traffic can be challenging to predict and manage, as traffic can be bursty and not always correlate with resource demand. In many cases, CPU utilization is a more reliable indicator of resource demand.</p><p><br></p><p>Configure GKE autoscaling to scale based on memory utilization, set at 70%. -&gt; Incorrect. Memory utilization, similar to network traffic, can be an unpredictable metric for autoscaling. If a pod is memory-intensive but not CPU-intensive, scaling based on memory utilization could lead to over-provisioning.</p><p><br></p><p>Set up a custom autoscaling policy based on the number of incoming requests, triggering scaling after 1000 requests per second. -&gt; Incorrect. Autoscaling based on the number of incoming requests can be unpredictable and may not always correlate with resource demand. For example, 1000 light requests per second might not need as much resources as 500 heavy requests per second.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297736,
    "question_plain": "You are migrating a complex application to Google Cloud. The application consists of multiple microservices, each developed by different teams. Your goal is to implement a CI/CD pipeline that allows for independent release cycles for each microservice. Which approach should you take?",
    "answers": [
      "<p>Implement a separate Cloud Build trigger for each microservice and separate the deployments.</p>",
      "<p>Use Cloud Build with a single trigger to build and deploy all microservices simultaneously.</p>",
      "<p>Use a single Cloud Function to trigger the build and deployment for all microservices.</p>",
      "<p>Use Google App Engine to deploy all microservices as a single service.</p>"
    ],
    "explanation": "<p>Implement a separate Cloud Build trigger for each microservice and separate the deployments. -&gt;&nbsp;Correct. This approach allows each microservice to be built and deployed independently, facilitating separate release cycles for each service.</p><p><br></p><p>Use Cloud Build with a single trigger to build and deploy all microservices simultaneously. -&gt;&nbsp;Incorrect. This approach does not support independent release cycles for each microservice as it will trigger a build and deployment for all services simultaneously.</p><p><br></p><p>Use a single Cloud Function to trigger the build and deployment for all microservices. -&gt;&nbsp;Incorrect. While Cloud Functions can trigger Cloud Build, using a single Cloud Function for all services does not support independent release cycles.</p><p><br></p><p>Use Google App Engine to deploy all microservices as a single service. -&gt;&nbsp;Incorrect. While App Engine is a powerful platform for running applications, deploying all microservices as a single service does not support independent release cycles.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297738,
    "question_plain": "Your organization is planning to migrate a large on-premise data warehouse to Google Cloud Platform. The data warehouse is currently hosted on a Hadoop cluster with Hive. The management decided to migrate to Google Cloud to use the managed services offered by Google. Which service should you choose?",
    "answers": [
      "<p>Use BigQuery as it provides a highly scalable, serverless, and cost-effective multi-cloud data warehouse.</p>",
      "<p>Use Cloud Bigtable as it offers low latency access to large datasets.</p>",
      "<p>Use Cloud Spanner as it provides strong transactional consistency.</p>",
      "<p>Use Cloud Storage as it provides unified object storage.</p>"
    ],
    "explanation": "<p>Use BigQuery as it provides a highly scalable, serverless, and cost-effective multi-cloud data warehouse. -&gt;&nbsp;Correct. BigQuery is designed to handle data analysis on large datasets and would be a suitable replacement for a Hadoop/Hive-based data warehouse.</p><p><br></p><p>Use Cloud Bigtable as it offers low latency access to large datasets. -&gt;&nbsp;Incorrect. Cloud Bigtable is designed for high-throughput, low-latency workloads and is typically used as a NoSQL database for operational, not analytical, workloads.</p><p><br></p><p>Use Cloud Spanner as it provides strong transactional consistency. -&gt;&nbsp;Incorrect. Cloud Spanner is a highly scalable, enterprise-grade, globally-distributed, and strongly consistent database service built for the cloud specifically to combine the benefits of relational database structure with non-relational horizontal scale. It is not designed for analytical workloads typical for a data warehouse.</p><p><br></p><p>Use Cloud Storage as it provides unified object storage. -&gt;&nbsp;Incorrect. Cloud Storage is not a database or data warehouse service. It's an object storage service for storing and accessing data on Google Cloud Platform infrastructure.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  },
  {
    "id": 81297740,
    "question_plain": "As a cloud architect, you have been tasked to design a Google Cloud Platform (GCP) resource hierarchy for an organization with multiple departments. The organization aims to limit the permissions for modifying IAM policies to the fewest number of individuals. Which design would achieve this?",
    "answers": [
      "<p>Assign IAM roles at the organization level only.</p>",
      "<p>Assign IAM roles at the project level for each department.</p>",
      "<p>Assign IAM roles at the individual resource level.</p>",
      "<p>Assign IAM roles at the folder level for each department.</p>"
    ],
    "explanation": "<p>Assign IAM roles at the organization level only. -&gt;&nbsp;Correct. Assigning IAM roles at the organization level provides centralized control and limits the number of individuals who can modify IAM policies. It ensures that only designated individuals within the organization can grant access to resources.</p><p><br></p><p>Assign IAM roles at the project level for each department. -&gt;&nbsp;Incorrect. This might allow for too many individuals to have permissions to modify IAM policies, as each project's owner could potentially modify their project's IAM policies.</p><p><br></p><p>Assign IAM roles at the individual resource level. -&gt;&nbsp;Incorrect. This would make the IAM roles management process excessively complex and error-prone, and it would not necessarily reduce the number of people who can modify IAM policies.</p><p><br></p><p>Assign IAM roles at the folder level for each department. -&gt;&nbsp;Incorrect. This could still allow for department-level users to modify IAM policies within their respective folders.</p>",
    "correct_response": ["a"],
    "assessment_type": "multiple-choice",
    "related_lectures": []
  }
]
