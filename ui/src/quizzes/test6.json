{
    "count": 70,
    "next": null,
    "previous": null,
    "results": [
        {
            "_class": "assessment",
            "id": 81297600,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you set up billing for your project and want to prevent excessive consumption of resources due to an error or malicious attack. What should you recommend to do? </p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should set up quotas for the resources that your project will be using. -&gt; By setting up quotas for the resources used in your project, you can limit the maximum amount of resources that can be used in a given period of time. This can help prevent excessive consumption of resources due to an error or malicious attack. </p><p><br></p><p>You should set up budgets and alerts in your project. -&gt; Incorrect. Budgets and alerts can help you track your spending and alert you when your spending exceeds a certain threshold, but they do not prevent excessive resource consumption. </p><p><br></p><p>You should set up a spending limit on the credit card used in your billing account. -&gt; Incorrect. It can prevent unexpected charges but may not prevent excessive resource consumption. </p><p><br></p><p>You should label all resources, regularly export the billing reports, and analyze them with BigQuery. -&gt; Incorrect. Labeling resources and analyzing billing reports with BigQuery can provide insights into resource usage patterns, but it does not prevent excessive resource consumption.</p><p><br></p><p>https://cloud.google.com/compute/quotas</p>",
                "answers": [
                    "<p>You should set up quotas for the resources that your project will be using.</p>",
                    "<p>You should set up budgets and alerts in your project.</p>",
                    "<p>You should set up a spending limit on the credit card used in your billing account.</p>",
                    "<p>You should label all resources, regularly export the billing reports, and analyze them with BigQuery.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you set up billing for your project and want to prevent excessive consumption of resources due to an error or malicious attack. What should you recommend to do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297602,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect for a multinational corporation that has a wide array of legacy applications hosted on-premises. You have been tasked to devise a strategy to migrate these applications to Google Cloud. Considering the least disruption, the nature of the applications, and the organization's business objectives, which migration strategy should you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use the Strangler pattern where a new system slowly replaces the old one over time. -&gt; Correct. The Strangler pattern involves building a new system around the edges of the old, gradually replacing it. This method reduces risk, allows feedback to influence the process, and enables gradual skill, process, and even cultural changes.</p><p><br></p><p>Lift-and-Shift strategy for all applications regardless of their complexity. -&gt; Incorrect. The Lift-and-Shift approach, although the fastest, might not be suitable for all applications due to their complexity, differences in infrastructure, and possible compatibility issues with the cloud environment.</p><p><br></p><p>Prioritize a hybrid approach, maintaining a mix of on-premises and cloud-hosted applications. -&gt; Incorrect. A hybrid approach could be beneficial in certain cases. However, this approach requires maintaining two environments (on-premises and cloud), which can be complicated and might not fully realize the benefits of moving to the cloud.</p><p><br></p><p>Migrate all applications to serverless compute options, such as Google Cloud Functions or App Engine. -&gt; Incorrect. While serverless options provide scalability and operational benefits, migrating all applications to serverless might not be feasible or cost-effective, especially if applications have specific requirements that are not well-suited for a serverless architecture.</p>",
                "answers": [
                    "<p>Use the Strangler pattern where a new system slowly replaces the old one over time.</p>",
                    "<p>Lift-and-Shift strategy for all applications regardless of their complexity.</p>",
                    "<p>Prioritize a hybrid approach, maintaining a mix of on-premises and cloud-hosted applications.</p>",
                    "<p>Migrate all applications to serverless compute options, such as Google Cloud Functions or App Engine.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect for a multinational corporation that has a wide array of legacy applications hosted on-premises. You have been tasked to devise a strategy to migrate these applications to Google Cloud. Considering the least disruption, the nature of the applications, and the organization's business objectives, which migration strategy should you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297604,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company plans to deploy a new mission-critical web application on Google Cloud Platform (GCP) and it must be highly available and scalable. As a cloud architect, you've decided to use managed instance groups (MIGs). Which features should you include in your deployment to ensure high availability and scalability?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Multi-zone deployment, autohealing, and autoscaling. -&gt;&nbsp;Correct. A multi-zone deployment helps to ensure high availability by spreading instances across multiple zones within a region. Autohealing helps maintain high availability by automatically recreating instances that become unresponsive due to health-check failures. Autoscaling helps to handle increased load by automatically adding instances to the group, and to save costs by removing unneeded instances. </p><p><br></p><p>Single-zone deployment, autohealing, and autoscaling. -&gt;&nbsp;Incorrect. While autohealing and autoscaling ensure high availability and scalability respectively, deploying instances in a single zone does not provide high availability. If the zone experiences an outage, the application becomes unavailable.</p><p><br></p><p>Multi-zone deployment, no autohealing, and autoscaling. -&gt;&nbsp;Incorrect. Although multi-zone deployment and autoscaling are used, the absence of autohealing means that if an instance becomes unresponsive, it won't be automatically recreated, affecting the application's availability.</p><p><br></p><p>Multi-zone deployment, autohealing, and no autoscaling. -&gt;&nbsp;Incorrect. While multi-zone deployment and autohealing ensure high availability, not using autoscaling would mean the application may not handle increased load efficiently.</p>",
                "answers": [
                    "<p>Multi-zone deployment, autohealing, and autoscaling.</p>",
                    "<p>Single-zone deployment, autohealing, and autoscaling.</p>",
                    "<p>Multi-zone deployment, no autohealing, and autoscaling.</p>",
                    "<p>Multi-zone deployment, autohealing, and no autoscaling.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company plans to deploy a new mission-critical web application on Google Cloud Platform (GCP) and it must be highly available and scalable. As a cloud architect, you've decided to use managed instance groups (MIGs). Which features should you include in your deployment to ensure high availability and scalability?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297606,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>As a cloud architect, you need to do a presentation on how to interact with the Google Cloud. Select all possible ways to interact with GCP.</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Platform Console -&gt; Correct. Cloud Platform Console is the web-based interface for interacting with GCP. It provides a graphical user interface (GUI) for managing resources and services. </p><p><br></p><p>Cloud Shell and Cloud SDK -&gt; Correct. Cloud Shell and Cloud SDK provide a command-line interface (CLI) for interacting with GCP. Cloud Shell is a web-based terminal that includes the Cloud SDK pre-installed, while Cloud SDK is a set of tools that allows developers to interact with GCP programmatically. </p><p><br></p><p>Cloud Console Mobile App -&gt; Correct. Cloud Console Mobile App is a mobile application that allows users to manage their GCP resources and services from their mobile devices. </p><p><br></p><p>REST-based API -&gt; Correct. REST-based API allows developers to interact with GCP programmatically using HTTP requests. This allows developers to build custom applications that integrate with GCP. </p><p><br></p><p>CloudFormation -&gt; Incorrect. CloudFormation is not a valid option as it is an AWS service and not a GCP service.</p><p><br></p><p>https://cloud.google.com/docs/overview#ways_to_interact_with_the_services</p>",
                "answers": [
                    "<p>Cloud Platform Console</p>",
                    "<p>Cloud Shell and Cloud SDK</p>",
                    "<p>Cloud Console Mobile App</p>",
                    "<p>REST-based API</p>",
                    "<p>CloudFormation</p>"
                ]
            },
            "correct_response": [
                "a",
                "b",
                "c",
                "d"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you need to do a presentation on how to interact with the Google Cloud. Select all possible ways to interact with GCP.",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297608,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>Users of your application complaints about long wait times while loading application pages with images. As a cloud architect, you want to reduce latency. Which of the following options would you use? (select 2)</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Multi-Regional Storage -&gt; Correct. Multi-Regional Storage is a good option because it enables the application to serve images from a storage location that is geographically closer to the user, reducing the latency of image loading.</p><p><br></p><p>Cloud CDN -&gt; Correct. Cloud CDN is also a good option because it caches images at edge locations closer to the user, reducing the latency and the number of requests to the application's origin server.</p><p><br></p><p>Coldline Storage -&gt; Incorrect. Coldline Storage is not a good option for reducing latency in this scenario because it is designed for infrequently accessed data, and accessing it may take longer due to retrieval times.</p><p><br></p><p>Cloud Pub/Sub -&gt; Incorrect. Cloud Pub/Sub is not a good option for reducing latency in this scenario because it is a messaging service for asynchronous communication between services and does not help with image loading times.</p><p><br></p><p>Cloud VPN -&gt; Incorrect. Cloud VPN is not a good option for reducing latency in this scenario because it is a service that provides secure connectivity between on-premises networks and Google Cloud resources and does not help with image loading times.</p><p><br></p><p>https://cloud.google.com/storage/docs/storage-classes#legacy</p><p>https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview</p>",
                "answers": [
                    "<p>Multi-Regional Storage</p>",
                    "<p>Cloud CDN</p>",
                    "<p>Coldline Storage</p>",
                    "<p>Cloud Pub/Sub</p>",
                    "<p>Cloud VPN</p>"
                ]
            },
            "correct_response": [
                "a",
                "b"
            ],
            "section": "",
            "question_plain": "Users of your application complaints about long wait times while loading application pages with images. As a cloud architect, you want to reduce latency. Which of the following options would you use? (select 2)",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297610,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are working as a cloud architect for a large organization. The organization is structured into various departments and you have been tasked with assigning the correct Identity and Access Management (IAM) roles to ensure proper access control. The sales team in your organization has requested access to view and download sales data stored in Cloud Storage but they should not be allowed to delete or modify any data. What IAM role should you assign to the sales team?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Storage Object Viewer -&gt;&nbsp;Correct. The Storage Object Viewer role allows for read-only access to objects in Google Cloud Storage. This fits the requirements of the sales team to view and download data, without the ability to modify or delete it.</p><p><br></p><p>Storage Admin -&gt;&nbsp;Incorrect. The Storage Admin role grants full control over objects and buckets, including the ability to delete and modify data. This would provide more access than is required.</p><p><br></p><p>Storage Object Creator -&gt;&nbsp;Incorrect. The Storage Object Creator role allows for the creation of new objects but does not provide read or download permissions on existing objects. This would not meet the needs of the sales team.</p><p><br></p><p>Storage Object Admin -&gt;&nbsp;Incorrect. The Storage Object Admin role grants full control over objects, including the ability to delete and modify data. This would provide more access than is required.</p>",
                "answers": [
                    "<p>Storage Object Viewer</p>",
                    "<p>Storage Admin</p>",
                    "<p>Storage Object Creator</p>",
                    "<p>Storage Object Admin</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are working as a cloud architect for a large organization. The organization is structured into various departments and you have been tasked with assigning the correct Identity and Access Management (IAM) roles to ensure proper access control. The sales team in your organization has requested access to view and download sales data stored in Cloud Storage but they should not be allowed to delete or modify any data. What IAM role should you assign to the sales team?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297612,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a Google Professional Cloud Architect working with a large e-commerce company. The company wants to optimize its data processing workflows by implementing Google Cloud Dataflow. They aim to leverage the power of Dataflow to process large volumes of streaming and batch data efficiently and reliably. In the context of this complex scenario, which of the following statements about Google Cloud Dataflow is correct?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Dataflow pipelines are written using Apache Beam, an open-source unified programming model for batch and stream processing. -&gt; Correct. Dataflow pipelines are written using Apache Beam, an open-source unified programming model. Apache Beam provides a consistent API for developing data processing pipelines, enabling developers to write code once and run it on multiple processing engines, including Cloud Dataflow.</p><p><br></p><p>Cloud Dataflow is a managed service that supports only batch processing of data. -&gt; Incorrect. Cloud Dataflow is a fully managed service provided by Google Cloud that supports both batch and streaming data processing. It offers flexibility in handling different data processing requirements.</p><p><br></p><p>Dataflow pipelines can only process data stored in Cloud Storage and Cloud Bigtable. -&gt; Incorrect. Dataflow pipelines can process data from various sources, including Cloud Storage, Cloud Pub/Sub, Cloud BigQuery, and external systems using connectors. It is not limited to Cloud Storage and Cloud Bigtable alone.</p><p><br></p><p>Dataflow is primarily designed for small-scale data processing and may not handle high-volume data efficiently. -&gt; Incorrect. Dataflow is designed to handle both small-scale and large-scale data processing. It can scale automatically to process high-volume data efficiently and reliably.</p>",
                "answers": [
                    "<p>Dataflow pipelines are written using Apache Beam, an open-source unified programming model for batch and stream processing.</p>",
                    "<p>Cloud Dataflow is a managed service that supports only batch processing of data.</p>",
                    "<p>Dataflow pipelines can only process data stored in Cloud Storage and Cloud Bigtable.</p>",
                    "<p>Dataflow is primarily designed for small-scale data processing and may not handle high-volume data efficiently.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a Google Professional Cloud Architect working with a large e-commerce company. The company wants to optimize its data processing workflows by implementing Google Cloud Dataflow. They aim to leverage the power of Dataflow to process large volumes of streaming and batch data efficiently and reliably. In the context of this complex scenario, which of the following statements about Google Cloud Dataflow is correct?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297614,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you work with Kubernetes in your new cloud project. Select progression of abstraction from the lowest to the highest level in Kubernetes.</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Pods -&gt; Deployments -&gt;&nbsp;Services -&gt; Correct. In Kubernetes, a Pod is the smallest and simplest unit in the object model and it represents a single instance of a running process in a cluster. A Deployment is a higher-level abstraction that manages Pods and provides declarative updates to their configurations, allowing them to be scaled up or down. Services define a logical set of Pods and enable network access to them, allowing the communication between different components in a cluster. So the correct progression of abstraction is Pods -&gt; Deployments -&gt; Services.</p><p><br></p><p>https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/</p><p>https://kubernetes.io/docs/concepts/overview/components/</p><p>https://cloud.google.com/kubernetes-engine/docs/quickstart</p>",
                "answers": [
                    "<p>Pods -&gt; Deployments -&gt;&nbsp;Services</p>",
                    "<p>Deployments -&gt;&nbsp;Pods -&gt;&nbsp;Services</p>",
                    "<p>Deployments -&gt;&nbsp;Services -&gt;&nbsp;Pods</p>",
                    "<p>Pods -&gt;&nbsp;Services -&gt; Deployments</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you work with Kubernetes in your new cloud project. Select progression of abstraction from the lowest to the highest level in Kubernetes.",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297616,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are working on a project for a large multinational company that has numerous APIs with different processing requirements. The architecture team decided to host each API on a separate set of instances and use a single Global HTTP(S) Load Balancer to route requests to the appropriate backend. To ensure the requests reach the right backend service, what should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use separate backend services for each API path and configure URL maps to route requests. -&gt; Correct. URL maps in Google Cloud's HTTP(S) Load Balancer allows traffic to be routed based on the URL of the request. Each host and path rule directs a set of URLs to one backend service.</p><p><br></p><p>Assign different static external IP addresses to each API backend. -&gt; Incorrect. Using different static external IP addresses for each API backend would not make sense as we're trying to have a single point of entry (load balancer) to manage all API paths.</p><p><br></p><p>Create separate subnetworks for each API backend and route requests based on the subnetwork IP range. -&gt; Incorrect. Routing requests based on the subnetwork IP range is not applicable here. Load balancer configuration should handle the request routing based on the request content (URL), not the backend network configuration.</p><p><br></p><p>Create a separate VPC network for each API path and configure the Load Balancer to route based on VPC. -&gt; Incorrect. Creating a separate VPC for each API path is an unnecessary over-complication. Routing in Load Balancer is not based on VPCs but on HTTP request parameters such as host and path.</p>",
                "answers": [
                    "<p>Use separate backend services for each API path and configure URL maps to route requests.</p>",
                    "<p>Assign different static external IP addresses to each API backend.</p>",
                    "<p>Create separate subnetworks for each API backend and route requests based on the subnetwork IP range.</p>",
                    "<p>Create a separate VPC network for each API path and configure the Load Balancer to route based on VPC.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are working on a project for a large multinational company that has numerous APIs with different processing requirements. The architecture team decided to host each API on a separate set of instances and use a single Global HTTP(S) Load Balancer to route requests to the appropriate backend. To ensure the requests reach the right backend service, what should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297618,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>Your objective is to decrease the frequency of unscheduled rollbacks for flawed production deployments within your company's web hosting platform. By enhancing QA/Test processes, you were able to achieve a substantial 80% reduction in rollbacks. Now, what are two additional approaches you can adopt to further minimize the occurrence of rollbacks?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement a green-blue deployment strategy. -&gt; Correct. Green-blue deployment is a technique that reduces downtime by creating two identical production environments. While one environment is active and serving requests, the other is on standby, waiting for the next deployment. Once the new deployment is tested and verified, traffic is routed to the new environment, and the old one is decommissioned. This method can significantly reduce the chances of unscheduled rollbacks.</p><p><br></p><p>Decompose the monolithic platform into microservices. -&gt; Correct. Microservices architecture breaks down large applications into smaller, independent components, each with its own development, deployment, and scaling process. This approach can help to isolate issues and prevent them from affecting the entire platform, making it easier to identify and resolve issues before they become catastrophic. It also makes it easier to roll back a single component without affecting the entire platform.</p><p><br></p><p>Replace the QA environment with canary releases. -&gt; Incorrect. Canary releases involve rolling out new features or changes to a small subset of users or servers, rather than the entire user base. This allows you to test the new features in a real-world environment without risking a full-scale rollout. However, this approach may not be effective in reducing rollbacks because it still involves some level of risk.</p><p><br></p><p>Reduce the platform's dependency on relational database systems. -&gt; Incorrect. While this may improve the platform's performance, it may not necessarily reduce unscheduled rollbacks.</p><p><br></p><p>Replace the platform's relational database systems with a NoSQL database. -&gt; Incorrect. This may improve the platform's performance, but it may not necessarily reduce unscheduled rollbacks.</p><p><br></p><p>You can introduce a green-blue deployment model and fragment the monolithic platform into microservices.</p>",
                "answers": [
                    "<p>Implement a green-blue deployment strategy.</p>",
                    "<p>Decompose the monolithic platform into microservices.</p>",
                    "<p>Replace the QA environment with canary releases.</p>",
                    "<p>Reduce the platform's dependency on relational database systems.</p>",
                    "<p>Replace the platform's relational database systems with a NoSQL database.</p>"
                ]
            },
            "correct_response": [
                "a",
                "b"
            ],
            "section": "",
            "question_plain": "Your objective is to decrease the frequency of unscheduled rollbacks for flawed production deployments within your company's web hosting platform. By enhancing QA/Test processes, you were able to achieve a substantial 80% reduction in rollbacks. Now, what are two additional approaches you can adopt to further minimize the occurrence of rollbacks?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297620,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect at a multinational company and have been asked to set up an HTTP(S) load balancer to route traffic to backends in multiple regions. However, the company wants to ensure that user requests are always routed to the closest healthy backend to minimize latency. Additionally, there should be a fallback mechanism if the closest backend is unhealthy. Which of the following strategies would you implement to fulfill these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Configure HTTP(S) Load Balancer with multiple backend services in each region and implement Global Load Balancing. -&gt; Correct. Creating multiple backend services in each region and implementing Global Load Balancing allows traffic to be directed to the closest healthy backend. If a backend within the closest region is unhealthy, Global Load Balancing would direct traffic to the next closest healthy backend, thereby providing a fallback mechanism.</p><p><br></p><p>Configure HTTP(S) Load Balancer with a single backend service and health checks. -&gt; Incorrect. While health checks are important, a single backend service doesn't support the multi-region requirement.</p><p><br></p><p>Configure HTTP(S) Load Balancer with multiple backend services in each region and utilize Cross-Region Load Balancing. -&gt; Incorrect. Cross-Region Load Balancing enables serving content from the geographically closest backend. However, it does not inherently provide a fallback mechanism.</p><p><br></p><p>Configure HTTP(S) Load Balancer with a single global backend service and use Global Load Balancing. -&gt; Incorrect. While Global Load Balancing does provide a fallback mechanism, a single global backend service does not meet the requirement of routing requests to the closest healthy backend.</p>",
                "answers": [
                    "<p>Configure HTTP(S) Load Balancer with multiple backend services in each region and implement Global Load Balancing.</p>",
                    "<p>Configure HTTP(S) Load Balancer with a single backend service and health checks.</p>",
                    "<p>Configure HTTP(S) Load Balancer with multiple backend services in each region and utilize Cross-Region Load Balancing.</p>",
                    "<p>Configure HTTP(S) Load Balancer with a single global backend service and use Global Load Balancing.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect at a multinational company and have been asked to set up an HTTP(S) load balancer to route traffic to backends in multiple regions. However, the company wants to ensure that user requests are always routed to the closest healthy backend to minimize latency. Additionally, there should be a fallback mechanism if the closest backend is unhealthy. Which of the following strategies would you implement to fulfill these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297622,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are working for an ad tech company that requires both real-time and batch processing of ad click data. The data needs to be analyzed in real-time for near instant reporting to advertisers, but also needs to be processed in a batch for daily and monthly reporting. Which combination of GCP services would be the most efficient for this use case?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Dataflow with both batch and streaming pipelines for processing data. -&gt; Correct. Cloud Dataflow is designed for both batch and stream data processing paradigms and can handle these use cases efficiently.</p><p><br></p><p>Use Pub/Sub for real-time processing and Cloud Functions for batch processing. -&gt; Incorrect. Pub/Sub is well-suited for real-time processing but Cloud Functions is event-driven and may not be best suited for batch processing.</p><p><br></p><p>Use BigQuery for real-time processing and Cloud Storage for batch processing. -&gt; Incorrect. BigQuery is an analytic data warehouse and while it can handle streaming data, it's not designed for real-time processing. Cloud Storage is an object storage service and doesn't inherently provide batch processing capabilities.</p><p><br></p><p>Use Firestore for real-time processing and Cloud Spanner for batch processing. -&gt; Incorrect. Firestore is a NoSQL database for building web, mobile, and server applications. It is not designed for real-time data processing. Cloud Spanner is a fully managed relational database service and doesn't inherently provide batch processing capabilities.</p>",
                "answers": [
                    "<p>Use Dataflow with both batch and streaming pipelines for processing data.</p>",
                    "<p>Use Pub/Sub for real-time processing and Cloud Functions for batch processing.</p>",
                    "<p>Use BigQuery for real-time processing and Cloud Storage for batch processing.</p>",
                    "<p>Use Firestore for real-time processing and Cloud Spanner for batch processing.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are working for an ad tech company that requires both real-time and batch processing of ad click data. The data needs to be analyzed in real-time for near instant reporting to advertisers, but also needs to be processed in a batch for daily and monthly reporting. Which combination of GCP services would be the most efficient for this use case?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297624,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, your client has informed you that their recently updated App Engine application is experiencing prolonged loading times of around 30 seconds for certain users. This issue was not present prior to the update. What approach should you adopt to address this problem effectively?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should roll back to an earlier known good release initially, then use Cloud Trace and Logging to diagnose the problem in a development/test/staging environment. -&gt; Correct. Rolling back to an earlier known good release is a recommended approach to address the issue of prolonged loading times. By reverting to a previous version of the application that did not have this problem, you can quickly restore the desired performance. Afterward, using Cloud Trace and Logging in a development/test/staging environment allows you to diagnose the problem and identify the specific cause of the issue.</p><p><br></p><p>You should work with your Internet Service Provider (ISP) to diagnose the problem. -&gt;&nbsp;Incorrect. It may be a consideration if the issue is related to network connectivity or external factors. However, given that the problem started after the application update, it is more likely that the root cause lies within the application itself.</p><p><br></p><p>You should open a support ticket to ask for network capture and flow data to diagnose the problem, then roll back your application. -&gt;&nbsp;Incorrect. Opening a support ticket to request network capture and flow data may be an option to investigate network-related issues. However, it is important to first roll back to a previous version of the application to restore performance. Additionally, analyzing network data alone may not provide sufficient insights into the specific application-level issue causing the prolonged loading times.</p><p><br></p><p>You should roll back to an earlier known good release, then push the release again at a quieter period to investigate. Then use Cloud Trace and Logging to diagnose the problem. -&gt;&nbsp;Incorrect. Rolling back to an earlier known good release is a recommended approach to address the immediate performance issue. Pushing the release again at a quieter period to investigate allows you to observe the application behavior under different conditions. Using Cloud Trace and Logging can then help diagnose the problem. However, it is advisable to first roll back and restore performance before proceeding with further investigation.</p><p><br></p><p>https://cloud.google.com/logging/</p><p>https://cloud.google.com/trace/docs</p>",
                "answers": [
                    "<p>You should roll back to an earlier known good release initially, then use Cloud Trace and Logging to diagnose the problem in a development/test/staging environment.</p>",
                    "<p>You should work with your Internet Service Provider (ISP) to diagnose the problem.</p>",
                    "<p>You should open a support ticket to ask for network capture and flow data to diagnose the problem, then roll back your application.</p>",
                    "<p>You should roll back to an earlier known good release, then push the release again at a quieter period to investigate. Then use Cloud Trace and Logging to diagnose the problem.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, your client has informed you that their recently updated App Engine application is experiencing prolonged loading times of around 30 seconds for certain users. This issue was not present prior to the update. What approach should you adopt to address this problem effectively?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297626,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect of a global online retail company that uses a production database hosted on Cloud SQL. You received an alert that the database is about to run out of storage space. What is the best approach to ensure that the production database does not run out of storage and there is minimal impact on the performance of the application?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Enable automatic storage increases for the database instance. -&gt;&nbsp;Correct. Enabling automatic storage increases for the database instance is the best approach as it allows the system to automatically add storage capacity when the available storage falls below a certain threshold. This allows the application to continue functioning without interruption.</p><p><br></p><p>Increase the size of the database instance manually. -&gt; Incorrect. Increasing the size of the database instance manually could result in downtime and would require constant monitoring of the storage usage. </p><p><br></p><p>Export the data to BigQuery and delete the data from the production database. -&gt; Incorrect. Exporting data to BigQuery and deleting it from the production database is not recommended as it could lead to data loss and inconsistency between the production database and the backup in BigQuery.</p><p><br></p><p>Create a snapshot of the database, delete some data from the production database, and then restore the data from the snapshot if needed. -&gt; Incorrect. Creating a snapshot and deleting some data from the production database is risky as it could lead to data loss. In addition, this approach does not provide a long-term solution to the problem of storage space running out.</p>",
                "answers": [
                    "<p>Enable automatic storage increases for the database instance.</p>",
                    "<p>Increase the size of the database instance manually.</p>",
                    "<p>Export the data to BigQuery and delete the data from the production database.</p>",
                    "<p>Create a snapshot of the database, delete some data from the production database, and then restore the data from the snapshot if needed.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect of a global online retail company that uses a production database hosted on Cloud SQL. You received an alert that the database is about to run out of storage space. What is the best approach to ensure that the production database does not run out of storage and there is minimal impact on the performance of the application?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297628,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect for a rapidly growing e-commerce company, you are tasked with handling Payment Card Industry Data Security Standard (PCI DSS) compliance for the storage and processing of payment card data. The company uses Compute Engine for their application servers, Cloud SQL for their transaction databases, and Cloud Storage for long-term data retention. Which of the following strategies is the most suitable to address this requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Utilize Google Cloud's Data Loss Prevention (DLP) API to discover, classify, and de-identify sensitive data. -&gt;&nbsp;Correct. The Data Loss Prevention (DLP) API is designed to help discover, classify, and de-identify sensitive data, which are key aspects of PCI DSS compliance. This is the best approach to handle sensitive payment card data, ensuring that this data is properly identified and managed wherever it might exist within the Google Cloud environment.</p><p><br></p><p>Implement a third-party key management system and utilize customer-supplied encryption keys for all storage systems. -&gt; Incorrect. Using a third-party key management system and customer-supplied encryption keys can provide an additional layer of control and security. However, this alone doesn't satisfy the broad requirements of PCI DSS compliance, which includes protecting data in transit and use, maintaining a vulnerability management program, and implementing strong access control measures.</p><p><br></p><p>Store all payment card data in Cloud Storage buckets configured with uniform bucket-level access. -&gt; Incorrect. Storing payment card data in Cloud Storage buckets with uniform bucket-level access can simplify permission management. However, this doesn't address many other requirements of PCI DSS compliance, such as protecting data in transit and use, vulnerability management, and regular security assessments.</p><p><br></p><p>Restrict network access to all services using Firewall Rules and Cloud Armor. -&gt; Incorrect. While restricting network access is an important security measure, it is just one aspect of PCI DSS compliance. It doesn't address the need for secure handling, storage, and transmission of payment card data, among other requirements.</p>",
                "answers": [
                    "<p>Utilize Google Cloud's Data Loss Prevention (DLP) API to discover, classify, and de-identify sensitive data.</p>",
                    "<p>Implement a third-party key management system and utilize customer-supplied encryption keys for all storage systems.</p>",
                    "<p>Store all payment card data in Cloud Storage buckets configured with uniform bucket-level access.</p>",
                    "<p>Restrict network access to all services using Firewall Rules and Cloud Armor.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect for a rapidly growing e-commerce company, you are tasked with handling Payment Card Industry Data Security Standard (PCI DSS) compliance for the storage and processing of payment card data. The company uses Compute Engine for their application servers, Cloud SQL for their transaction databases, and Cloud Storage for long-term data retention. Which of the following strategies is the most suitable to address this requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297630,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect working for a large e-commerce company. The company is generating massive amounts of clickstream data from its online platform and wants to implement an efficient and scalable solution for storing and analyzing this data. The primary goal is to gain insights into user behavior and improve the overall user experience. Which of the following approaches would be the most suitable for storing and analyzing the clickstream data in this complex scenario?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Storage for storing the raw clickstream data and BigQuery for performing real-time analytics. -&gt; Correct. It is a suitable choice as it leverages Cloud Storage for storing the raw clickstream data, which provides durability and scalability. BigQuery is a powerful analytics tool that can handle real-time queries on large datasets, making it an ideal choice for analyzing the clickstream data.</p><p><br></p><p>Store the raw clickstream data in Cloud Spanner and use Cloud Dataflow for batch processing and analysis. -&gt; Incorrect. It is not the best choice for this scenario. Cloud Spanner is a distributed relational database, which may not be the most efficient solution for storing unstructured clickstream data. Cloud Dataflow is suitable for data processing but may not be the best fit for analyzing clickstream data.</p><p><br></p><p>Implement a serverless architecture using Cloud Functions to directly process and store the clickstream data in Cloud Firestore. -&gt;&nbsp;Incorrect. It is not the optimal approach for this scenario. While Cloud Functions can be used to process and store the clickstream data, Cloud Firestore may not be the most suitable database for complex analysis and querying of the data.</p><p><br></p><p>Set up a self-managed Apache Hadoop cluster on Compute Engine to handle the storage and analysis of the clickstream data. -&gt; Incorrect. It is not the recommended choice. Setting up a self-managed Apache Hadoop cluster on Compute Engine would require significant manual management and may not provide the scalability and ease of use offered by Google Cloud's fully managed services.</p>",
                "answers": [
                    "<p>Use Cloud Storage for storing the raw clickstream data and BigQuery for performing real-time analytics.</p>",
                    "<p>Store the raw clickstream data in Cloud Spanner and use Cloud Dataflow for batch processing and analysis.</p>",
                    "<p>Implement a serverless architecture using Cloud Functions to directly process and store the clickstream data in Cloud Firestore.</p>",
                    "<p>Set up a self-managed Apache Hadoop cluster on Compute Engine to handle the storage and analysis of the clickstream data.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect working for a large e-commerce company. The company is generating massive amounts of clickstream data from its online platform and wants to implement an efficient and scalable solution for storing and analyzing this data. The primary goal is to gain insights into user behavior and improve the overall user experience. Which of the following approaches would be the most suitable for storing and analyzing the clickstream data in this complex scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297632,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is considering an Infrastructure as Code (IaC) approach for deploying resources on Google Cloud Platform (GCP) and has a requirement for creating reusable templates for resource provisioning. As a cloud architect, which tool would you recommend that best aligns with their requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Google Cloud Deployment Manager -&gt;&nbsp;Correct. Google Cloud Deployment Manager allows you to specify all the resources needed for your application in a declarative format using yaml. You can also create reusable templates using Python or Jinja2.</p><p><br></p><p>Google Cloud Console -&gt;&nbsp;Incorrect. Google Cloud Console is an interface to manage Google Cloud resources. While it can be used to create and manage resources, it does not support the creation of reusable templates or the IaC approach.</p><p><br></p><p>Google Cloud SDK -&gt;&nbsp;Incorrect. While you can script resource creation with the gcloud command-line tool, it does not natively support the creation of reusable templates, making it less optimal for IaC practices.</p><p><br></p><p>Cloud Build -&gt;&nbsp;Incorrect. Cloud Build is a service that executes your builds on Google Cloud. While it can be used in conjunction with other services to implement IaC, it doesn't natively provide reusable template functionality.</p>",
                "answers": [
                    "<p>Google Cloud Deployment Manager</p>",
                    "<p>Google Cloud Console</p>",
                    "<p>Google Cloud SDK</p>",
                    "<p>Cloud Build</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company is considering an Infrastructure as Code (IaC) approach for deploying resources on Google Cloud Platform (GCP) and has a requirement for creating reusable templates for resource provisioning. As a cloud architect, which tool would you recommend that best aligns with their requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297634,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company's website experiences high traffic during business hours and almost no traffic during the night. The website is hosted on the Google Cloud Platform, and you have been asked to optimize the configuration for cost efficiency without sacrificing availability during peak hours. Which of the following actions would be the best practice to achieve this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Deploy a Managed Instance Group (MIG) and configure autoscaling based on the CPU utilization. -&gt; Correct. Deploying a Managed Instance Group and configuring autoscaling based on CPU utilization is an efficient way to handle variable traffic. The number of instances will increase during peak hours to handle the load, and decrease during periods of low traffic to save costs.</p><p><br></p><p>Use Compute Engine instances with maximum possible machine type for higher capacity. -&gt; Incorrect. Using instances with the maximum machine type may lead to overprovisioning and unnecessary costs during periods of low traffic. It is generally more cost-effective to scale out (add more instances) than to scale up (use bigger instances).</p><p><br></p><p>Keep all the Compute Engine instances running 24/7 to avoid startup delays. -&gt; Incorrect. Keeping all Compute Engine instances running 24/7 will lead to unnecessary costs, especially during periods of low traffic. It's more cost-effective to turn off or delete unneeded instances, especially if they are not covered by committed use contracts.</p><p><br></p><p>Use Cloud Storage to serve the website content instead of Compute Engine instances. -&gt;&nbsp;Incorrect. Using Cloud Storage to serve the website might be a good choice for static websites, but it is not suitable for dynamic websites that require server-side processing. Therefore, this option is not generally applicable.</p>",
                "answers": [
                    "<p>Deploy a Managed Instance Group (MIG) and configure autoscaling based on the CPU utilization.</p>",
                    "<p>Use Compute Engine instances with maximum possible machine type for higher capacity.</p>",
                    "<p>Keep all the Compute Engine instances running 24/7 to avoid startup delays.</p>",
                    "<p>Use Cloud Storage to serve the website content instead of Compute Engine instances.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company's website experiences high traffic during business hours and almost no traffic during the night. The website is hosted on the Google Cloud Platform, and you have been asked to optimize the configuration for cost efficiency without sacrificing availability during peak hours. Which of the following actions would be the best practice to achieve this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297636,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is planning to shift their operations to Google Cloud Platform. As a cloud architect, you are tasked with setting up the environment following the best practices. Which of the following is the best strategy to implement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement IAM policies at the organizational level and fine-tune them at the project level. -&gt; Correct. IAM policies are inheritable and hierarchical, so it's a good practice to set broad policies at the organization level and then fine-tune them at the project level. This aligns with the principle of least privilege, ensuring that users only have the permissions necessary to perform their tasks.</p><p><br></p><p>Grant all users the 'Owner' role for simplicity and ease of access. -&gt; Incorrect. This is not a good practice as it contradicts the principle of least privilege. Granting everyone the 'Owner' role can lead to accidental deletions or modifications, and it opens the possibility for a major security breach.</p><p><br></p><p>Create a single Virtual Private Cloud (VPC) network for all the company's projects and applications. -&gt; Incorrect. Having a single VPC for all projects and applications may not be ideal due to potential conflicts with firewall rules, IP ranges, and it could create an unnecessary single point of failure.</p><p><br></p><p>Employ a flat project structure where all resources are deployed in a single project for centralized control. -&gt; Incorrect. A flat project structure is not considered a best practice in GCP. It's better to have multiple projects, segregating resources based on their purpose, team, or environment (production, staging, development, etc.), providing better control, security, and billing tracking.</p>",
                "answers": [
                    "<p>Implement IAM policies at the organizational level and fine-tune them at the project level.</p>",
                    "<p>Grant all users the 'Owner' role for simplicity and ease of access.</p>",
                    "<p>Create a single Virtual Private Cloud (VPC) network for all the company's projects and applications.</p>",
                    "<p>Employ a flat project structure where all resources are deployed in a single project for centralized control.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company is planning to shift their operations to Google Cloud Platform. As a cloud architect, you are tasked with setting up the environment following the best practices. Which of the following is the best strategy to implement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297638,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization has decided to run its LAMP (Linux, Apache, MySQL, PHP) stack on Google Cloud Platform. They need a solution that is scalable, easy to manage, and highly available. Which of the following options would you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Kubernetes Engine to manage the Apache and PHP components in separate containers, with Cloud SQL for MySQL. -&gt;&nbsp;Correct. Using Google Kubernetes Engine for managing Apache and PHP components in separate containers, with Cloud SQL for MySQL database, is a robust and scalable solution. It provides high availability, easy management, and the necessary scalability.</p><p><br></p><p>Deploy each component of the LAMP stack on separate Compute Engine instances. -&gt; Incorrect. Deploying each component of the LAMP stack on separate Compute Engine instances would increase management overhead and might not provide the desired level of scalability and high availability.</p><p><br></p><p>Use Cloud Functions to host the PHP application and Cloud SQL for MySQL, with Apache running on a Compute Engine instance. -&gt; Incorrect. While Cloud Functions and Cloud SQL could be used for hosting PHP applications and MySQL databases respectively, Apache cannot be effectively run on Compute Engine for scalable and resilient web serving needs in a LAMP stack scenario.</p><p><br></p><p>Use Cloud Run to deploy the Apache and PHP components, with Firestore for MySQL. -&gt; Incorrect. Cloud Run is a managed compute platform that enables you to run stateless containers, and Firestore is a NoSQL document database, both are not suitable for running a traditional LAMP stack.</p>",
                "answers": [
                    "<p>Use Kubernetes Engine to manage the Apache and PHP components in separate containers, with Cloud SQL for MySQL.</p>",
                    "<p>Deploy each component of the LAMP stack on separate Compute Engine instances.</p>",
                    "<p>Use Cloud Functions to host the PHP application and Cloud SQL for MySQL, with Apache running on a Compute Engine instance.</p>",
                    "<p>Use Cloud Run to deploy the Apache and PHP components, with Firestore for MySQL.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization has decided to run its LAMP (Linux, Apache, MySQL, PHP) stack on Google Cloud Platform. They need a solution that is scalable, easy to manage, and highly available. Which of the following options would you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81298112,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is planning to deploy a new web application on Google Cloud Platform (GCP). The application is expected to have fluctuating usage patterns and significant spikes in traffic. The development team is committed to following best practices for continuous integration and continuous deployment (CI/CD) to enhance the application’s scalability and manageability. As a cloud architect, you are tasked with recommending a CI/CD strategy that ensures the application is always available, scalable, and up to date. Which of the following CI/CD strategies is most appropriate for this scenario?</p>",
                "relatedLectureIds": [],
                "links": [],
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Configure a blue-green deployment model using Kubernetes Engine to allow testing in a live environment before full rollout. -&gt;&nbsp;Correct. A blue-green deployment model using Kubernetes Engine effectively supports CI/CD by enabling two identical environments where one (green) can serve as a live testing environment before switching traffic to it (turning it blue). This approach allows for immediate rollbacks if issues arise and supports high availability and scalability.</p><p><br></p><p>Use Cloud Build to automate deployments, employing Cloud Functions for lightweight processing tasks that scale automatically. -&gt; Incorrect. While Cloud Build and Cloud Functions are useful tools, this strategy may not fully address the application's need for managing significant traffic spikes, as Cloud Functions are primarily used for event-driven and asynchronous tasks.</p><p><br></p><p>Manually deploy updated versions to App Engine standard environment during off-peak hours to minimize disruption. -&gt; Incorrect. Manual deployments during off-peak hours do not leverage the benefits of CI/CD for continuous updates and automation, making this approach less suitable for dynamic scaling and frequent updates.</p><p><br></p><p>Implement a rolling update strategy using Compute Engine managed instance groups to ensure zero downtime during deployments. -&gt; Incorrect. Rolling updates with Compute Engine managed instance groups provide seamless updates but might not be the most efficient way to handle unpredictable, high-traffic spikes as it's primarily VM-based and not as agile as containerized solutions.</p>",
                "answers": [
                    "<p>Configure a blue-green deployment model using Kubernetes Engine to allow testing in a live environment before full rollout.</p>",
                    "<p>Use Cloud Build to automate deployments, employing Cloud Functions for lightweight processing tasks that scale automatically.</p>",
                    "<p>Manually deploy updated versions to App Engine standard environment during off-peak hours to minimize disruption.</p>",
                    "<p>Implement a rolling update strategy using Compute Engine managed instance groups to ensure zero downtime during deployments.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company is planning to deploy a new web application on Google Cloud Platform (GCP). The application is expected to have fluctuating usage patterns and significant spikes in traffic. The development team is committed to following best practices for continuous integration and continuous deployment (CI/CD) to enhance the application’s scalability and manageability. As a cloud architect, you are tasked with recommending a CI/CD strategy that ensures the application is always available, scalable, and up to date. Which of the following CI/CD strategies is most appropriate for this scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297642,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect at a software company that has an application deployed on Google Cloud. The application has been experiencing performance issues both in the testing and production environments. The DevOps team is unsure if the problem is due to the application's code or the underlying infrastructure. You've been asked to identify an efficient way to isolate and diagnose these performance issues. Which approach would you suggest?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Profiler to identify performance bottlenecks in the code, while also leveraging Cloud Monitoring and Logging to analyze infrastructure performance. -&gt;&nbsp;Correct. Cloud Profiler allows you to analyze how your application's code runs in production and can help you identify performance bottlenecks in the code. Cloud Monitoring and Logging can be used in parallel to analyze infrastructure metrics and logs, allowing you to determine if the infrastructure is affecting performance. This combination provides a comprehensive solution for diagnosing performance issues.</p><p><br></p><p>Create a detailed log for every function call in the application code to identify any bottlenecks. -&gt; Incorrect. While logging function calls can be useful for debugging, it is not efficient for identifying performance issues as it may significantly impact the application's performance and create a large volume of logs.</p><p><br></p><p>Upgrade the machine types of all Compute Engine instances in the project to increase performance. -&gt; Incorrect. Upgrading machine types without first identifying the cause of the performance issues may result in unnecessary costs and may not solve the problem if it is not related to the infrastructure.</p><p><br></p><p>Use only Cloud Monitoring to analyze both the code and the infrastructure performance. -&gt; Incorrect. While Cloud Monitoring is a powerful tool for analyzing infrastructure performance, it does not provide insights into how the application's code runs. It is therefore not sufficient on its own for diagnosing the reported performance issues.</p>",
                "answers": [
                    "<p>Use Cloud Profiler to identify performance bottlenecks in the code, while also leveraging Cloud Monitoring and Logging to analyze infrastructure performance.</p>",
                    "<p>Create a detailed log for every function call in the application code to identify any bottlenecks.</p>",
                    "<p>Upgrade the machine types of all Compute Engine instances in the project to increase performance.</p>",
                    "<p>Use only Cloud Monitoring to analyze both the code and the infrastructure performance.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect at a software company that has an application deployed on Google Cloud. The application has been experiencing performance issues both in the testing and production environments. The DevOps team is unsure if the problem is due to the application's code or the underlying infrastructure. You've been asked to identify an efficient way to isolate and diagnose these performance issues. Which approach would you suggest?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297644,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you have observed that a few API requests in your microservices application experience significant delays. You are aware that each API request may pass through multiple services. To identify the specific service causing the longest delays in such cases, what course of action should you pursue?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Instrument your application with Cloud Trace in order to break down the request latencies at each microservice. -&gt; Correct. Instrumenting the application with Cloud Trace enables the tracking of the entire request journey, including each service involved in processing the request. Cloud Trace provides detailed information about request latency and service performance to help isolate the bottleneck and identify the service(s) causing the issue. </p><p><br></p><p>Use Cloud Monitoring to look for insights that show when your API latencies are high. -&gt; Incorrect. Cloud Monitoring could help to monitor the overall performance of the application and track specific metrics, but they may not provide the same level of detail as Cloud Trace. </p><p><br></p><p>Send custom metrics for each of your requests to Cloud Monitoring. -&gt; Incorrect. Cloud Monitoring and custom metrics could help to monitor the overall performance of the application and track specific metrics, but they may not provide the same level of detail as Cloud Trace. </p><p><br></p><p>Set timeouts on your application so that you can fail requests faster. -&gt; Incorrect. Setting timeouts on the application may help to fail requests faster, but it doesn't help to identify the root cause of the latency issue.</p><p><br></p><p>https://cloud.google.com/trace/docs/overview</p>",
                "answers": [
                    "<p>Instrument your application with Cloud Trace in order to break down the request latencies at each microservice.</p>",
                    "<p>Use Cloud Monitoring to look for insights that show when your API latencies are high.</p>",
                    "<p>Send custom metrics for each of your requests to Cloud Monitoring.</p>",
                    "<p>Set timeouts on your application so that you can fail requests faster.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you have observed that a few API requests in your microservices application experience significant delays. You are aware that each API request may pass through multiple services. To identify the specific service causing the longest delays in such cases, what course of action should you pursue?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297646,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are tasked with designing a solution to backup an on-premises PostgreSQL database to Google Cloud Platform. The objective is to create a replica of the on-premises database on GCP for backup purposes, so the data is easily recoverable and accessible in case of an on-premises failure. Which method would be the most efficient way to accomplish this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud SQL for PostgreSQL and setup Cloud SQL external server replication -&gt;&nbsp;Correct. Cloud SQL for PostgreSQL supports the replication of an external PostgreSQL server. This allows you to create a replica of an on-premises PostgreSQL database on Google Cloud, meeting the requirement of this scenario.</p><p><br></p><p>Use Cloud Dataflow to stream data from PostgreSQL to BigQuery -&gt; Incorrect. Cloud Dataflow is primarily used for processing and transforming large data sets, not for creating a replica of a PostgreSQL database. Streaming data to BigQuery wouldn't maintain the relational structure of the PostgreSQL database and isn't suitable for real-time backup.</p><p><br></p><p>Use Cloud Spanner to replicate the PostgreSQL database -&gt;&nbsp;Incorrect. Cloud Spanner is a highly scalable, multi-region database service, but it doesn't support direct replication from an on-premises PostgreSQL database.</p><p><br></p><p>Use Google Cloud Storage to store PostgreSQL dump files -&gt;&nbsp;Incorrect. Storing PostgreSQL dump files in Google Cloud Storage could serve as a form of backup, but it wouldn't create a readily accessible and live replica of the database.</p>",
                "answers": [
                    "<p>Use Cloud SQL for PostgreSQL and setup Cloud SQL external server replication</p>",
                    "<p>Use Cloud Dataflow to stream data from PostgreSQL to BigQuery</p>",
                    "<p>Use Cloud Spanner to replicate the PostgreSQL database</p>",
                    "<p>Use Google Cloud Storage to store PostgreSQL dump files</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are tasked with designing a solution to backup an on-premises PostgreSQL database to Google Cloud Platform. The objective is to create a replica of the on-premises database on GCP for backup purposes, so the data is easily recoverable and accessible in case of an on-premises failure. Which method would be the most efficient way to accomplish this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297648,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>After creating multiple preemptible Linux VM instances through Google Compute Engine, your objective is to ensure the appropriate shutdown of the application prior to the VMs being preempted. What actions are recommended in this situation?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should create a shutdown script and use it as the value for a new metadata entry with the key shutdown-script in the Cloud Platform Console when you create the new virtual machine instance. -&gt; Correct. This option is the best fit for this scenario, as it allows you to specify a script that will be executed before the VM is terminated. You can also use this option to ensure that the script is executed in a timely manner and to make sure that it is executed consistently across all instances.</p><p><br></p><p>You should create a shutdown script, registered as a xinetd service in Linux, and use the gcloud compute instances add-metadata command to specify the service URL as the value for a new metadata entry with the key shutdown-script-url. -&gt; Incorrect. This option may not be the best fit for shutting down the application before the VMs are preempted as it may not be reliable enough to guarantee that the script will be executed before the VM is terminated.</p><p><br></p><p>Create a shutdown script registered as a xinetd service in Linux and configure an endpoint check to call the service. -&gt; Incorrect. While this option may work in some scenarios, it can be complex to set up and may not be the most reliable solution for shutting down the application before the VMs are preempted.</p><p><br></p><p>Create a shutdown script in the <code>/etc/rc.6.d/</code> directory. -&gt; Incorrect. While this option may work, it can be unreliable as the script may not be executed in time before the VM is terminated.</p><p><br></p><p>https://cloud.google.com/compute/docs/instances/startup-scripts</p>",
                "answers": [
                    "<p>You should create a shutdown script and use it as the value for a new metadata entry with the key shutdown-script in the Cloud Platform Console when you create the new virtual machine instance.</p>",
                    "<p>You should create a shutdown script, registered as a xinetd service in Linux, and use the gcloud compute instances add-metadata command to specify the service URL as the value for a new metadata entry with the key shutdown-script-url.</p>",
                    "<p>Create a shutdown script registered as a xinetd service in Linux and configure an endpoint check to call the service.</p>",
                    "<p>Create a shutdown script in the <code>/etc/rc.6.d/</code> directory.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "After creating multiple preemptible Linux VM instances through Google Compute Engine, your objective is to ensure the appropriate shutdown of the application prior to the VMs being preempted. What actions are recommended in this situation?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297652,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you've been tasked with setting up an e-commerce application on Google Cloud Platform for a multinational company. The application will handle credit card transactions and customer data, so security is of utmost importance. The application consists of various components running on Compute Engine, Cloud Storage, and Cloud SQL. What should be your primary focus in terms of security?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use VPC Service Controls to establish a security perimeter around sensitive resources. -&gt;&nbsp;Correct. VPC Service Controls allows you to define a security perimeter around Google Cloud resources to mitigate data exfiltration risks, which is extremely important when handling sensitive data such as credit card transactions and customer data.</p><p><br></p><p>Utilize Cloud Armor to protect the application against DDoS attacks. -&gt; Incorrect. While using Cloud Armor can protect the application against DDoS attacks, it's only a part of a comprehensive security strategy and it's not the primary focus for securing an application that handles sensitive data across various GCP services.</p><p><br></p><p>Apply IAM roles and policies at the organization level to manage resource access. -&gt; Incorrect. Applying IAM roles and policies at the organization level is good practice, but it does not provide a security perimeter around sensitive data, which is critical when handling credit card transactions and customer data.</p><p><br></p><p>Enable Secure Boot on all Compute Engine instances to ensure the integrity of the boot process. -&gt; Incorrect. Secure Boot is a feature that ensures the integrity of the boot process to protect against threats such as rootkits and bootkits. Although it's a good security practice, it is not the primary focus in this scenario where securing sensitive data across various GCP services is the key.</p>",
                "answers": [
                    "<p>Use VPC Service Controls to establish a security perimeter around sensitive resources.</p>",
                    "<p>Utilize Cloud Armor to protect the application against DDoS attacks.</p>",
                    "<p>Apply IAM roles and policies at the organization level to manage resource access.</p>",
                    "<p>Enable Secure Boot on all Compute Engine instances to ensure the integrity of the boot process.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you've been tasked with setting up an e-commerce application on Google Cloud Platform for a multinational company. The application will handle credit card transactions and customer data, so security is of utmost importance. The application consists of various components running on Compute Engine, Cloud Storage, and Cloud SQL. What should be your primary focus in terms of security?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297654,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>When considering strong security during the operation of fully autonomous vehicles within your agricultural division, there are two key architecture characteristics that should be taken into account. Which two characteristics should you prioritize in your architecture design?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Treat every microservice call between modules on the vehicle as untrusted. -&gt; Correct. This improves system security by making it more resistant to hacking, especially through man-in-the-middle attacks between modules.</p><p><br></p><p>Use a Trusted Platform Module (TPM) and verify firmware and binaries on boot. -&gt; Correct. This improves system security by making it more resistant to hacking, especially rootkits or other kinds of corruption by malicious actors.</p><p><br></p><p>Use multiple connectivity subsystems for redundancy. -&gt; Incorrect. This improves system durability, but it doesn't have any impact on the security during vehicle operation.</p><p><br></p><p>Require IPv6 for connectivity to ensure a secure address space. -&gt; Incorrect. IPv6 doesn't have any impact on the security during vehicle operation, although it improves system scalability and simplicity.</p><p><br></p><p>Use a functional programming language to isolate code execution cycles. -&gt;&nbsp;Incorrect. Merely using a functional programming language doesn't guarantee a more secure level of execution isolation. Any impact on security from this decision would be incidental at best.</p>",
                "answers": [
                    "<p>Treat every microservice call between modules on the vehicle as untrusted.</p>",
                    "<p>Use a Trusted Platform Module (TPM) and verify firmware and binaries on boot.</p>",
                    "<p>Use multiple connectivity subsystems for redundancy.</p>",
                    "<p>Require IPv6 for connectivity to ensure a secure address space.</p>",
                    "<p>Use a functional programming language to isolate code execution cycles.</p>"
                ]
            },
            "correct_response": [
                "a",
                "b"
            ],
            "section": "",
            "question_plain": "When considering strong security during the operation of fully autonomous vehicles within your agricultural division, there are two key architecture characteristics that should be taken into account. Which two characteristics should you prioritize in your architecture design?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297656,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdf</p><p><br></p><p>Mountkirk Games is seeking to develop a real-time analytics platform for their upcoming game while ensuring that all their technical requirements are met. Which combination of Google technologies would satisfy these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Dataflow, Cloud Storage, Cloud Pub/Sub, and BigQuery -&gt; Correct. Cloud Dataflow dynamically scales up or down, can process data in real time, and is ideal for processing data that arrives late using Beam windows and triggers. Cloud Storage can be the landing space for files that are regularly uploaded by users’ mobile devices. Cloud Pub/Sub can ingest the streaming data from the mobile users. BigQuery can query more than 10 TB of historical data.</p><p><br></p><p>Kubernetes Engine, Cloud Pub/Sub, and Cloud SQL -&gt; Incorrect. Cloud SQL is the only storage listed, is limited to 10 TB of storage, and is better suited for transactional workloads. Mountkirk Games needs queries to access at least 10 TB of historical data for analytic purposes.</p><p><br></p><p>Cloud SQL, Cloud Storage, Cloud Pub/Sub, and Cloud Dataflow -&gt; Incorrect. Cloud SQL is the only storage listed, is limited to 10TB of storage, and is better suited for transactional workloads. Mountkirk Games needs queries to access at least 10 TB of historical data for analytic purposes.</p><p><br></p><p>Cloud Pub/Sub, Compute Engine, Cloud Storage, and Cloud Dataproc -&gt; Incorrect. Mountkirk Games needs the ability to query historical data. While this might be possible using workarounds, such as BigQuery federated queries for Cloud Storage or Hive queries for Cloud Dataproc, these approaches are more complex. BigQuery is a simpler and more flexible product that fulfills those requirements.</p>",
                "answers": [
                    "<p>Cloud Dataflow, Cloud Storage, Cloud Pub/Sub, and BigQuery</p>",
                    "<p>Kubernetes Engine, Cloud Pub/Sub, and Cloud SQL</p>",
                    "<p>Cloud SQL, Cloud Storage, Cloud Pub/Sub, and Cloud Dataflow</p>",
                    "<p>Cloud Pub/Sub, Compute Engine, Cloud Storage, and Cloud Dataproc</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdfMountkirk Games is seeking to develop a real-time analytics platform for their upcoming game while ensuring that all their technical requirements are met. Which combination of Google technologies would satisfy these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297658,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are responsible for setting up a continuous deployment pipeline for a project hosted in a Git source repository. Your objective is to guarantee that code modifications can be validated prior to being deployed to the production environment. What steps should you take to achieve this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Jenkins to monitor tags in the repository. Deploy staging tags to a staging environment for testing. After testing, tag the repository for production and deploy that to the production environment. -&gt; Correct. It suggests using Jenkins to monitor tags in the Git repository. When code changes are made, the developer creates a tag and pushes it to the Git repository. Jenkins will monitor these tags and deploy the staging tags to a staging environment for testing. Once the code has been tested and verified, the developer will tag the repository for production and deploy it to the production environment. This process ensures that code changes are verified before being deployed to production, reducing the risk of errors.</p><p><br></p><p>Use Spinnaker to deploy builds to production using the red/black deployment strategy so that changes can easily be rolled back. -&gt; Incorrect. While this strategy is useful for rolling back changes, it does not address the need for testing code changes before deploying them to production.</p><p><br></p><p>Use Spinnaker to deploy builds to production and run tests on production deployments. -&gt;&nbsp;Incorrect. This approach is risky because any issues that arise during testing could affect production users.</p><p><br></p><p>Use Jenkins to build the staging branches and the master branch. Build and deploy changes to production for 10% of users before doing a complete rollout. -&gt; Incorrect. While this approach is useful for gradually rolling out changes, it does not address the need for testing code changes before deploying them to production.</p><p><br></p><p>https://cloud.google.com/architecture/continuous-delivery-jenkins-kubernetes-engine</p>",
                "answers": [
                    "<p>Use Jenkins to monitor tags in the repository. Deploy staging tags to a staging environment for testing. After testing, tag the repository for production and deploy that to the production environment.</p>",
                    "<p>Use Spinnaker to deploy builds to production using the red/black deployment strategy so that changes can easily be rolled back.</p>",
                    "<p>Use Spinnaker to deploy builds to production and run tests on production deployments.</p>",
                    "<p>Use Jenkins to build the staging branches and the master branch. Build and deploy changes to production for 10% of users before doing a complete rollout.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are responsible for setting up a continuous deployment pipeline for a project hosted in a Git source repository. Your objective is to guarantee that code modifications can be validated prior to being deployed to the production environment. What steps should you take to achieve this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297660,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization's application is deployed on App Engine. Users have started reporting that some transactions are failing, and the rate of failure seems to be increasing. You need to identify the root cause and mitigate the problem. Which of the following steps is the most efficient way to proceed?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Monitoring and Logging to identify the source of the problem. -&gt; Correct. Cloud Monitoring and Logging can help you identify issues with your application by monitoring its metrics and logs. This approach allows you to understand what is happening with your application and troubleshoot effectively.</p><p><br></p><p>Change the runtime environment of the application and observe if the issue persists. -&gt;&nbsp;Incorrect. Changing the runtime environment is not a recommended immediate action for troubleshooting. It could lead to additional complications without necessarily addressing the root cause of the problem.</p><p><br></p><p>Migrate the application to Google Kubernetes Engine (GKE) to better handle the load. -&gt; Incorrect. Migrating to GKE is a significant operation and should not be considered as an immediate action for troubleshooting an application issue. Also, moving to GKE doesn't ensure the problem will be resolved, especially if the problem lies within the application's code or configuration.</p><p><br></p><p>Utilize Cloud Pub/Sub to decouple and distribute the transactions across various services. -&gt; Incorrect. Cloud Pub/Sub might help distribute workload across different services, but it doesn't directly address the issue at hand. Furthermore, integrating Pub/Sub into the application requires significant changes to the application's code and is not suitable as an immediate troubleshooting step.</p>",
                "answers": [
                    "<p>Use Cloud Monitoring and Logging to identify the source of the problem.</p>",
                    "<p>Change the runtime environment of the application and observe if the issue persists.</p>",
                    "<p>Migrate the application to Google Kubernetes Engine (GKE) to better handle the load.</p>",
                    "<p>Utilize Cloud Pub/Sub to decouple and distribute the transactions across various services.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization's application is deployed on App Engine. Users have started reporting that some transactions are failing, and the rate of failure seems to be increasing. You need to identify the root cause and mitigate the problem. Which of the following steps is the most efficient way to proceed?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297662,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company has chosen to adopt Kubernetes for managing its containerized application deployments. As a cloud architect, you have been tasked with creating a Kubernetes cluster using <code>gcloud</code> command-line tool in a way that allows you to perform rolling updates without downtime, supports automatic scaling, and ensures data persistency for stateful apps. Which of the following gcloud commands would be most appropriate for this task?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p><code>gcloud container clusters create my-cluster --zone=us-central1-a --num-nodes=3 --enable-autoscaling --min-nodes=1 --max-nodes=5 --scopes=gke-default</code> -&gt; Correct. It creates a cluster, enables autoscaling and assigns the correct scope for a GKE cluster. Autoscaling allows the cluster to automatically adjust the number of nodes as workload demand changes.</p><p><br></p><p><code>gcloud container clusters create my-cluster --region=us-central1 --num-nodes=3 --scopes=cloud-platform</code>&nbsp; -&gt; Incorrect. This command creates a cluster but does not enable autoscaling which is a requirement. The scopes flag is too broad, granting unnecessary permissions.</p><p><br></p><p><code>gcloud container clusters create my-cluster --region=us-central1 --num-nodes=3 --scopes=cloud-platform --enable-autoscaling --min-nodes=1 --max-nodes=5</code>&nbsp; -&gt; Incorrect. While this command enables autoscaling, it uses a broader scope than necessary and also creates the cluster in a region instead of a zone which might not be optimal for the requirements.</p><p><br></p><p><code>gcloud container clusters create my-cluster --zone=us-central1-a --num-nodes=3 --scopes=gke-default --enable-autoscaling --min-nodes=1 --max-nodes=5 --enable-cloud-logging</code>&nbsp; -&gt; Incorrect. While this command enables autoscaling and assigns the correct scope, it enables cloud logging which is not a requirement and may unnecessarily increase costs.</p>",
                "answers": [
                    "<p><code>gcloud container clusters create my-cluster --zone=us-central1-a --num-nodes=3 --enable-autoscaling --min-nodes=1 --max-nodes=5 --scopes=gke-default</code> </p>",
                    "<p><code>gcloud container clusters create my-cluster --region=us-central1 --num-nodes=3 --scopes=cloud-platform</code> </p>",
                    "<p><code>gcloud container clusters create my-cluster --region=us-central1 --num-nodes=3 --scopes=cloud-platform --enable-autoscaling --min-nodes=1 --max-nodes=5</code> </p>",
                    "<p><code>gcloud container clusters create my-cluster --zone=us-central1-a --num-nodes=3 --scopes=gke-default --enable-autoscaling --min-nodes=1 --max-nodes=5 --enable-cloud-logging</code> </p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company has chosen to adopt Kubernetes for managing its containerized application deployments. As a cloud architect, you have been tasked with creating a Kubernetes cluster using gcloud command-line tool in a way that allows you to perform rolling updates without downtime, supports automatic scaling, and ensures data persistency for stateful apps. Which of the following gcloud commands would be most appropriate for this task?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297664,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your team is developing a high-performance computing application that specifically needs to run on a Debian Linux environment. The application is compute-intensive and processes a large amount of data. Given the need for compute resources to be scaled up and down in response to the changing volume of data, as a cloud architect, what deployment strategy would you suggest on Google Cloud?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Compute Engine with Debian Linux images and configure them in a Managed Instance Group. -&gt; Correct. Compute Engine allows you to create instances with custom images including Debian Linux. By configuring the instances in a Managed Instance Group, the compute resources can be automatically scaled up and down in response to load, making this the best choice for this scenario.</p><p><br></p><p>Deploy the application on App Engine standard environment. -&gt; Incorrect. App Engine standard environment only supports specific runtime environments and Debian Linux is not one of them. Hence, this is not a suitable choice.</p><p><br></p><p>Deploy the application on Google Kubernetes Engine with Debian containers. -&gt; Incorrect. While Google Kubernetes Engine (GKE) allows deploying applications in containers, setting up a Kubernetes cluster for a single application can be overkill and would add unnecessary complexity, especially if the application doesn’t specifically require containerization or Kubernetes features.</p><p><br></p><p>Use Cloud Functions with a custom runtime to mimic the Debian Linux environment. -&gt; Incorrect. Cloud Functions are designed for lightweight, single-purpose functions and are not suitable for high-performance computing applications. Furthermore, while you can specify a custom runtime, mimicking a full Debian Linux environment in a function is not feasible.</p>",
                "answers": [
                    "<p>Use Compute Engine with Debian Linux images and configure them in a Managed Instance Group.</p>",
                    "<p>Deploy the application on App Engine standard environment.</p>",
                    "<p>Deploy the application on Google Kubernetes Engine with Debian containers.</p>",
                    "<p>Use Cloud Functions with a custom runtime to mimic the Debian Linux environment.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your team is developing a high-performance computing application that specifically needs to run on a Debian Linux environment. The application is compute-intensive and processes a large amount of data. Given the need for compute resources to be scaled up and down in response to the changing volume of data, as a cloud architect, what deployment strategy would you suggest on Google Cloud?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297666,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, your company has asked you to architect and deploy a highly scalable web application using Google App Engine. The application will be used globally and should be able to handle large spikes in traffic. The application also needs to be updated frequently with zero downtime. Moreover, the company is very cost-conscious and wants to ensure that they are only billed for the compute resources they actually use. Which of the following App Engine environment and scaling type combinations would you recommend for this situation?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>App Engine Standard Environment with Automatic Scaling. -&gt;&nbsp;Correct. The App Engine Standard Environment is designed to scale instances out when traffic increases, and to scale instances back when traffic decreases, thereby only charging for the actual resources used. It also allows seamless deployments with zero downtime, which makes it the best option in this scenario.</p><p><br></p><p>App Engine Flexible Environment with Basic Scaling. -&gt; Incorrect. The App Engine Flexible Environment is more appropriate for applications that require more customization and have specific runtime requirements. However, Basic Scaling may not handle large spikes in traffic efficiently and could lead to higher costs.</p><p><br></p><p>App Engine Standard Environment with Manual Scaling. -&gt; Incorrect. While the App Engine Standard Environment is a good choice for scalability and cost-efficiency, Manual Scaling doesn't meet the requirement of automatic scalability to handle large traffic spikes.</p><p><br></p><p>App Engine Flexible Environment with Automatic Scaling. -&gt; Incorrect. The App Engine Flexible Environment with Automatic Scaling might handle traffic spikes, but it could lead to higher costs as it's more appropriate for applications with specific runtime requirements and the resources aren't scaled down as efficiently when not in use.</p>",
                "answers": [
                    "<p>App Engine Standard Environment with Automatic Scaling.</p>",
                    "<p>App Engine Flexible Environment with Basic Scaling.</p>",
                    "<p>App Engine Standard Environment with Manual Scaling.</p>",
                    "<p>App Engine Flexible Environment with Automatic Scaling.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, your company has asked you to architect and deploy a highly scalable web application using Google App Engine. The application will be used globally and should be able to handle large spikes in traffic. The application also needs to be updated frequently with zero downtime. Moreover, the company is very cost-conscious and wants to ensure that they are only billed for the compute resources they actually use. Which of the following App Engine environment and scaling type combinations would you recommend for this situation?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297668,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are leading the team responsible for managing an application hosted on Cloud Run. You are planning to release a new version of the application. To ensure minimal disruption and maintain high availability, you need to define a strategy for deploying the new version. Which of the following would be the most appropriate strategy for this scenario?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Perform a canary deployment, gradually directing a small percentage of traffic to the new version while monitoring for issues. -&gt; Correct. Canary deployments are a best practice in software development. They allow a small percentage of traffic to be directed to the new version, while the majority of traffic continues to interact with the stable version. This provides real-world testing of the new version and allows for problems to be detected and corrected without affecting the entire user base.</p><p><br></p><p>Perform a blue-green deployment, keeping the current version (blue) running while the new version (green) is fully deployed and tested. Once the new version is validated, traffic is redirected to it. -&gt; Incorrect. While blue-green deployments can help minimize downtime, Cloud Run does not natively support this type of deployment. However, you can implement it using a combination of services (like Traffic Splitting), but it's more complex and may not be necessary.</p><p><br></p><p>Deploy the new version directly to production, testing it in the live environment and rolling it back if issues are found. -&gt; Incorrect. Directly deploying to production without testing can lead to unforeseen issues affecting all users. This approach should be avoided whenever possible.</p><p><br></p><p>Shut down the application, deploy the new version, perform extensive testing, and then bring the application back online. -&gt; Incorrect. Shutting down the application entirely for an update is not advisable, especially for mission-critical applications. This could result in downtime and poor user experience.</p>",
                "answers": [
                    "<p>Perform a canary deployment, gradually directing a small percentage of traffic to the new version while monitoring for issues.</p>",
                    "<p>Perform a blue-green deployment, keeping the current version (blue) running while the new version (green) is fully deployed and tested. Once the new version is validated, traffic is redirected to it.</p>",
                    "<p>Deploy the new version directly to production, testing it in the live environment and rolling it back if issues are found.</p>",
                    "<p>Shut down the application, deploy the new version, perform extensive testing, and then bring the application back online.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are leading the team responsible for managing an application hosted on Cloud Run. You are planning to release a new version of the application. To ensure minimal disruption and maintain high availability, you need to define a strategy for deploying the new version. Which of the following would be the most appropriate strategy for this scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297670,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect working for a global enterprise that runs an e-commerce application. As part of the design, the application layer is hosted on Compute Engine and needs to be scalable to handle potential spikes in traffic during high-usage times. In this scenario, you decided to implement a managed instance group (MIG) for automated scaling. You need to create a process to automate the creation of the managed instance group, ensure it scales based on load, is distributed across multiple zones for high availability, and uses predefined instance templates for uniformity. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Terraform to create a regional managed instance group using an instance template. Configure autoscaling based on Cloud Monitoring metrics. -&gt; Correct. Terraform can be used to automate the creation of resources and manage infrastructure as code. A regional managed instance group provides the required high availability across multiple zones. Configuring autoscaling based on Cloud Monitoring metrics will allow the MIG to scale based on application-specific requirements.</p><p><br></p><p>Use Deployment Manager to create and manage a zonal managed instance group. Configure autoscaling based on HTTP load balancing. -&gt; Incorrect. While Deployment Manager can be used to automate the creation of resources, the use of a zonal managed instance group doesn't provide the high availability required across multiple zones.</p><p><br></p><p>Use the Cloud SDK to create a zonal managed instance group with a template, and then manually add instances when needed. -&gt; Incorrect. Manually adding instances when needed contradicts the need for automation and auto-scaling based on load.</p><p><br></p><p>Use Cloud Functions to create an instance template and a zonal managed instance group. Configure autoscaling based on Cloud Monitoring metrics. -&gt; Incorrect. While Cloud Functions could theoretically be used, they are serverless execution environments for building and connecting cloud services, and are not generally used for this kind of infrastructure management task.</p>",
                "answers": [
                    "<p>Use Terraform to create a regional managed instance group using an instance template. Configure autoscaling based on Cloud Monitoring metrics.</p>",
                    "<p>Use Deployment Manager to create and manage a zonal managed instance group. Configure autoscaling based on HTTP load balancing.</p>",
                    "<p>Use the Cloud SDK to create a zonal managed instance group with a template, and then manually add instances when needed.</p>",
                    "<p>Use Cloud Functions to create an instance template and a zonal managed instance group. Configure autoscaling based on Cloud Monitoring metrics. </p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect working for a global enterprise that runs an e-commerce application. As part of the design, the application layer is hosted on Compute Engine and needs to be scalable to handle potential spikes in traffic during high-usage times. In this scenario, you decided to implement a managed instance group (MIG) for automated scaling. You need to create a process to automate the creation of the managed instance group, ensure it scales based on load, is distributed across multiple zones for high availability, and uses predefined instance templates for uniformity. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297672,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you have been assigned the task of setting up a Compute Engine application in a single Virtual Private Cloud (VPC) spanning across two regions for a global e-commerce company. The objective is to ensure high availability and seamless connectivity between instances in these two regions, while also keeping latency and cost to a minimum. Which approach would be the most effective to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Create a single VPC and deploy two regional subnets with custom dynamic routing. -&gt; Correct. This is the best solution as it would ensure seamless connectivity between instances in different regions within the same VPC. Custom dynamic routing allows traffic to be efficiently routed within the VPC, minimizing latency and cost.</p><p><br></p><p>Create two separate VPCs, one for each region, and connect them using Cloud VPN. -&gt; Incorrect. Cloud VPN allows secure connections between your on-premises network and your VPCs, or between two VPCs, but it introduces additional complexity and cost, which may not be necessary if instances are within the same VPC.</p><p><br></p><p>Create a single VPC and deploy two unconnected regional subnets. -&gt; Incorrect. This would allow the instances to operate in the same VPC, but without proper routing, there wouldn't be seamless connectivity between the two subnets, which doesn't meet the requirements.</p><p><br></p><p>Use shared VPC to connect the two regions. -&gt; Incorrect. Shared VPC can be useful when you want to keep resources isolated in different projects, but it doesn't inherently ensure seamless connectivity or minimize latency and cost across regions.</p>",
                "answers": [
                    "<p>Create a single VPC and deploy two regional subnets with custom dynamic routing.</p>",
                    "<p>Create two separate VPCs, one for each region, and connect them using Cloud VPN.</p>",
                    "<p>Create a single VPC and deploy two unconnected regional subnets.</p>",
                    "<p>Use shared VPC to connect the two regions.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you have been assigned the task of setting up a Compute Engine application in a single Virtual Private Cloud (VPC) spanning across two regions for a global e-commerce company. The objective is to ensure high availability and seamless connectivity between instances in these two regions, while also keeping latency and cost to a minimum. Which approach would be the most effective to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297650,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is structuring their Google Cloud Platform (GCP) resources. The company consists of three distinct departments: Finance, Marketing, and Operations. Each department should only be able to access and manage their own resources. As a cloud architect, what is the most appropriate way to design the GCP resource hierarchy?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Create one organization, and within it create a folder for each department. In each folder, create projects for different needs of the department. -&gt;&nbsp;Correct. This approach gives a clear separation of resources, makes it easier to manage IAM roles and policies, and aligns with best practices for designing resource hierarchies in GCP. It provides flexibility and control at the right levels.</p><p><br></p><p>Create one project for the whole company and use IAM policies to restrict access for each department. -&gt; Incorrect. This approach can theoretically work with finely tuned IAM policies, but it can become complex and hard to manage as the organization grows. It's better to segregate resources on the project level to maintain clear separation between departments.</p><p><br></p><p>Create three different organizations for each department. -&gt; Incorrect. Creating separate organizations for each department can result in unnecessary complexity and difficulty in managing company-wide policies and resources. Organizations should typically represent an entire company.</p><p><br></p><p>Do not create an organization. Instead, create separate projects for each department. -&gt; Incorrect. This approach would not take full advantage of GCP's resource hierarchy. An Organization node is the root node in the Google Cloud resource hierarchy and it provides central visibility and control over all of the GCP resources.</p>",
                "answers": [
                    "<p>Create one organization, and within it create a folder for each department. In each folder, create projects for different needs of the department.</p>",
                    "<p>Create one project for the whole company and use IAM policies to restrict access for each department.</p>",
                    "<p>Create three different organizations for each department.</p>",
                    "<p>Do not create an organization. Instead, create separate projects for each department.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company is structuring their Google Cloud Platform (GCP) resources. The company consists of three distinct departments: Finance, Marketing, and Operations. Each department should only be able to access and manage their own resources. As a cloud architect, what is the most appropriate way to design the GCP resource hierarchy?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297674,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization collects telemetry data from thousands of IoT devices. The data needs to be stored in a time-series database on Google Cloud for future analysis and predictions. The data volume is expected to grow exponentially over the next couple of years. Which Google Cloud service should you use to handle this situation?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Bigtable -&gt;&nbsp;Correct. Cloud Bigtable is a great option for time-series data due to its design, which can handle massive workloads, up to billions of rows and thousands of columns, thus enabling the solution to scale with the growing data.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. Cloud Storage is an object storage service that's great for storing large amounts of unstructured data, but it doesn't provide the database-like querying capabilities typically needed with time-series data.</p><p><br></p><p>Firestore -&gt; Incorrect. Firestore is a NoSQL document database for mobile, web, and server development. It's not optimal for time-series data storage, particularly at a large scale.</p><p><br></p><p>BigQuery -&gt; Incorrect. BigQuery is a great service for analytical workloads, but it is not optimized for time-series data, which could lead to inefficient storage and querying of the growing data.</p>",
                "answers": [
                    "<p>Cloud Bigtable</p>",
                    "<p>Cloud Storage</p>",
                    "<p>Firestore</p>",
                    "<p>BigQuery</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization collects telemetry data from thousands of IoT devices. The data needs to be stored in a time-series database on Google Cloud for future analysis and predictions. The data volume is expected to grow exponentially over the next couple of years. Which Google Cloud service should you use to handle this situation?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297676,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are designing a multi-tier web application that is hosted on Google Cloud Platform (GCP). The web application has a high number of users, and you expect that the number of users will increase significantly in the near future. What GCP services would you use to ensure that the web application can scale dynamically to meet the changing demand?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Google Kubernetes Engine and Google Cloud Load Balancing. -&gt;&nbsp;Correct. Google Kubernetes Engine (GKE) and Google Cloud Load Balancing are both designed to dynamically scale with changing demand. GKE allows you to manage and scale containerized applications, while Cloud Load Balancing provides intelligent load balancing and auto-scaling capabilities. This combination enables the web application to automatically scale based on incoming traffic and ensure high availability, even during periods of high demand. </p><p><br></p><p>Google App Engine and Google Compute Engine -&gt; Incorrect. It is incorrect because Google Compute Engine does not have automatic scaling capabilities. </p><p><br></p><p>Google Compute Engine and Google Cloud Load Balancing -&gt; Incorrect. It is incorrect because Cloud Load Balancing alone does not provide the necessary scalability features to automatically adjust resources based on traffic patterns. </p><p><br></p><p>Google App Engine and Google Kubernetes Engine -&gt; Incorrect. It is incorrect because while App Engine can scale dynamically, it may not be the best fit for a multi-tier web application, and Kubernetes provides more advanced scaling and orchestration features.</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview</p><p>https://cloud.google.com/load-balancing/docs</p>",
                "answers": [
                    "<p>Google App Engine and Google Compute Engine</p>",
                    "<p>Google Compute Engine and Google Cloud Load Balancing</p>",
                    "<p>Google App Engine and Google Kubernetes Engine</p>",
                    "<p>Google Kubernetes Engine and Google Cloud Load Balancing.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "You are designing a multi-tier web application that is hosted on Google Cloud Platform (GCP). The web application has a high number of users, and you expect that the number of users will increase significantly in the near future. What GCP services would you use to ensure that the web application can scale dynamically to meet the changing demand?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297678,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A company is planning to deploy a multi-tier web application on Google Cloud Platform (GCP) that will be accessed by users globally. The application must be highly available, provide fast and reliable performance, and meet strict security and compliance requirements. Which of the following options would be the most effective approach to meet these requirements while also optimizing cost?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use App Engine Flexible Environment to host the web and application tiers, and store data in Cloud Datastore. -&gt; Correct. App Engine Flexible Environment is a fully-managed platform for building scalable and highly available web applications. It automatically scales the resources based on demand and provides automatic load balancing across multiple regions, which makes it highly available. In addition, it meets strict security and compliance requirements as it provides features such as Identity and Access Management (IAM), audit logging, and encryption of data at rest and in transit. Cloud Datastore is a highly-scalable NoSQL document database that can store and serve structured data for the application, making it fast and reliable.</p><p><br></p><p>Use Compute Engine instances in a global load-balanced network to host the web and application tiers, and store data in Cloud SQL. -&gt; Incorrect. While Compute Engine instances in a global load-balanced network can also provide high availability and fast performance, they require more management effort and are generally more expensive than App Engine Flexible Environment. </p><p><br></p><p>Use Kubernetes Engine to deploy and manage containers for the web and application tiers, and store data in Cloud Bigtable. -&gt; Incorrect. Using Kubernetes Engine to deploy and manage containers for the web and application tiers would require more management effort and cost than App Engine Flexible Environment. Cloud Bigtable is also a highly-scalable NoSQL database, but it is better suited for use cases that require high throughput and low latency, such as time-series data processing or real-time analytics. </p><p><br></p><p>Use Cloud Functions to host the web and application tiers, and store data in Cloud Firestore. -&gt; Incorrect. Cloud Functions is a serverless computing platform, and while it can be used for building web applications, it is better suited for event-driven use cases, such as processing data or triggering actions based on certain events.</p>",
                "answers": [
                    "<p>Use Compute Engine instances in a global load-balanced network to host the web and application tiers, and store data in Cloud SQL.</p>",
                    "<p>Use Kubernetes Engine to deploy and manage containers for the web and application tiers, and store data in Cloud Bigtable.</p>",
                    "<p>Use App Engine Flexible Environment to host the web and application tiers, and store data in Cloud Datastore.</p>",
                    "<p>Use Cloud Functions to host the web and application tiers, and store data in Cloud Firestore.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "A company is planning to deploy a multi-tier web application on Google Cloud Platform (GCP) that will be accessed by users globally. The application must be highly available, provide fast and reliable performance, and meet strict security and compliance requirements. Which of the following options would be the most effective approach to meet these requirements while also optimizing cost?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297680,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A company is planning to deploy a highly scalable and secure cloud infrastructure on Google Cloud Platform (GCP) to support their growing cloud-based product offerings. The infrastructure must be able to handle sudden spikes in demand, provide fast and reliable performance, and meet strict security and compliance requirements. Which of the following options would be the most effective approach to meet these requirements while also optimizing cost?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Google Kubernetes Engine with network security policies and encrypted secrets to host the application, and use Cloud Interconnect to securely connect to on-premises resources. -&gt;&nbsp;Correct. The scenario described in the question involves deploying a highly scalable and secure cloud infrastructure on Google Cloud Platform that can handle sudden spikes in demand, provide fast and reliable performance, and meet strict security and compliance requirements while also optimizing cost. Google Kubernetes Engine (GKE) is a fully managed container orchestration system that provides automatic scaling, self-healing, and rolling updates. It can handle sudden spikes in demand and provide fast and reliable performance. GKE also integrates well with other GCP services, including Cloud Interconnect, which can be used to securely connect to on-premises resources.</p><p><br></p><p>Use Compute Engine instances with custom firewall rules and encrypted disks to host the application, and use Cloud VPN to securely connect to on-premises resources. -&gt; Incorrect. It is a viable approach for hosting applications, but it does not provide the same level of scalability and reliability as GKE. Cloud VPN is also a valid solution for securely connecting to on-premises resources, but Cloud Interconnect provides a more reliable and faster connection.</p><p><br></p><p>Use App Engine Flexible Environment with custom firewall rules and encrypted environment variables to host the application, and use Cloud Armor to protect against network threats. -&gt; Incorrect. It is a good solution for hosting web applications that require automatic scaling and self-healing. However, GKE provides more control over the underlying infrastructure, making it a better fit for highly scalable and secure cloud infrastructures.</p><p><br></p><p>Use Cloud Functions with encrypted secrets and environment variables to host the application, and use Cloud Load Balancer with SSL/TLS termination to provide secure access to the application. -&gt; Incorrect. It is a good solution for deploying event-driven applications that do not require a persistent server. However, it is not suitable for hosting web applications that require a persistent server and automatic scaling. Additionally, Cloud Load Balancer with SSL/TLS termination can provide secure access to the application, but it does not provide the same level of security and compliance as network security policies provided by GKE.</p>",
                "answers": [
                    "<p>Use Compute Engine instances with custom firewall rules and encrypted disks to host the application, and use Cloud VPN to securely connect to on-premises resources.</p>",
                    "<p>Use Google Kubernetes Engine with network security policies and encrypted secrets to host the application, and use Cloud Interconnect to securely connect to on-premises resources.</p>",
                    "<p>Use App Engine Flexible Environment with custom firewall rules and encrypted environment variables to host the application, and use Cloud Armor to protect against network threats.</p>",
                    "<p>Use Cloud Functions with encrypted secrets and environment variables to host the application, and use Cloud Load Balancer with SSL/TLS termination to provide secure access to the application.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "A company is planning to deploy a highly scalable and secure cloud infrastructure on Google Cloud Platform (GCP) to support their growing cloud-based product offerings. The infrastructure must be able to handle sudden spikes in demand, provide fast and reliable performance, and meet strict security and compliance requirements. Which of the following options would be the most effective approach to meet these requirements while also optimizing cost?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297682,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A company is planning to deploy a real-time data processing solution on Google Cloud Platform (GCP) to process large amounts of incoming sensor data. The solution must be able to scale elastically to accommodate varying levels of incoming data, provide low latency processing, and ensure high data reliability. Additionally, the solution must be able to support complex data transformations and integrations with other data sources. Which of the following options would be the most effective approach to meet these requirements while also optimizing cost?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Apache Beam on Cloud Dataflow to process the data in real-time and store the results in BigQuery. -&gt;&nbsp;Correct. This option suits all requirements. Apache Beam on Cloud Dataflow allows for real-time processing and can automatically scale to accommodate varying data volumes. The processed data is stored in BigQuery, which supports complex queries and ensures high data reliability.</p><p><br></p><p>Use Cloud Dataflow to process the data in a batch mode and store the results in BigQuery. -&gt;&nbsp;Incorrect. This option doesn't satisfy the real-time data processing requirement. Although BigQuery is a good choice for storage and analysis, processing the data in batch mode wouldn't provide the low-latency processing needed.</p><p><br></p><p>Use Cloud Pub/Sub to ingest the data and feed it into Cloud Dataproc clusters for processing and storing the results in BigQuery. -&gt;&nbsp;Incorrect. This approach could potentially work, but managing Cloud Dataproc clusters could be more complex and costlier due to the need for explicit scaling, compared to serverless solutions like Dataflow.</p><p><br></p><p>Use Cloud Pub/Sub to ingest the data, Cloud Functions to process the data in real-time, and Cloud Firestore to store the results. -&gt;&nbsp;Incorrect. While Cloud Pub/Sub can effectively ingest real-time data and Cloud Functions can process it, this architecture may not be the most effective or cost-optimized for large amounts of incoming sensor data because Cloud Functions are priced per invocation and it may become costly with a large amount of data. Cloud Firestore, while a powerful NoSQL database, is not specifically designed for analytics or for handling very large amounts of data, and the costs can add up quickly for high-volume read and write operations.</p>",
                "answers": [
                    "<p>Use Cloud Dataflow to process the data in a batch mode and store the results in BigQuery.</p>",
                    "<p>Use Cloud Pub/Sub to ingest the data and feed it into Cloud Dataproc clusters for processing and storing the results in BigQuery.</p>",
                    "<p>Use Cloud Pub/Sub to ingest the data, Cloud Functions to process the data in real-time, and Cloud Firestore to store the results.</p>",
                    "<p>Use Apache Beam on Cloud Dataflow to process the data in real-time and store the results in BigQuery.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "A company is planning to deploy a real-time data processing solution on Google Cloud Platform (GCP) to process large amounts of incoming sensor data. The solution must be able to scale elastically to accommodate varying levels of incoming data, provide low latency processing, and ensure high data reliability. Additionally, the solution must be able to support complex data transformations and integrations with other data sources. Which of the following options would be the most effective approach to meet these requirements while also optimizing cost?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297684,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A multinational corporation has a large number of remote employees working in different countries. They need to provide secure access to company resources, such as Google Workspace and GCP resources, to these employees. The solution should also meet the following requirements:</p><ul><li><p>provide a secure and scalable solution that can handle a large number of remote users</p></li><li><p>enforce strong authentication and authorization policies</p></li><li><p>automatically provide employees with the appropriate level of access to company resources based on their job function</p></li><li><p>provide centralized management and auditing of user access</p></li><li><p>be cost-effective</p></li></ul><p><br></p><p>Which solution would you recommend to meet these requirements and why?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implementing Google Workspace Domain-Wide Delegation of Authority and Google Cloud Identity-Aware Proxy (IAP) -&gt; Correct. Google Workspace Domain-Wide Delegation of Authority allows service accounts to impersonate users in a Google Workspace domain. This enables the service accounts to perform actions on behalf of users, thus aligning with the requirement of automatic provisioning based on job function. Coupled with Google Cloud Identity-Aware Proxy (IAP), which controls access to cloud applications running on Google Cloud Platform by verifying user identity and the context of the request, this combination provides a robust solution for secure, scalable access, centralized management, and auditing of user access.</p><p><br></p><p>Implementing Google Workspace Domain-Wide Delegation of Authority and Google Cloud VPN -&gt; Incorrect. Google Cloud VPN creates an encrypted IPsec VPN connection for securely connecting remote users to the GCP resources. However, it does not provide fine-grained control over user access to resources based on job function as required, nor does it handle authentication to Google Workspace.</p><p><br></p><p>Implementing Google Workspace Groups and Google Cloud IAM roles -&gt; Incorrect. Google Workspace Groups and Google Cloud IAM roles could be used to manage access to resources, but these tools alone don't provide a complete solution for enforcing strong authentication and authorization policies, and they are not designed to handle a large number of remote users.</p><p><br></p><p>Implementing Google Workspace Single Sign-On (SSO) and Google Cloud IAM roles -&gt; Incorrect. Google Workspace Single Sign-On (SSO) and Google Cloud IAM roles provide a means of access control, but the SSO itself isn't specifically tailored to provide access based on job function or handle the large scale of remote users. Also, IAM roles alone don't handle authentication or context-aware access control.</p>",
                "answers": [
                    "<p>Implementing Google Workspace Domain-Wide Delegation of Authority and Google Cloud Identity-Aware Proxy (IAP)</p>",
                    "<p>Implementing Google Workspace Domain-Wide Delegation of Authority and Google Cloud VPN</p>",
                    "<p>Implementing Google Workspace Groups and Google Cloud IAM roles</p>",
                    "<p>Implementing Google Workspace Single Sign-On (SSO) and Google Cloud IAM roles</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A multinational corporation has a large number of remote employees working in different countries. They need to provide secure access to company resources, such as Google Workspace and GCP resources, to these employees. The solution should also meet the following requirements:provide a secure and scalable solution that can handle a large number of remote usersenforce strong authentication and authorization policiesautomatically provide employees with the appropriate level of access to company resources based on their job functionprovide centralized management and auditing of user accessbe cost-effectiveWhich solution would you recommend to meet these requirements and why?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297686,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are tasked with deploying a global e-commerce application on Google Cloud. To ensure optimal latency and high availability, you decided to use the Global HTTP(S) Load Balancer. The application needs to route incoming requests to the nearest healthy backend that has sufficient capacity to handle the load. How should you configure the load balancer?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Configure Global HTTP(S) Load Balancer with backend services in multiple regions and use a balancing mode based on the capacity of the backends. -&gt;&nbsp;Correct. This is the correct answer as setting up backend services in multiple regions allows for proximity-based routing. A balancing mode based on capacity ensures traffic is sent to backends that are healthy and have sufficient capacity.</p><p><br></p><p>Configure Global HTTP(S) Load Balancer with backend services in a single region and enable Cloud CDN. -&gt; Incorrect. While Cloud CDN can improve latency and reduce load on your backends for cacheable content, it does not fulfill the requirement of routing to the nearest healthy backend with sufficient capacity.</p><p><br></p><p>Configure Global HTTP(S) Load Balancer with instance groups in different regions and enable session affinity. -&gt; Incorrect. Session affinity is typically used to route all requests from a particular client to the same backend for the duration of a session. It does not inherently route to the nearest backend or account for backend capacity.</p><p><br></p><p>Configure Global HTTP(S) Load Balancer with a single instance group and enable Cross-Region load balancing. -&gt; Incorrect. While cross-region load balancing can provide high availability, it does not inherently provide proximity-based routing or account for the capacity of backends.</p>",
                "answers": [
                    "<p>Configure Global HTTP(S) Load Balancer with backend services in multiple regions and use a balancing mode based on the capacity of the backends.</p>",
                    "<p>Configure Global HTTP(S) Load Balancer with backend services in a single region and enable Cloud CDN.</p>",
                    "<p>Configure Global HTTP(S) Load Balancer with instance groups in different regions and enable session affinity.</p>",
                    "<p>Configure Global HTTP(S) Load Balancer with a single instance group and enable Cross-Region load balancing.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are tasked with deploying a global e-commerce application on Google Cloud. To ensure optimal latency and high availability, you decided to use the Global HTTP(S) Load Balancer. The application needs to route incoming requests to the nearest healthy backend that has sufficient capacity to handle the load. How should you configure the load balancer?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297688,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization is planning to design a cloud solution architecture for a new application that will have significant variations in demand and should be cost-effective. As a cloud architect, what strategy would you propose for this scenario?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Deploy the application on App Engine standard environment. -&gt;&nbsp;Correct. App Engine standard environment is a fully managed serverless platform that automatically scales your app up and down while balancing the load. This matches the requirements of significant variations in demand and cost-effectiveness.</p><p><br></p><p>Use Compute Engine instances with fixed resources, manually adding or removing instances as demand fluctuates. -&gt;&nbsp;Incorrect. This strategy does not provide automatic scaling in response to demand and would require significant overhead in monitoring and manual intervention.</p><p><br></p><p>Use preemptible VMs for all parts of the application. -&gt;&nbsp;Incorrect. While preemptible VMs can be cost-effective, they are not guaranteed to be available when needed, and they can be terminated at any time, making them unsuitable for a critical application that needs to be highly available.</p><p><br></p><p>Use sole-tenant nodes for hosting the application. -&gt;&nbsp;Incorrect. Sole-tenant nodes are physical Compute Engine servers dedicated to hosting VM instances for your specific project. While they provide isolation, they are not cost-effective for applications with fluctuating demand since you are paying for the entire node regardless of usage.</p>",
                "answers": [
                    "<p>Deploy the application on App Engine standard environment.</p>",
                    "<p>Use Compute Engine instances with fixed resources, manually adding or removing instances as demand fluctuates.</p>",
                    "<p>Use preemptible VMs for all parts of the application.</p>",
                    "<p>Use sole-tenant nodes for hosting the application.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization is planning to design a cloud solution architecture for a new application that will have significant variations in demand and should be cost-effective. As a cloud architect, what strategy would you propose for this scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297690,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>How would you design a highly available and scalable solution for storing and processing large amounts of time-series data in Google Cloud, taking into consideration the requirement to minimize latency for data retrieval and processing, and cost-effectiveness?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Bigtable for the primary storage, with Cloud Pub/Sub for real-time data processing and analysis. -&gt;&nbsp;Correct. Bigtable is a highly scalable NoSQL database designed for storing and processing large amounts of time-series data with low latency. It provides automatic sharding and replication, ensuring high availability and durability. Cloud Pub/Sub allows for real-time data processing and analysis. This combination of Bigtable and Pub/Sub provides a highly available and scalable solution for storing and processing time-series data in Google Cloud while also minimizing latency and cost.</p><p><br></p><p>Use Cloud Datastore as the primary storage, with occasional backups to Cloud Storage for data durability. -&gt; Incorrect. It is not the best solution because Cloud Datastore is not optimized for time-series data and may not provide the low-latency required for real-time analysis.</p><p><br></p><p>Use Cloud Storage as the primary storage, with Cloud Dataflow for batch data processing and analysis. -&gt; Incorrect. It may work for batch data processing and analysis, but may not provide the low-latency required for real-time analysis of time-series data.</p><p><br></p><p>Use Cloud SQL for the primary storage, with Cloud Functions for real-time data processing and analysis. -&gt; Incorrect. It is not the best solution because Cloud SQL is not optimized for time-series data and may not provide the scalability required for storing and processing large amounts of time-series data.</p><p><br></p><p>https://cloud.google.com/bigtable/docs</p><p>https://cloud.google.com/bigtable/docs/schema-design-time-series</p>",
                "answers": [
                    "<p>Use Cloud Datastore as the primary storage, with occasional backups to Cloud Storage for data durability.</p>",
                    "<p>Use Bigtable for the primary storage, with Cloud Pub/Sub for real-time data processing and analysis.</p>",
                    "<p>Use Cloud Storage as the primary storage, with Cloud Dataflow for batch data processing and analysis.</p>",
                    "<p>Use Cloud SQL for the primary storage, with Cloud Functions for real-time data processing and analysis.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "How would you design a highly available and scalable solution for storing and processing large amounts of time-series data in Google Cloud, taking into consideration the requirement to minimize latency for data retrieval and processing, and cost-effectiveness?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297692,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>How would you design a solution for securely storing and accessing sensitive data in Google Cloud, considering the requirement for encryption at rest, secure data transfer, and fine-grained access control?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Key Management Service for encryption key management, with Cloud Storage for data storage and Cloud IAM for access control. -&gt;&nbsp;Correct. It provides encryption at rest through Cloud Storage's default encryption, secure data transfer with Cloud Storage's built-in HTTPS support, and fine-grained access control through Cloud IAM's role-based access control. Additionally, using Cloud Key Management Service for encryption key management allows for better control and management of encryption keys. Overall, this option covers all the requirements for securely storing and accessing sensitive data in Google Cloud.</p><p><br></p><p>Use Cloud Storage with customer-supplied encryption keys and Cloud VPN for secure data transfer. -&gt; Incorrect. While this provides encryption at rest and secure data transfer, it doesn't address the need for fine-grained access control.</p><p><br></p><p>Use Cloud Bigtable with customer-supplied encryption keys and Cloud Armor for network security. -&gt; Incorrect. It provides encryption at rest and network security, but doesn't address the need for fine-grained access control.</p><p><br></p><p>Use Cloud SQL with server-side encryption and Cloud Identity-Aware Proxy for fine-grained access control. -&gt; Incorrect. While this provides encryption at rest and fine-grained access control, it doesn't address the need for secure data transfer.</p>",
                "answers": [
                    "<p>Use Cloud Storage with customer-supplied encryption keys and Cloud VPN for secure data transfer.</p>",
                    "<p>Use Cloud Bigtable with customer-supplied encryption keys and Cloud Armor for network security.</p>",
                    "<p>Use Cloud SQL with server-side encryption and Cloud Identity-Aware Proxy for fine-grained access control.</p>",
                    "<p>Use Cloud Key Management Service for encryption key management, with Cloud Storage for data storage and Cloud IAM for access control.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "How would you design a solution for securely storing and accessing sensitive data in Google Cloud, considering the requirement for encryption at rest, secure data transfer, and fine-grained access control?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297694,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>How would you design a solution for running a large-scale, highly-available data warehousing platform on Google Cloud, considering the requirement for fast query performance, cost-effectiveness, and the ability to handle petabyte-scale data?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use BigQuery for data warehousing and analysis, with Cloud Dataflow for data processing and Cloud Storage for data storage. -&gt;&nbsp;Correct. BigQuery is a fully-managed, cloud-native data warehousing solution that is optimized for fast query performance and can handle petabyte-scale data. Cloud Dataflow can be used for processing large datasets in a fully-managed way, and Cloud Storage provides a cost-effective and scalable way to store large volumes of data. Overall, this option covers all the requirements for running a large-scale, highly-available data warehousing platform on Google Cloud.</p><p><br></p><p>Use Apache Hive on Compute Engine for data warehousing, with Apache Hadoop HDFS for data storage and Apache Zeppelin for data analysis. -&gt; Incorrect. While this approach may provide some cost savings, it can be complex to manage and maintain at scale. Additionally, Hive may not be the most performant option for querying large datasets.</p><p><br></p><p>Use Cloud Dataproc for data processing, with Cloud Bigtable for data storage and Cloud Datalab for data analysis. -&gt; Incorrect. While Dataproc can be a good option for processing large datasets using Apache Hadoop and Spark, it may not be the most cost-effective solution for data warehousing at petabyte scale. Cloud Bigtable is optimized for handling large volumes of structured data, but may not be the best fit for unstructured data.</p><p><br></p><p>Use Apache Impala on Google Kubernetes Engine for data warehousing and analysis, with Apache Kudu for data storage and Apache Superset for data visualization. -&gt; Incorrect. While Impala can provide fast SQL querying of large datasets, managing the infrastructure for Impala and Kudu can be complex and time-consuming. Additionally, Kubernetes may not be the best fit for managing large-scale data processing workloads.</p>",
                "answers": [
                    "<p>Use BigQuery for data warehousing and analysis, with Cloud Dataflow for data processing and Cloud Storage for data storage.</p>",
                    "<p>Use Apache Hive on Compute Engine for data warehousing, with Apache Hadoop HDFS for data storage and Apache Zeppelin for data analysis.</p>",
                    "<p>Use Cloud Dataproc for data processing, with Cloud Bigtable for data storage and Cloud Datalab for data analysis.</p>",
                    "<p>Use Apache Impala on Google Kubernetes Engine for data warehousing and analysis, with Apache Kudu for data storage and Apache Superset for data visualization.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "How would you design a solution for running a large-scale, highly-available data warehousing platform on Google Cloud, considering the requirement for fast query performance, cost-effectiveness, and the ability to handle petabyte-scale data?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297696,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>How would you design a solution for running a large-scale, highly-available, and scalable transactional application on Google Cloud, considering the requirement for fast page load times, and the ability to handle billions of daily transactions?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud CDN for content delivery, with Cloud Functions for serverless computing and Cloud Spanner for database management. -&gt; Correct. Cloud CDN for content delivery ensures fast page load times. Cloud Functions, a serverless compute solution, can scale automatically to handle large loads. Cloud Spanner, a globally-distributed, horizontally scalable relational database service, is designed to handle high transaction rates and ensure high availability, which fits the requirement perfectly.</p><p><br></p><p>Use Cloud Storage for static asset storage, with Cloud CDN for content delivery and Cloud SQL for database management. -&gt;&nbsp;Incorrect. Using Cloud Storage for static asset storage and Cloud CDN for content delivery are good choices for ensuring fast page load times. However, while Cloud SQL is a reliable relational database service, it might struggle to handle billions of daily transactions at scale as compared to a globally distributed, horizontally scalable database service like Cloud Spanner.</p><p><br></p><p>Use Cloud Storage for static asset storage, with Cloud Load Balancer for traffic distribution and BigQuery for database management. -&gt;&nbsp;Incorrect. While Cloud Storage and Cloud Load Balancer are suitable for static asset storage and traffic distribution respectively, BigQuery is not ideal for transactional applications. It's a data warehousing solution designed for analytics and not optimized for transactional workloads.</p><p><br></p><p>Use Cloud CDN for content delivery, with Cloud Load Balancer for traffic distribution and Cloud Firestore for database management. -&gt;&nbsp;Incorrect. Cloud CDN and Cloud Load Balancer are suitable for content delivery and traffic distribution respectively. However, Cloud Firestore, although a powerful NoSQL database, is not optimized for handling billions of daily transactions as efficiently as Cloud Spanner. Firestore is more suited for real-time updates and sync across app clients, not for heavy transactional loads.</p>",
                "answers": [
                    "<p>Use Cloud Storage for static asset storage, with Cloud CDN for content delivery and Cloud SQL for database management.</p>",
                    "<p>Use Cloud Storage for static asset storage, with Cloud Load Balancer for traffic distribution and BigQuery for database management.</p>",
                    "<p>Use Cloud CDN for content delivery, with Cloud Functions for serverless computing and Cloud Spanner for database management.</p>",
                    "<p>Use Cloud CDN for content delivery, with Cloud Load Balancer for traffic distribution and Cloud Firestore for database management.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "How would you design a solution for running a large-scale, highly-available, and scalable transactional application on Google Cloud, considering the requirement for fast page load times, and the ability to handle billions of daily transactions?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297698,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is planning to design a new cloud solution architecture that not only addresses its current needs but is also robust enough to accommodate future improvements and technological advancements. As a Google Professional Cloud Architect, you are tasked with ensuring the solution is scalable, cost-effective, and able to integrate future cloud and technology innovations. Considering Google Cloud Platform's (GCP) capabilities, which of the following approaches would best align with these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Using Google Kubernetes Engine (GKE) to containerize and orchestrate microservices. -&gt;&nbsp;Correct. GKE provides a scalable and flexible environment for managing microservices, making it easier to implement future solution improvements and integrate new technologies.</p><p><br></p><p>Implementing a monolithic architecture on Compute Engine for all services. -&gt;&nbsp;Incorrect. A monolithic architecture may hinder scalability and flexibility, making it challenging to adopt future improvements and technology changes.</p><p><br></p><p>Relying solely on preemptible VMs to reduce costs, without considering high availability. -&gt;&nbsp;Incorrect. Using preemptible VMs can lower costs, but it may compromise high availability and reliability, which are crucial for future-proofing and scalability in response to technology advancements.</p><p><br></p><p>Designing the architecture to rely on a single region and availability zone for all services. -&gt;&nbsp;Incorrect. This approach does not consider the high availability and disaster recovery aspects of cloud architecture. Relying on a single region and availability zone makes the system vulnerable to outages and fails to leverage the global infrastructure of GCP, limiting the ability to adapt to future geographical or technological expansions.</p>",
                "answers": [
                    "<p>Using Google Kubernetes Engine (GKE) to containerize and orchestrate microservices.</p>",
                    "<p>Implementing a monolithic architecture on Compute Engine for all services.</p>",
                    "<p>Relying solely on preemptible VMs to reduce costs, without considering high availability.</p>",
                    "<p>Designing the architecture to rely on a single region and availability zone for all services.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company is planning to design a new cloud solution architecture that not only addresses its current needs but is also robust enough to accommodate future improvements and technological advancements. As a Google Professional Cloud Architect, you are tasked with ensuring the solution is scalable, cost-effective, and able to integrate future cloud and technology innovations. Considering Google Cloud Platform's (GCP) capabilities, which of the following approaches would best align with these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297700,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>How would you design a solution for running a large-scale, multi-cloud, and secure big data analytics platform on Google Cloud, considering the requirement for real-time data processing, scalability, and the ability to handle petabytes of structured and unstructured data from multiple sources?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Dataproc for batch processing, with Cloud Dataflow for real-time data processing and Cloud BigQuery for data warehousing. -&gt; Correct. Cloud Dataproc is a managed Hadoop and Spark service designed for batch processing, and Cloud Dataflow is designed for both batch and real-time data processing. Cloud BigQuery is a fully-managed, serverless data warehouse that enables super-fast SQL queries and provides the ability to process petabytes of data, making it suitable for handling structured and unstructured data from multiple sources.</p><p><br></p><p>Use Cloud Dataproc for batch processing, with Cloud Dataflow for real-time data processing and Cloud Bigtable for data warehousing. -&gt;&nbsp;Incorrect. While Cloud Dataproc and Cloud Dataflow are suitable for batch and real-time processing, respectively, Cloud Bigtable, though a high-performance NoSQL database, is not designed for data warehousing needs. It doesn't provide the SQL interface and powerful analytical capabilities offered by BigQuery.</p><p><br></p><p>Use Cloud Tasks for batch processing, with Cloud Pub/Sub for real-time data streaming and Cloud BigQuery for data warehousing. -&gt;&nbsp;Incorrect. Cloud Tasks is designed for asynchronous task execution, which is not a fit for the requirement of batch processing large-scale data. Cloud Pub/Sub is good for real-time data streaming, but it does not offer processing capabilities. BigQuery is an excellent choice for a data warehousing solution, but other components of this option don't meet the requirements.</p><p><br></p><p>Use Cloud Dataproc for batch processing, with Cloud Pub/Sub for real-time data streaming and Cloud Bigtable for data warehousing. -&gt;&nbsp;Incorrect. Although Cloud Dataproc and Cloud Pub/Sub are appropriate for batch processing and real-time data streaming, Cloud Bigtable isn't a suitable choice for a data warehousing solution as it doesn't provide the SQL interface and powerful analytical capabilities offered by BigQuery.</p>",
                "answers": [
                    "<p>Use Cloud Dataproc for batch processing, with Cloud Dataflow for real-time data processing and Cloud Bigtable for data warehousing.</p>",
                    "<p>Use Cloud Tasks for batch processing, with Cloud Pub/Sub for real-time data streaming and Cloud BigQuery for data warehousing.</p>",
                    "<p>Use Cloud Dataproc for batch processing, with Cloud Dataflow for real-time data processing and Cloud BigQuery for data warehousing.</p>",
                    "<p>Use Cloud Dataproc for batch processing, with Cloud Pub/Sub for real-time data streaming and Cloud Bigtable for data warehousing.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "How would you design a solution for running a large-scale, multi-cloud, and secure big data analytics platform on Google Cloud, considering the requirement for real-time data processing, scalability, and the ability to handle petabytes of structured and unstructured data from multiple sources?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297702,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>How would you design a solution for running a large-scale, multi-cloud, and secure disaster recovery plan for a global enterprise, considering the requirement for real-time data replication, low recovery time objective (RTO), and the ability to handle multiple TBs of data from multiple locations?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Storage Transfer Service for data replication, with Cloud Dedicated Interconnect for network connectivity and Cloud Spanner for database management. -&gt;&nbsp;Correct. Cloud Storage Transfer Service is suitable for data replication, and Cloud Dedicated Interconnect provides high-bandwidth, low-latency connections for network connectivity. Cloud Spanner is a globally-distributed, horizontally scalable, relational database service that is designed for high transaction rates and can handle multiple TBs of data, ensuring high availability. It can effectively serve as the database management system in a disaster recovery plan.</p><p><br></p><p>Use Cloud Storage Transfer Service for data replication, with Cloud VPN for network connectivity and Cloud Storage for database management. -&gt;&nbsp;Incorrect. Cloud Storage Transfer Service and Cloud VPN are suitable for data replication and network connectivity, respectively. However, Cloud Storage isn't a database management system, so it doesn't offer the transactional and querying capabilities typical databases provide. It's more suitable for storing unstructured data.</p><p><br></p><p>Use Cloud Functions for data replication, with Cloud Interconnect for network connectivity and Cloud Bigtable for database management. -&gt;&nbsp;Incorrect. Cloud Functions is a serverless execution environment and isn't primarily designed for data replication tasks. Cloud Interconnect is suitable for network connectivity, but Cloud Bigtable, while powerful for large-scale, low-latency read/write workloads, is not globally-distributed, which could pose a problem for a global enterprise.</p><p><br></p><p>Use Cloud Storage Transfer Service for data replication, with Cloud Dedicated Interconnect for network connectivity and Cloud Filestore for database management. -&gt;&nbsp;Incorrect. Cloud Storage Transfer Service and Cloud Dedicated Interconnect are appropriate for data replication and network connectivity. However, Cloud Filestore is a managed file storage service for applications that require a filesystem interface and a shared filesystem for data, but it's not designed for handling database workloads.</p>",
                "answers": [
                    "<p>Use Cloud Storage Transfer Service for data replication, with Cloud VPN for network connectivity and Cloud Storage for database management.</p>",
                    "<p>Use Cloud Functions for data replication, with Cloud Interconnect for network connectivity and Cloud Bigtable for database management.</p>",
                    "<p>Use Cloud Storage Transfer Service for data replication, with Cloud Dedicated Interconnect for network connectivity and Cloud Filestore for database management.</p>",
                    "<p>Use Cloud Storage Transfer Service for data replication, with Cloud Dedicated Interconnect for network connectivity and Cloud Spanner for database management.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "How would you design a solution for running a large-scale, multi-cloud, and secure disaster recovery plan for a global enterprise, considering the requirement for real-time data replication, low recovery time objective (RTO), and the ability to handle multiple TBs of data from multiple locations?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297704,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Given a scenario where a company wants to migrate their on-premises infrastructure to Google Cloud, which solution is most appropriate to securely transfer sensitive data to Google Cloud while meeting the company's compliance requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud VPN to establish a secure connection from the on-premises infrastructure to Google Cloud, then use <code>gsutil</code> to transfer data to Cloud Storage buckets. -&gt;&nbsp;Correct. Using Cloud VPN to establish a secure connection from the on-premises infrastructure to Google Cloud ensures that data transfer occurs over an encrypted tunnel. This approach provides a secure and compliant method for transferring sensitive data. After establishing the VPN connection, gsutil can be used to transfer the data to Cloud Storage buckets securely.</p><p><br></p><p>Use <code>gsutil</code> to transfer data directly from the on-premises infrastructure to Cloud Storage buckets. -&gt;&nbsp;Incorrect. It may not provide the necessary security and compliance measures required for transferring sensitive data securely. It lacks the encryption and secure connection provided by other options.</p><p><br></p><p>Use Cloud Storage Transfer Service to transfer data from the on-premises infrastructure to Cloud Storage buckets. -&gt;&nbsp;Incorrect. It may not address the need for secure connection and compliance requirements directly.</p><p><br></p><p>Use <code>gcloud</code> to transfer data directly from the on-premises infrastructure to Cloud Storage buckets. -&gt;&nbsp;Incorrect. It may not provide the necessary security measures required for transferring sensitive data securely. It lacks the encryption and secure connection provided by other options.</p>",
                "answers": [
                    "<p>Use <code>gsutil</code> to transfer data directly from the on-premises infrastructure to Cloud Storage buckets.</p>",
                    "<p>Use Cloud Storage Transfer Service to transfer data from the on-premises infrastructure to Cloud Storage buckets.</p>",
                    "<p>Use Cloud VPN to establish a secure connection from the on-premises infrastructure to Google Cloud, then use <code>gsutil</code> to transfer data to Cloud Storage buckets.</p>",
                    "<p>Use <code>gcloud</code> to transfer data directly from the on-premises infrastructure to Cloud Storage buckets.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "Given a scenario where a company wants to migrate their on-premises infrastructure to Google Cloud, which solution is most appropriate to securely transfer sensitive data to Google Cloud while meeting the company's compliance requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297706,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Consider a scenario where a global financial services company wants to move their legacy monolithic application to the cloud. The application is currently hosted on-premise and relies on a combination of custom-built software, third-party software, and hardware appliances. The new cloud-based solution must meet the following requirements:</p><ul><li><p>support high availability and disaster recovery</p></li><li><p>maintain the current level of security and compliance</p></li><li><p>ensure data privacy and data residency requirements are met for all regions</p></li><li><p>optimize cost while providing scalable and elastic resources</p></li><li><p>minimize downtime during migration</p></li></ul><p><br></p><p>What is the most appropriate solution to meet the requirements outlined above?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Migrate the application to Google Cloud using Compute Engine virtual machines and Cloud Storage for data storage. Use Load Balancer for traffic management and Cloud VPN for secure communication between on-premise and cloud resources. Implement a multi-region deployment with active-active replication. -&gt;&nbsp;Correct. It is the correct answer for the following reasons:</p><ul><li><p>Compute Engine virtual machines and Cloud Storage can handle legacy monolithic applications with ease and provide high availability and disaster recovery features. They also allow the company to maintain the current level of security and compliance by implementing various security measures.</p></li><li><p>Load Balancer can help in traffic management, and Cloud VPN can provide secure communication between on-premise and cloud resources.</p></li><li><p>Implementing a multi-region deployment with active-active replication ensures data privacy and data residency requirements are met for all regions.</p></li><li><p>The solution provided with A meets the requirement to optimize cost while providing scalable and elastic resources.</p></li><li><p>By migrating the application to Google Cloud using Compute Engine virtual machines and Cloud Storage, the company can minimize downtime during migration.</p></li></ul><p><br></p><p>Rebuild the application using App Engine and Google Cloud SQL for data storage. Use Cloud CDN for traffic management and Cloud Interconnect for secure communication between on-premise and cloud resources. Implement a multi-zone deployment with active-standby replication. -&gt; Incorrect. Rebuilding the application may require a lot of resources, and it may take a considerable amount of time to rebuild the application.</p><p><br></p><p>Migrate the application to Google Cloud using Compute Engine virtual machines and Cloud Storage for data storage. Use Google Cloud DNS for traffic management and Virtual Private Cloud (VPC) for secure communication between on-premise and cloud resources. Implement a multizone deployment with active-active replication. -&gt; Incorrect. Google Cloud DNS does not provide traffic management capabilities like Load Balancer.</p><p><br></p><p>Rebuild the application using Cloud Functions and Firestore for data storage. Use Cloud CDN for traffic management and loud VPN for secure communication between on-premise and cloud resources. Implement a multi-region deployment with active-standby replication. -&gt; Incorrect. Cloud Functions is used for serverless computing and may not be the best fit for a legacy monolithic application. Firestore also may not be suitable for data storage for such applications.</p>",
                "answers": [
                    "<p>Migrate the application to Google Cloud using Compute Engine virtual machines and Cloud Storage for data storage. Use Load Balancer for traffic management and Cloud VPN for secure communication between on-premise and cloud resources. Implement a multi-region deployment with active-active replication.</p>",
                    "<p>Rebuild the application using App Engine and Google Cloud SQL for data storage. Use Cloud CDN for traffic management and Cloud Interconnect for secure communication between on-premise and cloud resources. Implement a multi-zone deployment with active-standby replication.</p>",
                    "<p>Migrate the application to Google Cloud using Compute Engine virtual machines and Cloud Storage for data storage. Use Google Cloud DNS for traffic management and Virtual Private Cloud (VPC) for secure communication between on-premise and cloud resources. Implement a multizone deployment with active-active replication.</p>",
                    "<p>Rebuild the application using Cloud Functions and Firestore for data storage. Use Cloud CDN for traffic management and loud VPN for secure communication between on-premise and cloud resources. Implement a multi-region deployment with active-standby replication.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Consider a scenario where a global financial services company wants to move their legacy monolithic application to the cloud. The application is currently hosted on-premise and relies on a combination of custom-built software, third-party software, and hardware appliances. The new cloud-based solution must meet the following requirements:support high availability and disaster recoverymaintain the current level of security and complianceensure data privacy and data residency requirements are met for all regionsoptimize cost while providing scalable and elastic resourcesminimize downtime during migrationWhat is the most appropriate solution to meet the requirements outlined above?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297708,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You have been assigned to design a storage system for a multinational company that anticipates substantial growth in data over the next two years. The data will be accessed frequently and changes infrequently. It is crucial to minimize latency and ensure the data's availability globally. Which Cloud Storage class should you choose?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Multi-Regional Storage -&gt;&nbsp;Correct. Multi-Regional Storage is designed for maximum availability and redundancy, with data geo-replicated to multiple regions. It is ideal for frequently accessed data, such as serving web content, interactive workloads, or data accessed by users around the world.</p><p><br></p><p>Regional Storage -&gt;&nbsp;Incorrect. Regional Storage is for data accessed frequently within a specific geographic region. It does not offer the global availability that is needed in this case.</p><p><br></p><p>Archive Storage -&gt;&nbsp;Incorrect. Archive Storage is for data that will be stored for long periods without the need for access, such as for long-term backups and compliance. It is not suitable for data that needs to be accessed frequently.</p><p><br></p><p>Nearline Storage -&gt;&nbsp;Incorrect. Nearline Storage is for infrequently accessed data, such as backups and long-tail multimedia content. It is not suitable for data that needs to be accessed frequently.</p>",
                "answers": [
                    "<p>Multi-Regional Storage</p>",
                    "<p>Regional Storage</p>",
                    "<p>Archive Storage</p>",
                    "<p>Nearline Storage</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You have been assigned to design a storage system for a multinational company that anticipates substantial growth in data over the next two years. The data will be accessed frequently and changes infrequently. It is crucial to minimize latency and ensure the data's availability globally. Which Cloud Storage class should you choose?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297712,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A company wants to migrate their existing on-premise data center to Google Cloud. The company's current data center has 100TB of data, with 1000 servers and 100 network devices. They require a high-availability solution with the lowest possible latency and maximum security. What would be the most cost-effective, scalable and secure solution to meet the company's requirements for data center migration to Google Cloud?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Storage with Cloud Interconnect for data transfer and storage. Deploy VMs in multiple zones.. -&gt;&nbsp;Correct. The company wants a high-availability solution with the lowest possible latency and maximum security for their data center migration to Google Cloud. To achieve this, the company can use Cloud Storage with Cloud Interconnect for data transfer and storage, which offers a highly available and low-latency solution with secure connectivity between on-premise data centers and Google Cloud. Deploying VMs in multiple zones will increase the availability and reduce the risk of downtime. This solution is also cost-effective and scalable, as it allows for the company to pay for only what they use and scale up as needed.</p><p><br></p><p>Use Cloud Storage with Cloud VPN for data transfer and storage. Deploy VMs in a single zone. -&gt; Incorrect. Using Cloud Storage with Cloud VPN for data transfer and storage, while still secure, does not offer the same low latency and high availability as Cloud Interconnect. Deploying VMs in a single zone is also not a highly available solution.</p><p><br></p><p>Use Cloud BigQuery with Cloud VPN for data transfer and storage. Deploy VMs in a single zone. -&gt; Incorrect. Using Cloud BigQuery, is not suitable for storing large amounts of unstructured data, as it is designed for analysis of structured data. Deploying VMs in a single zone is also not a highly available solution.</p><p><br></p><p>Use Compute Engine for storage. Deploy VMs in multiple regions with load balancing. -&gt; Incorrect. Using Compute Engine for storage, is not as cost-effective as using Cloud Storage, and deploying VMs in multiple regions with load balancing is not the most secure or low-latency solution.</p>",
                "answers": [
                    "<p>Use Cloud Storage with Cloud VPN for data transfer and storage. Deploy VMs in a single zone.</p>",
                    "<p>Use Cloud Storage with Cloud Interconnect for data transfer and storage. Deploy VMs in multiple zones.</p>",
                    "<p>Use Cloud BigQuery with Cloud VPN for data transfer and storage. Deploy VMs in a single zone.</p>",
                    "<p>Use Compute Engine for storage. Deploy VMs in multiple regions with load balancing.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "A company wants to migrate their existing on-premise data center to Google Cloud. The company's current data center has 100TB of data, with 1000 servers and 100 network devices. They require a high-availability solution with the lowest possible latency and maximum security. What would be the most cost-effective, scalable and secure solution to meet the company's requirements for data center migration to Google Cloud?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297714,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are designing a cloud solution for a multinational retail company with a complex multi-tier architecture. The company has a central web application that serves as a customer-facing e-commerce platform. The web application integrates with multiple backend systems, including a payment gateway, a warehouse management system, and an inventory management system. The company wants to ensure that the platform is highly available and secure, with a disaster recovery strategy in place in case of regional outages. In addition, the company wants to be able to deploy new features and updates to the platform with minimal downtime. Which of the following design patterns would you recommend to achieve this design?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement a multi-region active-active setup with multiple regional load balancers and backend systems in different regions, with real-time replication between regions. -&gt; Correct. It is the most effective way to achieve high availability and disaster recovery for a complex multi-tier architecture like the one described. This approach involves deploying the web application and backend systems across multiple regions and using load balancers to distribute traffic across the different regions. Real-time replication between regions ensures that data is consistent across all regions and that the platform remains highly available even in the event of regional outages.</p><p><br></p><p>Implement a regional active-active setup with a regional load balancer and multiple backend systems in different regions. -&gt; Incorrect. It may provide some degree of redundancy, but it may not be as effective as a multi-region setup in ensuring high availability and disaster recovery.</p><p><br></p><p>Implement a regional active-passive setup with a regional load balancer and multiple backend systems in different regions, with real-time replication between regions. -&gt; Incorrect. It may provide some level of disaster recovery, but it may not be as effective in ensuring high availability as an active-active setup.</p><p><br></p><p>Implement a global active-active setup with a global load balancer and multiple backend systems in different regions, with real-time replication between regions. -&gt; Incorrect. It may be more expensive and complex than a multi-region setup and may not provide significant benefits in terms of high availability and disaster recovery.</p>",
                "answers": [
                    "<p>Implement a regional active-active setup with a regional load balancer and multiple backend systems in different regions.</p>",
                    "<p>Implement a regional active-passive setup with a regional load balancer and multiple backend systems in different regions, with real-time replication between regions.</p>",
                    "<p>Implement a global active-active setup with a global load balancer and multiple backend systems in different regions, with real-time replication between regions.</p>",
                    "<p>Implement a multi-region active-active setup with multiple regional load balancers and backend systems in different regions, with real-time replication between regions.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "You are designing a cloud solution for a multinational retail company with a complex multi-tier architecture. The company has a central web application that serves as a customer-facing e-commerce platform. The web application integrates with multiple backend systems, including a payment gateway, a warehouse management system, and an inventory management system. The company wants to ensure that the platform is highly available and secure, with a disaster recovery strategy in place in case of regional outages. In addition, the company wants to be able to deploy new features and updates to the platform with minimal downtime. Which of the following design patterns would you recommend to achieve this design?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297716,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A large multinational corporation has an e-commerce platform with multiple microservices hosted in Google Cloud Platform. Each microservice has its own separate database, but the corporation wants to centralize the authentication process for all of its services. What is the most secure and scalable solution for centralized authentication in this scenario?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implementing Google Cloud IAP for each microservice. -&gt; Correct. Google Cloud IAP (Identity-Aware Proxy) is a solution that controls access to cloud applications running on Google Cloud Platform. It can secure the application by verifying user identity and the context of the request to determine if a user should be allowed access. It can act as a centralized point for authenticating all requests coming into the microservices, making it the most secure and scalable solution for centralized authentication in this scenario.</p><p><br></p><p>Implementing Google Cloud IAM for each microservice. -&gt;&nbsp;Incorrect. Google Cloud IAM (Identity and Access Management) is used to control who (users or services) has what type of access to which resources. While it's important for controlling access within a cloud environment, it doesn't provide the functionality to centralize authentication across multiple services.</p><p><br></p><p>Implementing a single OAuth 2.0 server using Google Cloud Endpoints. -&gt;&nbsp;Incorrect. An OAuth 2.0 server using Google Cloud Endpoints could handle authentication, but it wouldn't provide the same level of security as Cloud IAP. Also, setting up and maintaining a single OAuth server may not be as scalable or simple as using a managed service like Cloud IAP.</p><p><br></p><p>Implementing a single OAuth 2.0 server using Google Cloud Functions. -&gt;&nbsp;Incorrect. While a single OAuth 2.0 server could centralize authentication, Cloud Functions is more designed to execute code in response to events and isn't specifically tailored for managing secure and scalable authentication across multiple microservices.</p><p><br></p><p>https://cloud.google.com/iap/docs</p>",
                "answers": [
                    "<p>Implementing Google Cloud IAM for each microservice.</p>",
                    "<p>Implementing Google Cloud IAP for each microservice.</p>",
                    "<p>Implementing a single OAuth 2.0 server using Google Cloud Endpoints.</p>",
                    "<p>Implementing a single OAuth 2.0 server using Google Cloud Functions.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "A large multinational corporation has an e-commerce platform with multiple microservices hosted in Google Cloud Platform. Each microservice has its own separate database, but the corporation wants to centralize the authentication process for all of its services. What is the most secure and scalable solution for centralized authentication in this scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297718,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A financial company wants to move its existing data warehouse infrastructure to Google Cloud Platform to take advantage of its scalability and cost-effectiveness. The data warehouse processes large amounts of financial data and the company wants to ensure the data is secure and available at all times. Which of the following is the best solution for this requirement in Google Cloud Platform?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Using BigQuery for data warehousing -&gt; Correct. BigQuery is a fully-managed, highly scalable, and cost-effective data warehouse solution provided by Google Cloud Platform. It is designed to process and analyze large amounts of data in real-time, making it an ideal solution for financial companies that deal with large amounts of financial data. BigQuery provides multiple layers of security to ensure the data is secure, including encryption at rest and in transit, IAM roles and permissions, and audit logging. It also offers high availability, with multi-regional replication and automatic failover, ensuring that the data is available at all times.</p><p><br></p><p>Using Cloud SQL for data warehousing -&gt; Incorrect. Cloud SQL is a managed service for relational databases, but it is not designed for large-scale data warehousing. It is more suitable for smaller-scale databases.</p><p><br></p><p>Using Cloud Datastore for data warehousing -&gt; Incorrect. Cloud Datastore is a NoSQL document database, which is not designed for data warehousing. It is more suitable for applications that require high scalability and low-latency.</p><p><br></p><p>Using Cloud Storage for data warehousing -&gt; Incorrect. Cloud Storage is a highly scalable object storage solution that is suitable for storing unstructured data, but it is not designed for data warehousing. It is more suitable for storing files, backups, and archives.</p><p><br></p><p>https://cloud.google.com/bigquery/docs</p>",
                "answers": [
                    "<p>Using BigQuery for data warehousing</p>",
                    "<p>Using Cloud SQL for data warehousing</p>",
                    "<p>Using Cloud Datastore for data warehousing</p>",
                    "<p>Using Cloud Storage for data warehousing</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A financial company wants to move its existing data warehouse infrastructure to Google Cloud Platform to take advantage of its scalability and cost-effectiveness. The data warehouse processes large amounts of financial data and the company wants to ensure the data is secure and available at all times. Which of the following is the best solution for this requirement in Google Cloud Platform?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297720,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company has just acquired another business, and you've been tasked with integrating their existing DNS setup into Google Cloud DNS. The other business already has several registered domains that are critical to its operations, and it's important that the transition is smooth with zero downtime. How would you accomplish this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Create a new zone for each domain in Google Cloud DNS, manually recreate the DNS records, then update the name servers at the domain registrar to those of the Google Cloud DNS zones. -&gt; Correct. After manually creating the records in Google Cloud DNS, updating the name servers at the domain registrar to point to Google's DNS servers ensures a seamless transition.</p><p><br></p><p>Export DNS records from the existing provider, create a new zone for each domain in Google Cloud DNS, and import the records. -&gt; Incorrect. While it is true that Google Cloud DNS can import and export DNS records, this isn't done automatically and there's no direct way to import from an existing provider, especially given that different DNS providers have different export formats.</p><p><br></p><p>Use the <code>gcloud dns record-sets import</code> command to import the records from the existing provider, then update the name servers at the domain registrar. -&gt; Incorrect. This command is not available and cannot be used to directly import records from another DNS provider.</p><p><br></p><p>Contact Google support to have them manually transition the DNS records for you. -&gt; Incorrect. Google support would not manually transition DNS records for you. It's the responsibility of the Google Cloud user to manage their DNS records.</p>",
                "answers": [
                    "<p>Create a new zone for each domain in Google Cloud DNS, manually recreate the DNS records, then update the name servers at the domain registrar to those of the Google Cloud DNS zones.</p>",
                    "<p>Export DNS records from the existing provider, create a new zone for each domain in Google Cloud DNS, and import the records.</p>",
                    "<p>Use the <code>gcloud dns record-sets import</code> command to import the records from the existing provider, then update the name servers at the domain registrar.</p>",
                    "<p>Contact Google support to have them manually transition the DNS records for you.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company has just acquired another business, and you've been tasked with integrating their existing DNS setup into Google Cloud DNS. The other business already has several registered domains that are critical to its operations, and it's important that the transition is smooth with zero downtime. How would you accomplish this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297722,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization operates a large on-premise data warehouse that's currently based on a traditional RDBMS. You have been tasked with migrating this system to the Google Cloud Platform. You're concerned about downtime during the migration and want to minimize it. What would be the best approach?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use BigQuery Data Transfer Service to regularly update BigQuery with changes from the on-premise system, followed by a brief downtime to finalize the transfer and switch over. -&gt;&nbsp;Correct. This approach minimizes downtime by keeping BigQuery synchronized with the on-premise system until you're ready to switch over.</p><p><br></p><p>Use a manual process to extract, transform, and load (ETL) the data from the on-premise system to BigQuery. -&gt;&nbsp;Incorrect. This approach is time-consuming and error-prone, and it does not handle ongoing changes to the on-premise system, which may result in extended downtime.</p><p><br></p><p>Set up a VPN between the on-premise data center and Google Cloud, then use Database Migration Service to move the data. -&gt;&nbsp;Incorrect. Database Migration Service can be used to migrate from on-premises or another cloud to Cloud SQL, not BigQuery. It doesn't align with the typical use case of a data warehouse.</p><p><br></p><p>Use Cloud SQL as the destination for the migration, then transfer the data from Cloud SQL to BigQuery. -&gt;&nbsp;Incorrect. Cloud SQL is an operational database service and not designed for analytical workloads typical of data warehouses. This adds an unnecessary step and potential point of failure to the migration process.</p>",
                "answers": [
                    "<p>Use BigQuery Data Transfer Service to regularly update BigQuery with changes from the on-premise system, followed by a brief downtime to finalize the transfer and switch over.</p>",
                    "<p>Use a manual process to extract, transform, and load (ETL) the data from the on-premise system to BigQuery.</p>",
                    "<p>Set up a VPN between the on-premise data center and Google Cloud, then use Database Migration Service to move the data.</p>",
                    "<p>Use Cloud SQL as the destination for the migration, then transfer the data from Cloud SQL to BigQuery.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization operates a large on-premise data warehouse that's currently based on a traditional RDBMS. You have been tasked with migrating this system to the Google Cloud Platform. You're concerned about downtime during the migration and want to minimize it. What would be the best approach?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297724,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A financial services company is looking to store and process large amounts of sensitive customer data in the cloud, while meeting the requirements of strict regulatory compliance. What is the best solution for meeting these requirements using Google Cloud Platform?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Store the data in Cloud Storage and process it using Cloud Functions, while ensuring data encryption and access controls using Cloud Key Management Service and Cloud Identity and Access Management. -&gt; Correct. Cloud Storage allows you to store large amounts of data, while Cloud Functions can be used to process it. In addition, Cloud Key Management Service provides tools for managing cryptographic keys for cloud services, which can be used to encrypt sensitive data. Cloud Identity and Access Management ensures proper access control by allowing you to determine who (users or services) has what access to specific resources. This solution addresses not only the data storage and processing requirements but also the data security and compliance requirements, making it the most suitable option for a financial services company dealing with sensitive data.</p><p><br></p><p>Store the data in Cloud SQL for MySQL and process it using Cloud Functions. -&gt;&nbsp;Incorrect. Cloud SQL for MySQL and Cloud Functions can be used for storing and processing data. However, this option doesn't mention any specific services for handling sensitive data or maintaining strict regulatory compliance such as encryption, access control, or key management.</p><p><br></p><p>Store the data in Bigtable and process it using Cloud Dataflow. -&gt;&nbsp;Incorrect. Bigtable and Cloud Dataflow are good for storing and processing large amounts of data, but they don't specify solutions for data security or compliance.</p><p><br></p><p>Store the data in Cloud SQL for PostgreSQL and process it using Cloud Functions. -&gt;&nbsp;Incorrect. While Cloud SQL for PostgreSQL and Cloud Functions can handle data storage and processing, they don't cover additional requirements for sensitive data and regulatory compliance.</p>",
                "answers": [
                    "<p>Store the data in Cloud SQL for MySQL and process it using Cloud Functions.</p>",
                    "<p>Store the data in Bigtable and process it using Cloud Dataflow.</p>",
                    "<p>Store the data in Cloud SQL for PostgreSQL and process it using Cloud Functions.</p>",
                    "<p>Store the data in Cloud Storage and process it using Cloud Functions, while ensuring data encryption and access controls using Cloud Key Management Service and Cloud Identity and Access Management.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "A financial services company is looking to store and process large amounts of sensitive customer data in the cloud, while meeting the requirements of strict regulatory compliance. What is the best solution for meeting these requirements using Google Cloud Platform?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297726,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A healthcare organization is looking to implement a cloud-based electronic medical records (EMR) OLTP system that can securely store and process large amounts of sensitive patient data. The solution should also be able to support real-time data access and updates by authorized healthcare providers. Which of the following Google Cloud Platform services would you recommend for this requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud SQL for PostgreSQL -&gt;&nbsp;Correct. Cloud SQL for PostgreSQL is a fully managed relational database service that can provide secure storage and processing of large amounts of sensitive patient data. It is designed for OLTP workloads and can provide real-time data access and updates by authorized healthcare providers. </p><p><br></p><p>Cloud Storage -&gt; Incorrect. It is a durable and highly available object storage service but is not a relational database and may not be the best choice for OLTP workloads. </p><p><br></p><p>Cloud Bigtable -&gt; Incorrect. It is a NoSQL database that can be used for high-performance OLTP workloads, but it may require more operations work to maintain compared to Cloud SQL for PostgreSQL. </p><p><br></p><p>Cloud Firestore -&gt; Incorrect. It is a NoSQL document database that is designed for mobile and web application development and may not be the best choice for a healthcare organization's EMR system.</p><p><br></p><p>https://cloud.google.com/sql/docs/postgres</p>",
                "answers": [
                    "<p>Cloud Storage</p>",
                    "<p>Cloud Bigtable</p>",
                    "<p>Cloud SQL for PostgreSQL</p>",
                    "<p>Cloud Firestore</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "A healthcare organization is looking to implement a cloud-based electronic medical records (EMR) OLTP system that can securely store and process large amounts of sensitive patient data. The solution should also be able to support real-time data access and updates by authorized healthcare providers. Which of the following Google Cloud Platform services would you recommend for this requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297728,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A company is looking to process real-time data from multiple sources in order to provide real-time analytics to their clients. The company is expecting to process a large volume of data with an estimated peak of up to 1 million requests per second. The solution must be able to scale elastically to handle varying levels of incoming data, ensure low latency processing, and support the processing of data in real-time. Which of the following options would be the most effective approach to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Pub/Sub to ingest the data and feed it into Cloud Dataflow for real-time processing. -&gt; Correct. It is the most effective approach for this scenario. Cloud Pub/Sub is a fully-managed real-time messaging service that can handle high throughput and automatically scales to meet the incoming data demands. Cloud Dataflow can be used to process the data in real-time and provides low-latency processing capabilities. Additionally, Cloud Dataflow can easily scale up or down to handle varying volumes of incoming data.</p><p><br></p><p>Use Cloud Dataproc with Apache Spark Streaming to process the data in real-time. -&gt; Incorrect. It is a valid option for processing real-time data, but may not be the most effective approach for this particular scenario. While Cloud Dataproc is highly scalable and can support processing of real-time data, the company is expecting to process a large volume of data, which may not be efficiently processed using Apache Spark Streaming in a single cluster.</p><p><br></p><p>Use Cloud Dataflow with Apache Beam to process the data in batch mode and store the results in Cloud Bigtable. -&gt;&nbsp;Incorrect. It is not an effective approach for processing real-time data. Cloud Dataflow is designed for batch processing and cannot provide low-latency processing capabilities.</p><p><br></p><p>Use Cloud Dataflow to ingest the data and feed it into Cloud Bigtable for real-time processing. -&gt; Incorrect. It is not an effective approach for processing real-time data. Cloud Dataflow is designed for batch processing and cannot provide low-latency processing capabilities. Additionally, Cloud Bigtable is a NoSQL database designed for high-throughput and high-scalability of large amounts of data, but may not be an ideal choice for real-time processing of data.</p>",
                "answers": [
                    "<p>Use Cloud Dataproc with Apache Spark Streaming to process the data in real-time.</p>",
                    "<p>Use Cloud Dataflow with Apache Beam to process the data in batch mode and store the results in Cloud Bigtable.</p>",
                    "<p>Use Cloud Pub/Sub to ingest the data and feed it into Cloud Dataflow for real-time processing.</p>",
                    "<p>Use Cloud Dataflow to ingest the data and feed it into Cloud Bigtable for real-time processing.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "A company is looking to process real-time data from multiple sources in order to provide real-time analytics to their clients. The company is expecting to process a large volume of data with an estimated peak of up to 1 million requests per second. The solution must be able to scale elastically to handle varying levels of incoming data, ensure low latency processing, and support the processing of data in real-time. Which of the following options would be the most effective approach to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297730,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is developing an application that requires a high volume of read and write operations on the database. The application will be hosted on Google Compute Engine instances. However, the database is expected to grow significantly over time, and the management wants to ensure that the disk performance remains high. As a cloud architect, what type of persistent disk would you recommend for optimal performance?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>SSD persistent disk, because it provides high IOPS and throughput. -&gt; Correct. SSD persistent disks provide high IOPS and throughput, which makes them ideal for databases with high read/write operations. They also persist data across instance lifecycle events, making them the best choice in this scenario.</p><p><br></p><p>Standard persistent disk, because it provides cost-effective storage. -&gt; Incorrect. While Standard persistent disks are cost-effective and deliver consistent performance, they do not provide the high IOPS (input/output operations per second) and throughput needed for a database with a high volume of read/write operations.</p><p><br></p><p>Local SSD, because it provides high IOPS and low latency. -. Incorrect. Although Local SSDs offer high IOPS and low latency, they are ephemeral. This means that they do not persist data across instance stop, restart, or deletion, making them not ideal for database storage where data persistence is critical.</p><p><br></p><p>Use Cloud Storage instead of a persistent disk for high performance. -&gt; Incorrect. Cloud Storage is an object storage service and it is not designed for high-volume read/write operations as required by most databases.</p>",
                "answers": [
                    "<p>SSD persistent disk, because it provides high IOPS and throughput.</p>",
                    "<p>Standard persistent disk, because it provides cost-effective storage.</p>",
                    "<p>Local SSD, because it provides high IOPS and low latency.</p>",
                    "<p>Use Cloud Storage instead of a persistent disk for high performance.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company is developing an application that requires a high volume of read and write operations on the database. The application will be hosted on Google Compute Engine instances. However, the database is expected to grow significantly over time, and the management wants to ensure that the disk performance remains high. As a cloud architect, what type of persistent disk would you recommend for optimal performance?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297732,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization has a multi-tier application running on Google Cloud and follows an Agile development process. You are assigned the task of setting up a testing environment that is identical to the production environment. This testing environment should be isolated and must not impact the production environment in any way. It should also be easily reproducible and scalable to facilitate multiple parallel testing efforts. How should you approach this task?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Google Deployment Manager to replicate the infrastructure of the production environment in the testing environment. -&gt; Correct. Using Google Deployment Manager is the most effective approach here. Deployment Manager allows you to define and deploy resources in a repeatable way using configuration files, which makes it easier to recreate identical environments and scale the testing efforts.</p><p><br></p><p>Use Google Compute Engine to manually create and configure VMs for the testing environment that match the production environment. -&gt; Incorrect. While it is possible to use Google Compute Engine to manually create and configure VMs for the testing environment, this process can be labor-intensive and may not be easily reproducible or scalable.</p><p><br></p><p>Use Google Kubernetes Engine and set up a separate namespace for testing while using the same cluster as the production environment. -&gt; Incorrect. While Google Kubernetes Engine could provide an isolated environment by using namespaces, using the same cluster as the production environment could have implications on the cluster resources which could inadvertently affect the production environment.</p><p><br></p><p>Use Google Cloud Functions and create separate functions for testing. -&gt; Incorrect. Cloud Functions is a serverless execution environment for building and connecting cloud services, and it's not designed to recreate complete multi-tier environments like a testing environment.</p>",
                "answers": [
                    "<p>Use Google Deployment Manager to replicate the infrastructure of the production environment in the testing environment.</p>",
                    "<p>Use Google Compute Engine to manually create and configure VMs for the testing environment that match the production environment.</p>",
                    "<p>Use Google Kubernetes Engine and set up a separate namespace for testing while using the same cluster as the production environment.</p>",
                    "<p>Use Google Cloud Functions and create separate functions for testing.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization has a multi-tier application running on Google Cloud and follows an Agile development process. You are assigned the task of setting up a testing environment that is identical to the production environment. This testing environment should be isolated and must not impact the production environment in any way. It should also be easily reproducible and scalable to facilitate multiple parallel testing efforts. How should you approach this task?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297734,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is using a Google Kubernetes Engine (GKE) to run a mission-critical web application. The application's traffic patterns are inconsistent, with significant spikes in demand at unpredictable intervals. You are tasked with ensuring the application is highly available and responsive, while maintaining cost-effectiveness. Which of the following autoscaling configurations should you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Configure GKE autoscaling with CPU utilization as the primary metric, set at 80%. -&gt;&nbsp;Correct. GKE autoscaling with CPU utilization as the primary metric, set at 80%, ensures that new pods are added when the CPU utilization exceeds 80%. This helps in managing sudden increases in traffic, thereby ensuring high availability and responsiveness of the application.</p><p><br></p><p>Set up a custom autoscaling policy based on network traffic, triggering scaling at 90% of peak traffic. -&gt; Incorrect. Autoscaling based on network traffic can be challenging to predict and manage, as traffic can be bursty and not always correlate with resource demand. In many cases, CPU utilization is a more reliable indicator of resource demand.</p><p><br></p><p>Configure GKE autoscaling to scale based on memory utilization, set at 70%. -&gt; Incorrect. Memory utilization, similar to network traffic, can be an unpredictable metric for autoscaling. If a pod is memory-intensive but not CPU-intensive, scaling based on memory utilization could lead to over-provisioning.</p><p><br></p><p>Set up a custom autoscaling policy based on the number of incoming requests, triggering scaling after 1000 requests per second. -&gt; Incorrect. Autoscaling based on the number of incoming requests can be unpredictable and may not always correlate with resource demand. For example, 1000 light requests per second might not need as much resources as 500 heavy requests per second.</p>",
                "answers": [
                    "<p>Configure GKE autoscaling with CPU utilization as the primary metric, set at 80%.</p>",
                    "<p>Set up a custom autoscaling policy based on network traffic, triggering scaling at 90% of peak traffic.</p>",
                    "<p>Configure GKE autoscaling to scale based on memory utilization, set at 70%.</p>",
                    "<p>Set up a custom autoscaling policy based on the number of incoming requests, triggering scaling after 1000 requests per second.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company is using a Google Kubernetes Engine (GKE) to run a mission-critical web application. The application's traffic patterns are inconsistent, with significant spikes in demand at unpredictable intervals. You are tasked with ensuring the application is highly available and responsive, while maintaining cost-effectiveness. Which of the following autoscaling configurations should you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297736,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are migrating a complex application to Google Cloud. The application consists of multiple microservices, each developed by different teams. Your goal is to implement a CI/CD pipeline that allows for independent release cycles for each microservice. Which approach should you take?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement a separate Cloud Build trigger for each microservice and separate the deployments. -&gt;&nbsp;Correct. This approach allows each microservice to be built and deployed independently, facilitating separate release cycles for each service.</p><p><br></p><p>Use Cloud Build with a single trigger to build and deploy all microservices simultaneously. -&gt;&nbsp;Incorrect. This approach does not support independent release cycles for each microservice as it will trigger a build and deployment for all services simultaneously.</p><p><br></p><p>Use a single Cloud Function to trigger the build and deployment for all microservices. -&gt;&nbsp;Incorrect. While Cloud Functions can trigger Cloud Build, using a single Cloud Function for all services does not support independent release cycles.</p><p><br></p><p>Use Google App Engine to deploy all microservices as a single service. -&gt;&nbsp;Incorrect. While App Engine is a powerful platform for running applications, deploying all microservices as a single service does not support independent release cycles.</p>",
                "answers": [
                    "<p>Implement a separate Cloud Build trigger for each microservice and separate the deployments.</p>",
                    "<p>Use Cloud Build with a single trigger to build and deploy all microservices simultaneously.</p>",
                    "<p>Use a single Cloud Function to trigger the build and deployment for all microservices.</p>",
                    "<p>Use Google App Engine to deploy all microservices as a single service.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are migrating a complex application to Google Cloud. The application consists of multiple microservices, each developed by different teams. Your goal is to implement a CI/CD pipeline that allows for independent release cycles for each microservice. Which approach should you take?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297738,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization is planning to migrate a large on-premise data warehouse to Google Cloud Platform. The data warehouse is currently hosted on a Hadoop cluster with Hive. The management decided to migrate to Google Cloud to use the managed services offered by Google. Which service should you choose?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use BigQuery as it provides a highly scalable, serverless, and cost-effective multi-cloud data warehouse. -&gt;&nbsp;Correct. BigQuery is designed to handle data analysis on large datasets and would be a suitable replacement for a Hadoop/Hive-based data warehouse.</p><p><br></p><p>Use Cloud Bigtable as it offers low latency access to large datasets. -&gt;&nbsp;Incorrect. Cloud Bigtable is designed for high-throughput, low-latency workloads and is typically used as a NoSQL database for operational, not analytical, workloads.</p><p><br></p><p>Use Cloud Spanner as it provides strong transactional consistency. -&gt;&nbsp;Incorrect. Cloud Spanner is a highly scalable, enterprise-grade, globally-distributed, and strongly consistent database service built for the cloud specifically to combine the benefits of relational database structure with non-relational horizontal scale. It is not designed for analytical workloads typical for a data warehouse.</p><p><br></p><p>Use Cloud Storage as it provides unified object storage. -&gt;&nbsp;Incorrect. Cloud Storage is not a database or data warehouse service. It's an object storage service for storing and accessing data on Google Cloud Platform infrastructure.</p>",
                "answers": [
                    "<p>Use BigQuery as it provides a highly scalable, serverless, and cost-effective multi-cloud data warehouse.</p>",
                    "<p>Use Cloud Bigtable as it offers low latency access to large datasets.</p>",
                    "<p>Use Cloud Spanner as it provides strong transactional consistency.</p>",
                    "<p>Use Cloud Storage as it provides unified object storage.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization is planning to migrate a large on-premise data warehouse to Google Cloud Platform. The data warehouse is currently hosted on a Hadoop cluster with Hive. The management decided to migrate to Google Cloud to use the managed services offered by Google. Which service should you choose?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 81297740,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you have been tasked to design a Google Cloud Platform (GCP) resource hierarchy for an organization with multiple departments. The organization aims to limit the permissions for modifying IAM policies to the fewest number of individuals. Which design would achieve this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Assign IAM roles at the organization level only. -&gt;&nbsp;Correct. Assigning IAM roles at the organization level provides centralized control and limits the number of individuals who can modify IAM policies. It ensures that only designated individuals within the organization can grant access to resources.</p><p><br></p><p>Assign IAM roles at the project level for each department. -&gt;&nbsp;Incorrect. This might allow for too many individuals to have permissions to modify IAM policies, as each project's owner could potentially modify their project's IAM policies.</p><p><br></p><p>Assign IAM roles at the individual resource level. -&gt;&nbsp;Incorrect. This would make the IAM roles management process excessively complex and error-prone, and it would not necessarily reduce the number of people who can modify IAM policies.</p><p><br></p><p>Assign IAM roles at the folder level for each department. -&gt;&nbsp;Incorrect. This could still allow for department-level users to modify IAM policies within their respective folders.</p>",
                "answers": [
                    "<p>Assign IAM roles at the organization level only.</p>",
                    "<p>Assign IAM roles at the project level for each department.</p>",
                    "<p>Assign IAM roles at the individual resource level.</p>",
                    "<p>Assign IAM roles at the folder level for each department.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you have been tasked to design a Google Cloud Platform (GCP) resource hierarchy for an organization with multiple departments. The organization aims to limit the permissions for modifying IAM policies to the fewest number of individuals. Which design would achieve this?",
            "related_lectures": []
        }
    ]
}