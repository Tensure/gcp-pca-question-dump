{
    "count": 70,
    "next": null,
    "previous": null,
    "results": [
        {
            "_class": "assessment",
            "id": 71681520,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with a rapidly expanding startup that uses Google Cloud. They have been manually creating individual Google user accounts for each employee, but now they are scaling and need a more efficient and secure way to manage access to Google Cloud resources. Which of the following options should you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Set up a Cloud Identity or G Suite domain, then use Google Cloud Directory Sync to synchronize the users to Google Cloud. -&gt; Correct. Setting up a Cloud Identity or G Suite domain allows the organization to manage user accounts more efficiently. Google Cloud Directory Sync can then be used to synchronize these users to Google Cloud, and IAM policies can be applied as needed.</p><p><br></p><p>Continue creating individual user accounts for each employee manually. -&gt; Incorrect. Continuing to manually create individual user accounts is inefficient and does not scale well.</p><p><br></p><p>Use the Google Cloud IAM service to create a custom IAM role for each employee. -&gt; Incorrect. Creating a custom IAM role for each employee can be useful for fine-grained access control, but it doesn't solve the problem of efficiently managing multiple user accounts.</p><p><br></p><p>Create service accounts for each employee and distribute the private keys. -&gt; Incorrect. Service accounts are designed for authenticating applications and services, not individual users. Distributing private keys is also a security risk.</p>",
                "answers": [
                    "<p>Set up a Cloud Identity or G Suite domain, then use Google Cloud Directory Sync to synchronize the users to Google Cloud.</p>",
                    "<p>Continue creating individual user accounts for each employee manually.</p>",
                    "<p>Use the Google Cloud IAM service to create a custom IAM role for each employee.</p>",
                    "<p>Create service accounts for each employee and distribute the private keys.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with a rapidly expanding startup that uses Google Cloud. They have been manually creating individual Google user accounts for each employee, but now they are scaling and need a more efficient and secure way to manage access to Google Cloud resources. Which of the following options should you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681522,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with a client that needs an automated solution to back up their Compute Engine workloads. They want to ensure minimal downtime during the backup process and be able to restore their workloads to any point in the last 30 days. Which of the following solutions would be the most effective for this scenario?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Persistent Disk Snapshots scheduled at regular intervals. -&gt; Correct. Persistent Disk Snapshots can be scheduled to run at regular intervals, providing an automated solution for backing up Compute Engine workloads.</p><p><br></p><p>Use Cloud Storage to store periodic snapshots of the Compute Engine instances. -&gt; Incorrect. While Cloud Storage is a versatile and durable storage solution, it doesn't provide an automated mechanism for creating instance snapshots.</p><p><br></p><p>Use Cloud Datastore to keep the state of the Compute Engine instances. -&gt; Incorrect. Cloud Datastore is a NoSQL database and isn't designed for backing up Compute Engine instances.</p><p><br></p><p>Use Google Cloud's operations suite to monitor and backup Compute Engine instances. -&gt;&nbsp;Incorrect. Google Cloud's operations suite provides monitoring, troubleshooting, and application performance management, but it doesn't offer automated backup of Compute Engine instances.</p>",
                "answers": [
                    "<p>Use Persistent Disk Snapshots scheduled at regular intervals.</p>",
                    "<p>Use Cloud Storage to store periodic snapshots of the Compute Engine instances.</p>",
                    "<p>Use Cloud Datastore to keep the state of the Compute Engine instances.</p>",
                    "<p>Use Google Cloud's operations suite to monitor and backup Compute Engine instances.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with a client that needs an automated solution to back up their Compute Engine workloads. They want to ensure minimal downtime during the backup process and be able to restore their workloads to any point in the last 30 days. Which of the following solutions would be the most effective for this scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681524,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You run a small startup and want to optimize costs in GCP. As a cloud architect, you need to research resource consumption charges and provide a summary of your expenses. You want to do it in the most efficient way. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should attach labels to resources to reflect the purpose. Than, export Cloud Billing data into BigQuery, and analyze it with Data Studio. -&gt; Correct. Attaching labels to resources to reflect their purpose allows you to track and categorize resource usage. By exporting Cloud Billing data into BigQuery, you can store and analyze the billing data in a structured format. Data Studio can then be used to create visualizations and reports based on the analyzed billing data. This approach provides an efficient and effective way to understand resource consumption charges and optimize costs.</p><p><br></p><p>You should rename resources to reflect the purpose. Write a Python script to analyze resource consumption. -&gt;&nbsp;Incorrect. Renaming resources to reflect their purpose may help with identification but does not directly address the task of analyzing resource consumption charges and optimizing costs. Writing a Python script to analyze resource consumption would require significant development effort and may not be as efficient as using tools specifically designed for this purpose.</p><p><br></p><p>You should assign tags to resources to reflect the purpose. Export Cloud Billing data into Cloud SQL, and analyze it with Data Studio. -&gt;&nbsp;Incorrect. Assigning tags to resources to reflect their purpose is similar to attaching labels and can be helpful for organizing and categorizing resources. Exporting Cloud Billing data into Cloud SQL is not a recommended approach.</p><p><br></p><p>You should use Google Cloud Recommender to see if expenses can be reduced. -&gt;&nbsp;Incorrect. It does not directly address the task of researching resource consumption charges and providing a summary of expenses. Google Cloud Recommender focuses on providing recommendations for improving performance, security, and cost-efficiency based on analyzing the current state of resources and configurations.</p><p><br></p><p>https://cloud.google.com/compute/docs/labeling-resources</p><p>https://cloud.google.com/blog/topics/cost-management/use-labels-to-gain-visibility-into-gcp-resource-usage-and-spending</p>",
                "answers": [
                    "<p>You should attach labels to resources to reflect the purpose. Than, export Cloud Billing data into BigQuery, and analyze it with Data Studio.</p>",
                    "<p>You should rename resources to reflect the purpose. Write a Python script to analyze resource consumption.</p>",
                    "<p>You should assign tags to resources to reflect the purpose. Export Cloud Billing data into Cloud SQL, and analyze it with Data Studio.</p>",
                    "<p>You should use Google Cloud Recommender to see if expenses can be reduced.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You run a small startup and want to optimize costs in GCP. As a cloud architect, you need to research resource consumption charges and provide a summary of your expenses. You want to do it in the most efficient way. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681526,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization is planning to create a custom VPC network on Google Cloud for deploying a multi-tier application. The application includes frontend servers, backend servers, and database servers, each of which requires a separate subnet. What considerations should you keep in mind while creating this VPC?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Assign one large CIDR block to the VPC and divide it into smaller CIDR blocks for each subnet. -&gt;&nbsp;Correct. By assigning a large CIDR block to the VPC, you can then create smaller subnets for each tier of your application, giving you greater control over IP address ranges.</p><p><br></p><p>Use Shared VPC to host all the tiers of the application. -&gt; Incorrect. Shared VPC allows resources from multiple projects to communicate with each other securely, but it may not be necessary if all tiers of your application are within the same project.</p><p><br></p><p>Use the same CIDR block for all the subnets. -&gt;&nbsp;Incorrect. If you use the same CIDR block for all subnets, it would result in overlapping IP addresses which is not allowed in GCP.</p><p><br></p><p>Use Cloud NAT for the frontend servers to connect with the internet. -&gt;&nbsp;Incorrect. The need for Cloud NAT depends on whether the frontend servers need to initiate communication with the internet without having public IP addresses. It's not mandatory for all cases.</p>",
                "answers": [
                    "<p>Assign one large CIDR block to the VPC and divide it into smaller CIDR blocks for each subnet.</p>",
                    "<p>Use Shared VPC to host all the tiers of the application.</p>",
                    "<p>Use the same CIDR block for all the subnets.</p>",
                    "<p>Use Cloud NAT for the frontend servers to connect with the internet.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization is planning to create a custom VPC network on Google Cloud for deploying a multi-tier application. The application includes frontend servers, backend servers, and database servers, each of which requires a separate subnet. What considerations should you keep in mind while creating this VPC?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681528,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>When deploying your application to App Engine, your development team aims to scale the number of instances in response to the request rate. It is necessary to maintain a minimum of five idle instances at all times. Which scaling method should they employ?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Automatic Scaling with <code>min_idle_instances</code> set to 5. -&gt; Correct. Automatic Scaling is an App Engine scaling type that automatically adjusts the number of instances based on request rate, and scales down to zero when there are no incoming requests. The <code>min_idle_instances</code> setting controls the minimum number of idle instances that the service should maintain at all times, which in this case is 5. This ensures that the service has enough instances to handle incoming requests without any delays.</p><p><br></p><p>Basic Scaling with <code>min_instances</code> set to 5. -&gt; Incorrect. Basic Scaling with <code>min_instances</code> set to 5 would not work in this scenario because Basic Scaling only scales up to a fixed number of instances based on incoming request traffic, and does not scale down to zero.</p><p><br></p><p>Manual Scaling with 5 instances. -&gt; Incorrect. Manual Scaling with 5 instances would require the development team to manually manage the number of instances, which can be time-consuming and prone to errors.</p><p><br></p><p>Basic Scaling with <code>max_instances</code> set to 5. -&gt; Incorrect. Basic Scaling with <code>max_instances</code> set to 5 would not work in this scenario because it does not ensure that there are at least 5 instances available at all times. Additionally, the number of instances would not automatically scale up based on incoming request traffic.</p><p><br></p><p>https://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed#apps_with_automatic_scaling</p>",
                "answers": [
                    "<p>Automatic Scaling with <code>min_idle_instances</code> set to 5.</p>",
                    "<p>Basic Scaling with <code>min_instances</code> set to 5.</p>",
                    "<p>Manual Scaling with 5 instances.</p>",
                    "<p>Basic Scaling with <code>max_instances</code> set to 5.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "When deploying your application to App Engine, your development team aims to scale the number of instances in response to the request rate. It is necessary to maintain a minimum of five idle instances at all times. Which scaling method should they employ?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681530,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are creating a VPC network to host a set of compute resources for a multi-tier web application in Google Cloud. How should you configure the network to optimize security and manageability?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Create separate subnets for each tier of the application. -&gt; Correct. By creating separate subnets for each tier of the application, you can use firewall rules to control traffic between different tiers, enhancing security.</p><p><br></p><p>Use automatic mode to create the VPC network. -&gt; Incorrect. Automatic mode creates a subnet in each region and sets up the appropriate IP ranges. This may not provide the granularity of control you need for a multi-tier web application.</p><p><br></p><p>Assign public IP addresses to all compute instances to facilitate direct access. -&gt; Incorrect. Assigning public IP addresses to all instances can expose them to potential threats. It's better to limit public IP addresses and use Cloud NAT or bastion hosts for access.</p><p><br></p><p>Use Shared VPC to host all the compute resources. -&gt; Incorrect. Shared VPC allows resources from multiple projects to communicate with each other securely, but it may not be necessary if all your compute resources are within the same project.</p>",
                "answers": [
                    "<p>Create separate subnets for each tier of the application.</p>",
                    "<p>Use automatic mode to create the VPC network.</p>",
                    "<p>Assign public IP addresses to all compute instances to facilitate direct access.</p>",
                    "<p>Use Shared VPC to host all the compute resources.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are creating a VPC network to host a set of compute resources for a multi-tier web application in Google Cloud. How should you configure the network to optimize security and manageability?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681532,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with a cost-conscious client who regularly runs large, complex queries on BigQuery. They want to understand how they can estimate the cost of running a BigQuery query before they execute it to avoid unexpected expenses. Which of the following is the most suitable solution for this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Preview the query in the BigQuery web UI, which displays an estimate of the amount of data the query will process. -&gt; Correct. When you paste a query into the BigQuery web UI and click on the 'Compose Query' button without running the query, it will show an estimate of the amount of data that the query will process. You can then use this data to calculate the cost based on BigQuery's pricing.</p><p><br></p><p>Use Google Cloud Pricing Calculator by entering the estimated amount of data processed by the query. -&gt; Incorrect. The Google Cloud Pricing Calculator can help estimate costs for cloud services based on usage predictions, but it is not designed to predict the cost of individual BigQuery queries.</p><p><br></p><p>Use the BigQuery command-line tool's <code>bq show</code> command to inspect the query. -&gt; Incorrect. The <code>bq show</code> command in BigQuery command-line tool is used to display information about dataset, table, or view, not to estimate the cost of a query.</p><p><br></p><p>Use Cloud Monitoring to estimate the cost based on previous query execution times. -&gt; Incorrect. Cloud Monitoring is for monitoring resource usage and performance, not for estimating the cost of individual BigQuery queries.</p>",
                "answers": [
                    "<p>Preview the query in the BigQuery web UI, which displays an estimate of the amount of data the query will process.</p>",
                    "<p>Use Google Cloud Pricing Calculator by entering the estimated amount of data processed by the query.</p>",
                    "<p>Use the BigQuery command-line tool's <code>bq show</code> command to inspect the query.</p>",
                    "<p>Use Cloud Monitoring to estimate the cost based on previous query execution times.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with a cost-conscious client who regularly runs large, complex queries on BigQuery. They want to understand how they can estimate the cost of running a BigQuery query before they execute it to avoid unexpected expenses. Which of the following is the most suitable solution for this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681534,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with an e-commerce client who expects a significant increase in traffic to their application hosted on Google Kubernetes Engine (GKE) during an upcoming sales event. To manage the increased load, the client wishes to enable autoscaling for their application. However, they want to ensure that any newly created pods during autoscaling are fully ready to handle traffic before being included in the service. Which of the following solutions should you implement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Configure a readiness probe in your pod specification. -&gt; Correct. A readiness probe is used in Kubernetes to determine when a pod is ready to accept traffic. The load balancer only sends traffic to pods that pass their readiness probe, ensuring that new pods are fully ready before they start receiving traffic.</p><p><br></p><p>Enable Cluster Autoscaler on your GKE cluster. -&gt; Incorrect. Enabling Cluster Autoscaler on your GKE cluster would indeed help manage the number of nodes based on the traffic, but it would not ensure that new pods are ready before they receive traffic.</p><p><br></p><p>Configure a startup script in the pod specification to delay service registration. -&gt; Incorrect. It would not be an effective solution, as it would delay all pods, not just the ones that are not ready. It would also require manual intervention to manage the delay.</p><p><br></p><p>Configure a liveness probe in your pod specification. -&gt; Incorrect. Configuring a liveness probe in your pod specification would be used to know when to restart a pod, but it does not determine when a pod is ready to accept traffic.</p>",
                "answers": [
                    "<p>Configure a readiness probe in your pod specification.</p>",
                    "<p>Enable Cluster Autoscaler on your GKE cluster.</p>",
                    "<p>Configure a startup script in the pod specification to delay service registration.</p>",
                    "<p>Configure a liveness probe in your pod specification.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with an e-commerce client who expects a significant increase in traffic to their application hosted on Google Kubernetes Engine (GKE) during an upcoming sales event. To manage the increased load, the client wishes to enable autoscaling for their application. However, they want to ensure that any newly created pods during autoscaling are fully ready to handle traffic before being included in the service. Which of the following solutions should you implement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681536,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>In the <code>europe-central2-a</code> zone, your team has an application server running on Compute Engine. What should you do to ensure high availability and replicate this server to <code>europe-central2-b</code> zone in as few steps as possible?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should create a snapshot from the disk and then create a disk from this snapshot in the <code>europe-central2-b</code> zone. Finally, create a new virtual machine with that disk. -&gt; Correct. Creating a snapshot of the disk is the quickest and easiest way to replicate an application server running on Compute Engine from one zone to another. A snapshot is a point-in-time copy of a disk that can be used to create a new disk in another zone or region. By creating a snapshot of the disk and then creating a new disk in the <code>europe-central2-b</code> zone, you can ensure that the data is replicated in as few steps as possible.</p><p><br></p><p>You should create a snapshot from the disk and then create a disk from this snapshot in the <code>europe-central2-a</code> zone, move this disk to <code>europe-central2-b</code> and finally create a new virtual machine with that disk. -&gt; Incorrect. It suggests creating a disk from the snapshot in the same zone, moving the disk to the <code>europe-central2-b</code> zone and then creating a new virtual machine with that disk. This option adds an extra step of moving the disk to the new zone, which is not necessary since a disk can be created directly in the desired zone from the snapshot.</p><p><br></p><p>You should use <code>gcloud</code> tool to copy the drive to the <code>europe-central2-b</code> zone and then create a new virtual machine with this disk. -&gt; Incorrect. It suggests using the <code>gcloud</code> tool to copy the drive to the <code>europe-central2-b</code> zone and then creating a new virtual machine with this disk. This option is not as efficient as creating a snapshot because it requires copying the entire drive, which may take longer and consume more resources.</p><p><br></p><p>You should use <code>gcloud</code> tool to copy the drive to the <code>europe-central2-a</code> zone, move this disk to <code>europe-central2-b</code> and finally create a new virtual machine with this disk. -&gt; Incorrect. It suggests using the <code>gcloud</code> tool to copy the drive to the <code>europe-central2-a</code> zone, moving the disk to <code>europe-central2-b</code> and then creating a new virtual machine with that disk. This option is also not as efficient as creating a snapshot since it involves an additional step of moving the disk between zones.</p><p><br></p><p>https://cloud.google.com/compute/docs/disks/create-snapshots</p>",
                "answers": [
                    "<p>You should create a snapshot from the disk and then create a disk from this snapshot in the <code>europe-central2-b</code> zone. Finally, create a new virtual machine with that disk.</p>",
                    "<p>You should create a snapshot from the disk and then create a disk from this snapshot in the <code>europe-central2-a</code> zone, move this disk to <code>europe-central2-b</code> and finally create a new virtual machine with that disk.</p>",
                    "<p>You should use <code>gcloud</code> tool to copy the drive to the <code>europe-central2-b</code> zone and then create a new virtual machine with this disk.</p>",
                    "<p>You should use <code>gcloud</code> tool to copy the drive to the <code>europe-central2-a</code> zone, move this disk to <code>europe-central2-b</code> and finally create a new virtual machine with this disk.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "In the europe-central2-a zone, your team has an application server running on Compute Engine. What should you do to ensure high availability and replicate this server to europe-central2-b zone in as few steps as possible?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681538,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you're helping a large organization deploy an update to their mission-critical application on App Engine. The application is used 24/7 by users worldwide, and it is crucial that the update does not affect application availability or user experience. Which of the following deployment strategies should you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Canary Deployment: Create a new version of the application and gradually migrate user traffic from the old version to the new version. -&gt; Correct. This strategy allows you to gradually shift traffic from the old version to the new version, allowing for real-time monitoring and the ability to roll back if issues are detected.</p><p><br></p><p>Blue-Green Deployment: Create a new version of the application, migrate all traffic to the new version, and delete the old version. -&gt; Incorrect. Blue-Green Deployment minimizes downtime but doesn't allow for gradual roll-out or testing of the new version with a subset of users before all traffic is moved.</p><p><br></p><p>Rolling Update: Update the application code in the existing version and restart the application. -&gt; Incorrect. Rolling Update would likely cause downtime and wouldn't allow for testing the new version with a subset of users before all users are moved to the new version.</p><p><br></p><p>Big Bang Deployment: Replace the existing application with the new version in a single operation. -&gt; Incorrect. Big Bang Deployment has the highest risk because it doesn't allow for testing the new version with a subset of users and has the potential for major downtime if issues arise.</p>",
                "answers": [
                    "<p>Canary Deployment: Create a new version of the application and gradually migrate user traffic from the old version to the new version.</p>",
                    "<p>Blue-Green Deployment: Create a new version of the application, migrate all traffic to the new version, and delete the old version.</p>",
                    "<p>Rolling Update: Update the application code in the existing version and restart the application.</p>",
                    "<p>Big Bang Deployment: Replace the existing application with the new version in a single operation.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you're helping a large organization deploy an update to their mission-critical application on App Engine. The application is used 24/7 by users worldwide, and it is crucial that the update does not affect application availability or user experience. Which of the following deployment strategies should you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681540,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect tasked with architecting a data storage solution for a media company. The company has the following data storage requirements:</p><ul><li><p>store large media files that are regularly accessed for the first month</p></li><li><p>archive media files that have not been accessed for over a year</p></li><li><p>ensure redundancy and high availability</p></li><li><p>keep costs optimized based on the frequency of data access</p></li></ul><p><br></p><p>Which combination of storage classes should be used for Google Cloud Storage to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Standard Storage for the large media files and enable Object Lifecycle Management to change the storage class to Archive for files not accessed in over a year. -&gt; Correct. Standard Storage is appropriate for frequently accessed data and offers high performance. By enabling Object Lifecycle Management to automatically change the storage class to Archive for files not accessed in over a year, the company can optimize costs for data that is essentially archival. Archive Storage is the most cost-effective storage class for long-term archiving where data is not expected to be accessed for over a year.</p><p><br></p><p>Use Nearline Storage for all media files and enable Object Lifecycle Management to change the storage class to Coldline for files not accessed in over a year. -&gt; Incorrect. Nearline Storage is not the best choice for frequently accessed data, as it is meant for data that is accessed about once a month. Additionally, Coldline Storage is meant for colder data but is not as cost-effective as Archive Storage for data not accessed in over a year.</p><p><br></p><p>Use Standard Storage for all media files and enable Object Lifecycle Management to change the storage class to Nearline for files not accessed in over a year. -&gt; Incorrect. Nearline Storage is meant for data that is accessed less frequently (about once a month), and using it for files not accessed in over a year would not be the most cost-effective solution.</p><p><br></p><p>Use Multi-Regional Storage for the large media files and Coldline Storage for archiving files not accessed in over a year. -&gt; Incorrect. Multi-Regional Storage is primarily used for serving content to geographically distributed users and would likely be overkill for this scenario. Coldline Storage is used for archiving, but this option doesn't mention the usage of Object Lifecycle Management for automation.</p>",
                "answers": [
                    "<p>Use Standard Storage for the large media files and enable Object Lifecycle Management to change the storage class to Archive for files not accessed in over a year.</p>",
                    "<p>Use Nearline Storage for all media files and enable Object Lifecycle Management to change the storage class to Coldline for files not accessed in over a year.</p>",
                    "<p>Use Standard Storage for all media files and enable Object Lifecycle Management to change the storage class to Nearline for files not accessed in over a year.</p>",
                    "<p>Use Multi-Regional Storage for the large media files and Coldline Storage for archiving files not accessed in over a year.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect tasked with architecting a data storage solution for a media company. The company has the following data storage requirements:store large media files that are regularly accessed for the first montharchive media files that have not been accessed for over a yearensure redundancy and high availabilitykeep costs optimized based on the frequency of data accessWhich combination of storage classes should be used for Google Cloud Storage to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681542,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization uses Google Kubernetes Engine (GKE) for its microservices-based application. The number of services is expected to grow significantly over the next two years. What would be the best approach to ensure manageability and operational efficiency as the number of services increases?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement a Service Mesh using Istio for fine-grained control and observability. -&gt;&nbsp;Correct. Istio is a service mesh that provides traffic management, policy enforcement, and telemetry collection. It would allow for improved control and observability of the growing number of services.</p><p><br></p><p>Migrate to Compute Engine instances to reduce the complexity of managing Kubernetes. -&gt;&nbsp;Incorrect. While Compute Engine instances might be simpler to manage than a Kubernetes cluster, they wouldn't provide the benefits of container orchestration for a growing microservices architecture.</p><p><br></p><p>Increase the size of the GKE cluster nodes to accommodate the growing number of services. -&gt;&nbsp;Incorrect. While having larger nodes could potentially accommodate more pods, it doesn't address the complexity of managing a growing number of services.</p><p><br></p><p>Implement Cloud Logging for better log management. -&gt;&nbsp;Incorrect. While proper log management is crucial, it's only one aspect of managing a growing microservices-based application and wouldn't address other aspects like traffic management or policy enforcement.</p>",
                "answers": [
                    "<p>Implement a Service Mesh using Istio for fine-grained control and observability.</p>",
                    "<p>Migrate to Compute Engine instances to reduce the complexity of managing Kubernetes.</p>",
                    "<p>Increase the size of the GKE cluster nodes to accommodate the growing number of services.</p>",
                    "<p>Implement Cloud Logging for better log management.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization uses Google Kubernetes Engine (GKE) for its microservices-based application. The number of services is expected to grow significantly over the next two years. What would be the best approach to ensure manageability and operational efficiency as the number of services increases?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681544,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are designing a Google Cloud architecture for an organization that operates in a heavily regulated industry. It needs to meet strict compliance requirements regarding data storage and access. Which approach should you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use customer-managed encryption keys (CMEK) and set up access controls for the keys. -&gt;&nbsp;Correct. Using CMEK provides greater control over the encryption keys. Combined with proper access controls, it can help meet strict compliance requirements regarding data storage and access.</p><p><br></p><p>Encrypt all data at rest using Google-managed encryption keys. -&gt;&nbsp;Incorrect. While this provides some level of security, it might not meet the strict requirements of heavily regulated industries that often require more control over the encryption keys.</p><p><br></p><p>Enable VPC Service Controls to limit data exfiltration. -&gt;&nbsp;Incorrect. While VPC Service Controls can prevent data exfiltration, they don't control who has access to the data within the secured perimeter.</p><p><br></p><p>Use Cloud Audit Logs to monitor and audit data access. -&gt;&nbsp;Incorrect. Cloud Audit Logs can help track who did what, where, and when, but it's a reactive approach and doesn't prevent unauthorized access from happening in the first place.</p>",
                "answers": [
                    "<p>Use customer-managed encryption keys (CMEK) and set up access controls for the keys.</p>",
                    "<p>Encrypt all data at rest using Google-managed encryption keys.</p>",
                    "<p>Enable VPC Service Controls to limit data exfiltration.</p>",
                    "<p>Use Cloud Audit Logs to monitor and audit data access.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are designing a Google Cloud architecture for an organization that operates in a heavily regulated industry. It needs to meet strict compliance requirements regarding data storage and access. Which approach should you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681546,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are consulting for a manufacturing company implementing an IoT solution. They plan to collect real-time sensor data from their machinery across multiple plants, with high frequency. This data will be used to monitor machine health, and it's crucial to have low-latency access to the last hour of data. Which of the following would be the most effective way to use Google Cloud Bigtable in this scenario?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Bigtable to store all sensor data, and use row keys that include the machine ID and timestamp to enable fast access to recent data. -&gt; Correct. Cloud Bigtable is designed for handling large quantities of high-frequency, single-keyed data like IoT sensor readings. By using row keys that include the machine ID and a reverse timestamp, you can ensure fast access to recent data for each machine.</p><p><br></p><p>Use Cloud Bigtable to store all sensor data, and use row keys that include the sensor type and machine ID to enable fast access to recent data. -&gt; Incorrect. Using row keys that include sensor type and machine ID might not allow for efficient retrieval of recent data, since it would not take into account the timestamp.</p><p><br></p><p>Use Cloud Bigtable to store only the last hour of sensor data, and use a separate system for long-term storage. -&gt; Incorrect. While it's feasible to use Cloud Bigtable for recent data and another system for long-term storage, this choice doesn't address how to structure the row keys for efficient access to recent data.</p><p><br></p><p>Use Cloud Bigtable to store all sensor data, but partition the data by machine ID and timestamp. -&gt; Incorrect. Cloud Bigtable doesn't support partitioning data in the same way as a relational database. It primarily relies on row key design for efficient data access.</p>",
                "answers": [
                    "<p>Use Cloud Bigtable to store all sensor data, and use row keys that include the machine ID and timestamp to enable fast access to recent data.</p>",
                    "<p>Use Cloud Bigtable to store all sensor data, and use row keys that include the sensor type and machine ID to enable fast access to recent data.</p>",
                    "<p>Use Cloud Bigtable to store only the last hour of sensor data, and use a separate system for long-term storage.</p>",
                    "<p>Use Cloud Bigtable to store all sensor data, but partition the data by machine ID and timestamp.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are consulting for a manufacturing company implementing an IoT solution. They plan to collect real-time sensor data from their machinery across multiple plants, with high frequency. This data will be used to monitor machine health, and it's crucial to have low-latency access to the last hour of data. Which of the following would be the most effective way to use Google Cloud Bigtable in this scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681548,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with a company to deploy their multi-tier application on Google Kubernetes Engine (GKE). The application includes a front-end service that needs to maintain high availability. The company has requested that at least five replicas of the front-end service run at all times to meet their availability requirements. Which field in the Kubernetes deployment manifest should you use to specify this requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p><code>spec.replicas</code> -&gt;&nbsp;Correct. In a Kubernetes deployment manifest, the <code>spec.replicas</code> field is used to specify the number of desired pods to run for the deployment. In this scenario, setting <code>spec.replicas</code> to 5 ensures that at least five replicas of the front-end service are always running.</p><p><br></p><p><code>metadata.replicas</code> -&gt; Incorrect. There is no <code>replicas</code> field in the <code>metadata</code> section of a Kubernetes deployment manifest. The metadata field is used to provide data about the deployment such as its name, labels, and namespace.</p><p><br></p><p><code>spec.pods</code> -&gt; Incorrect. There is no <code>pods</code> field in the <code>spec</code> section of a Kubernetes deployment manifest. The <code>spec</code> section describes the desired state of the objects, but the number of pods is controlled with the <code>replicas</code> field.</p><p><br></p><p><code>metadata.scale</code> -&gt; Incorrect. There is no <code>scale</code> field in the <code>metadata</code> section of a Kubernetes deployment manifest. The <code>metadata</code> field provides data about the deployment itself, not about its scaling configuration.</p>",
                "answers": [
                    "<p><code>spec.replicas</code> </p>",
                    "<p><code>metadata.replicas</code> </p>",
                    "<p><code>spec.pods</code> </p>",
                    "<p><code>metadata.scale</code> </p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with a company to deploy their multi-tier application on Google Kubernetes Engine (GKE). The application includes a front-end service that needs to maintain high availability. The company has requested that at least five replicas of the front-end service run at all times to meet their availability requirements. Which field in the Kubernetes deployment manifest should you use to specify this requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681550,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with a media company that runs a complex application on a Kubernetes cluster in Google Kubernetes Engine (GKE). After evaluating the application's resource usage over the past few months, you've determined that the cluster is over-provisioned and you need to resize it to save costs. The application is stateless, and there are no user sessions to maintain. The application is also globally distributed and must maintain high availability during the resizing process. What is the best approach to resize the cluster?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use the <code>gcloud container clusters resize</code> command to reduce the number of nodes in the cluster. -&gt; Correct. This command allows you to safely reduce the size of the cluster. Google Kubernetes Engine takes care of gracefully draining nodes and ensuring that pods are properly rescheduled onto other nodes.</p><p><br></p><p>Delete the existing cluster and create a new one with fewer nodes. -&gt; Incorrect. Deleting the existing cluster and creating a new one would cause downtime for the application, which is not acceptable for a high-availability requirement.</p><p><br></p><p>Manually remove nodes from the existing cluster until you reach the desired size. -&gt; Incorrect. Manually removing nodes from the cluster might lead to errors and inconsistencies. It is not recommended.</p><p><br></p><p>Adjust the number of vCPUs and memory allocated to each node in the cluster. -&gt;&nbsp;Incorrect. Adjusting the number of vCPUs and memory allocated to each node would change the capacity of each node, but not the number of nodes in the cluster. This might also lead to over-provisioning or under-provisioning for certain workloads.</p>",
                "answers": [
                    "<p>Use the <code>gcloud container clusters resize</code> command to reduce the number of nodes in the cluster.</p>",
                    "<p>Delete the existing cluster and create a new one with fewer nodes.</p>",
                    "<p>Manually remove nodes from the existing cluster until you reach the desired size.</p>",
                    "<p>Adjust the number of vCPUs and memory allocated to each node in the cluster.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with a media company that runs a complex application on a Kubernetes cluster in Google Kubernetes Engine (GKE). After evaluating the application's resource usage over the past few months, you've determined that the cluster is over-provisioned and you need to resize it to save costs. The application is stateless, and there are no user sessions to maintain. The application is also globally distributed and must maintain high availability during the resizing process. What is the best approach to resize the cluster?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681552,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A marketing company creates a lot of landing pages (static websites). As a cloud architect, which storage service would you recommend in this case to minimize costs?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Storage -&gt; Correct. It is a Google Cloud service designed for storing and accessing unstructured data such as images, videos, and static web content. It offers a simple and cost-effective solution for storing large amounts of data and making it accessible via the internet. In the given scenario, since the marketing company creates a lot of landing pages (static websites), the content is typically unstructured and can be stored in a simple file-based system. Cloud Storage is an ideal solution for storing and serving static web content as it allows you to host static websites directly from a Cloud Storage bucket. With this approach, you can minimize costs and reduce the complexity of managing web servers.</p><p><br></p><p>Cloud SDK -&gt; Incorrect. It is a command-line interface tool for managing Google Cloud services and resources. It is not a storage service and cannot be used to store or serve static web content.</p><p><br></p><p>Cloud Endpoints -&gt; Incorrect. It is a Google Cloud service designed for creating, deploying, and managing APIs. It is not a storage service and cannot be used to store or serve static web content.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. It is a NoSQL document database designed for storing non-relational data. It is not optimized for storing and serving static web content and is more suited for applications that require complex data structures.</p><p><br></p><p>Compute Engine + Persistent Disk -&gt; Incorrect. It is a more complex and expensive solution for serving static web content compared to Cloud Storage. Compute Engine is designed for running virtual machines in the cloud and is more suitable for running complex applications and workloads that require a higher level of customization and control.</p><p><br></p><p>https://cloud.google.com/storage/docs/hosting-static-website</p>",
                "answers": [
                    "<p>Cloud Storage</p>",
                    "<p>Cloud SDK</p>",
                    "<p>Cloud Endpoints</p>",
                    "<p>Cloud Datastore</p>",
                    "<p>Compute Engine + Persistent Disk</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A marketing company creates a lot of landing pages (static websites). As a cloud architect, which storage service would you recommend in this case to minimize costs?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681554,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You have a web application operating on a Managed Instance Group, which receives a high volume of requests every minute. Your objective is to apply patches to the application without reducing the number of instances in the MIG. What action should you take?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should perform a rolling-action start-update with <code>max-unavailable</code> set to 0 and <code>max-surge</code> set to 1. -&gt; Correct. Performing a rolling-action start-update with <code>max-unavailable</code> set to 0 and <code>max-surge</code> set to 1 allows you to apply patches to the application without reducing the number of instances in the Managed Instance Group (MIG). The rolling update process will replace instances one by one while maintaining the desired number of instances. With <code>max-unavailable</code> set to 0, no instances will be unavailable during the update process, ensuring continuous availability.</p><p><br></p><p>You should perform a rolling-action start-update with <code>max-unavailable</code> set to 1 and <code>max-surge</code> set to 0. -&gt;&nbsp;Incorrect. Performing a rolling-action start-update with <code>max-unavailable</code> set to 1 and <code>max-surge</code> set to 0 means that during the update process, one instance will be unavailable at a time, potentially reducing the number of instances in the MIG. This contradicts the objective of applying patches without reducing the number of instances.</p><p><br></p><p>You should deploy the update in a new Managed Instance Group and add it as a backend service to the existing production Load Balancer. Then remove the old Managed Instance Group from the Load Balancer backend and remove the group. -&gt;&nbsp;Incorrect. It involves creating a new group and Load Balancer configuration, which is not necessary if the goal is to apply patches without reducing the number of instances in the current MIG.</p><p><br></p><p>You should update the existing Managed Instance Group to point to a new instance template containing the updated version. Terminate all existing instances in this group and wait until they are all replaced by new instances created from the new template. -&gt;&nbsp;Incorrect. This approach does not align with the objective of applying patches without reducing the number of instances.</p><p><br></p><p>https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups</p>",
                "answers": [
                    "<p>You should perform a rolling-action start-update with <code>max-unavailable</code> set to 0 and <code>max-surge</code> set to 1.</p>",
                    "<p>You should perform a rolling-action start-update with <code>max-unavailable</code> set to 1 and <code>max-surge</code> set to 0.</p>",
                    "<p>You should deploy the update in a new Managed Instance Group and add it as a backend service to the existing production Load Balancer. Then remove the old Managed Instance Group from the Load Balancer backend and remove the group.</p>",
                    "<p>You should update the existing Managed Instance Group to point to a new instance template containing the updated version. Terminate all existing instances in this group and wait until they are all replaced by new instances created from the new template.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You have a web application operating on a Managed Instance Group, which receives a high volume of requests every minute. Your objective is to apply patches to the application without reducing the number of instances in the MIG. What action should you take?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681556,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A mobile game developer wants to launch a new mobile game that will be available to users around the world. The game requires RDBMS for storing player profiles. As a cloud architect, which storage service should&nbsp;you recommend so they can scale to a global audience with minimal configuration updates?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Spanner -&gt; Correct. It is a fully managed, globally distributed, and horizontally scalable relational database service, specifically designed to scale horizontally across multiple regions and zones while maintaining strong consistency and high availability. It is suitable for applications that require a relational data model and ACID transactions. On the other hand, Cloud SQL, Cloud Datastore, and Cloud Firestore are also storage services provided by Google Cloud Platform (GCP), but they are designed for different use cases.</p><p><br></p><p>Cloud SQL -&gt; Incorrect. It is a fully managed relational database service that allows developers to choose the database engine that best fits their application requirements (e.g., MySQL, PostgreSQL, SQL Server). It is suitable for small to medium-sized workloads that require a traditional relational database.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. It is a NoSQL document database service that provides a schemaless data model and eventually consistent reads, making it suitable for storing non-relational data such as user-generated content, metadata, and logs.</p><p><br></p><p>Cloud Firestore -&gt; Incorrect. It is a NoSQL document database service that provides real-time updates, offline support, and mobile sync capabilities. It is designed for mobile and web application developers who need to store and synchronize data across multiple devices and platforms.</p><p><br></p><p>https://cloud.google.com/spanner/docs/quickstart-console</p>",
                "answers": [
                    "<p>Cloud Spanner</p>",
                    "<p>Cloud SQL</p>",
                    "<p>Cloud Datastore</p>",
                    "<p>Cloud Firestore</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A mobile game developer wants to launch a new mobile game that will be available to users around the world. The game requires RDBMS for storing player profiles. As a cloud architect, which storage service should&nbsp;you recommend so they can scale to a global audience with minimal configuration updates?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681558,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with a global gaming company that needs a database for their new real-time, multi-player game. They require low latency, high transaction throughput, and the ability to scale to millions of users worldwide. Which of the following approaches would be the most effective way to use Google Cloud Spanner to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Spanner's multi-region configurations and configure the game to direct traffic based on user location. -&gt; Correct. Cloud Spanner's multi-region configurations allow for low latency, high availability, and high throughput. Directing traffic based on user location helps to reduce latency.</p><p><br></p><p>Create a single-region Cloud Spanner instance for the lowest possible latency and manually partition the database by user location. -&gt; Incorrect. Creating a single-region Cloud Spanner instance could introduce higher latencies for users who are far from the region, and manually partitioning the database by user location can be complex and difficult to manage.</p><p><br></p><p>Create a single Cloud Spanner instance and rely on Google's global network for low latency. -&gt; Incorrect. While Cloud Spanner provides high scalability and Google's global network can deliver low latencies, for a global user base a multi-region configuration would be better suited to provide consistently low latencies across the world.</p><p><br></p><p>Set up multiple Cloud Spanner instances in each region where the game has users, and manually replicate data between instances. -&gt; Incorrect. Setting up multiple Cloud Spanner instances and manually replicating data between them would be complex and likely more expensive. It doesn't leverage Cloud Spanner's built-in ability to replicate data across regions.</p>",
                "answers": [
                    "<p>Use Cloud Spanner's multi-region configurations and configure the game to direct traffic based on user location.</p>",
                    "<p>Create a single-region Cloud Spanner instance for the lowest possible latency and manually partition the database by user location.</p>",
                    "<p>Create a single Cloud Spanner instance and rely on Google's global network for low latency.</p>",
                    "<p>Set up multiple Cloud Spanner instances in each region where the game has users, and manually replicate data between instances.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with a global gaming company that needs a database for their new real-time, multi-player game. They require low latency, high transaction throughput, and the ability to scale to millions of users worldwide. Which of the following approaches would be the most effective way to use Google Cloud Spanner to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681560,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you need to establish connection between your on-premises network and Google Cloud. Your company will need 5 Gbps of bandwidth in total between the on-premises data center and Google Cloud. The traffic may be split between multiple connections. How many VPN endpoints will you need?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>2 -&gt;&nbsp;Correct. For Google Cloud VPN, each VPN tunnel can provide up to 3 Gbps of throughput, which means that you will need at least two VPN endpoints to achieve the required 5 Gbps of bandwidth.</p><p><br></p><p>https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview#network-bandwidth</p>",
                "answers": [
                    "<p>2</p>",
                    "<p>1</p>",
                    "<p>3</p>",
                    "<p>4</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you need to establish connection between your on-premises network and Google Cloud. Your company will need 5 Gbps of bandwidth in total between the on-premises data center and Google Cloud. The traffic may be split between multiple connections. How many VPN endpoints will you need?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681562,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>When planning your migration, you find out that some members of the network management team will need to be able to manage all network components, but other team members will only need read access. What mechanism should you use to control this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>IAM&nbsp;roles -&gt; Correct. IAM (Identity and Access Management) roles are a mechanism in cloud computing that allow you to manage permissions for different users or groups of users within your network. With IAM roles, you can grant specific access privileges to different members of your team based on their role, such as read-only access or full access to manage all network components.</p><p><br></p><p>Firewall rules -&gt; Incorrect. Firewall rules are used to control incoming and outgoing network traffic, but they do not provide access control for individual users or groups of users.</p><p><br></p><p>Virtual Private Clouds -&gt; Incorrect. Virtual Private Clouds (VPCs) is a network-related technology, but it is not directly related to access control for individual users or groups of users. VPCs are used to create isolated virtual networks within a cloud computing environment.</p><p><br></p><p>Virtual Private Networks -&gt; Incorrect. Virtual Private Networks (VPNs) is a network-related technology, but it is not directly related to access control for individual users or groups of users. VPNs are used to establish secure connections between different networks or devices.</p><p><br></p><p>https://cloud.google.com/iam/docs/understanding-roles</p>",
                "answers": [
                    "<p>IAM&nbsp;roles</p>",
                    "<p>Firewall rules</p>",
                    "<p>Virtual Private Clouds</p>",
                    "<p>Virtual Private Networks</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "When planning your migration, you find out that some members of the network management team will need to be able to manage all network components, but other team members will only need read access. What mechanism should you use to control this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681564,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company's data science team anticipates a significant increase in the number and complexity of machine learning (ML) models they plan to train over the next year. Currently, the team uses Compute Engine instances for training. What would be the best approach to accommodate this increase?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use AI Platform Training to offload and scale model training tasks. -&gt;&nbsp;Correct. AI Platform Training is a managed service that allows data scientists to run ML training jobs on Google Cloud at scale and without the need to manage infrastructure.</p><p><br></p><p>Upgrade the Compute Engine instances to have more CPUs and more memory. -&gt;&nbsp;Incorrect. This solution, also known as vertical scaling, could accommodate some growth, but it has limitations and can lead to overprovisioning and higher costs. Also, it doesn't fully utilize the benefits of distributed ML training.</p><p><br></p><p>Use Cloud Functions to train individual models in response to events. -&gt;&nbsp;Incorrect. Cloud Functions is primarily designed to handle lightweight, event-driven serverless compute tasks and wouldn't be appropriate for resource-intensive ML training jobs.</p><p><br></p><p>Increase the disk size for the Compute Engine instances to store more training data. -&gt;&nbsp;Incorrect. Increasing disk size may allow more data to be stored locally but does not address the computational requirements of training more complex ML models.</p>",
                "answers": [
                    "<p>Use AI Platform Training to offload and scale model training tasks.</p>",
                    "<p>Upgrade the Compute Engine instances to have more CPUs and more memory.</p>",
                    "<p>Use Cloud Functions to train individual models in response to events.</p>",
                    "<p>Increase the disk size for the Compute Engine instances to store more training data.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company's data science team anticipates a significant increase in the number and complexity of machine learning (ML) models they plan to train over the next year. Currently, the team uses Compute Engine instances for training. What would be the best approach to accommodate this increase?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681566,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company has a robust e-commerce application running on Compute Engine instances and using Cloud Storage. As the business expands globally, you notice latency issues affecting customers in different regions. What approach should you consider to improve your solution?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement a Content Delivery Network (CDN) using Cloud CDN to cache static content closer to users. -&gt;&nbsp;Correct. Implementing a CDN would cache static content closer to the users, significantly reducing the latency experienced by users in different geographical locations.</p><p><br></p><p>Increase the size of the Compute Engine instances to enhance the application's performance. -&gt;&nbsp;Incorrect. While this could marginally improve performance, it would not significantly impact the latency issues experienced by global customers.</p><p><br></p><p>Move the application to App Engine to leverage its automatic scaling features. -&gt;&nbsp;Incorrect. Although App Engine provides automatic scaling, this doesn't directly address the latency issue for global users.</p><p><br></p><p>Migrate from Cloud Storage to Persistent Disk for faster I/O operations. -&gt;&nbsp;Incorrect. Persistent Disks may provide faster I/O for specific use cases, but they wouldn't solve the latency problem for users in different geographical locations.</p>",
                "answers": [
                    "<p>Implement a Content Delivery Network (CDN) using Cloud CDN to cache static content closer to users.</p>",
                    "<p>Increase the size of the Compute Engine instances to enhance the application's performance.</p>",
                    "<p>Move the application to App Engine to leverage its automatic scaling features.</p>",
                    "<p>Migrate from Cloud Storage to Persistent Disk for faster I/O operations.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company has a robust e-commerce application running on Compute Engine instances and using Cloud Storage. As the business expands globally, you notice latency issues affecting customers in different regions. What approach should you consider to improve your solution?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681568,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company has a multi-tier web application running on Google Cloud. You've been tasked with planning for future improvements. User feedback indicates that the application occasionally experiences latency issues, which you suspect is due to increasing database load. What would be the best approach to address this issue and improve the application?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Migrate the database from Cloud SQL to Cloud Spanner to take advantage of horizontal scaling. -&gt;&nbsp;Correct. Cloud Spanner is Google's horizontally scalable, globally-distributed database service, which is designed to handle high transaction loads with low latency.</p><p><br></p><p>Increase the size of the Cloud SQL instance to accommodate the higher load. -&gt;&nbsp;Incorrect. This is an example of vertical scaling. While it could provide some temporary relief, it's not a long-term solution as it has limitations and can lead to overprovisioning and higher costs.</p><p><br></p><p>Implement a caching layer using Cloud Memorystore to reduce database load. -&gt;&nbsp;Incorrect. While adding a caching layer could help improve performance for read-heavy workloads, it may not solve issues with write-heavy loads or complex queries, and it adds complexity to the application.</p><p><br></p><p>Use Cloud Dataflow to preprocess and aggregate data before storing it in Cloud SQL. -&gt;&nbsp;Incorrect. While this could potentially reduce the amount of data written to Cloud SQL, it might not solve the problem if the database load is due to a large number of queries or complex joins, and it adds complexity to the data processing pipeline.</p>",
                "answers": [
                    "<p>Migrate the database from Cloud SQL to Cloud Spanner to take advantage of horizontal scaling.</p>",
                    "<p>Increase the size of the Cloud SQL instance to accommodate the higher load.</p>",
                    "<p>Implement a caching layer using Cloud Memorystore to reduce database load.</p>",
                    "<p>Use Cloud Dataflow to preprocess and aggregate data before storing it in Cloud SQL.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company has a multi-tier web application running on Google Cloud. You've been tasked with planning for future improvements. User feedback indicates that the application occasionally experiences latency issues, which you suspect is due to increasing database load. What would be the best approach to address this issue and improve the application?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681570,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are planning to run stateful applications in Kubernetes Engine. What should you use to support stateful applications?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>StatefulSets -&gt; Correct. StatefulSets is a Kubernetes feature that provides guarantees about the ordering and uniqueness of pods when deploying stateful applications. It ensures that each pod in the StatefulSet gets a stable network identity, which is necessary for stateful applications such as databases. This means that the StatefulSet creates a unique hostname and persistent storage for each pod in the set, and ensures that the pods are created and deleted in a predictable order.</p><p><br></p><p>Pods -&gt; Incorrect. Pods, which are the smallest deployable units in Kubernetes, are not suitable for stateful applications because they do not provide guarantees about the ordering and uniqueness of pods. Pods are ephemeral, meaning they are created and destroyed frequently in Kubernetes, which can lead to data loss or inconsistencies for stateful applications.</p><p><br></p><p>StatefulPods -&gt; Incorrect. StatefulPods is not a valid Kubernetes feature or resource, so it is not a correct answer to the question.</p><p><br></p><p>DeamonSet -&gt; Incorrect. DeamonSet is a Kubernetes feature that ensures that all (or some) nodes run a copy of a pod. It is useful for running background or monitoring tasks on each node in a cluster, but it is not specifically designed for stateful applications.</p><p><br></p><p>ReplicaSet -&gt; Incorrect. ReplicaSet is a Kubernetes feature that ensures a specified number of replicas of a pod are running at any given time. While it can be used to ensure high availability and scalability of stateful applications, it does not provide guarantees about the ordering and uniqueness of pods, which are necessary for stateful applications</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/concepts/statefulset</p>",
                "answers": [
                    "<p>StatefulSets</p>",
                    "<p>Pods</p>",
                    "<p>StatefulPods</p>",
                    "<p>DeamonSet</p>",
                    "<p>ReplicaSet</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are planning to run stateful applications in Kubernetes Engine. What should you use to support stateful applications?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681572,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with a client who wants to migrate a large number of files from their on-premises data center to a Cloud Storage bucket. The client's on-premises data center is located in a region with a slow and unstable network connection. They would like the transfer to happen as quickly as possible, while also ensuring that the transfer can continue from where it left off in case of network interruptions. Which gsutil command option should you recommend for this scenario?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p><code>-m</code> -&gt; Correct. It stands for \"multi-threading\". This option causes gsutil to perform operations (like cp and rsync) in parallel, which can significantly speed up the upload process. It also helps in case of network interruptions, as the already transferred files are not re-transferred.</p><p><br></p><p><code>-c</code> -&gt; Incorrect. It is used for continuing a transfer after a process has been interrupted. While this is helpful, it does not speed up the process, and it can be combined with -m for better performance in this scenario.</p><p><br></p><p><code>-r</code> -&gt; Incorrect. It is used for copying directories recursively, but it does not provide the benefits needed in this scenario.</p><p><br></p><p><code>-o</code> -&gt; Incorrect. This option is used to specify options for gsutil, but it doesn't help to speed up the process or handle network interruptions.</p>",
                "answers": [
                    "<p><code>-m</code> </p>",
                    "<p><code>-c</code> </p>",
                    "<p><code>-r</code> </p>",
                    "<p><code>-o</code> </p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with a client who wants to migrate a large number of files from their on-premises data center to a Cloud Storage bucket. The client's on-premises data center is located in a region with a slow and unstable network connection. They would like the transfer to happen as quickly as possible, while also ensuring that the transfer can continue from where it left off in case of network interruptions. Which gsutil command option should you recommend for this scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681574,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your compliance team is concerned about using a public cloud service because other companies will be running their systems in the same cloud. You assure them that your company's resources will be isolated and inaccessible to others. Which resource is used for this purpose?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Virtual Private Clouds - VPCs -&gt; Correct. Virtual Private Clouds (VPCs) are a mechanism in cloud computing that allow you to create isolated virtual networks within a public cloud environment. Each VPC is logically separated from other VPCs and the public internet, providing a secure and isolated environment for your company's resources. With a VPC, you can define your own IP address range, create subnets, and configure routing tables, security groups, and network access control lists (ACLs) to control access to your resources. You can also use VPC peering to connect VPCs within the same cloud provider, or use a VPN connection to establish a secure connection between your VPC and your on-premises network.</p><p><br></p><p>CIDR&nbsp;blocks -&gt; Incorrect. CIDR blocks are a method for allocating and addressing IP networks, but they do not provide isolation or security for your resources within a public cloud environment.</p><p><br></p><p>Cloud Interconnect -&gt; Incorrect. Cloud Interconnect is a method for establishing secure connection between your on-premises network and a public cloud environment, but it does not provide isolation or security for your resources within the cloud environment itself.</p><p><br></p><p>Cloud VPN -&gt; Incorrect. Cloud VPN is a method for establishing secure connection between your on-premises network and a public cloud environment, but it does not provide isolation or security for your resources within the cloud environment itself.</p><p><br></p><p>https://cloud.google.com/vpc/docs/vpc</p>",
                "answers": [
                    "<p>Virtual Private Clouds - VPCs</p>",
                    "<p>CIDR&nbsp;blocks</p>",
                    "<p>Cloud Interconnect</p>",
                    "<p>Cloud VPN</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your compliance team is concerned about using a public cloud service because other companies will be running their systems in the same cloud. You assure them that your company's resources will be isolated and inaccessible to others. Which resource is used for this purpose?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681576,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A SaaS solution for enterprise customers needs to be updated. Many components of the service are stateful and the system wasn't designed to allow incremental rollout of new code. The entire environment has to be running the same version of the deployed code. What implementation strategy would you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Blue/Green deployment strategy -&gt; Correct. In a Blue/Green deployment, two environments (the \"blue\" and the \"green\") are maintained. One of them is live (let's say blue) and serving users while the other (green) is used for deploying and testing the new version of the software. Once the new version has been fully tested and is ready to go live, the router is switched to redirect all traffic to the green environment. Therefore, all instances are running the same version of the software at all times, which suits the stated requirement.</p><p><br></p><p>Canary deployment strategy -&gt;&nbsp;Incorrect. This strategy involves releasing the new version of software to a small subset of users before rolling it out to the entire user base. This approach wouldn't work in this scenario since the system needs to run the same version of the code across the entire environment, and canary deployment inherently involves running multiple versions at the same time.</p><p><br></p><p>Rolling deployment strategy -&gt;&nbsp;Incorrect. In a rolling deployment, the new version of software is gradually rolled out to all instances, replacing the old version. But during the rollout process, different instances will be running different versions of the software, which contradicts the system requirements described.</p><p><br></p><p>A/B deployment strategy -&gt;&nbsp;Incorrect. Similar to the Canary strategy, A/B testing involves running two (or more) versions of the software simultaneously to compare their performance. This also doesn't fit the requirement of running the same code version across all instances.</p><p><br></p><p>https://cloud.google.com/architecture/implementing-deployment-and-testing-strategies-on-gke#perform_a_bluegreen_deployment</p>",
                "answers": [
                    "<p>Blue/Green deployment strategy</p>",
                    "<p>Canary deployment strategy</p>",
                    "<p>Rolling deployment strategy</p>",
                    "<p>A/B deployment strategy</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A SaaS solution for enterprise customers needs to be updated. Many components of the service are stateful and the system wasn't designed to allow incremental rollout of new code. The entire environment has to be running the same version of the deployed code. What implementation strategy would you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681578,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company offers online services that collect data about users and operates in North America. You want to start a business in Europe. What regulations must your company meet?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>GDPR -&gt; Correct. It is a regulation in EU law on data protection and privacy for all individuals within the European Union (EU) and the European Economic Area (EEA). It also addresses the export of personal data outside the EU and EEA. Any company that collects data from individuals in the EU, regardless of where the company is based, must comply with GDPR.</p><p><br></p><p>COPPA -&gt;&nbsp;Incorrect. (Children's Online Privacy Protection Act) is a U.S. law that applies to websites or online services that collect personal information from children under 13. </p><p><br></p><p>HIPAA/HITECH -&gt;&nbsp;Incorrect. (Health Insurance Portability and Accountability Act/Health Information Technology for Economic and Clinical Health Act) are U.S. laws that govern the use and disclosure of protected health information (PHI). </p><p><br></p><p>SOX -&gt; Incorrect. (Sarbanes-Oxley Act) is a U.S. law that governs financial reporting and accounting practices for publicly traded companies.</p><p><br></p><p>https://cloud.google.com/privacy/gdpr</p>",
                "answers": [
                    "<p>GDPR</p>",
                    "<p>COPPA</p>",
                    "<p>HIPAA/HITECH</p>",
                    "<p>SOX</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company offers online services that collect data about users and operates in North America. You want to start a business in Europe. What regulations must your company meet?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681580,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>In your role as a cloud architect, you're preparing to transition your on-site data warehouse to Google Cloud, utilizing BigQuery. You have to create a presentation for the management team to explain the cost structure of BigQuery. There are two primary elements to consider when it comes to the pricing of BigQuery. Select the costs you incur when using BigQuery.</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>The cost to process queries&nbsp;(analysis pricing). -&gt;&nbsp;Correct. BigQuery charges based on the amount of data processed by each query. This is often referred to as \"analysis\" or \"query\" pricing. BigQuery does not charge for queries that access cached results or metadata operations.</p><p><br></p><p>The cost to store data that you load into BigQuery (storage pricing). -&gt;&nbsp;Correct. BigQuery charges for storing data. The costs are incurred monthly and vary depending on the amount of data stored.</p><p><br></p><p>The cost of Identity and Access Management (IAM pricing). -&gt;&nbsp;Incorrect. Google Cloud Platform does not charge for IAM. IAM is a feature provided by Google Cloud to manage access to resources, and there's no direct cost associated with it.</p><p><br></p><p>The cost of viewing table schemas (schema pricing). -&gt;&nbsp;Incorrect. There is no cost associated with viewing table schemas. BigQuery does not charge for metadata operations, which includes viewing table schemas.</p><p><br></p><p>https://cloud.google.com/bigquery/pricing</p>",
                "answers": [
                    "<p>The cost to process queries&nbsp;(analysis pricing).</p>",
                    "<p>The cost to store data that you load into BigQuery (storage pricing).</p>",
                    "<p>The cost of Identity and Access Management (IAM pricing).</p>",
                    "<p>The cost of viewing table schemas (schema pricing).</p>"
                ]
            },
            "correct_response": [
                "a",
                "b"
            ],
            "section": "",
            "question_plain": "In your role as a cloud architect, you're preparing to transition your on-site data warehouse to Google Cloud, utilizing BigQuery. You have to create a presentation for the management team to explain the cost structure of BigQuery. There are two primary elements to consider when it comes to the pricing of BigQuery. Select the costs you incur when using BigQuery.",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681582,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are responsible for preparing migration strategy. Your company needs to mount a shared filesystem to Compute Engine instances. Which GCP&nbsp;service should you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Filestore -&gt; Correct. It is the best GCP service to use when mounting a shared file system to Compute Engine instances. Cloud Filestore is a fully-managed file storage service that provides a high-performance, NFSv3-compatible file system. It is ideal for enterprise applications that require high-performance file storage for applications that need low-latency access to a shared file system.</p><p><br></p><p>Cloud Firestore -&gt; Incorrect. It is a NoSQL document database that is optimized for mobile and web application development. It is not designed for file storage.</p><p><br></p><p>Local SSD -&gt; Incorrect. It is a type of temporary, high-performance storage that is directly attached to a virtual machine instance. It is not suitable for shared file systems that need to be accessed by multiple instances.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. It is a fully-managed object storage service that is designed for storing and retrieving large unstructured data objects. While it can be used for file storage, it is not optimized for high-performance file sharing among multiple instances.</p><p><br></p><p>https://cloud.google.com/filestore/docs/concepts</p>",
                "answers": [
                    "<p>Cloud Filestore</p>",
                    "<p>Cloud Firestore</p>",
                    "<p>Local SSD</p>",
                    "<p>Cloud Storage</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are responsible for preparing migration strategy. Your company needs to mount a shared filesystem to Compute Engine instances. Which GCP&nbsp;service should you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681584,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with a client who operates a popular web service. Their traffic patterns are highly variable, and they need a deployment strategy that will scale in response to the load on their system. The client wants to deploy a Managed Instance Group (MIG) with a minimum of 5 instances and a maximum of 50 instances. Which of the following strategies should you use to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Deploy the MIG and configure it with an Autoscaler, setting the target utilization level to match the desired load. -&gt; Correct. The Autoscaler in a Managed Instance Group automatically adds or removes instances based on the load. By setting a target utilization level, the Autoscaler will maintain the number of instances between the minimum and maximum number of instances defined.</p><p><br></p><p>Deploy the MIG without Autoscaler, manually adding instances as needed. -&gt; Incorrect. This approach does not satisfy the requirement for automatic scaling. The number of instances would need to be manually adjusted based on the load, which is not practical or efficient.</p><p><br></p><p>Deploy the MIG with a minimum of 5 instances, but do not specify a maximum number of instances. -&gt; Incorrect. While you can deploy a MIG with a specified minimum number of instances, without defining a maximum number of instances and using an Autoscaler, the number of instances could scale beyond what is required or affordable for the client.</p><p><br></p><p>Deploy the MIG with Autoscaler, and use Instance Templates to specify the maximum number of instances. -&gt; Incorrect. Instance Templates define the properties of the instances that will be created in a MIG, but they do not control the number of instances. The Autoscaler uses the Instance Template to create new instances, but the maximum number of instances must be defined in the Autoscaler configuration, not in the Instance Template.</p>",
                "answers": [
                    "<p>Deploy the MIG and configure it with an Autoscaler, setting the target utilization level to match the desired load.</p>",
                    "<p>Deploy the MIG without Autoscaler, manually adding instances as needed.</p>",
                    "<p>Deploy the MIG with a minimum of 5 instances, but do not specify a maximum number of instances.</p>",
                    "<p>Deploy the MIG with Autoscaler, and use Instance Templates to specify the maximum number of instances.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with a client who operates a popular web service. Their traffic patterns are highly variable, and they need a deployment strategy that will scale in response to the load on their system. The client wants to deploy a Managed Instance Group (MIG) with a minimum of 5 instances and a maximum of 50 instances. Which of the following strategies should you use to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681586,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>As a cloud architect, you need to plan your enterprise network in Google Cloud. Why should enterprises use custom VPC networks rather than the default network? (select 2)</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Custom VPC networks integrate better with existing IP address management schemes. The default networks use the same set of internal IP ranges. IP ranges might overlap when connected with your on-premises corporate networks. -&gt; Correct. Custom VPC networks can be designed to fit an organization's specific IP address management scheme, which may help avoid IP range overlaps when connecting with on-premises networks. The default network uses a predefined set of internal IP ranges that might conflict with the organization's IP addresses.</p><p><br></p><p>Because you cannot connect two auto mode VPC networks to each other using VPC Network Peering because their subnets use identical primary IP ranges. -&gt;&nbsp;Correct. Auto mode VPC networks use a specific range of IP addresses that are the same across all networks. This makes it impossible to peer two auto mode networks with identical primary IP ranges, which limits network connectivity options.</p><p><br></p><p>Because in custom mode you cannot choose unique, descriptive names for custom mode subnets. -&gt; Incorrect. In custom mode, subnets can have unique, descriptive names.</p><p><br></p><p>Because in custom mode you automatically get some pre-populated firewall rules. -&gt; Incorrect. In custom mode, users must manually create all firewall rules.</p><p><br></p><p>https://cloud.google.com/architecture/best-practices-vpc-design#custom-mode</p>",
                "answers": [
                    "<p>Custom VPC networks integrate better with existing IP address management schemes. The default networks use the same set of internal IP ranges. IP ranges might overlap when connected with your on-premises corporate networks.</p>",
                    "<p>Because you cannot connect two auto mode VPC networks to each other using VPC Network Peering because their subnets use identical primary IP ranges.</p>",
                    "<p>Because in custom mode you cannot choose unique, descriptive names for custom mode subnets.</p>",
                    "<p>Because in custom mode you automatically get some pre-populated firewall rules.</p>"
                ]
            },
            "correct_response": [
                "a",
                "b"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you need to plan your enterprise network in Google Cloud. Why should enterprises use custom VPC networks rather than the default network? (select 2)",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681588,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdf</p><p><br></p><p>As the Data Compliance Officer for Mountkirk Games, your responsibility is to safeguard customers' personally identifiable information (PII). The company seeks to generate anonymized usage reports for its new game and implement a data deletion policy for PII after a designated timeframe. Your role is to ensure compliance while considering the business and technical requirements with minimal cost implications. What course of action would you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should archive audit logs in BigQuery, and generate reports using Google Data Studio. -&gt;&nbsp;Correct. BigQuery allows easy querying for report generation, with low storage costs.</p><p><br></p><p>You should archive audit logs in Cloud Storage, and manually generate reports. -&gt; Incorrect. Cloud Storage is an object store with no query language access for report generation.</p><p><br></p><p>You should write a Cloud Logging filter to export specific date ranges to Pub/Sub. -&gt; Incorrect. It does not address log storage for data retention.</p><p><br></p><p>You should archive user logs on a locally attached persistent disk, and cat them to a text file for auditing. -&gt; Incorrect. Long term storage in persistent disks is expensive.</p><p><br></p><p>https://cloud.google.com/logging/docs/audit</p>",
                "answers": [
                    "<p>You should archive audit logs in BigQuery, and generate reports using Google Data Studio.</p>",
                    "<p>You should archive audit logs in Cloud Storage, and manually generate reports.</p>",
                    "<p>You should write a Cloud Logging filter to export specific date ranges to Pub/Sub.</p>",
                    "<p>You should archive user logs on a locally attached persistent disk, and cat them to a text file for auditing.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdfAs the Data Compliance Officer for Mountkirk Games, your responsibility is to safeguard customers' personally identifiable information (PII). The company seeks to generate anonymized usage reports for its new game and implement a data deletion policy for PII after a designated timeframe. Your role is to ensure compliance while considering the business and technical requirements with minimal cost implications. What course of action would you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681590,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Refer to the Helicopter Racing League (HRL) case study for this question: https://services.google.com/fh/files/blogs/master_case_study_helicopter_racing_league.pdf</p><p><br></p><p>The Helicopter Racing League (HRL) is seeking your assistance in expanding the reach of their existing recorded video content to attract new fans in emerging regions. In light of the business and technical requirements of HRL, what actions do you need to take?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should use Cloud CDN to cache the video content from HRL’s existing public cloud provider. -&gt; Correct. Cloud CDN can be used to cache data hosted on other cloud providers and supports large objects such as video.</p><p><br></p><p>You should serve the video content directly from a multi-region Cloud Storage bucket. -&gt;&nbsp;Incorrect. A multi-region bucket does not serve all global areas with similar latency.</p><p><br></p><p>You should use Apigee Edge to cache the video content from HRL’s existing public cloud provider. -&gt; Incorrect. Apigee Edge is not designed to cache data larger than 512 KB.</p><p><br></p><p>You should replicate the video content in Google Kubernetes Engine clusters in regions close to the fans. -&gt; Incorrect. Replicating the video content introduces unnecessary complexity.</p><p><br></p><p>https://cloud.google.com/storage/docs/locations#considerations</p><p>https://cloud.google.com/cdn/docs/caching#maximum-size</p>",
                "answers": [
                    "<p>You should use Cloud CDN to cache the video content from HRL’s existing public cloud provider.</p>",
                    "<p>You should serve the video content directly from a multi-region Cloud Storage bucket.</p>",
                    "<p>You should use Apigee Edge to cache the video content from HRL’s existing public cloud provider.</p>",
                    "<p>You should replicate the video content in Google Kubernetes Engine clusters in regions close to the fans.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Refer to the Helicopter Racing League (HRL) case study for this question: https://services.google.com/fh/files/blogs/master_case_study_helicopter_racing_league.pdfThe Helicopter Racing League (HRL) is seeking your assistance in expanding the reach of their existing recorded video content to attract new fans in emerging regions. In light of the business and technical requirements of HRL, what actions do you need to take?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681592,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are assisting a client with an e-commerce application hosted on Google Cloud Platform (GCP). The client's application is expected to receive an increased volume of traffic during an upcoming sale. They want to distribute incoming traffic across multiple Compute Engine instances using Cloud Load Balancer. Additionally, they want to ensure that if any instance is unresponsive or unable to handle requests, it is automatically removed from the pool of available instances. Which of the following configurations would best achieve this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Configure an HTTP(S) Load Balancer with global backend services and set up an HTTP health check. -&gt; Correct. An HTTP(S) Load Balancer with global backend services allows traffic to be distributed across instances in multiple regions. Setting up an HTTP health check will ensure that only responsive instances receive traffic.</p><p><br></p><p>Configure a Network Load Balancer and set up an HTTPS health check. -&gt; Incorrect. Network Load Balancer is a good option for distributing traffic, but it is not designed for global traffic distribution and does not support HTTP(S) health checks.</p><p><br></p><p>Configure a TCP/SSL Proxy Load Balancer and set up a TCP health check. -&gt; Incorrect. TCP/SSL Proxy Load Balancer can be used to distribute TCP traffic, but in this case, the application is likely to be HTTP(S) based as it is an e-commerce site, making HTTP(S) Load Balancer a more appropriate choice.</p><p><br></p><p>Configure a Network Load Balancer and set up a TCP health check. -&gt;&nbsp;Incorrect. Network Load Balancer would be beneficial for distributing TCP and UDP traffic. However, for an e-commerce application which is HTTP(S) based, HTTP(S) Load Balancer with HTTP health checks would be more appropriate.</p>",
                "answers": [
                    "<p>Configure an HTTP(S) Load Balancer with global backend services and set up an HTTP health check.</p>",
                    "<p>Configure a Network Load Balancer and set up an HTTPS health check.</p>",
                    "<p>Configure a TCP/SSL Proxy Load Balancer and set up a TCP health check.</p>",
                    "<p>Configure a Network Load Balancer and set up a TCP health check.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are assisting a client with an e-commerce application hosted on Google Cloud Platform (GCP). The client's application is expected to receive an increased volume of traffic during an upcoming sale. They want to distribute incoming traffic across multiple Compute Engine instances using Cloud Load Balancer. Additionally, they want to ensure that if any instance is unresponsive or unable to handle requests, it is automatically removed from the pool of available instances. Which of the following configurations would best achieve this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681594,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Refer to the TerramEarth case study for this question: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdf</p><p><br></p><p>Upon evaluating TerramEarth's business requirements to minimize downtime, it was determined that a substantial amount of time-saving could be achieved by decreasing customers' wait time for parts. Consequently, the decision has been made to concentrate efforts on reducing the aggregate reporting time of three weeks. What changes to the company's processes would you suggest?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Increase fleet cellular connectivity to 80%, migrate from FTP to streaming transport, and develop machine learning analysis of metrics. -&gt; Correct. Using cellular connectivity will greatly improve the freshness of data used for analysis from where it is now, collected when the machines are in for maintenance. Streaming transport instead of periodic FTP will tighten the feedback loop even more. Machine learning is ideal for predictive maintenance workloads.</p><p><br></p><p>Migrate from CSV to binary format, migrate from FTP to SFTP transport, and develop machine learning analysis of metrics. -&gt;&nbsp;Incorrect. Machine learning analysis is a good means toward the end of reducing downtime, but shuffling formats and transport doesn't directly help at all.</p><p><br></p><p>Migrate from FTP to streaming transport, migrate from CSV to binary format, and develop machine learning analysis of metrics. -&gt; Incorrect. Machine learning analysis is a good means toward the end of reducing downtime, and moving to streaming can improve the freshness of the information in that analysis, but changing the format doesn't directly help at all.</p><p><br></p><p>Migrate from FTP to SFTP transport, develop machine learning analysis of metrics, and increase dealer local inventory by a fixed factor. -&gt; Incorrect. Machine learning analysis is a good means toward the end of reducing downtime, but the rest of these changes don't directly help at all.</p>",
                "answers": [
                    "<p>Increase fleet cellular connectivity to 80%, migrate from FTP to streaming transport, and develop machine learning analysis of metrics.</p>",
                    "<p>Migrate from CSV to binary format, migrate from FTP to SFTP transport, and develop machine learning analysis of metrics.</p>",
                    "<p>Migrate from FTP to streaming transport, migrate from CSV to binary format, and develop machine learning analysis of metrics.</p>",
                    "<p>Migrate from FTP to SFTP transport, develop machine learning analysis of metrics, and increase dealer local inventory by a fixed factor.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Refer to the TerramEarth case study for this question: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdfUpon evaluating TerramEarth's business requirements to minimize downtime, it was determined that a substantial amount of time-saving could be achieved by decreasing customers' wait time for parts. Consequently, the decision has been made to concentrate efforts on reducing the aggregate reporting time of three weeks. What changes to the company's processes would you suggest?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681596,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you're helping a large organization configure IAM roles for their security team. The team needs to audit corporate applications deployed on GCP, identify security risks, and recommend improvements, but they should not be allowed to modify resources. Which of the following is the most suitable IAM role to assign to this team?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Security Auditor role -&gt; Correct. The Security Auditor role is designed for exactly this scenario - it provides permissions to view security configuration and state for all resources, making it perfect for auditing, identifying risks and suggesting improvements, but does not allow modification of resources.</p><p><br></p><p>Project Editor role -&gt;&nbsp;Incorrect. The Project Editor role would allow the team to modify resources in the project, which is not desired in this scenario.</p><p><br></p><p>Project Viewer role -&gt;&nbsp;Incorrect. The Project Viewer role would allow the team to view all resources in a project, but it might not provide the detailed security insights that a specific security-focused role like Security Auditor would provide.</p><p><br></p><p>Security Admin role -&gt;&nbsp;Incorrect. The Security Admin role provides broad control over security features in the project and would allow modifications, which is not desired in this case.</p><p><br></p><p>Security Reviewer role -&gt;&nbsp;Incorrect. No such role as \"Security Reviewer\" exists in Google Cloud IAM.</p>",
                "answers": [
                    "<p>Security Auditor role</p>",
                    "<p>Project Editor role</p>",
                    "<p>Project Viewer role</p>",
                    "<p>Security Admin role</p>",
                    "<p>Security Reviewer role</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you're helping a large organization configure IAM roles for their security team. The team needs to audit corporate applications deployed on GCP, identify security risks, and recommend improvements, but they should not be allowed to modify resources. Which of the following is the most suitable IAM role to assign to this team?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681598,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are helping an organization set up a secure, low-latency network connection between their Compute Engine instances in the <code>us-central1</code> region and their local data center. They want to ensure the connection is private and the data isn't exposed over the public internet. Which of the following options should you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Set up Dedicated Interconnect at a colocation facility near their data center to connect directly to Google's network. -&gt; Correct. Dedicated Interconnect provides a direct, private connection from the organization's on-premises network to Google's network, which does not involve the public internet.</p><p><br></p><p>Set up VPN Gateway on Google Cloud and enable IPsec for secure, encrypted communication. -&gt; Incorrect. VPN Gateway and IPsec would indeed encrypt the traffic, but it would still go over the public internet which does not fulfil the requirement for a private connection.</p><p><br></p><p>Use a NAT gateway to enable communication between the Compute Engine instances and the local data center. -&gt; Incorrect. NAT gateways are used for allowing instances without public IPs to connect to the internet, not for connecting a local data center to Google Cloud.</p><p><br></p><p>Set up a VPC Network Peering between their Google Cloud VPC and their local data center. -&gt; Incorrect. VPC Network Peering is used to connect two VPC networks, not an on-premises data center and a Google Cloud VPC.</p>",
                "answers": [
                    "<p>Set up Dedicated Interconnect at a colocation facility near their data center to connect directly to Google's network.</p>",
                    "<p>Set up VPN Gateway on Google Cloud and enable IPsec for secure, encrypted communication.</p>",
                    "<p>Use a NAT gateway to enable communication between the Compute Engine instances and the local data center.</p>",
                    "<p>Set up a VPC Network Peering between their Google Cloud VPC and their local data center.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are helping an organization set up a secure, low-latency network connection between their Compute Engine instances in the us-central1 region and their local data center. They want to ensure the connection is private and the data isn't exposed over the public internet. Which of the following options should you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681600,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with an organization that wants to have granular control over its Google Cloud resources. The organization has multiple projects, each managed by different teams and belonging to different departments. The company wants to enforce the principle of least privilege and ensure that the resources are only accessible to the team that needs them, while also keeping the management simple and efficient. Which of the following approaches should you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Create Google Groups for each team, assign roles to the groups at the project level, and add the team members to the appropriate groups. -&gt; Correct. By creating Google Groups for each team and assigning roles to these groups at the project level, you can ensure that only the teams that need access to a project get it. This approach is also efficient because when a team member joins or leaves, you only need to update the Google Group membership, not the IAM policy.</p><p><br></p><p>Assign individual roles to each team member at the project level. -&gt; Incorrect. Assigning individual roles to each team member at the project level is not efficient, as it would require a lot of manual effort to manage the permissions and does not scale well with the growth of the organization.</p><p><br></p><p>Assign roles to the departments at the organization level and let the departments manage their individual teams' access. -&gt; Incorrect. Assigning roles to the departments at the organization level would not provide the desired granularity. The resources of all projects under the organization would be accessible to all teams under a department, which does not adhere to the principle of least privilege.</p><p><br></p><p>Assign roles to the teams at the organization level. -&gt; Incorrect. Assigning roles to the teams at the organization level would not ensure that a team only has access to the projects it needs. This approach also doesn't adhere to the principle of least privilege.</p>",
                "answers": [
                    "<p>Create Google Groups for each team, assign roles to the groups at the project level, and add the team members to the appropriate groups.</p>",
                    "<p>Assign individual roles to each team member at the project level.</p>",
                    "<p>Assign roles to the departments at the organization level and let the departments manage their individual teams' access.</p>",
                    "<p>Assign roles to the teams at the organization level.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with an organization that wants to have granular control over its Google Cloud resources. The organization has multiple projects, each managed by different teams and belonging to different departments. The company wants to enforce the principle of least privilege and ensure that the resources are only accessible to the team that needs them, while also keeping the management simple and efficient. Which of the following approaches should you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681602,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with an organization to structure their BigQuery permissions. The company has three teams: data analysts, data scientists, and data engineers. The data analysts should be able to run SQL queries, the data scientists need to create and run machine learning models in BigQuery, and the data engineers should have full control over BigQuery resources. What is the most appropriate IAM role assignment for these teams?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Assign the roles/bigquery.user role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.admin role to the data engineers. -&gt;&nbsp;Correct. The roles/bigquery.user role allows data analysts to run queries and read dataset metadata, roles/bigquery.mlUser allows data scientists to create and run ML models, and roles/bigquery.admin gives data engineers full control over BigQuery resources.</p><p><br></p><p>Assign the roles/bigquery.dataViewer role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.dataOwner role to the data engineers. -&gt; Incorrect. The roles/bigquery.dataViewer role would not allow data analysts to run queries, it only provides read access to datasets. Also, roles/bigquery.dataOwner would not provide full control to data engineers over BigQuery resources.</p><p><br></p><p>Assign the roles/bigquery.dataEditor role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.admin role to the data engineers. -&gt; Incorrect. The roles/bigquery.dataEditor role allows users to edit (but not run) queries and provides read/write access to datasets, which is more permission than data analysts typically need.</p><p><br></p><p>Assign the roles/bigquery.jobUser role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.dataEditor role to the data engineers. -&gt;&nbsp;Incorrect. The roles/bigquery.jobUser role would not allow data analysts to run queries, it only provides the ability to run jobs. Also, roles/bigquery.dataEditor does not provide full control to data engineers over BigQuery resources.</p>",
                "answers": [
                    "<p>Assign the roles/bigquery.user role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.admin role to the data engineers.</p>",
                    "<p>Assign the roles/bigquery.dataViewer role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.dataOwner role to the data engineers.</p>",
                    "<p>Assign the roles/bigquery.dataEditor role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.admin role to the data engineers.</p>",
                    "<p>Assign the roles/bigquery.jobUser role to the data analysts, roles/bigquery.mlUser role to the data scientists, and roles/bigquery.dataEditor role to the data engineers.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with an organization to structure their BigQuery permissions. The company has three teams: data analysts, data scientists, and data engineers. The data analysts should be able to run SQL queries, the data scientists need to create and run machine learning models in BigQuery, and the data engineers should have full control over BigQuery resources. What is the most appropriate IAM role assignment for these teams?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681604,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you're working with an organization that operates a complex microservices application on Google Kubernetes Engine (GKE). They want to utilize Service Mesh visualization in the Google Cloud Console to gain insights into their services' performance and interactions. However, after setting up their environment with Istio, they are unable to see any traffic flow between services. Which of the following could be a possible reason and the appropriate fix?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>The required Envoy proxy sidecar containers might not have been injected into each relevant Kubernetes pod. They should ensure automatic or manual sidecar injection is configured. -&gt;&nbsp;Correct. Istio relies on the Envoy proxy sidecars to manage traffic between services in a Service Mesh. If these are not injected into the Kubernetes pods, service traffic will not be correctly managed and visualized.</p><p><br></p><p>The Service Mesh visualization doesn't support GKE. They should use Stackdriver for visualizing service interactions. -&gt; Incorrect. Service Mesh visualization does support GKE.</p><p><br></p><p>Service Mesh visualization does not support microservices. They should refactor their application into a monolith. -&gt; Incorrect. Service Mesh is designed specifically to help manage and understand microservices architectures.</p><p><br></p><p>Istio is not installed correctly. They should reinstall it and restart their services. -&gt; Incorrect. Reinstalling Istio might be necessary in some cases, but the problem could also be due to the missing Envoy proxy sidecars, which are responsible for routing and collecting telemetry for traffic between services.</p>",
                "answers": [
                    "<p>The required Envoy proxy sidecar containers might not have been injected into each relevant Kubernetes pod. They should ensure automatic or manual sidecar injection is configured.</p>",
                    "<p>The Service Mesh visualization doesn't support GKE. They should use Stackdriver for visualizing service interactions.</p>",
                    "<p>Service Mesh visualization does not support microservices. They should refactor their application into a monolith.</p>",
                    "<p>Istio is not installed correctly. They should reinstall it and restart their services.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you're working with an organization that operates a complex microservices application on Google Kubernetes Engine (GKE). They want to utilize Service Mesh visualization in the Google Cloud Console to gain insights into their services' performance and interactions. However, after setting up their environment with Istio, they are unable to see any traffic flow between services. Which of the following could be a possible reason and the appropriate fix?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681606,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are designing a secure architecture for a client who needs to limit the use of external IP addresses on their Compute Engine instances. The client wants to ensure that only specific, approved instances can be assigned external IP addresses. What method would be the most appropriate for enforcing this requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use an organization policy to restrict external IP addresses and apply it to the project. -&gt; Correct. An organization policy allows you to define fine-grained, location-specific policies for your Google Cloud resources. You can use the compute.vmExternalIpAccess policy constraint to restrict instances from obtaining external IP addresses, and apply exceptions to approved instances.</p><p><br></p><p>Use Shared VPC to restrict instances from obtaining external IP addresses. -&gt; Incorrect. While Shared VPC can be used to share network resources across multiple projects, it does not provide a direct way to restrict instances from obtaining external IP addresses.</p><p><br></p><p>Use firewall rules to block outbound traffic from instances without approved external IPs. -&gt; Incorrect. Firewall rules control network traffic, but they can't prevent instances from being assigned external IP addresses. They could only block outbound traffic, but the instances would still have external IPs.</p><p><br></p><p>Use VPC Service Controls to restrict instances from obtaining external IP addresses. -&gt; Incorrect. VPC Service Controls are mainly used to define a security perimeter around Google Cloud resources and services to mitigate data exfiltration risks. They do not limit instances from obtaining external IP addresses.</p>",
                "answers": [
                    "<p>Use an organization policy to restrict external IP addresses and apply it to the project.</p>",
                    "<p>Use Shared VPC to restrict instances from obtaining external IP addresses.</p>",
                    "<p>Use firewall rules to block outbound traffic from instances without approved external IPs.</p>",
                    "<p>Use VPC Service Controls to restrict instances from obtaining external IP addresses.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are designing a secure architecture for a client who needs to limit the use of external IP addresses on their Compute Engine instances. The client wants to ensure that only specific, approved instances can be assigned external IP addresses. What method would be the most appropriate for enforcing this requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681608,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with a large enterprise customer who is storing millions of daily logs in a Cloud Storage bucket. To manage costs and ensure data is retained appropriately, they want to automatically move files older than 30 days to Nearline storage and delete any files older than 365 days. Which of the following is the best approach to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use the lifecycle management feature of Cloud Storage to move older files to Nearline and delete files older than 365 days. -&gt;&nbsp;Correct. The lifecycle management feature of Cloud Storage is designed to handle this exact scenario, allowing you to set rules to automatically change the storage class of objects and delete them based on their age.</p><p><br></p><p>Use gsutil to move older files to Nearline storage and delete files older than 365 days manually. -&gt; Incorrect. This approach requires manual intervention and would not be practical or efficient for managing a large number of files.</p><p><br></p><p>Use Cloud Storage Transfer service to move files to Nearline storage and lifecycle management to delete files older than 365 days. -&gt; Incorrect. The Cloud Storage Transfer service is designed to move data into Cloud Storage from online and on-premises sources, not to manage the lifecycle of objects already in Cloud Storage.</p><p><br></p><p>Use gsutil to periodically move all files to Nearline storage and use lifecycle management to delete files older than 365 days. -&gt; Incorrect. Moving all files to Nearline storage with gsutil doesn't make sense if the files are being frequently accessed. Also, you'd still need to manage the deletion of older files.</p>",
                "answers": [
                    "<p>Use the lifecycle management feature of Cloud Storage to move older files to Nearline and delete files older than 365 days.</p>",
                    "<p>Use gsutil to move older files to Nearline storage and delete files older than 365 days manually.</p>",
                    "<p>Use Cloud Storage Transfer service to move files to Nearline storage and lifecycle management to delete files older than 365 days.</p>",
                    "<p>Use gsutil to periodically move all files to Nearline storage and use lifecycle management to delete files older than 365 days.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with a large enterprise customer who is storing millions of daily logs in a Cloud Storage bucket. To manage costs and ensure data is retained appropriately, they want to automatically move files older than 30 days to Nearline storage and delete any files older than 365 days. Which of the following is the best approach to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681610,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company offers a downloadable rendering software through its website, serving customers globally. To ensure optimal customer experience, you aim to minimize delays for all users while adhering to Google's recommended practices. What is the recommended approach for storing the files?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Save the files in multiple Multi-Regional Cloud Storage buckets, one bucket per multi-region. -&gt;&nbsp;Correct. Multi-Regional Cloud Storage buckets are designed for storing data that needs to be accessed frequently from anywhere in the world with low latency. By storing the files in multiple Multi-Regional Cloud Storage buckets, one bucket per multi-region, you can ensure that your customers will experience low latency when accessing your software files from anywhere in the world.</p><p><br></p><p>Save the files in multiple Regional Cloud Storage buckets, one bucket per zone per region. -&gt; Incorrect. It is not the best answer because Regional Cloud Storage buckets are designed for storing data that needs to be accessed frequently from a specific region. Therefore, using multiple Regional Cloud Storage buckets would not be optimal for a company that has customers all over the world.</p><p><br></p><p>Save the files in multiple Regional Cloud Storage buckets, one bucket per region. -&gt; Incorrect. It suggests storing files in multiple Regional Cloud Storage buckets, one per region, which is still not optimal for a company that has customers all over the world.</p><p><br></p><p>Save the files in a Multi-Regional Cloud Storage bucket. -&gt; Incorrect. It is also not the best answer because Multi-Regional Cloud Storage buckets are designed for storing data that needs to be accessed frequently from anywhere in the world, but it's recommended to use multiple buckets in different multi-regions to ensure low latency access for users in different regions.</p><p><br></p><p>https://cloud.google.com/storage/docs/locations</p>",
                "answers": [
                    "<p>Save the files in multiple Multi-Regional Cloud Storage buckets, one bucket per multi-region.</p>",
                    "<p>Save the files in multiple Regional Cloud Storage buckets, one bucket per zone per region.</p>",
                    "<p>Save the files in multiple Regional Cloud Storage buckets, one bucket per region.</p>",
                    "<p>Save the files in a Multi-Regional Cloud Storage bucket.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company offers a downloadable rendering software through its website, serving customers globally. To ensure optimal customer experience, you aim to minimize delays for all users while adhering to Google's recommended practices. What is the recommended approach for storing the files?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681612,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with an organization that frequently provisions and de-provisions a set of Google Cloud resources for temporary projects. They are looking for an automated and reproducible way to manage this. Which of the following approaches best utilizes Google Cloud Deployment Manager for this purpose?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Create a Deployment Manager configuration file for the required resources, and use Deployment Manager to create and delete these resources as needed. -&gt; Correct. Google Cloud Deployment Manager allows you to specify all the resources needed for an application in a declarative format using yaml. You can create a configuration file for your resources and use it to provision and de-provision these resources as needed.</p><p><br></p><p>Use the Deployment Manager API to manually create and delete resources as needed. -&gt; Incorrect. Using the Deployment Manager API directly would not be as simple or maintainable as using a configuration file.</p><p><br></p><p>Use Deployment Manager to periodically check for unused resources and delete them. -&gt; Incorrect. Deployment Manager does not have built-in functionality to check for unused resources. It is used to create and manage resources based on configuration files.</p><p><br></p><p>Use Deployment Manager to automate the process of manually creating and deleting resources in the Google Cloud Console. -&gt; Incorrect. Deployment Manager does not automate manual actions in the Google Cloud Console. It uses configuration files to create and manage resources.</p>",
                "answers": [
                    "<p>Create a Deployment Manager configuration file for the required resources, and use Deployment Manager to create and delete these resources as needed.</p>",
                    "<p>Use the Deployment Manager API to manually create and delete resources as needed.</p>",
                    "<p>Use Deployment Manager to periodically check for unused resources and delete them.</p>",
                    "<p>Use Deployment Manager to automate the process of manually creating and deleting resources in the Google Cloud Console.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with an organization that frequently provisions and de-provisions a set of Google Cloud resources for temporary projects. They are looking for an automated and reproducible way to manage this. Which of the following approaches best utilizes Google Cloud Deployment Manager for this purpose?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681614,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your customer is planning to run a data processing pipeline that ingests, processes, and stores large amounts of data in Google Cloud. The customer has the following requirements:</p><ul><li><p>the pipeline must be easily scalable and able to handle a large volume of data</p></li><li><p>the pipeline must be able to handle failures and recover gracefully</p></li><li><p>the pipeline must be easily integratable with other Google Cloud services</p></li><li><p>the pipeline must be cost-effective</p></li></ul><p><br></p><p>Which Google Cloud service should you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Dataflow -&gt; Correct. Cloud Dataflow is a fully-managed service for transforming and enriching data in stream (real-time) and batch (historical) modes with equal reliability and expressiveness. It provides a simple and cost-effective way to build and manage data processing pipelines that automatically scales up and down as needed. Cloud Dataflow also has built-in fault tolerance and recovery mechanisms, making it able to handle failures and recover gracefully. It can easily integrate with other Google Cloud services like Cloud Storage, BigQuery, and Pub/Sub.</p><p><br></p><p>Cloud Dataproc -&gt;&nbsp;Incorrect. While it can handle data processing tasks, it is more suitable for batch and interactive big data processing rather than building scalable data processing pipelines. It may require additional development and management efforts to achieve the desired pipeline functionality.</p><p><br></p><p>Cloud BigQuery -&gt;&nbsp;Incorrect. Cloud BigQuery is a fully managed data warehouse and analytics platform. It is excellent for querying and analyzing large datasets, but it is not designed specifically for building data processing pipelines. It does not provide the same level of pipeline orchestration and data transformation capabilities as Cloud Dataflow.</p><p><br></p><p>Cloud Pub/Sub -&gt;&nbsp;Incorrect. While it can be used as part of a data processing pipeline for message-based communication, it does not provide the full range of capabilities required for ingesting, processing, and storing large amounts of data in a scalable and cost-effective manner.</p><p><br></p><p>https://cloud.google.com/dataflow/docs</p>",
                "answers": [
                    "<p>Cloud Dataflow</p>",
                    "<p>Cloud Dataproc</p>",
                    "<p>Cloud BigQuery</p>",
                    "<p>Cloud Pub/Sub</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your customer is planning to run a data processing pipeline that ingests, processes, and stores large amounts of data in Google Cloud. The customer has the following requirements:the pipeline must be easily scalable and able to handle a large volume of datathe pipeline must be able to handle failures and recover gracefullythe pipeline must be easily integratable with other Google Cloud servicesthe pipeline must be cost-effectiveWhich Google Cloud service should you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681616,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your customer is planning to run a containerized workload in Google Cloud. The customer has the following requirements:</p><ul><li><p>the workload must be easily deployable and manageable</p></li><li><p>the workload must be easily scalable and able to handle a large volume of data</p></li><li><p>the workload must be able to handle failures and recover gracefully</p></li><li><p>the workload must be easily integratable with other Google Cloud services</p></li></ul><p><br></p><p>Which Google Cloud service should you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Google Kubernetes Engine -&gt; Correct. Google Kubernetes Engine (GKE) is a managed container orchestration service that allows customers to deploy, manage, and scale containerized applications in Google Cloud. GKE is designed to meet the requirements mentioned in the question. Firstly, GKE provides a simple and easy-to-use interface for deploying and managing containerized workloads. It supports various container images and provides integrated support for monitoring, logging, and debugging. Secondly, GKE provides horizontal scaling capabilities that allow customers to handle large volumes of data and traffic by adding or removing nodes to their cluster based on demand. Thirdly, GKE provides automated failover and recovery capabilities that ensure high availability of the applications. It automatically replaces failed nodes and reschedules containers in the event of a node failure. Finally, GKE is easily integratable with other Google Cloud services, such as Cloud Storage, Cloud SQL, and Cloud Pub/Sub. This allows customers to build scalable and reliable applications that can leverage other Google Cloud services.</p><p><br></p><p>Amazon EC2 -&gt; Incorrect. It is an Amazon Web Services (AWS) service for running virtual machines in the cloud. It is not a container orchestration service and cannot meet the requirements mentioned in the question.</p><p><br></p><p>Cloud Functions -&gt; Incorrect. It is a serverless compute service that allows customers to run code in response to events. It is not designed for managing containerized workloads and does not provide container orchestration capabilities.</p><p><br></p><p>Compute Engine -&gt; Incorrect. It is a Google Cloud service for running virtual machines in the cloud. While it is possible to run containers on Compute Engine, it does not provide container orchestration capabilities and cannot meet the requirements mentioned in the question.</p><p><br></p><p>https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview</p>",
                "answers": [
                    "<p>Amazon EC2</p>",
                    "<p>Google Kubernetes Engine</p>",
                    "<p>Cloud Functions</p>",
                    "<p>Compute Engine</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "Your customer is planning to run a containerized workload in Google Cloud. The customer has the following requirements:the workload must be easily deployable and manageablethe workload must be easily scalable and able to handle a large volume of datathe workload must be able to handle failures and recover gracefullythe workload must be easily integratable with other Google Cloud servicesWhich Google Cloud service should you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681618,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your customer is planning to run a highly available and scalable web application in Google Cloud. The customer has the following requirements:</p><ul><li><p>the application must be easily deployable and manageable</p></li><li><p>the application must be highly available and recover from failures automatically</p></li><li><p>the application must be able to handle incoming traffic spikes and scale dynamically</p></li><li><p>the application must be secure and protect against common web attacks</p></li></ul><p><br></p><p>Which Google Cloud service should you recommend to secure and protect the application against common web attacks?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Armor -&gt; Correct. Cloud Armor is a distributed denial-of-service (DDoS) and web application firewall (WAF) service that protects against external threats such as DDoS attacks, application-layer attacks, and SQL injection attacks. It provides a set of security policies that can be customized to allow or deny traffic based on IP address, geo-location, URL, or other attributes. Cloud Armor integrates with other Google Cloud services, such as HTTP(S) Load Balancing and Cloud CDN, to provide a comprehensive security solution for web applications.</p><p><br></p><p>App Engine -&gt; Incorrect. App Engine is a fully managed platform for building and deploying web applications and APIs. It provides automatic scaling, load balancing, and high availability out of the box. However, it does not provide native DDoS protection or WAF capabilities.</p><p><br></p><p>Google Kubernetes Engine -&gt; Incorrect. Google Kubernetes Engine (GKE) is a managed Kubernetes service that allows developers to deploy and manage containerized applications at scale. GKE provides a flexible and portable platform for running microservices-based applications. However, it does not provide native DDoS protection or WAF capabilities.</p><p><br></p><p>Cloud Functions -&gt; Incorrect. Cloud Functions is a serverless compute platform that allows developers to run code in response to events without managing any infrastructure. It is suitable for small to medium-sized workloads that require a short-lived, event-driven architecture. However, it does not provide native DDoS protection or WAF capabilities.</p><p><br></p><p>https://cloud.google.com/armor/docs/cloud-armor-overview</p>",
                "answers": [
                    "<p>App Engine</p>",
                    "<p>Google Kubernetes Engine</p>",
                    "<p>Cloud Functions</p>",
                    "<p>Cloud Armor</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "Your customer is planning to run a highly available and scalable web application in Google Cloud. The customer has the following requirements:the application must be easily deployable and manageablethe application must be highly available and recover from failures automaticallythe application must be able to handle incoming traffic spikes and scale dynamicallythe application must be secure and protect against common web attacksWhich Google Cloud service should you recommend to secure and protect the application against common web attacks?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681620,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>When migrating a legacy application to Google Cloud Platform (GCP), which of the following strategies is most likely to result in the quickest and least disruptive migration, while also taking into account future scalability needs?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Lift and shift the application to Compute Engine Virtual Machines (VMs) and use Cloud Storage for data storage. -&gt; Correct. Lifting and shifting the application to Compute Engine VMs and using Cloud Storage for data storage provides a relatively quick and least disruptive migration strategy while considering future scalability needs. It minimizes the need for significant code changes and allows for a straightforward migration process.</p><p><br></p><p>Re-architect the entire application to use Google Cloud Platform services and design patterns. -&gt;&nbsp;Incorrect. Re-architecting the entire application to use Google Cloud Platform (GCP) services and design patterns can be a valid approach for long-term scalability and optimization. However, it typically requires significant time, effort, and resources to re-engineer the application to take full advantage of GCP services. This option may not result in the quickest and least disruptive migration, as it involves substantial changes to the application architecture.</p><p><br></p><p>Use the Virtual Private Cloud (VPC) to create a dedicated network for the application and its components, and deploy the components to Compute Engine instances. -&gt;&nbsp;Incorrect. Using Virtual Private Cloud (VPC) to create a dedicated network for the application and deploying the components to Compute Engine instances can provide network isolation and control. However, this approach still requires managing and maintaining the application components on virtual machines, which may not be the quickest and least disruptive migration strategy. It also doesn't address scalability needs in an efficient manner.</p><p><br></p><p>Use App Engine to host the application and store data in Cloud Datastore or Cloud Firestore. -&gt;&nbsp;Incorrect. Migrating a legacy application to App Engine often requires making significant changes to the codebase and architecture to fit the PaaS model. This option may not be the quickest and least disruptive migration strategy, especially if the legacy application is not well-suited for the App Engine environment.</p>",
                "answers": [
                    "<p>Re-architect the entire application to use Google Cloud Platform services and design patterns.</p>",
                    "<p>Use the Virtual Private Cloud (VPC) to create a dedicated network for the application and its components, and deploy the components to Compute Engine instances.</p>",
                    "<p>Use App Engine to host the application and store data in Cloud Datastore or Cloud Firestore.</p>",
                    "<p>Lift and shift the application to Compute Engine Virtual Machines (VMs) and use Cloud Storage for data storage.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "When migrating a legacy application to Google Cloud Platform (GCP), which of the following strategies is most likely to result in the quickest and least disruptive migration, while also taking into account future scalability needs?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681622,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A financial services company wants to implement a secure and scalable solution for processing large volumes of financial transactions. The solution should also provide real-time visibility into transaction status, and enable employees to resolve issues and perform auditing tasks. The solution should be fully managed, and minimize the need for additional hardware and software investments. Which of the following options would be the most effective approach to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Spanner for storing transaction data and Cloud Dataflow for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data. -&gt; Correct. Cloud Spanner is a globally distributed, scalable, and fully managed relational database that provides strong consistency and horizontal scaling. It is an ideal solution for processing large volumes of financial transactions while providing real-time visibility into transaction status. Cloud Dataflow is a fully managed data processing service that can be used to process transactions in real-time. Cloud Pub/Sub can be used for real-time updates on transaction status, and Cloud IAM can be used to control access to the data. Cloud Spanner eliminates the need for additional hardware and software investments, making it a cost-effective solution for the financial services company.</p><p><br></p><p>Use Cloud Bigtable for storing transaction data and Cloud Functions for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data. -&gt;&nbsp;Incorrect. Cloud Bigtable is a NoSQL database that does not provide strong consistency and is not ideal for processing financial transactions that require strong consistency. Cloud Functions can be used to process transactions in real-time, Cloud Pub/Sub can be used for real-time updates on transaction status, and Cloud IAM can be used to control access to the data.</p><p><br></p><p>Use Cloud Datastore for storing transaction data and Cloud Functions for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data. -&gt;&nbsp;Incorrect. Cloud Datastore is a NoSQL database that does not provide strong consistency and is not ideal for processing financial transactions that require strong consistency. Cloud Functions can be used to process transactions in real-time, Cloud Pub/Sub can be used for real-time updates on transaction status, and Cloud IAM can be used to control access to the data.</p><p><br></p><p>Use Cloud SQL for storing transaction data and Cloud Functions for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data. -&gt; Incorrect. Cloud SQL is a relational database that is not globally distributed and does not provide strong consistency. It is not ideal for processing financial transactions that require strong consistency. Cloud Functions can be used to process transactions in real-time, Cloud Pub/Sub can be used for real-time updates on transaction status, and Cloud IAM can be used to control access to the data.</p>",
                "answers": [
                    "<p>Use Cloud Bigtable for storing transaction data and Cloud Functions for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data.</p>",
                    "<p>Use Cloud Datastore for storing transaction data and Cloud Functions for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data.</p>",
                    "<p>Use Cloud SQL for storing transaction data and Cloud Functions for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data.</p>",
                    "<p>Use Cloud Spanner for storing transaction data and Cloud Dataflow for processing transactions in real-time. Use Cloud Pub/Sub for real-time updates on transaction status and Cloud IAM for controlling access to the data.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "A financial services company wants to implement a secure and scalable solution for processing large volumes of financial transactions. The solution should also provide real-time visibility into transaction status, and enable employees to resolve issues and perform auditing tasks. The solution should be fully managed, and minimize the need for additional hardware and software investments. Which of the following options would be the most effective approach to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681624,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A large retail company wants to launch a new personalized shopping experience for customers using an e-commerce platform. The platform should allow customers to create a profile, save their preferences and purchase history, and receive personalized product recommendations in real-time. The platform should also provide real-time analytics and reporting on customer behavior and purchasing patterns. Which of the following solutions would be the most effective for meeting these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use BigQuery for storing customer data, Cloud Dataflow for processing customer preferences and purchase history, and Cloud Machine Learning Engine for generating product recommendations. Use Cloud Dataproc for analytics and reporting. -&gt;&nbsp;Correct. BigQuery is designed for handling and analyzing large datasets, perfect for storing customer data. Cloud Dataflow allows for real-time data processing, which meets the requirement of processing customer preferences and purchase history. Cloud Machine Learning Engine can indeed handle personalized product recommendations, and Cloud Dataproc offers more powerful processing for analytics and reporting compared to Data Studio.</p><p><br></p><p>Use Cloud Datastore for storing customer data, Cloud Functions for processing customer preferences and purchase history, and Cloud Machine Learning Engine for generating product recommendations. Use Cloud Data Studio for analytics and reporting. -&gt;&nbsp;Incorrect. Cloud Datastore is a NoSQL database, which is not the best option for handling complex queries or data analysis, and Cloud Functions might not provide the real-time processing needed. Cloud Data Studio is primarily a visualizing tool and may lack the processing power for complex real-time analytics.</p><p><br></p><p>Use Cloud SQL for storing customer data, Cloud Functions for processing customer preferences and purchase history, and Cloud Machine Learning Engine for generating product recommendations. Use Cloud Data Studio for analytics and reporting. -&gt;&nbsp;Incorrect. Although Cloud SQL can handle customer data, it's not designed for large-scale data analytics. Also, Cloud Functions might not provide the real-time processing required. Cloud Data Studio may not handle the analytical requirements.</p><p><br></p><p>Use Cloud Firestore for storing customer data, Cloud Functions for processing customer preferences and purchase history, and Cloud AI Platform for generating product recommendations. Use Cloud Data Studio for analytics and reporting. -&gt;&nbsp;Incorrect. While Cloud Firestore and Cloud Functions can handle the respective tasks, Cloud AI Platform is more oriented towards model training and deployment, not specifically towards generating product recommendations. Again, Data Studio may not be able to handle complex real-time analytics.</p>",
                "answers": [
                    "<p>Use Cloud Datastore for storing customer data, Cloud Functions for processing customer preferences and purchase history, and Cloud Machine Learning Engine for generating product recommendations. Use Cloud Data Studio for analytics and reporting.</p>",
                    "<p>Use BigQuery for storing customer data, Cloud Dataflow for processing customer preferences and purchase history, and Cloud Machine Learning Engine for generating product recommendations. Use Cloud Dataproc for analytics and reporting.</p>",
                    "<p>Use Cloud SQL for storing customer data, Cloud Functions for processing customer preferences and purchase history, and Cloud Machine Learning Engine for generating product recommendations. Use Cloud Data Studio for analytics and reporting.</p>",
                    "<p>Use Cloud Firestore for storing customer data, Cloud Functions for processing customer preferences and purchase history, and Cloud AI Platform for generating product recommendations. Use Cloud Data Studio for analytics and reporting.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "A large retail company wants to launch a new personalized shopping experience for customers using an e-commerce platform. The platform should allow customers to create a profile, save their preferences and purchase history, and receive personalized product recommendations in real-time. The platform should also provide real-time analytics and reporting on customer behavior and purchasing patterns. Which of the following solutions would be the most effective for meeting these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681626,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A global media company wants to implement a video streaming platform that can handle high traffic and deliver high-quality video to users. The platform should also be able to provide real-time analytics on video viewing trends and user engagement. The company wants to use the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud CDN to deliver high-quality video to users, Cloud Storage to store video data, and Cloud Bigtable to store video analytics data. -&gt; Correct. Cloud CDN (Content Delivery Network) can help deliver high-quality video to users by caching and delivering content from a location closest to the user, reducing latency and improving user experience. Cloud Storage is a highly scalable and durable storage solution that can store video data. Cloud Bigtable is a NoSQL database that can handle high read and write throughput and can be used to store analytics data for real-time analysis.</p><p><br></p><p>Use Cloud Pub/Sub to ingest video data, BigQuery to store video data, and Cloud Dataflow to process video analytics. -&gt; Incorrect. It is not the best solution because BigQuery is not suitable for storing large video files.</p><p><br></p><p>Use Cloud Storage to store video data, Cloud Functions to process video data, and Cloud SQL to store analytics data. -&gt; Incorrect. It is not the best solution because Cloud Functions may not be able to handle the processing requirements for video data and Cloud SQL may not be able to handle the high write throughput for analytics data.</p><p><br></p><p>Use Cloud Video Intelligence API to extract insights from video data, Cloud Pub/Sub to ingest video data, and Cloud Dataflow to process video analytics. -&gt; Incorrect. It is not the best solution because it relies on the Cloud Video Intelligence API for video analysis, which may not be sufficient for real-time analytics on high traffic video streaming platform.</p>",
                "answers": [
                    "<p>Use Cloud Pub/Sub to ingest video data, BigQuery to store video data, and Cloud Dataflow to process video analytics.</p>",
                    "<p>Use Cloud Storage to store video data, Cloud Functions to process video data, and Cloud SQL to store analytics data.</p>",
                    "<p>Use Cloud CDN to deliver high-quality video to users, Cloud Storage to store video data, and Cloud Bigtable to store video analytics data.</p>",
                    "<p>Use Cloud Video Intelligence API to extract insights from video data, Cloud Pub/Sub to ingest video data, and Cloud Dataflow to process video analytics.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "A global media company wants to implement a video streaming platform that can handle high traffic and deliver high-quality video to users. The platform should also be able to provide real-time analytics on video viewing trends and user engagement. The company wants to use the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681628,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A multinational retail company wants to implement a new e-commerce platform that can handle high traffic and provide real-time recommendations to users. The platform should also be able to track user behavior and provide insights into customer preferences. The company wants to use the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud SQL to store customer data, Cloud Pub/Sub to ingest customer data, and Cloud Dataflow to process customer analytics. -&gt; Correct. Cloud SQL is a fully-managed relational database service, good for storing customer data. Cloud Pub/Sub allows you to quickly ingest large amounts of data which can be especially useful for high traffic situations. Cloud Dataflow is a fully-managed service for stream and batch processing, making it ideal for processing customer analytics in real-time.</p><p><br></p><p>Use Cloud Storage to store customer data, Cloud Functions to process customer data, and Cloud Datastore to store analytics data. -&gt;&nbsp;Incorrect. Cloud Storage is not the best for storing structured customer data as it is an object storage service primarily used for unstructured data. Cloud Functions are serverless and event-driven, not best suited for high traffic real-time analytics. Cloud Datastore is a NoSQL document database, which can handle analytics data but not the best for real-time operations or complex querying.</p><p><br></p><p>Use Cloud Bigtable to store customer data, Cloud Pub/Sub to ingest customer data, and Cloud Functions to process customer analytics. -&gt;&nbsp;Incorrect. Cloud Bigtable is a NoSQL database service designed for large operational and analytical workloads, however, it may not be the best choice for storing structured customer data due to cost and complexity. Cloud Pub/Sub is good for ingesting data, but Cloud Functions may not provide the throughput and consistent performance needed for high traffic real-time analytics.</p><p><br></p><p>Use Cloud Firestore to store customer data, Cloud Functions to process customer data, and Cloud Datalab to store customer analytics. -&gt;&nbsp;Incorrect. Cloud Firestore is a NoSQL document database which may not be the best choice for real-time analytics or complex querying of customer data. Cloud Datalab is an interactive tool for exploration, transformation, analysis, and visualization of data but it's not ideal for storing analytics data.</p>",
                "answers": [
                    "<p>Use Cloud Storage to store customer data, Cloud Functions to process customer data, and Cloud Datastore to store analytics data.</p>",
                    "<p>Use Cloud SQL to store customer data, Cloud Pub/Sub to ingest customer data, and Cloud Dataflow to process customer analytics.</p>",
                    "<p>Use Cloud Bigtable to store customer data, Cloud Pub/Sub to ingest customer data, and Cloud Functions to process customer analytics.</p>",
                    "<p>Use Cloud Firestore to store customer data, Cloud Functions to process customer data, and Cloud Datalab to store customer analytics.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "A multinational retail company wants to implement a new e-commerce platform that can handle high traffic and provide real-time recommendations to users. The platform should also be able to track user behavior and provide insights into customer preferences. The company wants to use the Google Cloud Platform for this implementation. Which of the following options would be the best solution for this requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681630,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A large financial services company wants to build a new trading platform that can process millions of transactions per second. The platform should be able to handle high levels of volatility, and provide low latency data access for real-time decision making. Which of the following options would be the best solution for this requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Pub/Sub for data streaming and BigTable for data storage. -&gt; Correct. Using Cloud Pub/Sub for data streaming and BigTable for data storage is the recommended solution for this requirement. Cloud Pub/Sub provides a highly scalable and reliable messaging system for streaming large volumes of data in real-time. It is well-suited for handling high levels of volatility and processing millions of transactions per second. BigTable is a NoSQL database designed for low-latency, high-throughput applications. It can handle the high transactional workload and provide fast data access for real-time decision making.</p><p><br></p><p>Use BigQuery for data storage and processing. -&gt;&nbsp;Incorrect. Using BigQuery for data storage and processing is not the most suitable option for a trading platform that requires low latency and real-time decision making. While BigQuery is excellent for large-scale data analytics and querying, it may not provide the necessary speed and responsiveness required for processing millions of transactions per second in real-time.</p><p><br></p><p>Use Cloud Spanner for data storage and Cloud Dataflow for data processing. -&gt;&nbsp;Incorrect. Using Cloud Spanner for data storage and Cloud Dataflow for data processing is a valid option for building scalable and reliable applications. However, it may introduce additional complexity and overhead, and it may not provide the same level of low latency and high throughput as Cloud Pub/Sub and BigTable for a high-frequency trading platform.</p><p><br></p><p>Use Cloud Datastore for data storage and Cloud Functions for data processing. -&gt;&nbsp;Incorrect. Using Cloud Datastore for data storage and Cloud Functions for data processing may not be the most optimal solution for a high-frequency trading platform. Cloud Datastore, while providing scalability, may not deliver the necessary performance and latency required for processing millions of transactions per second. Cloud Functions are event-driven functions and may not provide the real-time processing capabilities needed for real-time decision making.</p><p><br></p><p>https://cloud.google.com/pubsub/docs</p><p>https://cloud.google.com/bigtable/docs</p>",
                "answers": [
                    "<p>Use BigQuery for data storage and processing.</p>",
                    "<p>Use Cloud Pub/Sub for data streaming and BigTable for data storage.</p>",
                    "<p>Use Cloud Spanner for data storage and Cloud Dataflow for data processing.</p>",
                    "<p>Use Cloud Datastore for data storage and Cloud Functions for data processing.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "A large financial services company wants to build a new trading platform that can process millions of transactions per second. The platform should be able to handle high levels of volatility, and provide low latency data access for real-time decision making. Which of the following options would be the best solution for this requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681632,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A large media company is looking to build a secure and scalable platform for hosting and streaming video content. The platform must meet the following requirements:</p><ul><li><p>support high volumes of concurrent video streams with low latency</p></li><li><p>ensure secure storage and processing of sensitive video content</p></li><li><p>enable real-time analytics and reporting on video usage and performance</p></li><li><p>minimize downtime during maintenance and upgrades</p></li><li><p>minimize costs while still providing high performance</p></li></ul><p><br></p><p>Which solution would you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implementing a managed solution using Video Intelligence API for video analysis, Cloud Storage for video storage, and Cloud Load Balancing for high-availability video streaming. -&gt;&nbsp;Correct. It is the recommended approach. Video Intelligence API provides specialized capabilities for video analysis, enabling real-time analytics and reporting on video usage and performance. Cloud Storage offers scalable and secure storage for video content. Cloud Load Balancing ensures high availability and low latency for video streaming, providing a seamless experience to users. This combination of services offers a managed and efficient solution specifically designed for hosting and streaming video content.</p><p><br></p><p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling. -&gt;&nbsp;Incorrect. It may not be the most efficient and cost-effective approach for hosting and streaming video content. These services are not specifically designed for video processing and streaming, and building a custom solution for such requirements can be complex and time-consuming.</p><p><br></p><p>Implementing a serverless solution using Cloud Functions for data processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing. -&gt;&nbsp;Incorrect. It may not be the most suitable option for hosting and streaming video content. Cloud Functions are primarily designed for lightweight, event-driven functions and may not provide the necessary scalability and performance for video streaming. Cloud Firestore may not be the most suitable data storage solution for large volumes of video content. It may not provide the low latency and high performance required for video streaming.</p><p><br></p><p>Implementing a hybrid solution using Compute Engine for video processing, Cloud Storage for video storage, and BigQuery for real-time analytics. -&gt;&nbsp;Incorrect. iI introduces additional complexity and management overhead. Compute Engine requires managing virtual machines for video processing, which may not be the most scalable and cost-effective solution for streaming high volumes of video content. BigQuery, while powerful for analytics, may not be the most optimal choice for real-time analytics and reporting on video usage and performance.</p><p><br></p><p>https://cloud.google.com/video-intelligence/docs</p>",
                "answers": [
                    "<p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling.</p>",
                    "<p>Implementing a serverless solution using Cloud Functions for data processing, Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing.</p>",
                    "<p>Implementing a managed solution using Video Intelligence API for video analysis, Cloud Storage for video storage, and Cloud Load Balancing for high-availability video streaming.</p>",
                    "<p>Implementing a hybrid solution using Compute Engine for video processing, Cloud Storage for video storage, and BigQuery for real-time analytics.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "A large media company is looking to build a secure and scalable platform for hosting and streaming video content. The platform must meet the following requirements:support high volumes of concurrent video streams with low latencyensure secure storage and processing of sensitive video contentenable real-time analytics and reporting on video usage and performanceminimize downtime during maintenance and upgradesminimize costs while still providing high performanceWhich solution would you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681634,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A large healthcare organization is looking to build a secure and scalable platform for storing and analyzing medical records. The platform must meet the following requirements:</p><ul><li><p>support high volumes of medical records with low latency</p></li><li><p>ensure secure storage and processing of sensitive medical information</p></li><li><p>enable real-time data analysis and reporting on patient health and treatment outcomes</p></li><li><p>minimize downtime during maintenance and upgrades</p></li><li><p>minimize costs while still providing high performance</p></li></ul><p><br></p><p>Which solution would you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implementing a managed solution using Cloud Healthcare API for data analysis, Cloud Storage for data storage, and Cloud Load Balancing for high-availability data access. -&gt;&nbsp;Correct. This solution meets the requirements by using Cloud Healthcare API, which is specifically designed for healthcare organizations to store, process, and analyze sensitive medical information securely and efficiently. Cloud Storage provides secure and scalable storage for the medical records. And, Cloud Load Balancing ensures high-availability of the data access, minimizing downtime during maintenance and upgrades.</p><p><br></p><p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling. -&gt;&nbsp;Incorrect. While this option provides flexibility and control over the solution components, it also requires significant development and maintenance effort. It may not be the most cost-effective and scalable option for meeting the specified requirements.</p><p><br></p><p>Implementing a serverless solution using Cloud Functions for data processing,&nbsp; Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing. -&gt;&nbsp;Incorrect. This option offers simplicity and scalability through serverless components. However, Cloud Firestore may not be the most suitable data storage solution for high volumes of medical records. It may not provide the low latency and performance required for real-time data analysis and reporting.</p><p><br></p><p>Implementing a hybrid solution using Compute Engine for data processing, Cloud SQL for data storage, and BigQuery for real-time analytics. -&gt;&nbsp;Incorrect. While this option combines different services for specific purposes, it introduces additional complexity and maintenance overhead. Compute Engine and Cloud SQL require managing virtual machines and databases, which may not be the most scalable and cost-effective approach.</p>",
                "answers": [
                    "<p>Implementing a custom-built solution using Cloud Pub/Sub for real-time data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling.</p>",
                    "<p>Implementing a serverless solution using Cloud Functions for data processing,&nbsp; Cloud Firestore for data storage, and Cloud Pub/Sub for real-time data processing.</p>",
                    "<p>Implementing a managed solution using Cloud Healthcare API for data analysis, Cloud Storage for data storage, and Cloud Load Balancing for high-availability data access.</p>",
                    "<p>Implementing a hybrid solution using Compute Engine for data processing, Cloud SQL for data storage, and BigQuery for real-time analytics.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "A large healthcare organization is looking to build a secure and scalable platform for storing and analyzing medical records. The platform must meet the following requirements:support high volumes of medical records with low latencyensure secure storage and processing of sensitive medical informationenable real-time data analysis and reporting on patient health and treatment outcomesminimize downtime during maintenance and upgradesminimize costs while still providing high performanceWhich solution would you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681636,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company has a massive amount of data (approx 50 TB) in its on-premises data center, which needs to be transferred to Cloud Storage for further analysis and machine learning processing. Due to security concerns, your company has decided not to use physical data transfer services like Transfer Appliance. Your company has a very high-speed internet connection, but the rate of data generation is also high, leading to a shrinking time window for data transfer. What would be the best way to migrate this data over the internet in a reasonable amount of time?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Storage Transfer Service with Cloud VPN for secure data transfer. -&gt;&nbsp;Correct. Storage Transfer Service is designed to move large amounts of data from online and on-premises sources to Cloud Storage. When combined with Cloud VPN, this option would provide both secure and efficient data transfer. Cloud VPN provides secure and private connections for transferring the data, which addresses the company's security concerns.</p><p><br></p><p>Use gsutil with multi-threaded/multi-processing options to copy data directly into Cloud Storage. -&gt; Incorrect. gsutil with multi-threaded/multi-processing options is a great way to move data into GCS, but with such a large amount of data (50 TB), it's not the most efficient or fastest option. Plus, it doesn't inherently include a mechanism for ensuring a secure connection.</p><p><br></p><p>Use Cloud Dataflow to transfer the data. -&gt; Incorrect. Cloud Dataflow is primarily used for stream and batch processing of data. While it could theoretically be used for the transfer, it is not its primary use case and would not be as efficient as the Storage Transfer Service for large volumes of data.</p><p><br></p><p>Use a combination of Google Cloud's Data Transfer Service and gsutil for multi-threaded transfers. -&gt;&nbsp;Incorrect. Using a combination of Google Cloud's Data Transfer Service and gsutil for multi-threaded transfers could potentially speed up the data transfer, but it would add unnecessary complexity. Additionally, it doesn't inherently address the security concerns, unlike the Cloud VPN.</p>",
                "answers": [
                    "<p>Use Storage Transfer Service with Cloud VPN for secure data transfer.</p>",
                    "<p>Use gsutil with multi-threaded/multi-processing options to copy data directly into Cloud Storage.</p>",
                    "<p>Use Cloud Dataflow to transfer the data.</p>",
                    "<p>Use a combination of Google Cloud's Data Transfer Service and gsutil for multi-threaded transfers.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company has a massive amount of data (approx 50 TB) in its on-premises data center, which needs to be transferred to Cloud Storage for further analysis and machine learning processing. Due to security concerns, your company has decided not to use physical data transfer services like Transfer Appliance. Your company has a very high-speed internet connection, but the rate of data generation is also high, leading to a shrinking time window for data transfer. What would be the best way to migrate this data over the internet in a reasonable amount of time?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681638,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is building a multi-tier web application in Google Cloud. The application has a web frontend, an application backend, and a database backend. The database backend is critical to the operation of the application and must be highly available and scalable. How would you design the database backend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use a Cloud Spanner instance for the database backend. -&gt;&nbsp;Correct. Using a Cloud Spanner instance for the database backend is the recommended approach for several reasons. Cloud Spanner is a globally distributed, horizontally scalable database service that offers strong consistency, high availability, and automatic scaling. It provides ACID transactions and allows for distributed data storage across multiple regions, ensuring data resilience and low latency access. Cloud Spanner is designed to handle critical workloads and can meet the requirements of a highly available and scalable database backend for a multi-tier web application.</p><p><br></p><p>Use a single Cloud SQL instance for the database backend. -&gt;&nbsp;Incorrect. Cloud SQL offers managed database services, a single instance can become a single point of failure, and scaling can be limited compared to other options.</p><p><br></p><p>Use a single Bigtable instance for the database backend. -&gt;&nbsp;Incorrect. Using a single Bigtable instance for the database backend may not be the most suitable choice for a multi-tier web application. Bigtable is a NoSQL database designed for high throughput and scalability, but it may not provide the same level of consistency and transactional capabilities required for critical database operations.</p><p><br></p><p>Use a Cloud SQL read replica for the database backend and configure automatic failover. -&gt;&nbsp;Incorrect. Using a Cloud SQL read replica for the database backend and configuring automatic failover can provide improved availability by having a standby replica. However, this approach may not provide the same level of scalability and performance as the correct option. Cloud SQL read replicas are primarily used for read scaling and may not be the best fit for high availability and scalability in critical database operations.</p><p><br></p><p>https://cloud.google.com/spanner/docs</p>",
                "answers": [
                    "<p>Use a single Cloud SQL instance for the database backend.</p>",
                    "<p>Use a single Bigtable instance for the database backend.</p>",
                    "<p>Use a Cloud SQL read replica for the database backend and configure automatic failover.</p>",
                    "<p>Use a Cloud Spanner instance for the database backend.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "Your company is building a multi-tier web application in Google Cloud. The application has a web frontend, an application backend, and a database backend. The database backend is critical to the operation of the application and must be highly available and scalable. How would you design the database backend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681640,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A company wants to build a data processing pipeline that ingests data from various sources, transforms the data, and then stores the processed data in a data warehouse for analysis. The pipeline should be scalable, easy to manage, and able to handle changes in data sources and data processing requirements. How would you design this data processing pipeline using Google Cloud services?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Dataflow to ingest data from the various sources, Cloud Dataproc to transform the data, and BigQuery to store the processed data. -&gt; Correct. Cloud Dataflow is a fully-managed data processing service that can ingest data from various sources and transform it before storing it in BigQuery, which is a scalable and cost-effective data warehouse solution. Cloud Dataproc can also be used for data transformation but is more suited for heavy compute workloads. </p><p><br></p><p>Use Cloud Functions to ingest data from the various sources, Cloud Dataflow to transform and store the data. -&gt; Incorrect. Using Cloud Functions for ingestion may not be suitable for large-scale data processing.</p><p><br></p><p>Use Cloud Pub/Sub to ingest data from the various sources, Cloud Dataproc to transform and store the data. -&gt; Incorrect. Using Cloud Pub/Sub for ingestion may require additional services for data transformation and storage. </p><p><br></p><p>Use Cloud Storage to ingest data from the various sources, Cloud Dataproc to transform the data, and BigQuery to store the processed data. -&gt; Incorrect. Using Cloud Storage for ingestion may not provide sufficient data processing capabilities.</p>",
                "answers": [
                    "<p>Use Cloud Functions to ingest data from the various sources, Cloud Dataflow to transform and store the data.</p>",
                    "<p>Use Cloud Dataflow to ingest data from the various sources, Cloud Dataproc to transform the data, and BigQuery to store the processed data.</p>",
                    "<p>Use Cloud Pub/Sub to ingest data from the various sources, Cloud Dataproc to transform and store the data.</p>",
                    "<p>Use Cloud Storage to ingest data from the various sources, Cloud Dataproc to transform the data, and BigQuery to store the processed data.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "A company wants to build a data processing pipeline that ingests data from various sources, transforms the data, and then stores the processed data in a data warehouse for analysis. The pipeline should be scalable, easy to manage, and able to handle changes in data sources and data processing requirements. How would you design this data processing pipeline using Google Cloud services?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681642,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is developing a new mobile application that needs to store user data in the cloud. The data must be highly available and durable, and the application must be able to read and write data even when the device is offline. How would you store the user data in Google Cloud to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Firestore in native mode to store the user data. -&gt;&nbsp;Correct. Cloud Firestore in Native mode is the most effective solution for storing user data in the cloud with high availability and durability, and allowing the application to read and write data even when the device is offline. Cloud Firestore is a document database that allows for flexible data modeling and querying. In Native mode, Cloud Firestore provides multi-region replication and automatic scaling, making it highly available and durable. It also provides offline data synchronization and conflict resolution, allowing the application to continue working even when the device is offline. This makes it a good fit for mobile applications that require offline functionality.</p><p><br></p><p>Use Cloud Datastore to store the user data. -&gt; Incorrect. Cloud Datastore is an older version of Cloud Firestore and does not provide the same level of offline functionality. </p><p><br></p><p>Use Cloud Firestore in Datastore mode to store the user data. -&gt; Incorrect. Cloud Firestore in Datastore mode is a compatibility mode that provides the same API as Cloud Datastore, but does not include the same features as Cloud Firestore in Native mode.</p><p><br></p><p>Use Cloud Bigtable to store the user data. -&gt; Incorrect. Cloud Bigtable is a NoSQL database designed for high throughput and low latency, but does not provide the same level of offline functionality as Cloud Firestore in Native mode.</p><p><br></p><p>https://cloud.google.com/datastore/docs/firestore-or-datastore</p>",
                "answers": [
                    "<p>Use Cloud Datastore to store the user data.</p>",
                    "<p>Use Cloud Firestore in Native mode to store the user data.</p>",
                    "<p>Use Cloud Firestore in Datastore mode to store the user data.</p>",
                    "<p>Use Cloud Bigtable to store the user data.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "Your company is developing a new mobile application that needs to store user data in the cloud. The data must be highly available and durable, and the application must be able to read and write data even when the device is offline. How would you store the user data in Google Cloud to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681644,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company wants to use Google Cloud to host a static website that will receive high amounts of traffic. The website must be highly available and must load quickly for users around the world. How would you host the website in Google Cloud to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Host the website on Cloud Storage and use Cloud CDN to distribute traffic. -&gt;&nbsp;Correct. Cloud CDN (Content Delivery Network) uses Google's globally distributed edge points of presence to cache external HTTP(S) load balanced content close to your users. Caching content at the edges of Google's network provides faster delivery of content to your users while reducing serving costs.</p><p><br></p><p>Host the website on a single Compute Engine virtual machine and use a global load balancer to distribute traffic. -&gt;&nbsp;Incorrect. Hosting the website on a single Compute Engine virtual machine and using a global load balancer to distribute traffic is not the most optimal solution for high availability and quick loading times. If the single virtual machine fails, the website will become inaccessible. Additionally, the response time for users located far away from the virtual machine's location may be slower.</p><p><br></p><p>Host the website on a single App Engine Standard environment instance. -&gt;&nbsp;Incorrect. Hosting the website on a single App Engine Standard environment instance can provide scalability and automatic load balancing, but it may not provide the same level of performance and global availability as other options. App Engine Standard is limited to specific regions and may not have the same level of caching and content delivery optimizations as Cloud CDN.</p><p><br></p><p>Host the website on multiple Compute Engine virtual machines in different regions and use a regional load balancer to distribute traffic. -&gt;&nbsp;Incorrect. Hosting the website on multiple Compute Engine virtual machines in different regions and using a regional load balancer can provide high availability and better performance by distributing traffic across regions. However, managing and synchronizing multiple virtual machines across regions can add complexity and increase costs. It may not be the most cost-effective and straightforward solution for a static website.</p><p><br></p><p>https://cloud.google.com/cdn/docs</p>",
                "answers": [
                    "<p>Host the website on a single Compute Engine virtual machine and use a global load balancer to distribute traffic.</p>",
                    "<p>Host the website on a single App Engine Standard environment instance.</p>",
                    "<p>Host the website on multiple Compute Engine virtual machines in different regions and use a regional load balancer to distribute traffic.</p>",
                    "<p>Host the website on Cloud Storage and use Cloud CDN to distribute traffic.</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "Your company wants to use Google Cloud to host a static website that will receive high amounts of traffic. The website must be highly available and must load quickly for users around the world. How would you host the website in Google Cloud to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681646,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company wants to deploy a batch processing workload in Google Cloud using Apache Spark. The workload consists of a large number of compute-intensive tasks that can be executed in parallel. The workload must be scalable and cost-effective. How would you deploy the batch processing workload in Google Cloud to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Dataproc to run the batch processing tasks. -&gt;&nbsp;Correct. Dataproc is a managed Apache Spark and Apache Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data.</p><p><br></p><p>Use Cloud Functions to run the batch processing tasks. -&gt;&nbsp;Incorrect. Using Cloud Functions to run the batch processing tasks is not the most suitable option for a compute-intensive workload. Cloud Functions are primarily designed for lightweight, event-driven functions and may not provide the necessary scalability and performance for parallel execution of compute-intensive tasks.</p><p><br></p><p>Use a fleet of Compute Engine virtual machines to run the batch processing tasks. -&gt;&nbsp;Incorrect. Using a fleet of Compute Engine virtual machines to run the batch processing tasks can be a viable option for parallel execution. However, it requires manual management of the virtual machines, scaling, and configuration. This approach may not be as cost-effective and easy to manage compared to dedicated managed services designed specifically for batch processing workloads.</p><p><br></p><p>Use Cloud Batch to run the batch processing tasks. -&gt;&nbsp;Incorrect. Batch is a fully managed service that lets you schedule, queue, and execute batch processing workloads on Google Cloud resources, but it does not allow you to use Apache Spark.</p><p><br></p><p>https://cloud.google.com/dataproc/docs</p><p>https://cloud.google.com/dataproc-serverless/docs/quickstarts/spark-batch</p>",
                "answers": [
                    "<p>Use Cloud Functions to run the batch processing tasks.</p>",
                    "<p>Use a fleet of Compute Engine virtual machines to run the batch processing tasks.</p>",
                    "<p>Use Cloud Dataproc to run the batch processing tasks.</p>",
                    "<p>Use Cloud Batch to run the batch processing tasks.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "Your company wants to deploy a batch processing workload in Google Cloud using Apache Spark. The workload consists of a large number of compute-intensive tasks that can be executed in parallel. The workload must be scalable and cost-effective. How would you deploy the batch processing workload in Google Cloud to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681648,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are designing a cloud-based architecture for a company that requires high availability and fault tolerance for its critical application. Which of the following options provides the best solution?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use multiple instance virtual machines across different regions with a load balancer. -&gt; Correct. Using multiple instance virtual machines across different regions with a load balancer is the best solution for high availability and fault tolerance, as it ensures that the application is available even if one or more instances fail, and it distributes the load across multiple instances to ensure that the application can handle high traffic. By using different regions, the application is also protected against regional failures.</p><p><br></p><p>Use a single instance virtual machine in one region with an attached disk. -&gt; Incorrect. Using a single instance virtual machine in one region with an attached disk does not provide high availability or fault tolerance, as the application would be vulnerable to single points of failure.</p><p><br></p><p>Use a serverless computing approach with AWS Lambda. -&gt; Incorrect. Using a serverless computing approach with AWS Lambda may provide high availability, but it does not provide fault tolerance, as the application is still dependent on the underlying cloud infrastructure.</p><p><br></p><p>Use a container-based approach with Kubernetes. -&gt; Incorrect. Using a container-based approach with Kubernetes may provide both high availability and fault tolerance, but it requires more complex setup and management.</p>",
                "answers": [
                    "<p>Use a single instance virtual machine in one region with an attached disk.</p>",
                    "<p>Use multiple instance virtual machines across different regions with a load balancer.</p>",
                    "<p>Use a serverless computing approach with AWS Lambda.</p>",
                    "<p>Use a container-based approach with Kubernetes.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "You are designing a cloud-based architecture for a company that requires high availability and fault tolerance for its critical application. Which of the following options provides the best solution?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681650,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with a global company that uses Cloud Storage for data storage. The company has a large number of contractors who need to upload data to a specific Cloud Storage bucket, but they should not have any other access rights to the data or the other resources. At the same time, the data engineers should have the ability to manage all Cloud Storage resources. What IAM roles should be assigned to the contractors and the data engineers?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Assign roles/storage.objectCreator to contractors and roles/storage.admin to data engineers. -&gt; Correct. The roles/storage.objectCreator role would give contractors the ability to create objects (i.e., upload data) in the Cloud Storage bucket, but would not allow them to access or delete data. The roles/storage.admin role would give data engineers full control over Cloud Storage resources, allowing them to manage buckets and objects.</p><p><br></p><p>Assign roles/storage.objectViewer to contractors and roles/storage.objectAdmin to data engineers. -&gt; Incorrect. The roles/storage.objectViewer role would only give contractors the ability to view objects, not to upload data. The roles/storage.objectAdmin role would give data engineers the ability to manage objects but would not allow them to manage buckets.</p><p><br></p><p>Assign roles/storage.objectAdmin to contractors and roles/storage.admin to data engineers. -&gt; Incorrect. The roles/storage.objectAdmin role would give contractors the ability to read, write, and delete objects, which is more access than they need.</p><p><br></p><p>Assign roles/storage.objectCreator to contractors and roles/storage.objectAdmin to data engineers. -&gt; Incorrect. The roles/storage.objectCreator role would give contractors the ability to create objects, which is correct. However, the roles/storage.objectAdmin role would only give data engineers the ability to manage objects, not buckets.</p>",
                "answers": [
                    "<p>Assign roles/storage.objectCreator to contractors and roles/storage.admin to data engineers.</p>",
                    "<p>Assign roles/storage.objectViewer to contractors and roles/storage.objectAdmin to data engineers.</p>",
                    "<p>Assign roles/storage.objectAdmin to contractors and roles/storage.admin to data engineers.</p>",
                    "<p>Assign roles/storage.objectCreator to contractors and roles/storage.objectAdmin to data engineers.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with a global company that uses Cloud Storage for data storage. The company has a large number of contractors who need to upload data to a specific Cloud Storage bucket, but they should not have any other access rights to the data or the other resources. At the same time, the data engineers should have the ability to manage all Cloud Storage resources. What IAM roles should be assigned to the contractors and the data engineers?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681652,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are assisting a client to transfer a large set of files from their local data center to Cloud Storage. The client has asked to minimize the costs associated with network egress. They have a Cloud Interconnect connection set up and want to transfer files over this connection. What is the most appropriate gsutil command to use for this purpose?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p><code>gsutil -m cp -r ./local_directory gs://bucket_name</code> -&gt; Correct. The <code>-m</code> option enables parallel uploads and downloads, which can speed up the transfer process. It doesn't explicitly minimize egress costs, but the faster the files are transferred, the shorter the duration of the data transfer, potentially reducing costs associated with a prolonged transfer.</p><p><br></p><p><code>gsutil cp -r ./local_directory gs://bucket_name</code> -&gt; Incorrect. This command does not utilize parallel uploads and downloads, and therefore it won't be as efficient as using the -m option.</p><p><br></p><p><code>gsutil -o \"Boto:parallel_composite_upload_threshold=150M\" cp -r ./local_directory gs://bucket_name</code> -&gt; Incorrect. The <code>Boto:parallel_composite_upload_threshold=150M</code> option tells gsutil to use composite uploads for files larger than 150M, but it doesn't help with cost reduction directly nor makes use of the Cloud Interconnect setup.</p><p><br></p><p><code>gsutil -o \"GSUtil:parallel_composite_upload_threshold=150M\" cp -r ./local_directory gs://bucket_name</code> -&gt; Incorrect. There is no <code>GSUtil:parallel_composite_upload_threshold=150M</code> option in gsutil. The <code>parallel_composite_upload_threshold</code> parameter is part of the Boto configuration.</p>",
                "answers": [
                    "<p><code>gsutil -m cp -r ./local_directory gs://bucket_name</code> </p>",
                    "<p><code>gsutil cp -r ./local_directory gs://bucket_name</code> </p>",
                    "<p><code>gsutil -o \"Boto:parallel_composite_upload_threshold=150M\" cp -r ./local_directory gs://bucket_name</code> </p>",
                    "<p><code>gsutil -o \"GSUtil:parallel_composite_upload_threshold=150M\" cp -r ./local_directory gs://bucket_name</code> </p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are assisting a client to transfer a large set of files from their local data center to Cloud Storage. The client has asked to minimize the costs associated with network egress. They have a Cloud Interconnect connection set up and want to transfer files over this connection. What is the most appropriate gsutil command to use for this purpose?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681654,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is developing a new IoT application that is expected to produce massive amounts of data for real-time analytics and future predictive models. Currently, the IoT devices publish data to a Pub/Sub topic, which then triggers a Cloud Function to store the data in BigQuery. What can be done to enhance this solution as data volume increases?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Migrate the data processing from Cloud Function to Dataflow to better manage streaming data. -&gt;&nbsp;Correct. Cloud Dataflow is more suitable for processing large amounts of streaming or batch data compared to Cloud Function, which is designed for lightweight, event-driven computing tasks.</p><p><br></p><p>Increase the memory and CPU allocated to the Cloud Function to process more data. -&gt; Incorrect. Although this could marginally improve performance, it wouldn't significantly affect the system's ability to handle a significant increase in data volume.</p><p><br></p><p>Migrate from BigQuery to Cloud SQL for data storage. -&gt; Incorrect. Cloud SQL isn't designed to handle the same volume of data as BigQuery, and it's not optimized for analytics.</p><p><br></p><p>Use Firestore to store IoT data because of its real-time capabilities. -&gt; Incorrect. Firestore provides real-time updates and can be suitable for certain use cases, but it's not designed for massive-scale analytics like BigQuery.</p>",
                "answers": [
                    "<p>Migrate the data processing from Cloud Function to Dataflow to better manage streaming data.</p>",
                    "<p>Increase the memory and CPU allocated to the Cloud Function to process more data.</p>",
                    "<p>Migrate from BigQuery to Cloud SQL for data storage.</p>",
                    "<p>Use Firestore to store IoT data because of its real-time capabilities.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company is developing a new IoT application that is expected to produce massive amounts of data for real-time analytics and future predictive models. Currently, the IoT devices publish data to a Pub/Sub topic, which then triggers a Cloud Function to store the data in BigQuery. What can be done to enhance this solution as data volume increases?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681656,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You're creating a custom VPC in Google Cloud with several subnets. One of your subnets requires communication with an on-premises network via Cloud VPN. What should you consider when designing this scenario?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Ensure the CIDR range of the VPC subnet does not overlap with the on-premises network. -&gt;&nbsp;Correct. To set up Cloud VPN, the CIDR range of the VPC subnet and the on-premises network must not overlap.</p><p><br></p><p>The CIDR range of the on-premises network and the VPC subnet should be the same for easy routing. -&gt; Incorrect. Overlapping IP ranges between your on-premises network and VPC subnets would cause routing issues.</p><p><br></p><p>Assign public IP addresses to all instances in the VPC subnet. -&gt; Incorrect. Assigning public IP addresses to all instances is not necessary for communication with an on-premises network via Cloud VPN, as VPN connections operate at the network level.</p><p><br></p><p>Create a Shared VPC instead of a custom VPC. -&gt; Incorrect. Shared VPC allows resources from multiple projects to communicate with each other securely. However, it's not inherently necessary for a VPC subnet to communicate with an on-premises network.</p>",
                "answers": [
                    "<p>Ensure the CIDR range of the VPC subnet does not overlap with the on-premises network.</p>",
                    "<p>The CIDR range of the on-premises network and the VPC subnet should be the same for easy routing.</p>",
                    "<p>Assign public IP addresses to all instances in the VPC subnet.</p>",
                    "<p>Create a Shared VPC instead of a custom VPC.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You're creating a custom VPC in Google Cloud with several subnets. One of your subnets requires communication with an on-premises network via Cloud VPN. What should you consider when designing this scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 71681658,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are configuring a fleet of Compute Engine instances that need to communicate with each other for data processing tasks. These instances need to access a Google Cloud Storage bucket. You want to optimize network throughput and costs. What should you consider?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Place instances and Cloud Storage bucket in the same region. -&gt;&nbsp;Correct. Placing your Compute Engine instances and the Cloud Storage bucket in the same region reduces latency and egress costs.</p><p><br></p><p>Use Compute Engine instances with higher vCPU count and memory to increase network performance. -&gt;&nbsp;Incorrect. While instances with more vCPUs have better network performance, using larger instances isn't the only or the most cost-effective way to optimize network throughput.</p><p><br></p><p>Enable Cloud CDN on Compute Engine instances. -&gt;&nbsp;Incorrect. Cloud CDN is beneficial for content delivery to end users and would not optimize network throughput between Compute Engine instances and a Cloud Storage bucket.</p><p><br></p><p>Use dedicated Interconnect to connect instances and Cloud Storage. -&gt;&nbsp;Incorrect. Dedicated Interconnect provides direct physical connections to Google's network and would not be useful in this scenario, as both Compute Engine instances and Cloud Storage are already on Google's network.</p>",
                "answers": [
                    "<p>Place instances and Cloud Storage bucket in the same region.</p>",
                    "<p>Use Compute Engine instances with higher vCPU count and memory to increase network performance.</p>",
                    "<p>Enable Cloud CDN on Compute Engine instances.</p>",
                    "<p>Use dedicated Interconnect to connect instances and Cloud Storage.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are configuring a fleet of Compute Engine instances that need to communicate with each other for data processing tasks. These instances need to access a Google Cloud Storage bucket. You want to optimize network throughput and costs. What should you consider?",
            "related_lectures": []
        }
    ]
}