{
    "count": 70,
    "next": null,
    "previous": null,
    "results": [
        {
            "_class": "assessment",
            "id": 78430128,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is restructuring its operations and you've been tasked with reorganizing your Google Cloud resources to match the new business units. You currently have all resources in one project. Your company is divided into several departments, each containing multiple teams. You need to ensure that billing reports can be generated per department, and resources can be isolated per team. What is the most effective way to reorganize your resources?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Create a folder for each department and then create projects under those folders for each team. -&gt;&nbsp;Correct. By creating a folder for each department and a project for each team, you can manage access controls at the project level and isolate resources for each team. Billing can be set up at the project level, allowing you to generate billing reports per department.</p><p><br></p><p>Create a separate organization for each department and projects for each team under the organizations. -&gt;&nbsp;Incorrect. In GCP, organizations are intended to represent an entire company, so creating an organization for each department would be an inappropriate use of the resource hierarchy.</p><p><br></p><p>Create a separate project for each team and use labels to identify the departments. -&gt;&nbsp;Incorrect. Although this would isolate resources for each team, using labels to identify departments would not allow you to generate billing reports per department.</p><p><br></p><p>Keep all resources in one project but use different network subnets and labels to differentiate between departments and teams. -&gt;&nbsp;Incorrect. This approach doesn't properly utilize the resource hierarchy for isolation and would not allow for billing reports per department.</p>",
                "answers": [
                    "<p>Create a folder for each department and then create projects under those folders for each team.</p>",
                    "<p>Create a separate organization for each department and projects for each team under the organizations.</p>",
                    "<p>Create a separate project for each team and use labels to identify the departments.</p>",
                    "<p>Keep all resources in one project but use different network subnets and labels to differentiate between departments and teams.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company is restructuring its operations and you've been tasked with reorganizing your Google Cloud resources to match the new business units. You currently have all resources in one project. Your company is divided into several departments, each containing multiple teams. You need to ensure that billing reports can be generated per department, and resources can be isolated per team. What is the most effective way to reorganize your resources?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430130,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>Compute Engine instance has a single boot persistent disk (PD) that contains the operating system by default. Suppose your application requires additional storage space and you should add additional storage to your instance. Which storage options can you use with Compute Engine virtual machines to do this? Select all that apply.</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Compute Engine offers several types of storage options for your instances. Each of the following storage options has unique price and performance characteristics:</p><p>- Zonal persistent disk: Efficient, reliable block storage.</p><p>- Regional persistent disk: Regional block storage replicated in two zones.</p><p>- Local SSD: High performance, transient, local block storage.</p><p>- Cloud Storage buckets: Affordable object storage.</p><p>- Filestore: High performance file storage for Google Cloud users.</p><p><br></p><p>https://cloud.google.com/compute/docs/disks</p>",
                "answers": [
                    "<p>Zonal/Regional persistent disk</p>",
                    "<p>Local&nbsp;SSD</p>",
                    "<p>Cloud Storage bucket</p>",
                    "<p>BigQuery</p>",
                    "<p>Bigtable</p>"
                ]
            },
            "correct_response": [
                "a",
                "b",
                "c"
            ],
            "section": "",
            "question_plain": "Compute Engine instance has a single boot persistent disk (PD) that contains the operating system by default. Suppose your application requires additional storage space and you should add additional storage to your instance. Which storage options can you use with Compute Engine virtual machines to do this? Select all that apply.",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430132,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Compute Engine instance has a single boot persistent disk (PD) that contains the operating system by default. Suppose your application requires additional storage space. As a cloud architect, you want to choose the best solution in terms of read/write IOPS per instance. Which storage options should you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Local SSD -&gt; Correct. Local SSDs are physically attached to the instance's host server and provide high-speed, low-latency storage for applications that require high-performance storage. Local SSDs provide the highest read/write IOPS per instance compared to the other options listed in the question. However, local SSDs are not persistent, meaning that their data is lost when the instance is terminated.</p><p><br></p><p>Regional SSD -&gt; Incorrect. Regional SSD is network-attached disk that provide lower performance than local SSDs. However, it is persistent and can be used for storing data that needs to be retained even after the instance is terminated.</p><p><br></p><p>Zonal SSD -&gt; Incorrect. Zonal SSD provides higher performance than regional SSD, but it is only available in a single zone and cannot be attached to instances in other zones.</p><p><br></p><p>Zonal Standard Persistent Disk -&gt; Incorrect. Zonal standard persistent disk provides a lower performance than zonal SSD, but it is persistent and can be used for storing data that needs to be retained even after the instance is terminated.</p><p><br></p><p>Regional Standard Persistent Disk -&gt; Incorrect. Regional standard persistent disk is a network-attached disk that provide lower performance than local SSDs. However, it is persistent and can be used for storing data that needs to be retained even after the instance is terminated.</p><p><br></p><p>https://cloud.google.com/compute/docs/disks</p>",
                "answers": [
                    "<p>Local SSD</p>",
                    "<p>Regional SSD</p>",
                    "<p>Zonal SSD</p>",
                    "<p>Zonal Standard Persistent Disk</p>",
                    "<p>Regional Standard Persistent Disk</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Compute Engine instance has a single boot persistent disk (PD) that contains the operating system by default. Suppose your application requires additional storage space. As a cloud architect, you want to choose the best solution in terms of read/write IOPS per instance. Which storage options should you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430134,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Suppose you run Spark jobs and machine learning models from time to time in your on-premises data center. As a cloud architect, you want to move to Google Cloud with your workload. Which service should you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Dataproc -&gt; Correct. Dataproc is a fully managed cloud service that provides Apache Spark and Apache Hadoop clusters on Google Cloud. It is an excellent choice for running Spark jobs and machine learning models on Google Cloud, especially if you are already using these technologies in your on-premises data center. Dataproc can help you take advantage of Google Cloud's scalability and ease of use while minimizing the need to re-architect your existing workloads.</p><p><br></p><p>BigQuery -&gt; Incorrect. It is a cloud data warehouse that can be used to analyze large datasets. However, it is not designed for running Spark jobs and machine learning models directly.</p><p><br></p><p>Bigtable -&gt; Incorrect. It is a NoSQL database service that is designed for storing and analyzing large datasets. It can be used for Spark and machine learning workloads, but it is not a direct replacement for Dataproc.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. It is a general-purpose object storage service that can be used to store and retrieve large datasets. It can be used as a data source for Spark and machine learning workloads, but it does not provide the Spark and Hadoop clusters that Dataproc offers.</p><p><br></p><p>https://cloud.google.com/dataproc/docs/concepts/overview</p>",
                "answers": [
                    "<p>Dataproc</p>",
                    "<p>BigQuery</p>",
                    "<p>Bigtable</p>",
                    "<p>Cloud Storage</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Suppose you run Spark jobs and machine learning models from time to time in your on-premises data center. As a cloud architect, you want to move to Google Cloud with your workload. Which service should you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430136,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A social media application allows users to upload pictures. You need to convert each image to your internal optimized binary format and store it. As a cloud architect, you want to use the most efficient, cost-effective solution. What should you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should save uploaded images in a Cloud Storage bucket, and monitor the bucket for uploads. Run a Cloud Function to convert the images and to store them in a Cloud Storage bucket. -&gt; Correct. Storing uploaded images in a Cloud Storage bucket is a cost-effective and scalable solution. Cloud Storage provides durable and highly available storage for objects like images. By monitoring the Cloud Storage bucket for uploads using a Cloud Function, you can trigger the image conversion process and store the optimized binary format of the images back in the same or a different Cloud Storage bucket.</p><p><br></p><p>You should store uploaded images in Cloud Bigtable, monitor Bigtable entries, and then run a Cloud Function to convert the images and store them in Bigtable. -&gt;&nbsp;Incorrect. Cloud Bigtable is a NoSQL database optimized for high-performance, low-latency read and write operations on large datasets. While it can store binary data, it may not be the most efficient and cost-effective solution for storing and processing images.</p><p><br></p><p>You should store uploaded images in Firestore, monitor Firestore entries, and then run a Cloud Function to convert the images and store them in Firestore. -&gt;&nbsp;Incorrect. Firestore is a flexible, scalable NoSQL document database, but it is primarily designed for storing structured data rather than binary objects like images. Storing and processing images in Firestore may not provide the optimal performance and cost-effectiveness compared to using Cloud Storage.</p><p><br></p><p>You should store uploaded images in Filestore, monitor Filestore entries, and then run a Cloud Function to convert the images and store them in Filestore. -&gt;&nbsp;Incorrect. Filestore is a managed file storage service for applications that need a file system interface. While it can store binary files like images, it may not be the most efficient and cost-effective solution for storing and processing images compared to using Cloud Storage.</p><p><br></p><p>https://cloud.google.com/functions/docs/calling/storage</p>",
                "answers": [
                    "<p>You should save uploaded images in a Cloud Storage bucket, and monitor the bucket for uploads. Run a Cloud Function to convert the images and to store them in a Cloud Storage bucket.</p>",
                    "<p>You should store uploaded images in Cloud Bigtable, monitor Bigtable entries, and then run a Cloud Function to convert the images and store them in Bigtable.</p>",
                    "<p>You should store uploaded images in Firestore, monitor Firestore entries, and then run a Cloud Function to convert the images and store them in Firestore.</p>",
                    "<p>You should store uploaded images in Filestore, monitor Filestore entries, and then run a Cloud Function to convert the images and store them in Filestore.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A social media application allows users to upload pictures. You need to convert each image to your internal optimized binary format and store it. As a cloud architect, you want to use the most efficient, cost-effective solution. What should you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430138,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>To leverage Cloud Pub/Sub messages within your App Engine application, you find that the Cloud Pub/Sub API is currently disabled. In order to enable your application to utilize Cloud Pub/Sub, you plan to authenticate it to the API using a service account. What measures should you take to ensure seamless usage of Cloud Pub/Sub by your application?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should enable the Cloud Pub/Sub API in the API Library on the GCP Console. -&gt; Correct. To use the Cloud Pub/Sub API in your App Engine application, you need to enable the API first. This can be done through the API Library on the GCP Console. Once the API is enabled, you can then use a service account to authenticate your application to the API and use Cloud Pub/Sub. </p><p><br></p><p>You should rely on the automatic enablement of the Cloud Pub/Sub API when the Service Account accesses it. -&gt; Incorrect. It is not a valid option, as the Cloud Pub/Sub API needs to be enabled before it can be accessed by the service account. </p><p><br></p><p>You should use Deployment Manager to deploy your application. Rely on the automatic enablement of all APIs used by the application being deployed. -&gt; Incorrect. It is not the best choice because it involves deploying the application with all APIs enabled, which is not a good security practice. </p><p><br></p><p>You should grant the App Engine Default service account the role of Cloud Pub/Sub Admin. Have your application enable the API on the first connection to Cloud Pub/Sub. -&gt; Incorrect. It is not necessary since granting the App Engine Default service account the Cloud Pub/Sub Admin role doesn't automatically enable the Cloud Pub/Sub API.</p><p><br></p><p>https://cloud.google.com/endpoints/docs/openapi/enable-api</p>",
                "answers": [
                    "<p>You should enable the Cloud Pub/Sub API in the API Library on the GCP Console.</p>",
                    "<p>You should rely on the automatic enablement of the Cloud Pub/Sub API when the Service Account accesses it.</p>",
                    "<p>You should use Deployment Manager to deploy your application. Rely on the automatic enablement of all APIs used by the application being deployed.</p>",
                    "<p>You should grant the App Engine Default service account the role of Cloud Pub/Sub Admin. Have your application enable the API on the first connection to Cloud Pub/Sub.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "To leverage Cloud Pub/Sub messages within your App Engine application, you find that the Cloud Pub/Sub API is currently disabled. In order to enable your application to utilize Cloud Pub/Sub, you plan to authenticate it to the API using a service account. What measures should you take to ensure seamless usage of Cloud Pub/Sub by your application?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430140,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization has two Google Cloud projects and you want them to share policies. What's the best practice in this case?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Place both projects into a folder, and define the policies on this folder. -&gt; Correct. In Google Cloud, policies can be defined at the project, folder, or organization level. By placing both projects into a folder and defining policies on the folder, those policies will apply to both projects within that folder.</p><p><br></p><p>You should duplicate all the policies on one project onto the other. -&gt; Incorrect. Duplicating policies onto the other project, is not the best practice as it can lead to inconsistencies if policies need to be updated or changed.</p><p><br></p><p>Combine two projects into one. -&gt; Incorrect. Combining two projects into one, may not be feasible or desirable depending on the specific needs and requirements of each project.</p><p><br></p><p>You cannot share policies across two different projects in GCP. -&gt; Incorrect. You can share policies across two different projects in GCP as long as they are in the same folder or organization.</p><p><br></p><p>https://cloud.google.com/resource-manager/docs/creating-managing-folders</p>",
                "answers": [
                    "<p>Place both projects into a folder, and define the policies on this folder.</p>",
                    "<p>You should duplicate all the policies on one project onto the other.</p>",
                    "<p>Combine two projects into one.</p>",
                    "<p>You cannot share policies across two different projects in GCP.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization has two Google Cloud projects and you want them to share policies. What's the best practice in this case?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430142,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization is moving its on-premises data center to Google Cloud. The existing setup involves multiple layers of firewalls and access control lists (ACLs) to ensure the security of the network. The organization wants to mirror the same level of security on Google Cloud. What should be your approach to designing the network on Google Cloud?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Shared VPC with subnets for each layer and apply Firewall Rules and Cloud Armor policies. -&gt;&nbsp;Correct. A Shared VPC allows for segregation of resources similar to a layered on-premises network. Firewall Rules and Cloud Armor can be used to control and protect the network traffic, similar to firewalls and ACLs in on-premises setups.</p><p><br></p><p>Implement multiple security groups to mirror the firewall layers. -&gt;&nbsp;Incorrect. Google Cloud doesn't use the concept of security groups. The correct approach would be to use Firewall Rules and VPC Service Controls.</p><p><br></p><p>Create a flat network with Firewall Rules for each service. -&gt;&nbsp;Incorrect. While Firewall Rules can be used to control traffic, a flat network doesn't provide the same level of segregation and control as a layered approach.</p><p><br></p><p>Use a single VPC for all resources and implement IAM policies for network control. -&gt;&nbsp;Incorrect. While IAM is an important part of Google Cloud security, it does not replace network-level controls such as Firewall Rules or Cloud Armor.</p>",
                "answers": [
                    "<p>Use Shared VPC with subnets for each layer and apply Firewall Rules and Cloud Armor policies.</p>",
                    "<p>Implement multiple security groups to mirror the firewall layers.</p>",
                    "<p>Create a flat network with Firewall Rules for each service.</p>",
                    "<p>Use a single VPC for all resources and implement IAM policies for network control.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization is moving its on-premises data center to Google Cloud. The existing setup involves multiple layers of firewalls and access control lists (ACLs) to ensure the security of the network. The organization wants to mirror the same level of security on Google Cloud. What should be your approach to designing the network on Google Cloud?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430144,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>In your new e-commerce application you need to use Cloud Storage. As a cloud architect, you need to have the lowest-cost, highly durable storage service for data archiving, online backup, and disaster recovery (365-day minimum storage duration). Which storage class should you select?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Archive -&gt; Correct. The reason for choosing the Archive storage class is that it provides the lowest-cost, highly durable storage service for long-term data retention, archiving, online backup, and disaster recovery scenarios. The Archive storage class is designed for data that is accessed less frequently, stored for longer periods (minimum of 365 days), and stored primarily for backup, archiving, or compliance purposes. This storage class has the lowest storage cost per gigabyte and the highest durability of all storage classes in Google Cloud Storage.</p><p><br></p><p>Standard -&gt; Incorrect. Standard storage class is designed for frequently accessed data, where fast access times and low latency are important. This option is not suitable for long-term data retention, archiving, or backup purposes.</p><p><br></p><p>Nearline -&gt; Incorrect. Nearline storage class is designed for infrequently accessed data that can tolerate slightly higher latency and access times than Standard storage class. This option is suitable for data that may need to be accessed once or twice a month and stored for a minimum of 30 days.</p><p><br></p><p>Coldline -&gt; Incorrect. Coldline storage class is designed for infrequently accessed data that can tolerate higher latency and access times than Nearline storage class. This option is suitable for data that may need to be accessed once or twice a year and stored for a minimum of 90 days. However, it is more expensive than the Archive storage class.</p><p><br></p><p>https://cloud.google.com/storage/docs/storage-classes#archive</p>",
                "answers": [
                    "<p>Archive</p>",
                    "<p>Standard</p>",
                    "<p>Nearline</p>",
                    "<p>Coldline</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "In your new e-commerce application you need to use Cloud Storage. As a cloud architect, you need to have the lowest-cost, highly durable storage service for data archiving, online backup, and disaster recovery (365-day minimum storage duration). Which storage class should you select?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430146,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you need to advise on how to store structured and unstructured binary data (images, media files) with Google Cloud. Which service should you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Storage -&gt; Correct. It is a fully managed object storage service that provides a durable and highly available way to store structured and unstructured data, including binary data like images and media files. Cloud Storage provides features such as versioning, lifecycle management, and access controls that make it easy to manage data at scale. Cloud Storage also integrates with other GCP services, such as Cloud Functions and Cloud Run, to enable serverless workflows.</p><p><br></p><p>BigQuery -&gt; Incorrect. It is a fully managed data warehouse service that is optimized for handling large datasets and performing analytics on them. BigQuery is not designed for storing binary data like images and media files.</p><p><br></p><p>Cloud Bigtable -&gt; Incorrect. It is a fully managed NoSQL database service that is optimized for handling large amounts of data with low latency. While Cloud Bigtable can store binary data, it is not designed for storing unstructured data like images and media files.</p><p><br></p><p>Cloud SQL -&gt; Incorrect. It is a fully managed relational database service that supports several popular database engines such as MySQL and PostgreSQL. While Cloud SQL can store binary data, it is not designed for storing unstructured data like images and media files.</p><p><br></p><p>Cloud Spanner -&gt; Incorrect. It is a fully managed relational database service that provides strong consistency and horizontal scalability. While Cloud Spanner can store binary data, it is not designed for storing unstructured data like images and media files.</p><p><br></p><p>https://cloud.google.com/storage/docs/json_api/v1/objects</p>",
                "answers": [
                    "<p>Cloud Storage</p>",
                    "<p>BigQuery</p>",
                    "<p>Cloud Bigtable</p>",
                    "<p>Cloud SQL</p>",
                    "<p>Cloud Spanner</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you need to advise on how to store structured and unstructured binary data (images, media files) with Google Cloud. Which service should you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430148,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Personally Identifiable Information (PII) and sensitive information about your company's customers should be stored securely in Cloud Storage. Several people from your company's compliance department need access to some of this information. As a cloud architect, what should you do to follow Google's best practices?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should use granular ACLs on the bucket. -&gt; Correct. The recommended best practice in this scenario is to use granular Access Control Lists (ACLs) on the bucket. You can use IAM to create custom roles that grant only the permissions that the compliance department needs to access the sensitive data, and then assign those roles to the appropriate users. By using granular ACLs, you can ensure that only authorized users have access to sensitive data, and you can limit the risk of data leaks or unauthorized access.</p><p><br></p><p>You should grant Storage Object Viewer role to the entire compliance department. -&gt; Incorrect. Granting the Storage Object Viewer role to the entire compliance department is not the best practice. Doing so would give them more permissions than they need to perform their tasks and could lead to unauthorized access to sensitive data.</p><p><br></p><p>You should create additional bucket, enable public access, and provide specific file URLs to the compliance department. -&gt; Incorrect. It is also not recommended because enabling public access to a bucket is a security risk, and providing specific file URLs to the compliance department can be difficult to manage and can lead to data leaks.</p><p><br></p><p>You should grant Storage Object Creator role to the entire compliance department. -&gt; Incorrect. Granting the Storage Object Viewer role to the Storage Object Creator role to the entire compliance department is not the best practice. Doing so would give them more permissions than they need to perform their tasks and could lead to unauthorized access to sensitive data.</p><p><br></p><p>https://cloud.google.com/storage/docs/access-control/lists</p>",
                "answers": [
                    "<p>You should use granular ACLs on the bucket.</p>",
                    "<p>You should grant Storage Object Viewer role to the entire compliance department.</p>",
                    "<p>You should create additional bucket, enable public access, and provide specific file URLs to the compliance department.</p>",
                    "<p>You should grant Storage Object Creator role to the entire compliance department.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Personally Identifiable Information (PII) and sensitive information about your company's customers should be stored securely in Cloud Storage. Several people from your company's compliance department need access to some of this information. As a cloud architect, what should you do to follow Google's best practices?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430150,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>An application is deployed using App Engine to serve production traffic. Your colleague have found a critical error in application and you need to fix this quickly, but you don't know if new solution will cause other problems. What do you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should deploy the new version of the application, and use traffic splitting to send a small percentage of traffic to it. -&gt;&nbsp;Correct. By deploying the new version of the application and using traffic splitting, you can gradually route a small percentage of production traffic to the new version while still serving the majority of traffic with the existing stable version. This allows you to test the new solution in a controlled manner and monitor its performance and impact on the application without affecting the entire user base.</p><p><br></p><p>You should set up a second App Engine service, and then update a subset of clients to hit the new version. -&gt;&nbsp;Incorrect. While this approach can be feasible in some cases, it may require additional configuration and maintenance overhead, especially if the application has a large number of clients or if fine-grained control is needed.</p><p><br></p><p>You should deploy the new version of the application temporarily, capture logs and then roll it back to the previous version. -&gt;&nbsp;Incorrect. While log capture can provide valuable insights, this approach does not address the need to test the new solution with live traffic and monitor its behavior in a controlled manner.</p><p><br></p><p>You should create a second Google App Engine project with the new application code, and migrate users gradually to the new application. -&gt;&nbsp;Incorrect. While this approach can be used for more complex migration scenarios, it may introduce additional complexity and overhead, especially when managing user migration and data synchronization between multiple projects.</p><p><br></p><p>https://cloud.google.com/appengine/docs</p><p>https://cloud.google.com/appengine/docs/standard/python/splitting-traffic</p>",
                "answers": [
                    "<p>You should deploy the new version of the application, and use traffic splitting to send a small percentage of traffic to it.</p>",
                    "<p>You should set up a second App Engine service, and then update a subset of clients to hit the new version.</p>",
                    "<p>You should deploy the new version of the application temporarily, capture logs and then roll it back to the previous version.</p>",
                    "<p>You should create a second Google App Engine project with the new application code, and migrate users gradually to the new application.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "An application is deployed using App Engine to serve production traffic. Your colleague have found a critical error in application and you need to fix this quickly, but you don't know if new solution will cause other problems. What do you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430152,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Production Compute Engine workload is running in a small subnet, which can be expanded. The recent spike in traffic has caused problems, but there are no free IP addresses for Managed Instances Group to autoscale. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should expand the subnet IP range. -&gt;&nbsp;Correct. By expanding the subnet IP range, you can accommodate more IP addresses within the existing subnet. This allows for additional instances to be provisioned and scaled within the Managed Instance Group to handle the increased traffic. Expanding the subnet IP range is a straightforward solution that avoids the need to create new subnets or projects.</p><p><br></p><p>You should create a new subnet with a larger, non-overlapping range. Move all instances to the new subnet and remove the old subnet. -&gt;&nbsp;Incorrect. While it can provide more IP addresses, it also requires migrating all instances to the new subnet, which can be complex and disruptive. Expanding the existing subnet is generally a more practical and straightforward solution.</p><p><br></p><p>You should create a new project and a new VPC. Share the new VPC with the existing project and configure all existing resources to use the new VPC. -&gt;&nbsp;Incorrect. While this can provide additional resources, it involves significant overhead and complexity. It is not necessary to create a new project and VPC in this scenario.</p><p><br></p><p>You should create a new subnet with a larger, overlapping range to automatically move all instances to the new subnet. Then, remove the old subnet. -&gt;&nbsp;Incorrect. Automatically moving instances to the new subnet can be complex and may introduce disruption to the workload. Expanding the existing subnet is a simpler and more appropriate solution.</p><p><br></p><p>https://cloud.google.com/vpc/docs/using-vpc#expand-subnet</p>",
                "answers": [
                    "<p>You should expand the subnet IP range.</p>",
                    "<p>You should create a new subnet with a larger, non-overlapping range. Move all instances to the new subnet and remove the old subnet.</p>",
                    "<p>You should create a new project and a new VPC. Share the new VPC with the existing project and configure all existing resources to use the new VPC.</p>",
                    "<p>You should create a new subnet with a larger, overlapping range to automatically move all instances to the new subnet. Then, remove the old subnet.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Production Compute Engine workload is running in a small subnet, which can be expanded. The recent spike in traffic has caused problems, but there are no free IP addresses for Managed Instances Group to autoscale. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430154,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you need to prepare a migration strategy for a company that wants to migrate all applications from its on-premise data center to GCP. The company's DevOps team currently use Jenkins to automate configuration updates. What should you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Jenkins can be delivered using Google Marketplace. -&gt;&nbsp;Correct. Cloud Marketplace provides a wide range of pre-configured software solutions, including Jenkins, that can be easily deployed and managed on Google Cloud Platform (GCP). By using Jenkins from the marketplace, the company can quickly provision and configure Jenkins on GCP without the need for manual installation and configuration.</p><p><br></p><p>Download the Jenkins binary from Jenkins website and deploy to the new Compute Engine instance. -&gt;&nbsp;Incorrect. While this approach is technically possible, it requires manual installation, configuration, and ongoing maintenance of the Jenkins instance on Compute Engine, which may be more time-consuming and error-prone compared to using a managed solution.</p><p><br></p><p>Download the Jenkins binary from Jenkins website and deploy in App Engine Standard environment. -&gt;&nbsp;Incorrect. App Engine Standard environment is a platform-as-a-service (PaaS) offering that is designed for running web applications, and it has limitations in terms of the available runtime and configuration options, which may not align well with the requirements of running Jenkins.</p><p><br></p><p>Create a YAML Kubernetes Deployment file referencing the Jenkins docker image and deploy to the new GKE cluster. -&gt;&nbsp;Incorrect. While this approach is technically possible, it requires more advanced knowledge of Kubernetes and containerization compared to using a managed solution from the marketplace.</p><p><br></p><p>https://cloud.google.com/jenkins</p>",
                "answers": [
                    "<p>Jenkins can be delivered using Google Marketplace.</p>",
                    "<p>Download the Jenkins binary from Jenkins website and deploy to the new Compute Engine instance.</p>",
                    "<p>Download the Jenkins binary from Jenkins website and deploy in App Engine Standard environment.</p>",
                    "<p>Create a YAML Kubernetes Deployment file referencing the Jenkins docker image and deploy to the new GKE cluster.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you need to prepare a migration strategy for a company that wants to migrate all applications from its on-premise data center to GCP. The company's DevOps team currently use Jenkins to automate configuration updates. What should you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430156,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company has a multistage CI/CD pipeline for deploying applications on Google Cloud. Recently, multiple faulty deployments have made it to the production stage. What quality control measures should you implement to avoid such scenarios?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement automated testing and validation at each stage of the pipeline. -&gt; Correct. Automated testing and validation at each stage can catch and prevent many issues before they reach production. It's a scalable and effective measure for maintaining high-quality deployments.</p><p><br></p><p>Implement more strict IAM policies to prevent unauthorized deployments. -&gt;&nbsp;Incorrect. While it's crucial to control who can deploy to production, stricter IAM policies would not necessarily prevent faulty deployments from authorized users.</p><p><br></p><p>Enforce manual approval before any deployment to the production environment. -&gt;&nbsp;Incorrect. While this measure can catch some issues, it heavily relies on human intervention, which may not scale well and can still overlook problems.</p><p><br></p><p>Increase the frequency of deployments to identify and fix issues quickly. -&gt;&nbsp;Incorrect. Simply increasing the deployment frequency doesn't necessarily improve quality control. Without proper testing and validation, this might lead to more faulty deployments.</p>",
                "answers": [
                    "<p>Implement automated testing and validation at each stage of the pipeline.</p>",
                    "<p>Implement more strict IAM policies to prevent unauthorized deployments.</p>",
                    "<p>Enforce manual approval before any deployment to the production environment.</p>",
                    "<p>Increase the frequency of deployments to identify and fix issues quickly.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company has a multistage CI/CD pipeline for deploying applications on Google Cloud. Recently, multiple faulty deployments have made it to the production stage. What quality control measures should you implement to avoid such scenarios?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430158,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A mission-critical application runs on several virtual machines in on-premise data center and needs to be migrated to GCP. As a cloud architect, you need to prepare a migration strategy. The company wants to benefit from<em> </em>Lift and Shift approach, and this application needs to be scaled automatically and efficiently based on the CPU utilization. What do you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>This company should deploy the application to Compute Engine Managed Instance Group with autoscaling enabled based on CPU utilization. -&gt;&nbsp;Correct. The Lift and Shift approach is a migration strategy where the applications are moved as they are, without any changes, to the cloud. The question states that the company wants to benefit from the Lift and Shift approach, which means that the application will be migrated without any changes. Since the application needs to be scaled automatically and efficiently based on the CPU utilization, a Managed Instance Group (MIG) with autoscaling enabled based on CPU utilization is the best option. Compute Engine MIG provides automatic scaling of virtual machines based on demand. Autoscaling based on CPU utilization will help the application to scale up or down automatically depending on the CPU usage, which will ensure that the application is running efficiently.</p><p><br></p><p>This company should deploy the application to GKE cluster with Horizontal Pod Autoscaling enabled based on CPU utilization. -&gt; Incorrect. It is incorrect because GKE is not the best option for Lift and Shift approach. GKE is a container orchestration system, which requires a re-architecture of the application to run in containers.</p><p><br></p><p>This company should deploy the application to Google Compute Engine Managed Instance Group with time-based autoscaling based on last months traffic patterns. -&gt; Incorrect. It is also incorrect because time-based autoscaling will not provide the required efficiency in scaling. Autoscaling based on last month's traffic patterns may not be the best predictor of future demand.</p><p><br></p><p>This company should deploy the application to Compute Engine Unmanaged Instance Group with autoscaling enabled based on CPU utilization. -&gt; Incorrect. It is incorrect because unmanaged instance groups require manual management and do not provide the automatic scaling based on demand.</p><p><br></p><p>https://cloud.google.com/compute/docs/autoscaler</p>",
                "answers": [
                    "<p>This company should deploy the application to Compute Engine Managed Instance Group with autoscaling enabled based on CPU utilization.</p>",
                    "<p>This company should deploy the application to GKE cluster with Horizontal Pod Autoscaling enabled based on CPU utilization.</p>",
                    "<p>This company should deploy the application to Google Compute Engine Managed Instance Group with time-based autoscaling based on last months traffic patterns.</p>",
                    "<p>This company should deploy the application to Compute Engine Unmanaged Instance Group with autoscaling enabled based on CPU utilization.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A mission-critical application runs on several virtual machines in on-premise data center and needs to be migrated to GCP. As a cloud architect, you need to prepare a migration strategy. The company wants to benefit from Lift and Shift approach, and this application needs to be scaled automatically and efficiently based on the CPU utilization. What do you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430160,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>An application needs to be migrated from your on-premise data center to Google Cloud App Engine. You modified your application to use Cloud Pub/Sub with a specific service account which has the necessary permissions to publish and subscribe on Pub/Sub. However, Cloud Pub/Sub API has not yet been enabled. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should navigate to the APIs &amp; Services section in Google Console and enable Cloud Pub/Sub API. -&gt;&nbsp;Correct. The user should navigate to the APIs &amp; Services section in Google Console and enable the Cloud Pub/Sub API. Cloud Pub/Sub is a managed messaging service that enables communication between different components of an application. Before an application can use Pub/Sub, the Cloud Pub/Sub API must be enabled in the Google Cloud Console.</p><p><br></p><p>You should use Deployment Manager to configure the App Engine to use a specific service account with the necessary permissions and rely on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe. -&gt; Incorrect. This option is incorrect because it mentions relying on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe. However, this automatic enablement does not occur until the Cloud Pub/Sub API is explicitly enabled.</p><p><br></p><p>You should configure the App Engine to use a specific service account with the necessary permissions and rely on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe. -&gt; Incorrect. This option is incorrect because it mentions relying on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe. However, this automatic enablement does not occur until the Cloud Pub/Sub API is explicitly enabled.</p><p><br></p><p>You should grant <code>roles/pubsub.admin</code> IAM role to the service account and modify the application code to enable the API before publishing or subscribing. -&gt; Incorrect. It is incorrect because granting the <code>roles/pubsub.admin</code> IAM role to the service account would give it too many permissions, and modifying the application code to enable the API before publishing or subscribing is unnecessary. The API can be enabled in the Google Cloud Console.</p><p><br></p><p>https://cloud.google.com/endpoints/docs/openapi/enable-api</p>",
                "answers": [
                    "<p>You should navigate to the APIs &amp; Services section in Google Console and enable Cloud Pub/Sub API.</p>",
                    "<p>You should use Deployment Manager to configure the App Engine to use a specific service account with the necessary permissions and rely on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe.</p>",
                    "<p>You should configure the App Engine to use a specific service account with the necessary permissions and rely on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe.</p>",
                    "<p>You should grant <code>roles/pubsub.admin</code> IAM role to the service account and modify the application code to enable the API before publishing or subscribing.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "An application needs to be migrated from your on-premise data center to Google Cloud App Engine. You modified your application to use Cloud Pub/Sub with a specific service account which has the necessary permissions to publish and subscribe on Pub/Sub. However, Cloud Pub/Sub API has not yet been enabled. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430162,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A social media company stores images in a Cloud Storage bucket for long term. Images older than 30 days are accessed only in exceptional circumstances, and images older than six months are no longer needed. As a cloud architect, how can you optimize lifecycle management policy to reduce costs?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should configure a lifecycle management policy to transition objects older than 30 days to Coldline storage class. Than, configure another lifecycle management policy to delete objects older than six months. -&gt;&nbsp;Correct. In this scenario, the images are stored in the cloud for long term, but they are accessed only in exceptional circumstances. This means that the data has a low access rate, and it is essential to optimize the storage cost. The company needs to keep the images for up to six months, after which they are no longer required. Google Cloud Storage provides different storage classes with different costs and performance characteristics. The Coldline storage class is designed for long-term storage of infrequently accessed data, and it provides a lower storage cost but with a longer retrieval time. Therefore, the first step is to configure a lifecycle management policy to transition objects older than 30 days to Coldline storage class, which is a cost-effective option for storing data that is not accessed frequently. Then, another lifecycle management policy should be configured to delete objects older than six months, which ensures that the data is retained for the required duration while minimizing storage costs.</p><p><br></p><p>You should configure a lifecycle management policy to transition objects older than 30 days to Coldline storage class. Than, configure another lifecycle management policy to delete objects older than 365 days. -&gt; Incorrect. It suggests to delete objects older than 365 days, which will cause additional costs.</p><p><br></p><p>You should configure a lifecycle management policy to transition objects older than 30 days to Archive storage class. Than, configure another lifecycle management policy to delete objects older than six months. -&gt; Incorrect. It suggests to transition objects to Archive storage class, which is intended for archiving data for disaster recovery. Also Archive storage class has a 365-day minimum storage duration.</p><p><br></p><p>You should use a Cloud Function to change the storage class to Coldline for objects older than 30 days. And use another Cloud Function to delete objects older than 6 months from Coldline Storage Class. -&gt; Incorrect. It proposes to use Cloud Functions to change the storage class and delete objects, which adds complexity and costs to the solution.</p><p><br></p><p>https://cloud.google.com/storage/docs/lifecycle</p>",
                "answers": [
                    "<p>You should configure a lifecycle management policy to transition objects older than 30 days to Coldline storage class. Than, configure another lifecycle management policy to delete objects older than six months.</p>",
                    "<p>You should configure a lifecycle management policy to transition objects older than 30 days to Coldline storage class. Than, configure another lifecycle management policy to delete objects older than 365 days.</p>",
                    "<p>You should configure a lifecycle management policy to transition objects older than 30 days to Archive storage class. Than, configure another lifecycle management policy to delete objects older than six months.</p>",
                    "<p>You should use a Cloud Function to change the storage class to Coldline for objects older than 30 days. And use another Cloud Function to delete objects older than 6 months from Coldline Storage Class.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A social media company stores images in a Cloud Storage bucket for long term. Images older than 30 days are accessed only in exceptional circumstances, and images older than six months are no longer needed. As a cloud architect, how can you optimize lifecycle management policy to reduce costs?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430164,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you need to advise what to do in the following situation. A development team needs to directly connect their on-premises resources to several virtual machines inside a VPC. They want fast and secure access to these virtual machines with minimal maintenance and cost. What do you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>They should use Cloud VPN to create a bridge between the VPC and their network. -&gt; Correct. Cloud VPN provides a secure and cost-effective way to establish a secure connection between the on-premises resources and the virtual machines inside a VPC. Cloud VPN uses the internet to connect to the VPC, and this eliminates the need for dedicated connections or expensive hardware. Additionally, Cloud VPN provides a fast and secure connection by encrypting all traffic between the on-premises resources and the VPC. </p><p><br></p><p>They should assign a public IP address to each virtual machine and assign a strong password to each of them. -&gt;&nbsp;Incorrect. Assigning public IP addresses to virtual machines is not a secure solution because it exposes the virtual machines to the public internet. </p><p><br></p><p>They should set up Cloud Interconnect. -&gt; Incorrect. Cloud Interconnect is a dedicated connection service that can provide higher bandwidth and lower latency, but it's more expensive and requires more maintenance. </p><p><br></p><p>They should start a Compute Engine virtual machine, install a software router, and create a direct tunnel to each virtual machine. -&gt; Incorrect. It is also not the best solution because it adds unnecessary complexity and maintenance overhead.</p><p><br></p><p>https://cloud.google.com/network-connectivity/docs/vpn</p>",
                "answers": [
                    "<p>They should use Cloud VPN to create a bridge between the VPC and their network.</p>",
                    "<p>They should set up Cloud Interconnect.</p>",
                    "<p>They should assign a public IP address to each virtual machine and assign a strong password to each of them.</p>",
                    "<p>They should start a Compute Engine virtual machine, install a software router, and create a direct tunnel to each virtual machine.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you need to advise what to do in the following situation. A development team needs to directly connect their on-premises resources to several virtual machines inside a VPC. They want fast and secure access to these virtual machines with minimal maintenance and cost. What do you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430166,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company needs to store audit logs of a project for at least 5 years. From time to time you would like to do some log analytics. What should you do in this case?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should route audit logs to BigQuery. -&gt; Correct. BigQuery is a fully-managed, highly-scalable, and cost-effective data warehouse designed for large-scale data analytics. It allows you to store and query large amounts of data quickly and easily using SQL-like syntax. By routing audit logs to BigQuery, you can store the logs for at least 5 years, and you can also perform log analytics using BigQuery's powerful querying capabilities.</p><p><br></p><p>You should route audit logs to Cloud Storage. -&gt; Incorrect. Cloud Storage is also a valid option for storing audit logs, but it may not be as well-suited for log analytics as BigQuery. Cloud Storage is a simple, durable, and highly available object storage service that can be used for storing large amounts of unstructured data. However, it does not provide the advanced querying capabilities that BigQuery offers.</p><p><br></p><p>You should route audit logs to Bigtable. -&gt; Incorrect. Bigtable is a NoSQL database that can be used for real-time analytics and high-performance, low-latency applications. While it can be used for storing audit logs, it may not be the best option for long-term storage and analytics.</p><p><br></p><p>You should route audit logs to Pub/Sub. -&gt; Incorrect. Pub/Sub is a messaging service that can be used for asynchronous communication between different components of a system. It may be used for routing audit logs to other services for processing or storage, but it is not designed for long-term storage or analytics.</p><p><br></p><p>https://cloud.google.com/resource-manager/docs/audit-logging#exporting_audit_logs</p>",
                "answers": [
                    "<p>You should route audit logs to BigQuery.</p>",
                    "<p>You should route audit logs to Cloud Storage.</p>",
                    "<p>You should route audit logs to Bigtable.</p>",
                    "<p>You should route audit logs to Pub/Sub.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company needs to store audit logs of a project for at least 5 years. From time to time you would like to do some log analytics. What should you do in this case?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430168,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you have been tasked with architecting a complex deployment scenario for SAP HANA on Google Cloud. The client requires high availability (HA) and disaster recovery (DR) for their mission-critical SAP HANA databases. They also need the deployment to be in multiple regions for serving international customers. How would you design this deployment?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Deploy a multi-node SAP HANA cluster in each region using Google Compute Engine and SAP HANA System Replication for HA and DR. -&gt;&nbsp;Correct. A multi-node SAP HANA cluster in each region would provide high availability. SAP HANA System Replication enables database replication across multiple nodes and regions, providing both high availability (HA) and disaster recovery (DR) for SAP HANA databases.</p><p><br></p><p>Deploy a single-node SAP HANA instance in each region using Google Compute Engine and replicate data between them. -&gt; Incorrect. While this deployment would provide multiple instances in different regions, a single-node deployment would not provide the high availability required by the client.</p><p><br></p><p>Deploy a multi-node SAP HANA cluster in a single region using Google Compute Engine and Google Cloud Storage for backups. -&gt; Incorrect. Deploying a multi-node SAP HANA cluster in a single region would not fulfill the requirement for multi-region deployment to serve international customers.</p><p><br></p><p>Deploy SAP HANA on Google Kubernetes Engine in multiple regions. -&gt; Incorrect. SAP HANA is not typically deployed on Kubernetes. SAP HANA is a high-performance in-memory database that requires specialized, high-memory instances which are provided by Google Compute Engine, not Google Kubernetes Engine.</p>",
                "answers": [
                    "<p>Deploy a multi-node SAP HANA cluster in each region using Google Compute Engine and SAP HANA System Replication for HA and DR.</p>",
                    "<p>Deploy a single-node SAP HANA instance in each region using Google Compute Engine and replicate data between them.</p>",
                    "<p>Deploy a multi-node SAP HANA cluster in a single region using Google Compute Engine and Google Cloud Storage for backups.</p>",
                    "<p>Deploy SAP HANA on Google Kubernetes Engine in multiple regions.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you have been tasked with architecting a complex deployment scenario for SAP HANA on Google Cloud. The client requires high availability (HA) and disaster recovery (DR) for their mission-critical SAP HANA databases. They also need the deployment to be in multiple regions for serving international customers. How would you design this deployment?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430170,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Suppose you work as a cloud architect and you want to create a group of virtual machines for processing big data (Hadoop, Spark). You can afford to interrupt jobs as they can be easily recreated. The solution must be as low cost as possible. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should create a Managed Instance Group that uses Preemptible VMs. -&gt;&nbsp;Correct. Preemptible VMs are a type of virtual machine on Google Cloud Platform that can be used at a significantly lower cost compared to regular VMs. The tradeoff is that Google Cloud can reclaim these instances with only a 30-second warning, but since the jobs can be easily recreated, this is not a problem. Managed instance groups allow you to create and manage groups of virtual machines that work together to run your workloads, and they can automatically create new VMs to replace the preemptible ones that get terminated.</p><p><br></p><p>You should create a Managed Instance Group with VMs in a single zone. -&gt; Incorrect. Creating a Managed Instance Group with VMs in a single zone, would not be the best solution because if there is an outage in that zone, the entire group would be affected.</p><p><br></p><p>You should create a Managed Instance Group with VMs in multiple zones. -&gt; Incorrect. Creating a Managed Instance Group with VMs in multiple zones, would be more resilient to outages but would also increase costs.</p><p><br></p><p>You should create a Unmanaged Instance Group. -&gt; Incorrect. Creating an unmanaged instance group, would not provide the automatic replacement of terminated instances that managed instance groups with preemptible VMs can provide.</p><p><br></p><p>https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances#create_managed_group</p>",
                "answers": [
                    "<p>You should create a Managed Instance Group that uses Preemptible VMs.</p>",
                    "<p>You should create a Managed Instance Group with VMs in a single zone.</p>",
                    "<p>You should create a Managed Instance Group with VMs in multiple zones.</p>",
                    "<p>You should create a Unmanaged Instance Group.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Suppose you work as a cloud architect and you want to create a group of virtual machines for processing big data (Hadoop, Spark). You can afford to interrupt jobs as they can be easily recreated. The solution must be as low cost as possible. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430172,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are responsible for the user management service for your global company. This service will perform basic operations such as viewing, adding, updating, deleting addresses. Each of these operations is implemented by a Docker container microservice. The processing load can vary from low to very high. You want to deploy the service on Google Cloud for scalability and minimal administration. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should deploy your Docker containers into Cloud Run. -&gt; Correct. The user management service needs to be scalable and easy to administer. Cloud Run is a managed compute platform that automatically scales containerized applications based on incoming requests. This makes it an excellent choice for deploying microservices, as each microservice can be containerized and deployed on its own. In addition, Cloud Run is serverless, which means that you don't have to worry about provisioning or managing any servers. You simply need to upload your container images and Cloud Run will handle the rest.</p><p><br></p><p>You should start each Docker container as a Managed Instance Group (MIG). -&gt;&nbsp;Incorrect. It is not the best approach for deploying microservices, as MIGs are designed for stateful applications that require a fixed number of instances.</p><p><br></p><p>You should deploy your Docker containers into GKE. -&gt; Incorrect. It is also a good option, but it requires more administration and management than Cloud Run. You would need to set up and manage the Kubernetes cluster, including nodes and pods.</p><p><br></p><p>You should combine four microservices into a single Docker image, and deploy it to the App Engine instance. -&gt;&nbsp;Incorrect. It is not a good approach for microservices architecture, as it goes against the microservices principle of each service being independent and scalable.</p><p><br></p><p>https://cloud.google.com/run/docs/deploying</p>",
                "answers": [
                    "<p>You should deploy your Docker containers into Cloud Run.</p>",
                    "<p>You should start each Docker container as a Managed Instance Group (MIG).</p>",
                    "<p>You should deploy your Docker containers into GKE.</p>",
                    "<p>You should combine four microservices into a single Docker image, and deploy it to the App Engine instance.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are responsible for the user management service for your global company. This service will perform basic operations such as viewing, adding, updating, deleting addresses. Each of these operations is implemented by a Docker container microservice. The processing load can vary from low to very high. You want to deploy the service on Google Cloud for scalability and minimal administration. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430174,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A legacy systems run in your on-premises data center. As a cloud architect, you are responsible for migration these systems to the cloud and you plan to decommission all existing applications and completely redesign and rewrite them as cloud-native applications. Which approach should you choose?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Remove and Replace -&gt; Correct. In this scenario, the goal is to completely redesign and rewrite the legacy applications as cloud-native applications. The \"Remove and Replace\" approach involves decommissioning the existing applications and building new cloud-native applications from scratch. This approach provides the opportunity to take full advantage of cloud-native technologies and design patterns, and can lead to more efficient and scalable applications that are better suited to the cloud environment.</p><p><br></p><p>Lift and Shift -&gt; Incorrect. This approach involves moving the existing applications to the cloud with minimal changes. It is not well-suited for completely redesigning and rewriting applications as cloud-native applications.</p><p><br></p><p>Improve and Move -&gt; Incorrect. This approach involves improving the existing applications and then migrating them to the cloud. While it may be suitable for some scenarios, it is not the best fit for completely redesigning and rewriting applications as cloud-native applications.</p><p><br></p><p>Blue-green -&gt; Incorrect. This approach involves deploying two identical environments (blue and green) and switching between them for upgrades or rollbacks. While it can be useful for reducing downtime during upgrades or rollbacks, it is not well-suited for completely redesigning and rewriting applications as cloud-native applications.</p><p><br></p><p>https://cloud.google.com/architecture/migration-to-gcp-getting-started#types_of_migrations</p>",
                "answers": [
                    "<p>Remove and Replace</p>",
                    "<p>Lift and Shift</p>",
                    "<p>Improve and Move</p>",
                    "<p>Blue-green</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A legacy systems run in your on-premises data center. As a cloud architect, you are responsible for migration these systems to the cloud and you plan to decommission all existing applications and completely redesign and rewrite them as cloud-native applications. Which approach should you choose?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430176,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization, a multinational bank, has decided to migrate its operations to Google Cloud. The applications include a transaction processing system, a customer relationship management (CRM) tool, an intranet site, a data warehouse, and an email server. Given the need for continuous high availability and security compliance, which application should be migrated first?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>The intranet site -&gt;&nbsp;Correct. The intranet site is a good candidate to be migrated first. It's typically less critical than the other systems and could allow the organization to gain familiarity with Google Cloud without risking core operational systems.</p><p><br></p><p>The transaction processing system -&gt;&nbsp;Incorrect. Migrating this system first might expose the organization to significant risk, as transaction systems are usually critical for day-to-day operations. It would be safer to start with less critical applications.</p><p><br></p><p>The customer relationship management (CRM) tool -&gt;&nbsp;Incorrect. The CRM tool is important for maintaining customer relationships and contains sensitive data. Migrating this system first could be risky.</p><p><br></p><p>The data warehouse -&gt;&nbsp;Incorrect. Migrating the data warehouse first might expose the organization to risks related to data loss or compliance issues. It would be safer to start with less critical applications.</p>",
                "answers": [
                    "<p>The intranet site</p>",
                    "<p>The transaction processing system</p>",
                    "<p>The customer relationship management (CRM) tool</p>",
                    "<p>The data warehouse</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization, a multinational bank, has decided to migrate its operations to Google Cloud. The applications include a transaction processing system, a customer relationship management (CRM) tool, an intranet site, a data warehouse, and an email server. Given the need for continuous high availability and security compliance, which application should be migrated first?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430178,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company runs a mission-critical application on Google Cloud with multiple microservices. Recently, the application experienced downtime due to a service failure, which had a cascading effect. As a Cloud Architect, what should you do to enhance the solution's reliability and prevent such incidents in the future?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement circuit breakers and retries with exponential backoff and jitter for inter-service communications. -&gt; Correct. Circuit breakers can prevent a single service failure from bringing down the whole system by stopping the flow of traffic to the failed service. Retries with exponential backoff and jitter can give the service time to recover, reducing the chances of cascading failure.</p><p><br></p><p>Implement regular disaster recovery drills to ensure the readiness of the backup system. -&gt;&nbsp;Incorrect. While disaster recovery drills are good practice for ensuring system recovery, they do not inherently prevent service failures or cascading effects.</p><p><br></p><p>Move the application to a monolithic architecture to simplify the complexity. -&gt;&nbsp;Incorrect. This approach does not inherently improve reliability and might in fact reduce the ability to scale and manage the application effectively.</p><p><br></p><p>Implement load balancing and auto-scaling for all microservices. -&gt;&nbsp;Incorrect. While load balancing and auto-scaling can help maintain service availability and performance, they do not inherently prevent cascading failures from a service failure.</p>",
                "answers": [
                    "<p>Implement circuit breakers and retries with exponential backoff and jitter for inter-service communications.</p>",
                    "<p>Implement regular disaster recovery drills to ensure the readiness of the backup system.</p>",
                    "<p>Move the application to a monolithic architecture to simplify the complexity.</p>",
                    "<p>Implement load balancing and auto-scaling for all microservices.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company runs a mission-critical application on Google Cloud with multiple microservices. Recently, the application experienced downtime due to a service failure, which had a cascading effect. As a Cloud Architect, what should you do to enhance the solution's reliability and prevent such incidents in the future?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430180,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect for a large media corporation that has decided to adopt Google Cloud. They have an extensive infrastructure including a content management system (CMS), a user registration system, an ad serving platform, an analytics platform, and an internal communication platform. Which one of these systems would you consider as a good candidate for the first migration?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>The analytics platform -&gt;&nbsp;Correct. The analytics platform is a good choice for a first migration because it typically doesn't directly affect the user experience. Also, there are robust tools in Google Cloud for analytics, making it a relatively straightforward migration.</p><p><br></p><p>The ad serving platform -&gt;&nbsp;Incorrect. This system directly impacts the organization's revenue, so any disruption during migration could lead to significant revenue loss.</p><p><br></p><p>The user registration system -&gt;&nbsp;Incorrect. The user registration system likely contains sensitive user data, and any disruption or data loss during migration could have significant consequences.</p><p><br></p><p>The content management system (CMS) -&gt;&nbsp;Incorrect. A CMS is often tightly integrated with other systems and might have significant dependencies, making it a challenging choice for a first migration.</p>",
                "answers": [
                    "<p>The analytics platform</p>",
                    "<p>The ad serving platform</p>",
                    "<p>The user registration system</p>",
                    "<p>The content management system (CMS)</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect for a large media corporation that has decided to adopt Google Cloud. They have an extensive infrastructure including a content management system (CMS), a user registration system, an ad serving platform, an analytics platform, and an internal communication platform. Which one of these systems would you consider as a good candidate for the first migration?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430182,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A mobile gaming company has a new mobile game with features that allow users to accumulate points by playing the game. Points can be use to make in-game purchases. You want to prevent bot activity (playing the game much faster than humans) and you don't want to ban users (if not necessary). What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should modify the game API to prevent more than 5-10 function calls per user, per minute. -&gt; Correct. By implementing rate limiting on the game API, you can restrict the number of function calls a user can make within a specified time period. This helps prevent bot activity by limiting the speed at which requests can be made. By setting a reasonable limit, you can strike a balance between preventing excessive activity from bots and allowing genuine users to enjoy the game without being unfairly restricted.</p><p><br></p><p>You should ban users with bot activity. Some users may be wrongly banned. -&gt;&nbsp;Incorrect. While banning users may effectively address the bot activity concern, there is a risk of false positives, where legitimate users may be wrongly identified as bots and banned. This approach may result in a negative user experience and potential loss of genuine users.</p><p><br></p><p>You should alert users with bot activity. Some users may be wrongly alerted. -&gt;&nbsp;Incorrect. While it's important to be transparent and inform users about any potential suspicious activity, relying solely on alerts may not be sufficient to prevent bot activity. It may result in some legitimate users receiving false alerts, leading to confusion and frustration.</p><p><br></p><p>There is nothing you can do in this case. -&gt;&nbsp;Incorrect. This answer choice implies that there are no actions that can be taken to address the bot activity issue. However, there are proactive measures that can be implemented, such as rate limiting, to mitigate the impact of bot activity and protect the integrity of the game.</p><p><br></p><p>https://cloud.google.com/compute/docs/api-rate-limits</p>",
                "answers": [
                    "<p>You should modify the game API to prevent more than 5-10 function calls per user, per minute.</p>",
                    "<p>You should ban users with bot activity. Some users may be wrongly banned.</p>",
                    "<p>You should alert users with bot activity. Some users may be wrongly alerted.</p>",
                    "<p>There is nothing you can do in this case.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A mobile gaming company has a new mobile game with features that allow users to accumulate points by playing the game. Points can be use to make in-game purchases. You want to prevent bot activity (playing the game much faster than humans) and you don't want to ban users (if not necessary). What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430184,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, it is your duty to establish industry best practices within your company. In the context of API error messages, what specific data and information would you suggest including?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>An API error message should return a status code form with the standard 400s and 500s HTTP status codes along with additional error details in the payload. -&gt;&nbsp;Correct. An API error message is a response that a client receives when an error occurs while making an API request. It is important to provide clear and useful information in the error message to help developers understand what went wrong and how to fix it. Returning an HTTP status code is essential to indicate whether the request was successful or not. Standard HTTP status codes such as 400s and 500s should be used to indicate the type of error that occurred, such as 400 for client-side errors or 500 for server-side errors. In addition to the status code, the API error message should also include additional error details in the payload, such as a message explaining what went wrong, and potentially additional information to help developers diagnose the problem.</p><p><br></p><p>An API error message should return HTTP status 200 with additional error details in the payload. -&gt; Incorrect. It is not recommended as returning HTTP status 200 would indicate a successful request, which is misleading in the case of an error. </p><p><br></p><p>An API error message should return error details in the payload, and don't return a status code. -&gt; Incorrect. It is not recommended as it would not provide any indication of whether the request was successful or not. </p><p><br></p><p>An API error message should define your own set of application-specific error codes. -&gt; Incorrect. It may be appropriate in some cases, but it is generally better to use standard HTTP status codes as they are widely recognized and understood.</p><p><br></p><p>https://cloud.google.com/apis/design/errors</p>",
                "answers": [
                    "<p>An API error message should return a status code form with the standard 400s and 500s HTTP status codes along with additional error details in the payload.</p>",
                    "<p>An API error message should return HTTP status 200 with additional error details in the payload.</p>",
                    "<p>An API error message should return error details in the payload, and don't return a status code.</p>",
                    "<p>An API error message should define your own set of application-specific error codes.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, it is your duty to establish industry best practices within your company. In the context of API error messages, what specific data and information would you suggest including?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430186,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company operates a large-scale e-commerce platform on Google Cloud with millions of daily users. You have been tasked with introducing a new feature to the platform. The feature has been thoroughly tested but has never been deployed in production. Given the potential for unforeseen issues, you have been asked to implement a deployment strategy that minimizes potential disruptions. Which approach should you take?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use a Canary deployment strategy. -&gt;&nbsp;Correct. Canary deployments allow for the new version of the application to be rolled out gradually to a small percentage of users. This allows you to monitor the new version in production, reducing the potential impact of any issues.</p><p><br></p><p>Use a Blue/Green deployment strategy. -&gt; Incorrect. While Blue/Green deployment strategy allows for quick rollback, it doesn't allow for gradual rollout of features, meaning if an issue arises, it could impact a large number of users immediately.</p><p><br></p><p>Use a Red/Black deployment strategy. -&gt; Incorrect. Red/Black deployment strategy is essentially the same as Blue/Green deployment strategy - it allows for easy rollback but doesn't provide a gradual rollout of the new feature.</p><p><br></p><p>Deploy the feature to all users at once without any gradual rollout. -&gt; Incorrect. Deploying a new, untested feature to all users at once is likely to lead to major disruptions if any unforeseen issues arise.</p>",
                "answers": [
                    "<p>Use a Canary deployment strategy.</p>",
                    "<p>Use a Blue/Green deployment strategy.</p>",
                    "<p>Use a Red/Black deployment strategy.</p>",
                    "<p>Deploy the feature to all users at once without any gradual rollout.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company operates a large-scale e-commerce platform on Google Cloud with millions of daily users. You have been tasked with introducing a new feature to the platform. The feature has been thoroughly tested but has never been deployed in production. Given the potential for unforeseen issues, you have been asked to implement a deployment strategy that minimizes potential disruptions. Which approach should you take?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430188,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization has just migrated their production environment to Google Cloud. As part of regulatory compliance, all administrative activities must be audited, and logs must be kept for at least one year. As a cloud architect, how would you ensure this requirement is met?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Enable Cloud Logging on all resources and export logs to Cloud Storage with a lifecycle policy of one year. -&gt;&nbsp;Correct. This approach meets the requirement to audit all administrative activities and keep logs for at least one year. Cloud Logging will capture the logs, and exporting them to Cloud Storage allows for cost-effective long-term storage.</p><p><br></p><p>Use Cloud Monitoring to audit all administrative activities. -&gt;&nbsp;Incorrect. Cloud Monitoring is useful for performance metrics and uptime, but it does not capture administrative activities in the same way as Cloud Logging does.</p><p><br></p><p>Enable VPC Flow Logs for auditing all administrative activities. -&gt;&nbsp;Incorrect. VPC Flow Logs capture network traffic, but do not capture other administrative activities, such as changes to IAM policies or resource configurations.</p><p><br></p><p>Use Cloud Functions to write log entries into BigQuery for long-term storage. -&gt;&nbsp;Incorrect. This approach might be too complex and costly. While BigQuery is a good option for querying large datasets, it's not typically used for long-term log storage.</p>",
                "answers": [
                    "<p>Enable Cloud Logging on all resources and export logs to Cloud Storage with a lifecycle policy of one year.</p>",
                    "<p>Use Cloud Monitoring to audit all administrative activities.</p>",
                    "<p>Enable VPC Flow Logs for auditing all administrative activities.</p>",
                    "<p>Use Cloud Functions to write log entries into BigQuery for long-term storage.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization has just migrated their production environment to Google Cloud. As part of regulatory compliance, all administrative activities must be audited, and logs must be kept for at least one year. As a cloud architect, how would you ensure this requirement is met?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430190,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are working with a client who wants to store their Git repositories within the Google Cloud environment. The client also requires functionality for code review, branch management, and collaborative development. Which service in Google Cloud should you recommend for these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Source Repositories -&gt; Correct. Cloud Source Repositories is a single place for your team to store, manage, and track code, providing features like code review and collaboration, making it an ideal choice for storing Git repositories.</p><p><br></p><p>Cloud Storage with object versioning enabled -&gt; Incorrect. Cloud Storage is a service for storing and retrieving any amount of data at any time, but it's not optimized for storing Git repositories and providing collaboration and code review features.</p><p><br></p><p>Cloud Pub/Sub -&gt; Incorrect. Cloud Pub/Sub is a messaging service for exchanging event data among services and applications, not for storing Git repositories.</p><p><br></p><p>Cloud Git -&gt; Incorrect. There is no such service in Google Cloud.</p>",
                "answers": [
                    "<p>Cloud Source Repositories</p>",
                    "<p>Cloud Storage with object versioning enabled</p>",
                    "<p>Cloud Pub/Sub</p>",
                    "<p>Cloud Git</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are working with a client who wants to store their Git repositories within the Google Cloud environment. The client also requires functionality for code review, branch management, and collaborative development. Which service in Google Cloud should you recommend for these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430192,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are managing a complex deployment scenario that requires high availability when using Google Cloud Storage. Which of the following practices would provide the highest level of data durability and availability?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Store all data in a multi-region Cloud Storage bucket. -&gt; Correct. Multi-region Cloud Storage buckets offer the highest availability as data is automatically stored redundantly in multiple geographically distant regions. This helps ensure that your data is still accessible even if one entire region goes offline.</p><p><br></p><p>Store all data in a single-region Cloud Storage bucket. -&gt; Incorrect. A single-region Cloud Storage bucket keeps your data in one region. While this offers lower latency and costs when the users are mainly in the same region, it does not provide the same level of high availability as multi-region storage.</p><p><br></p><p>Store all data in a dual-region Cloud Storage bucket. -&gt; Incorrect. Dual-region Cloud Storage provides higher availability compared to a single-region, but not as high as a multi-region bucket. Data in a dual-region bucket is stored in two specific regions.</p><p><br></p><p>Store all data in a local SSD. -&gt;&nbsp;Incorrect. Local SSDs provide high-performance storage but they are physically attached to the server hosting the VM instance. They do not offer the same durability and availability as replicated cloud storage, and data can be lost if the VM is stopped or if the SSD fails.</p>",
                "answers": [
                    "<p>Store all data in a multi-region Cloud Storage bucket.</p>",
                    "<p>Store all data in a single-region Cloud Storage bucket.</p>",
                    "<p>Store all data in a dual-region Cloud Storage bucket.</p>",
                    "<p>Store all data in a local SSD.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are managing a complex deployment scenario that requires high availability when using Google Cloud Storage. Which of the following practices would provide the highest level of data durability and availability?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430194,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are responsible for a Lift and Shift migration to the Google Cloud. You have several load-balanced clusters that use virtual machines that are not identically configured. You don't want to make unnecessary changes when moving to the cloud. Which GCP feature should you use?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Unmanaged Instance Groups -&gt; Correct. Unmanaged instance groups allow you to bring together virtual machines with non-identical configurations, which is required for your Lift and Shift migration as per the scenario given in the question. You do not want to make unnecessary changes and the virtual machines are not identically configured. Unmanaged instance groups provide this flexibility.</p><p><br></p><p>Managed Instance Groups -&gt;&nbsp;Incorrect. Managed Instance Groups (MIGs) in Google Cloud are a collection of identical instances. This would require your load-balanced clusters to have identically configured virtual machines, which contradicts the premise of your question.</p><p><br></p><p>Kubernetes clusters -&gt;&nbsp;Incorrect. Kubernetes is an open-source platform for managing containerized workloads and services. While it's a powerful tool, using it would entail changes to your existing VM-based architecture, which you are trying to avoid according to the question.</p><p><br></p><p>App Engine -&gt;&nbsp;Incorrect. App Engine is a fully managed serverless platform for developing and hosting web applications. Like Kubernetes, using App Engine would necessitate significant changes to your existing setup, which contradicts the desire to avoid unnecessary changes mentioned in the question.</p><p><br></p><p>https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-unmanaged-instances</p>",
                "answers": [
                    "<p>Unmanaged Instance Groups</p>",
                    "<p>Managed Instance Groups</p>",
                    "<p>Kubernetes clusters</p>",
                    "<p>App Engine</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are responsible for a Lift and Shift migration to the Google Cloud. You have several load-balanced clusters that use virtual machines that are not identically configured. You don't want to make unnecessary changes when moving to the cloud. Which GCP feature should you use?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430196,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are responsible for migrating stateless application written in Python to the Google Cloud. You want to avoid managing servers or containers and this application must be scalable. Which GCP&nbsp;service do you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>App Engine -&gt; Correct. App Engine is a fully managed Platform-as-a-Service (PaaS) offering from Google Cloud that allows you to deploy web applications and APIs written in several programming languages, including Python, without worrying about managing servers or containers. App Engine automatically scales your application based on traffic and provides a high level of availability.</p><p><br></p><p>Compute Engine -&gt; Incorrect. Compute Engine is an Infrastructure-as-a-Service (IaaS) offering that allows you to deploy virtual machines and manage them yourself. While Compute Engine provides a lot of flexibility, it requires more management and configuration than App Engine, making it less suitable for stateless applications that need to be scalable.</p><p><br></p><p>Kubernetes Engine -&gt; Incorrect. Kubernetes Engine is a managed Kubernetes service that allows you to deploy, manage, and scale containerized applications. While Kubernetes Engine provides a lot of flexibility and scalability, it requires more management and configuration than App Engine, making it less suitable for stateless applications that need to be scalable.</p><p><br></p><p>Cloud Function -&gt; Incorrect. Cloud Functions is a serverless compute service that allows you to run code in response to events without worrying about managing servers or containers. However, Cloud Functions is designed for short-lived, event-driven functions rather than stateless applications, making it less suitable for this scenario.</p><p><br></p><p>https://cloud.google.com/appengine/docs</p>",
                "answers": [
                    "<p>App Engine</p>",
                    "<p>Compute Engine</p>",
                    "<p>Kubernetes Engine</p>",
                    "<p>Cloud Function</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are responsible for migrating stateless application written in Python to the Google Cloud. You want to avoid managing servers or containers and this application must be scalable. Which GCP&nbsp;service do you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430198,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A monolithic Python application needs to be migrated to the Google Cloud. As a cloud architect, you are planning to use the Lift and Shift strategy. You have a Dockerfile created with this application and an image container. You want to minimize effort to maintain it. What should you do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should use App Engine Flexible to run the container image. -&gt; Correct. App Engine Flexible is the best option for running a Docker container on Google Cloud using the Lift and Shift strategy. App Engine Flexible allows developers to package and deploy their applications in a Docker container without the need to manage infrastructure or worry about scalability, while also providing access to many GCP services.</p><p><br></p><p>You should use App Engine Standard to run the container image. -&gt; Incorrect. App Engine Standard is not suitable in this case as it does not support running Docker containers directly. </p><p><br></p><p>You should create a Compute Engine instance and configure it from scratch. -&gt; Incorrect. Creating a Compute Engine instance from scratch would require manual configuration and maintenance, which is not ideal for minimizing effort. </p><p><br></p><p>You should use Managed Instance Group to run the container image. -&gt; Incorrect. Managed Instance Group could be an option, but it would require additional setup and configuration, making it less optimal than App Engine Flexible.</p><p><br></p><p>https://cloud.google.com/appengine/docs/flexible</p>",
                "answers": [
                    "<p>You should use App Engine Flexible to run the container image.</p>",
                    "<p>You should use App Engine Standard to run the container image.</p>",
                    "<p>You should create a Compute Engine instance and configure it from scratch.</p>",
                    "<p>You should use Managed Instance Group to run the container image.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A monolithic Python application needs to be migrated to the Google Cloud. As a cloud architect, you are planning to use the Lift and Shift strategy. You have a Dockerfile created with this application and an image container. You want to minimize effort to maintain it. What should you do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430200,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are managing a complex deployment scenario for a multinational enterprise. The company requires that all data at rest be encrypted using encryption keys that the company manages itself. The company also needs to use different encryption keys for its branches in North America, Europe, and Asia. How can you implement this requirement in Google Cloud?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use customer-managed encryption keys (CMEK), create separate keys for each region in Cloud KMS, and assign them accordingly. -&gt; Correct. The use of customer-managed encryption keys (CMEK) allows the company to create, control, and manage its encryption keys through Google Cloud KMS. The company can create separate keys for each region and assign them to resources accordingly, which fulfills its requirements.</p><p><br></p><p>Use Google-managed encryption keys and assign different keys to different regions. -&gt; Incorrect. Google-managed encryption keys do not give the company the control it needs over its keys and the ability to use different keys in different regions.</p><p><br></p><p>Use Cloud Storage with Uniform bucket-level access and provide different keys for different regions. -&gt; Incorrect. Cloud Storage with Uniform bucket-level access is a storage solution that controls access to the bucket as a whole, but this alone doesn't answer the need for the company to manage its own encryption keys and use different keys in different regions.</p><p><br></p><p>Use Cloud Datastore and provide different keys for different regions. -&gt; Incorrect. Cloud Datastore is a NoSQL document database built for automatic scaling and high performance, but it doesn't directly address the requirement for managing custom encryption keys per region.</p>",
                "answers": [
                    "<p>Use customer-managed encryption keys (CMEK), create separate keys for each region in Cloud KMS, and assign them accordingly.</p>",
                    "<p>Use Google-managed encryption keys and assign different keys to different regions.</p>",
                    "<p>Use Cloud Storage with Uniform bucket-level access and provide different keys for different regions.</p>",
                    "<p>Use Cloud Datastore and provide different keys for different regions.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are managing a complex deployment scenario for a multinational enterprise. The company requires that all data at rest be encrypted using encryption keys that the company manages itself. The company also needs to use different encryption keys for its branches in North America, Europe, and Asia. How can you implement this requirement in Google Cloud?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430202,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A data scientist has prepared a query in BigQuery and expects to process a large amount of data. Before executing the query, he wants to know how much data will be processed. What should a data scientist do?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>The correct answer is to use <code>--dry_run</code> flag. The Google Cloud Pricing Calculator cannot estimate the number of bytes read by the query. -&gt; Correct. When the data scientist runs a query in the bq command-line tool, they should use the <code>--dry_run</code> flag to estimate the number of bytes read by the query. The <code>--dry_run</code> flag in BigQuery's <code>bq</code> command-line tool is designed to simulate a query's execution, including the amount of data read, without actually executing the query. By using this flag, the data scientist can estimate the size of the data that the query will process before executing it.</p><p><br></p><p>When he runs a query in the <code>bq</code> command-line tool, he should use <code>--cost</code> flag to estimate the number of bytes read by the query. -&gt; Incorrect. It is incorrect because the <code>--cost</code> flag is used to estimate the cost of a query based on the amount of data processed, not the size of the data.</p><p><br></p><p>He should use Google Cloud Pricing Calculator to estimate the number of bytes read by the query. -&gt; Incorrect. It is incorrect because the Google Cloud Pricing Calculator is used to estimate the cost of running a workload on Google Cloud Platform, including BigQuery, but it does not estimate the amount of data processed by a specific query.</p><p><br></p><p>The number of bytes cannot be estimated without executing the query. -&gt; Incorrect. It is incorrect because the amount of data processed can be estimated using the <code>--dry_run</code> flag before executing the query.</p><p><br></p><p>https://cloud.google.com/bigquery/docs/dry-run-queries</p>",
                "answers": [
                    "<p>When he runs a query in the <code>bq</code> command-line tool, he should use <code>--dry_run</code> flag to estimate the number of bytes read by the query.</p>",
                    "<p>When he runs a query in the <code>bq</code> command-line tool, he should use <code>--cost</code> flag to estimate the number of bytes read by the query.</p>",
                    "<p>He should use Google Cloud Pricing Calculator to estimate the number of bytes read by the query.</p>",
                    "<p>The number of bytes cannot be estimated without executing the query.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A data scientist has prepared a query in BigQuery and expects to process a large amount of data. Before executing the query, he wants to know how much data will be processed. What should a data scientist do?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430204,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect at a multinational corporation that has recently decided to move its infrastructure to Google Cloud Platform. The company has several departments globally, with each department having multiple teams. You need to design a resource hierarchy that allows you to apply company-wide policies, segregate resources at the department level, and isolate resources at the team level. What would be the best way to set up your GCP resource hierarchy?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Create an organization for the company, a project for each team, and segregate departments using folders. -&gt;&nbsp;Correct. The organization node is the root node in the Google Cloud resource hierarchy and represents your company. Under the organization, creating folders for each department and then projects for each team would allow you to apply company-wide policies, segregate resources at the department level, and isolate resources at the team level.</p><p><br></p><p>Create a project for each department and use IAM roles to segregate resources at the team level. -&gt;&nbsp;Incorrect. This approach won't provide the level of isolation required at the team level, as resources within the same project are part of the same trust boundary.</p><p><br></p><p>Create an organization for the company, a project for each department, and folders for each team. -&gt;&nbsp;Incorrect. Folders should not be created under projects in GCP resource hierarchy. Folders can only be created under an organization or other folders.</p><p><br></p><p>Create an organization for each department and projects for each team under those organizations. -&gt;&nbsp;Incorrect. In GCP, an organization is typically used to represent a company rather than a department. Using an organization for each department is an inappropriate use of the resource hierarchy.</p>",
                "answers": [
                    "<p>Create an organization for the company, a project for each team, and segregate departments using folders.</p>",
                    "<p>Create a project for each department and use IAM roles to segregate resources at the team level.</p>",
                    "<p>Create an organization for the company, a project for each department, and folders for each team.</p>",
                    "<p>Create an organization for each department and projects for each team under those organizations.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect at a multinational corporation that has recently decided to move its infrastructure to Google Cloud Platform. The company has several departments globally, with each department having multiple teams. You need to design a resource hierarchy that allows you to apply company-wide policies, segregate resources at the department level, and isolate resources at the team level. What would be the best way to set up your GCP resource hierarchy?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430206,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>As a cloud architect, you are involved in a project that requires the utilization of numerous virtual machines in Compute Engine. These virtual machines can exist in various states such as Running, Suspended, Stopped, or Deleted. Please select all accurate statements that demonstrate how each state influences the billing aspect.</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>In Running state, you pay for the vCPU, memory, and disk utilization. -&gt;&nbsp;Correct. When a virtual machine is running, you are billed for the resources it consumes, including vCPU, memory, and disk utilization. The billing is based on the machine type and usage.</p><p><br></p><p>In Stopped state, you only pay for disk utilization. -&gt;&nbsp;Correct. When a virtual machine is stopped, you are not billed for vCPU and memory usage. However, you are still billed for the disk storage that is associated with the stopped virtual machine. This allows you to retain the disk data while not incurring costs for the compute resources.</p><p><br></p><p>In Deleted state, you only pay for disk utilization. -&gt;&nbsp;Incorrect. When a virtual machine is deleted, you are not billed for any resources, including disk utilization. Deleting a virtual machine removes all associated resources, and you are no longer charged for them.</p><p><br></p><p>In Suspended state, you pay for the vCPU, memory, and disk utilization. -&gt;&nbsp;Incorrect. During suspension, compute resources are generally not billed.</p><p><br></p><p>https://cloud.google.com/architecture/cost-optimization-using-automated-vm-management?hl=en#stopping_suspending_and_deleting_instances</p>",
                "answers": [
                    "<p>In Running state, you pay for the vCPU, memory, and disk utilization.</p>",
                    "<p>In Stopped state, you only pay for disk utilization.</p>",
                    "<p>In Deleted state, you only pay for disk utilization.</p>",
                    "<p>In Suspended state, you pay for the vCPU, memory, and disk utilization.</p>"
                ]
            },
            "correct_response": [
                "a",
                "b"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are involved in a project that requires the utilization of numerous virtual machines in Compute Engine. These virtual machines can exist in various states such as Running, Suspended, Stopped, or Deleted. Please select all accurate statements that demonstrate how each state influences the billing aspect.",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430208,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Refer to the EHR Healthcare case study for this question: https://services.google.com/fh/files/blogs/master_case_study_ehr_healthcare.pdf</p><p><br></p><p>EHR's client in the healthcare sector is a world-renowned research and hospital facility. A significant number of the patients at this renowned research and hospital facility are prominent public figures. There have been consistent attempts from both internal and external sources to illicitly access the health information of these patients. To safeguard patient privacy, the hospital has implemented a policy that restricts the movement of patient information stored in Cloud Storage buckets beyond the geographic boundaries where the buckets are located. It is crucial for you to ensure that the data stored in Cloud Storage buckets within the <code>europe-west2</code> region remains confined within this specific region and does not get transferred elsewhere. What actions are recommended?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should enable Virtual Private Network Service Controls, and create a service perimeter around the Cloud Storage resources. -&gt; Correct. VPC Service Controls improves your ability to mitigate the risk of data exfiltration from Google Cloud services.</p><p><br></p><p>You should encrypt the data in the application on-premises before the data is stored in the <code>europe-west2</code> region. -&gt; Incorrect. Encrypting the data does not stop data exfiltration.</p><p><br></p><p>You should assign the Identity and Access Management (IAM) <code>storage.objectViewer</code> role only to users and service accounts that need to use the data. -&gt; Incorrect. IAM roles deal with identity-based access control, not context-aware perimeter security.</p><p><br></p><p>You should create an access control list (ACL) that limits access to the bucket to authorized users only, and apply it to the buckets in the <code>europe-west2</code> region. -&gt; Incorrect. Cloud Storage ACLs are a mechanism you can use to define who has access to your buckets and objects, as well as their level of access. ACLs do not stop data exfiltration.</p><p><br></p><p>https://cloud.google.com/vpc-service-controls/docs/overview</p>",
                "answers": [
                    "<p>You should enable Virtual Private Network Service Controls, and create a service perimeter around the Cloud Storage resources.</p>",
                    "<p>You should encrypt the data in the application on-premises before the data is stored in the <code>europe-west2</code> region.</p>",
                    "<p>You should assign the Identity and Access Management (IAM) <code>storage.objectViewer</code> role only to users and service accounts that need to use the data.</p>",
                    "<p>You should create an access control list (ACL) that limits access to the bucket to authorized users only, and apply it to the buckets in the <code>europe-west2</code> region.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Refer to the EHR Healthcare case study for this question: https://services.google.com/fh/files/blogs/master_case_study_ehr_healthcare.pdfEHR's client in the healthcare sector is a world-renowned research and hospital facility. A significant number of the patients at this renowned research and hospital facility are prominent public figures. There have been consistent attempts from both internal and external sources to illicitly access the health information of these patients. To safeguard patient privacy, the hospital has implemented a policy that restricts the movement of patient information stored in Cloud Storage buckets beyond the geographic boundaries where the buckets are located. It is crucial for you to ensure that the data stored in Cloud Storage buckets within the europe-west2 region remains confined within this specific region and does not get transferred elsewhere. What actions are recommended?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430210,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdf</p><p><br></p><p>As a cloud architect, your task is to deploy Virtual Private Cloud (VPC) Service Controls for Mountkirk Games. The objective is to allow developers to utilize Cloud Shell while ensuring they do not have unrestricted access to managed services. It is crucial to find a balance between these conflicting goals while considering the business requirements of Mountkirk Games. What steps should you take in this scenario?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>You should create a service perimeter around only the projects that handle sensitive data, and do not grant your developers access to it. -&gt;&nbsp;Correct. VPC Service Controls protects data, and Cloud Shell facilitates rapid iteration of Google Cloud architecture changes.</p><p><br></p><p>You should use VPC Service Controls for the entire platform. -&gt;&nbsp;Incorrect. VPC Service Controls do not directly affect scaling of Google Cloud architecture.</p><p><br></p><p>You should prioritize VPC Service Controls implementation over Cloud Shell usage for the entire platform. -&gt;&nbsp;Incorrect. A security perimeter is not a business requirement, but rapid iteration is.</p><p><br></p><p>You should include all developers in an access level associated with the service perimeter, and allow them to use Cloud Shell. -&gt;&nbsp;Incorrect. It works but does not scale, and we do not want to give users this much access.</p><p><br></p><p>https://cloud.google.com/vpc-service-controls/docs/overview</p><p>https://cloud.google.com/vpc-service-controls/docs/supported-products#shell</p>",
                "answers": [
                    "<p>You should create a service perimeter around only the projects that handle sensitive data, and do not grant your developers access to it.</p>",
                    "<p>You should use VPC Service Controls for the entire platform.</p>",
                    "<p>You should prioritize VPC Service Controls implementation over Cloud Shell usage for the entire platform.</p>",
                    "<p>You should include all developers in an access level associated with the service perimeter, and allow them to use Cloud Shell. </p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdfAs a cloud architect, your task is to deploy Virtual Private Cloud (VPC) Service Controls for Mountkirk Games. The objective is to allow developers to utilize Cloud Shell while ensuring they do not have unrestricted access to managed services. It is crucial to find a balance between these conflicting goals while considering the business requirements of Mountkirk Games. What steps should you take in this scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430212,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>The objective of your company is to monitor the presence of individuals in meeting rooms that have been booked for scheduled meetings. There are a total of 5,000 conference rooms spread across six offices located on four continents. Each room is equipped with a motion sensor that provides status updates every second. As a cloud architect, your goal is to establish a data ingestion system that can handle the requirements of this sensor network. The receiving infrastructure needs to consider the fact that the devices may experience inconsistent connectivity. Which solution is worth designing?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Have devices poll for connectivity to Pub/Sub and publish the latest messages on a regular interval to a shared topic for all devices. -&gt;&nbsp;Correct. Pub/Sub can handle the frequency of this data, and consumers of the data can pull from the shared topic for further processing.</p><p><br></p><p>Have each device create a persistent connection to a Compute Engine instance and write messages to a custom application. -&gt; Incorrect. Having a persistent connection does not handle the case where the device is disconnected.</p><p><br></p><p>Have devices poll for connectivity to Cloud SQL and insert the latest messages on a regular interval to a device specific table. -&gt; Incorrect. Cloud SQL is a regional, relational database and not the best fit for sensor data. Additionally, the frequency of the writes has the potential to exceed the supported number of concurrent connections.</p><p><br></p><p>Have devices create a persistent connection to an App Engine application fronted by Cloud Endpoints, which ingest messages and write them to Datastore. -&gt;&nbsp;Incorrect. Having a persistent connection does not handle the case where the device is disconnected.</p><p><br></p><p>https://cloud.google.com/pubsub/docs/overview</p>",
                "answers": [
                    "<p>Have devices poll for connectivity to Pub/Sub and publish the latest messages on a regular interval to a shared topic for all devices.</p>",
                    "<p>Have each device create a persistent connection to a Compute Engine instance and write messages to a custom application.</p>",
                    "<p>Have devices poll for connectivity to Cloud SQL and insert the latest messages on a regular interval to a device specific table.</p>",
                    "<p>Have devices create a persistent connection to an App Engine application fronted by Cloud Endpoints, which ingest messages and write them to Datastore.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "The objective of your company is to monitor the presence of individuals in meeting rooms that have been booked for scheduled meetings. There are a total of 5,000 conference rooms spread across six offices located on four continents. Each room is equipped with a motion sensor that provides status updates every second. As a cloud architect, your goal is to establish a data ingestion system that can handle the requirements of this sensor network. The receiving infrastructure needs to consider the fact that the devices may experience inconsistent connectivity. Which solution is worth designing?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430214,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are managing a complex deployment scenario involving several Google Cloud projects under a single organization. You have a new team member who needs to view metadata about the organization and projects, but should not be allowed to modify resources. Which combination of IAM roles would best fulfill this requirement?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Assign 'Organization Viewer' at the organization level and 'Project Viewer' at the project level. -&gt; Correct. The 'Organization Viewer' role at the organization level gives the user the ability to view metadata about the organization. The 'Project Viewer' role at the project level allows the user to view but not modify project resources. This combination fulfills the requirement without granting additional permissions.</p><p><br></p><p>Assign 'Organization Viewer' at the organization level and 'Project Editor' at the project level. -&gt; Incorrect. The 'Project Editor' role at the project level would give the user the ability to modify project resources, which is not required in this scenario.</p><p><br></p><p>Assign 'Project Viewer' at both the organization and project levels. -&gt; Incorrect. 'Project Viewer' cannot be assigned at the organization level.</p><p><br></p><p>Assign 'Project Viewer' at the organization level and 'Organization Viewer' at the project level. -&gt;&nbsp;Incorrect. IAM roles are specific to resources. 'Project Viewer' cannot be assigned at the organization level, and 'Organization Viewer' cannot be assigned at the project level.</p>",
                "answers": [
                    "<p>Assign 'Organization Viewer' at the organization level and 'Project Viewer' at the project level.</p>",
                    "<p>Assign 'Organization Viewer' at the organization level and 'Project Editor' at the project level.</p>",
                    "<p>Assign 'Project Viewer' at both the organization and project levels.</p>",
                    "<p>Assign 'Project Viewer' at the organization level and 'Organization Viewer' at the project level.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are managing a complex deployment scenario involving several Google Cloud projects under a single organization. You have a new team member who needs to view metadata about the organization and projects, but should not be allowed to modify resources. Which combination of IAM roles would best fulfill this requirement?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430216,
            "assessment_type": "multi-select",
            "prompt": {
                "question": "<p>In order to cut down expenses, the chief engineering officer mandated that all developers shift their development infrastructure resources from on-premises virtual machines (VMs) to Google Cloud. These resources undergo frequent start/stop occurrences throughout the day and necessitate the persistence of their state. As a cloud architect, you have been tasked with designing a plan for running a development environment on Google Cloud that also allows the finance department to have clear visibility into the costs involved. Which two steps should you take?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use persistent disks to store the state. Start and stop the VM as needed. -&gt;&nbsp;Correct. Persistent disks will not be deleted when an instance is stopped.</p><p><br></p><p>Use BigQuery billing export and labels to relate cost to groups. -&gt;&nbsp;Correct. Exporting daily usage and cost estimates automatically throughout the day to a BigQuery dataset is a good way of providing visibility to the finance department. Labels can then be used to group the costs based on team or cost center.</p><p><br></p><p>Use the <code>gcloud --auto-delete</code> flag on all persistent disks before stopping the VM. -&gt; Incorrect. The <code>--auto-delete</code> flag has no effect unless the instance is deleted. Stopping an instance does not delete the instance or the attached persistent disks.</p><p><br></p><p>Apply VM CPU utilization label and include it in the BigQuery billing export. -&gt;&nbsp;Incorrect. Labels are used to organize instances, not to monitor metrics.</p><p><br></p><p>Store all state in a Local SSD, snapshot the persistent disks, and terminate the VM. -&gt; Incorrect. The state stored in local SSDs will be lost when the instance is stopped.</p><p><br></p><p>https://cloud.google.com/compute/docs/instances/instance-life-cycle</p><p>https://cloud.google.com/compute/docs/disks/local-ssd#data_persistence</p><p>https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-disk-auto-delete#--auto-delete</p>",
                "answers": [
                    "<p>Use persistent disks to store the state. Start and stop the VM as needed.</p>",
                    "<p>Use BigQuery billing export and labels to relate cost to groups.</p>",
                    "<p>Use the <code>gcloud --auto-delete</code> flag on all persistent disks before stopping the VM.</p>",
                    "<p>Apply VM CPU utilization label and include it in the BigQuery billing export.</p>",
                    "<p>Store all state in a Local SSD, snapshot the persistent disks, and terminate the VM.</p>"
                ]
            },
            "correct_response": [
                "a",
                "b"
            ],
            "section": "",
            "question_plain": "In order to cut down expenses, the chief engineering officer mandated that all developers shift their development infrastructure resources from on-premises virtual machines (VMs) to Google Cloud. These resources undergo frequent start/stop occurrences throughout the day and necessitate the persistence of their state. As a cloud architect, you have been tasked with designing a plan for running a development environment on Google Cloud that also allows the finance department to have clear visibility into the costs involved. Which two steps should you take?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430218,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Refer to the TerramEarth case study for this question: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdf</p><p><br></p><p>At present, maintenance personnel for TerramEarth obtain interactive performance graphs covering the past 24 hours (24 hours x 60 minutes x 60 seconds = 86,400 events) by connecting their maintenance tablets directly to the vehicle. The support group aims to enable remote viewing of this data by support technicians, while ensuring minimal latency for graph loading. What approach should be taken to offer this functionality?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Perform queries on data indexed by <code>vehicle_id.timestamp</code> in Cloud Bigtable. -&gt; Correct. Cloud Bigtable is optimized for time-series data. It is cost-efficient, highly available, and low-latency. It scales well. Best of all, it is a managed service that does not require significant operations work to keep running.</p><p><br></p><p>Perform queries on data stored in a Cloud SQL. -&gt; Incorrect. Cloud SQL provides relational database services that are well-suited to OLTP workloads, but not storage and low-latency retrieval of time-series data.</p><p><br></p><p>Perform queries against data stored on daily partitioned BigQuery tables. -&gt; Incorrect. BigQuery is fast for wide-range queries, but it is not as well-optimized for narrow-range queries as Cloud Bigtable is. Latency will be an order of magnitude shorter with Cloud Bigtable for this use.</p><p><br></p><p>Execute queries against BigQuery with data stored in Cloud Storage via BigQuery federation. -&gt; Incorrect. The objective is to minimize latency, and although BigQuery federation offers tremendous flexibility, it doesn't perform as well as native BigQuery storage[2], and will have longer latency than Cloud Bigtable for narrow-range queries. </p><p><br></p><p>https://cloud.google.com/bigtable/docs/schema-design-time-series#time-series-cloud-bigtable</p>",
                "answers": [
                    "<p>Perform queries on data indexed by <code>vehicle_id.timestamp</code> in Cloud Bigtable.</p>",
                    "<p>Perform queries on data stored in a Cloud SQL.</p>",
                    "<p>Perform queries against data stored on daily partitioned BigQuery tables.</p>",
                    "<p>Execute queries against BigQuery with data stored in Cloud Storage via BigQuery federation.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Refer to the TerramEarth case study for this question: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdfAt present, maintenance personnel for TerramEarth obtain interactive performance graphs covering the past 24 hours (24 hours x 60 minutes x 60 seconds = 86,400 events) by connecting their maintenance tablets directly to the vehicle. The support group aims to enable remote viewing of this data by support technicians, while ensuring minimal latency for graph loading. What approach should be taken to offer this functionality?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430220,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdf</p><p><br></p><p>Mountkirk Games has implemented their latest backend on the Google Cloud Platform. As a cloud architect, you aim to establish a comprehensive testing procedure for new iterations of the backend prior to their public release. Your objective is to design a testing environment that can scale effectively while keeping costs under control. How should you approach the process design?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Create a scalable environment in GCP for simulating production load. -&gt; Correct. Simulating production load in GCP can scale in an economical way.</p><p><br></p><p>Use the existing infrastructure to test the GCP-based backend at scale. -&gt; Incorrect. One of the pain points about the existing infrastructure was precisely that the environment did not scale well.</p><p><br></p><p>Build stress tests into each component of your application and use resources from the already deployed production backend to simulate load. -&gt; Incorrect. It is a best practice to have a clear separation between test and production environments. Generating test load should not be done from a production environment.</p><p><br></p><p>Create a set of static environments in GCP to test different levels of load—for example, high, medium, and low. -&gt; Incorrect. Mountkirk Games wants the testing environment to scale as needed. Defining several static environments for specific levels of load goes against this requirement.</p>",
                "answers": [
                    "<p>Create a scalable environment in GCP for simulating production load.</p>",
                    "<p>Use the existing infrastructure to test the GCP-based backend at scale.</p>",
                    "<p>Build stress tests into each component of your application and use resources from the already deployed production backend to simulate load.</p>",
                    "<p>Create a set of static environments in GCP to test different levels of load—for example, high, medium, and low.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Refer to the Mountkirk Games case study for this question: https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdfMountkirk Games has implemented their latest backend on the Google Cloud Platform. As a cloud architect, you aim to establish a comprehensive testing procedure for new iterations of the backend prior to their public release. Your objective is to design a testing environment that can scale effectively while keeping costs under control. How should you approach the process design?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430222,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>In your customer support tool, all email and chat conversations are logged to Bigtable for storage and analysis purposes. However, to ensure data privacy and compliance, it is essential to sanitize this data by removing any Personally Identifiable Information (PII) or payment card information before it is initially stored. What approach would you recommend for accomplishing this task?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>De-identify the data with the Cloud Data Loss Prevention API. -&gt; Correct. The Cloud Data Loss Prevention (DLP) API is a tool that can be used to scan, de-identify, and classify sensitive data in real-time. It has pre-built detectors for PII such as social security numbers, credit card numbers, email addresses, phone numbers, etc., and it can automatically mask, tokenize, or delete the sensitive data before it is stored in Bigtable. By using the Cloud DLP API, the data can be sanitized in a scalable and efficient way, without having to write custom code to handle each type of sensitive data.</p><p><br></p><p>Hash all data using SHA256. -&gt; Incorrect. It would not be recommended because hashing is a one-way function, and the original data cannot be recovered from the hash value. This may be suitable for some use cases, but it would not allow for any meaningful analysis of the data.</p><p><br></p><p>Encrypt all data using elliptic curve cryptography. -&gt; Incorrect. It would not be recommended because encryption only protects the data while it is in transit or at rest, and it would not prevent authorized users from seeing the sensitive data if they have access to the decryption key.</p><p><br></p><p>Use regular expressions to find and redact phone numbers, email addresses, and credit card numbers. -&gt; Incorrect. It would be a manual and error-prone process and may not capture all sensitive data. Additionally, it would not be scalable for large amounts of data.</p><p><br></p><p>https://cloud.google.com/architecture/pci-dss-compliance-in-gcp#using_data_loss_prevention_api_to_sanitize_data</p><p>https://cloud.google.com/dlp</p>",
                "answers": [
                    "<p>De-identify the data with the Cloud Data Loss Prevention API.</p>",
                    "<p>Hash all data using SHA256.</p>",
                    "<p>Encrypt all data using elliptic curve cryptography.</p>",
                    "<p>Use regular expressions to find and redact phone numbers, email addresses, and credit card numbers.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "In your customer support tool, all email and chat conversations are logged to Bigtable for storage and analysis purposes. However, to ensure data privacy and compliance, it is essential to sanitize this data by removing any Personally Identifiable Information (PII) or payment card information before it is initially stored. What approach would you recommend for accomplishing this task?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430224,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are tasked with designing a complex deployment scenario involving a GKE cluster that needs to be PCI DSS-compliant. Which of the following considerations is most critical to ensure that the deployment meets the PCI DSS compliance requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement network segmentation and isolate the Cardholder Data Environment (CDE) using network policies. -&gt; Correct. PCI DSS requires network segmentation to isolate the Cardholder Data Environment (CDE) from other networks. Implementing network policies in GKE allows you to control the network traffic to and from your containerized applications and is a critical step in creating a PCI DSS-compliant deployment.</p><p><br></p><p>Enable Cloud Logging at all times. -&gt; Incorrect. While enabling Cloud Logging is a good practice for monitoring and debugging, it is not the most critical consideration for achieving PCI DSS compliance in a GKE cluster.</p><p><br></p><p>Use GKE Autopilot for managing the GKE cluster. -&gt; Incorrect. While GKE Autopilot can simplify the management of GKE clusters, it does not directly ensure PCI DSS compliance.</p><p><br></p><p>Deploy all applications in the cluster in the default namespace. -&gt; Incorrect. Deploying applications in the default namespace does not help ensure PCI DSS compliance. It's generally a good practice to separate different applications or different parts of an application into different namespaces.</p>",
                "answers": [
                    "<p>Implement network segmentation and isolate the Cardholder Data Environment (CDE) using network policies.</p>",
                    "<p>Enable Cloud Logging at all times.</p>",
                    "<p>Use GKE Autopilot for managing the GKE cluster.</p>",
                    "<p>Deploy all applications in the cluster in the default namespace.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are tasked with designing a complex deployment scenario involving a GKE cluster that needs to be PCI DSS-compliant. Which of the following considerations is most critical to ensure that the deployment meets the PCI DSS compliance requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430226,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>In the process of deploying an application on App Engine that requires integration with an on-premises database, you encounter a security constraint where the on-premises database cannot be accessed through the public Internet. What steps would you recommend taking to address this issue?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Deploy your application on App Engine Flexible environment and use Cloud VPN to limit access to the on-premises database. -&gt; Correct. App Engine Flexible environment allows for more flexibility in configuring networking and infrastructure compared to the Standard environment. By deploying the application on App Engine Flexible, you can set up a secure connection between the App Engine environment and the on-premises database using Cloud VPN. Cloud VPN provides secure, encrypted connectivity over the public Internet, allowing you to limit access to the on-premises database while ensuring data privacy and security.</p><p><br></p><p>Deploy your application on App Engine Standard environment and use App Engine firewall rules to limit access to the open on-premises database. -&gt;&nbsp;Incorrect. App Engine Standard environment does not provide the same level of networking flexibility as the Flexible environment. While you can use firewall rules in App Engine Standard, they are primarily designed to restrict access to the application itself rather than controlling access to an on-premises database.</p><p><br></p><p>Deploy your application on App Engine Standard environment and use Cloud SQL for SQL&nbsp;Server to limit access to the on-premises database. -&gt;&nbsp;Incorrect. Cloud SQL for SQL Server is a managed database service provided by Google Cloud, but it is not specifically designed to integrate with on-premises databases. It may not provide the necessary connectivity and access control mechanisms required to connect to an on-premises database securely.</p><p><br></p><p>Deploy your application on App Engine Flexible environment and use App Engine firewall rules to limit access to the on-premises database. -&gt;&nbsp;Incorrect. While deploying on App Engine Flexible environment allows for more flexibility, App Engine firewall rules are designed to control access to the application itself rather than providing secure connectivity to an on-premises database.</p><p><br></p><p>https://cloud.google.com/appengine/docs/flexible</p>",
                "answers": [
                    "<p>Deploy your application on App Engine Flexible environment and use Cloud VPN to limit access to the on-premises database.</p>",
                    "<p>Deploy your application on App Engine Standard environment and use App Engine firewall rules to limit access to the open on-premises database.</p>",
                    "<p>Deploy your application on App Engine Standard environment and use Cloud SQL for SQL&nbsp;Server to limit access to the on-premises database.</p>",
                    "<p>Deploy your application on App Engine Flexible environment and use App Engine firewall rules to limit access to the on-premises database.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "In the process of deploying an application on App Engine that requires integration with an on-premises database, you encounter a security constraint where the on-premises database cannot be accessed through the public Internet. What steps would you recommend taking to address this issue?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430228,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you've implemented an autoscaling group for a client's application on Google Cloud Compute Engine. The client now wants to increase the maximum number of instances in the autoscaling group to handle peak demand. Which of the following steps would you take to achieve this?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Increase the value of the <code>maxNumReplicas</code> parameter in the Autoscaler configuration. -&gt; Correct. To increase the maximum number of instances in an autoscaling group, you need to increase the value of the <code>maxNumReplicas</code> parameter in the Autoscaler configuration.</p><p><br></p><p>Increase the maximum number of vCPUs for the project. -&gt; Incorrect. Increasing the maximum number of vCPUs for the project increases the project's quota for vCPUs but doesn't directly affect the number of instances in the autoscaling group.</p><p><br></p><p>Increase the disk size for the instance template used by the autoscaling group. -&gt; Incorrect. Increasing the disk size for the instance template would provide more storage for each instance but would not increase the number of instances in the autoscaling group.</p><p><br></p><p>Increase the number of zones in the region where the autoscaling group is deployed.-&gt; Incorrect. Increasing the number of zones in the region would potentially increase the availability of the application but doesn't directly affect the number of instances in the autoscaling group.</p>",
                "answers": [
                    "<p>Increase the value of the <code>maxNumReplicas</code> parameter in the Autoscaler configuration.</p>",
                    "<p>Increase the maximum number of vCPUs for the project.</p>",
                    "<p>Increase the disk size for the instance template used by the autoscaling group.</p>",
                    "<p>Increase the number of zones in the region where the autoscaling group is deployed.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you've implemented an autoscaling group for a client's application on Google Cloud Compute Engine. The client now wants to increase the maximum number of instances in the autoscaling group to handle peak demand. Which of the following steps would you take to achieve this?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430230,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are tasked with storing sensitive data in Google Cloud Storage buckets. What is the most secure approach to protect this data while at rest in the buckets?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Customer-Managed Encryption Keys (CMEK) to encrypt the data. -&gt; Correct. Customer-Managed Encryption Keys (CMEK) is a feature where you can generate and control your encryption keys using Cloud Key Management Service (KMS). This provides the highest level of control over the encryption and decryption of your data stored in Cloud Storage.</p><p><br></p><p>Use Google Cloud IAM to restrict bucket access only to authorized service accounts. -&gt; Incorrect. hile restricting bucket access with IAM to authorized service accounts is a good practice, it does not protect the data while it's at rest in the bucket. Encryption is required for that purpose.</p><p><br></p><p>Enable Object Versioning in the Cloud Storage bucket. -&gt; Incorrect. Object Versioning helps protect your data against accidental deletions and overwrites, but it does not protect your data while it's at rest in the bucket.</p><p><br></p><p>Enable Public Access Prevention on the Cloud Storage bucket. -&gt; Incorrect. Enabling Public Access Prevention is a good practice to prevent public access to a bucket, but it does not protect your data while it's at rest in the bucket.</p>",
                "answers": [
                    "<p>Use Customer-Managed Encryption Keys (CMEK) to encrypt the data.</p>",
                    "<p>Use Google Cloud IAM to restrict bucket access only to authorized service accounts.</p>",
                    "<p>Enable Object Versioning in the Cloud Storage bucket.</p>",
                    "<p>Enable Public Access Prevention on the Cloud Storage bucket.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are tasked with storing sensitive data in Google Cloud Storage buckets. What is the most secure approach to protect this data while at rest in the buckets?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430232,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are employed at a multinational corporation with employees aged from 18 to 60. The corporation maintains extensive personal data, including health conditions, in BigQuery. In compliance with current privacy regulations, the corporation must be able to delete such data when requested by an employee. How can you architect a solution to accommodate such requests?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Assign a distinct identifier to every employee. When a deletion request is made, remove all rows from BigQuery that correspond to this identifier. -&gt;&nbsp;Correct. It aligns with the requirement of deleting personal data when requested by an employee while complying with privacy regulations. By assigning a distinct identifier to every employee, you can easily identify and locate the specific rows in BigQuery that correspond to the employee's data. When a deletion request is received, you can simply remove all rows from BigQuery that have the identifier associated with the employee making the request. This ensures that the personal data is effectively deleted from the database.</p><p><br></p><p>Establish a BigQuery view on top of the table that holds all data. In the event of a deletion request, omit the rows related to the employee's data from this view. Employ this view for all analytical operations instead of the base table. -&gt; Incorrect. It suggests using a view on top of the table to omit rows related to the employee's data. While this may seem like a viable option, it doesn't actually delete the data from the underlying table. The data remains stored in BigQuery, which could potentially be a privacy violation.</p><p><br></p><p>Implement a data archiving strategy to move the personal data to a separate storage system when a deletion request is made. -&gt; Incorrect. It suggests moving the personal data to a separate storage system instead of deleting it from BigQuery. The requirement is to delete the data when requested by an employee, not just move it to another location.</p><p><br></p><p>During the ingestion of new data into BigQuery, process the data via the Data Loss Prevention (DLP) API to detect any personal information. During the DLP scan, store the result in the Data Catalog. In response to a deletion request, consult the Data Catalog to identify the column containing personal information. -&gt; Incorrect. It suggests using the Data Loss Prevention (DLP) API and Data Catalog to identify and delete personal information. While this approach may be useful for detecting and cataloging personal information, it doesn't specifically address the requirement of deleting the data from BigQuery when requested by an employee.</p>",
                "answers": [
                    "<p>Assign a distinct identifier to every employee. When a deletion request is made, remove all rows from BigQuery that correspond to this identifier.</p>",
                    "<p>Establish a BigQuery view on top of the table that holds all data. In the event of a deletion request, omit the rows related to the employee's data from this view. Employ this view for all analytical operations instead of the base table.</p>",
                    "<p>Implement a data archiving strategy to move the personal data to a separate storage system when a deletion request is made.</p>",
                    "<p>During the ingestion of new data into BigQuery, process the data via the Data Loss Prevention (DLP) API to detect any personal information. During the DLP scan, store the result in the Data Catalog. In response to a deletion request, consult the Data Catalog to identify the column containing personal information.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are employed at a multinational corporation with employees aged from 18 to 60. The corporation maintains extensive personal data, including health conditions, in BigQuery. In compliance with current privacy regulations, the corporation must be able to delete such data when requested by an employee. How can you architect a solution to accommodate such requests?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430234,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are designing a complex deployment scenario for a client that involves using an unmanaged instance group with an active instance and a standby instance in different zones. You are required to use a regional persistent disk and a network load balancer in front of the instances. How do you ensure that in the event of a failure, traffic will be redirected to the standby instance?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement a health check and use a regional network load balancer. -&gt;&nbsp;Correct. In this scenario, it's important to set up a health check for the network load balancer to determine the health of the instances. When the active instance fails, the load balancer would detect it via the health check, and traffic would then be redirected to the standby instance in the different zone.</p><p><br></p><p>Use Google Cloud Armor for traffic redirection. -&gt; Incorrect. Cloud Armor is a distributed denial of service (DDoS) defense, and web application firewall (WAF) service. It is not responsible for traffic redirection in the context of load balancing.</p><p><br></p><p>Implement Cloud CDN for traffic redirection. -&gt; Incorrect. Cloud CDN (Content Delivery Network) uses Google's global edge network to serve content closer to users, improving performance. However, it is not used for traffic redirection in the event of instance failure.</p><p><br></p><p>Use Cloud NAT for traffic redirection. -&gt; Incorrect. Cloud NAT (Network Address Translation) enables instances without external IP addresses to access the internet, but it is not responsible for traffic redirection between active and standby instances.</p>",
                "answers": [
                    "<p>Implement a health check and use a regional network load balancer.</p>",
                    "<p>Use Google Cloud Armor for traffic redirection.</p>",
                    "<p>Implement Cloud CDN for traffic redirection.</p>",
                    "<p>Use Cloud NAT for traffic redirection.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are designing a complex deployment scenario for a client that involves using an unmanaged instance group with an active instance and a standby instance in different zones. You are required to use a regional persistent disk and a network load balancer in front of the instances. How do you ensure that in the event of a failure, traffic will be redirected to the standby instance?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430236,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you've been asked to perform a \"lift-and-shift\" migration of a Linux RHEL virtual machine from an on-premises data center to Google Cloud. Which Google Cloud service would be most appropriate to facilitate this migration?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Google Cloud Migrate for Compute Engine -&gt; Correct. Google Cloud Migrate for Compute Engine is a tool designed to assist with migrating physical servers and virtual machines (from on-premises or other clouds) into Google Compute Engine. It supports different types of operating systems, including Linux RHEL.</p><p><br></p><p>Cloud Dataflow -&gt; Incorrect. Cloud Dataflow is a service for stream and batch processing of data. It is not specifically designed for migrating virtual machines.</p><p><br></p><p>Cloud Storage -&gt; Incorrect. Cloud Storage is used for storing and retrieving data, not for migrating virtual machines.</p><p><br></p><p>Cloud Pub/Sub -&gt; Incorrect. Cloud Pub/Sub is a messaging service designed for building event-driven systems and real-time data processing, rather than facilitating VM migrations.</p>",
                "answers": [
                    "<p>Google Cloud Migrate for Compute Engine</p>",
                    "<p>Cloud Dataflow</p>",
                    "<p>Cloud Storage</p>",
                    "<p>Cloud Pub/Sub</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you've been asked to perform a \"lift-and-shift\" migration of a Linux RHEL virtual machine from an on-premises data center to Google Cloud. Which Google Cloud service would be most appropriate to facilitate this migration?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430238,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are architecting a cloud-based solution for a financial institution that handles sensitive customer data. The solution requires encryption at various levels to ensure data security. Which encryption techniques should you consider for this scenario?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Database-level encryption for data at rest and transport layer encryption for data in transit -&gt;&nbsp;Correct. Database-level encryption encrypts data at rest within the database, while transport layer encryption (such as SSL/TLS) secures data during transit. This combination ensures data security at both stages and is a recommended option.</p><p><br></p><p>Homomorphic encryption for data in transit and data at rest -&gt;&nbsp;Incorrect. Homomorphic encryption allows computations to be performed on encrypted data without decrypting it, but it is computationally intensive and may not be practical for data in transit or large-scale data storage.</p><p><br></p><p>Asymmetric encryption for data at rest and hashing for data in transit -&gt; Incorrect. Asymmetric encryption uses public and private key pairs for encryption, but it is not commonly used for encrypting large amounts of data at rest. Hashing is a one-way function that generates a fixed-size output based on input data and is not suitable for data in transit encryption.</p><p><br></p><p>Tokenization for data at rest and symmetric encryption for data in transit -&gt; Incorrect. Tokenization replaces sensitive data with randomly generated tokens but does not provide encryption for data at rest. Symmetric encryption uses a single key to encrypt and decrypt data and is suitable for data in transit. However, this combination does not fully address the encryption requirements for data at rest.</p>",
                "answers": [
                    "<p>Database-level encryption for data at rest and transport layer encryption for data in transit</p>",
                    "<p>Homomorphic encryption for data in transit and data at rest</p>",
                    "<p>Asymmetric encryption for data at rest and hashing for data in transit</p>",
                    "<p>Tokenization for data at rest and symmetric encryption for data in transit</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are architecting a cloud-based solution for a financial institution that handles sensitive customer data. The solution requires encryption at various levels to ensure data security. Which encryption techniques should you consider for this scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430240,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your customer is planning to run a stateful, multi-tier application in Google Cloud. The customer has the following requirements:</p><ul><li><p>the application must store its data in a highly available, highly durable transactional storage system</p></li><li><p>the storage system must be able to handle random read/write workloads and scale dynamically</p></li><li><p>the storage system must be able to recover from failures quickly</p></li><li><p>the storage system must be transparent to the application, which should not have to modify its code to use it</p></li></ul><p><br></p><p>Which Google Cloud service should you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Spanner -&gt; Correct. Cloud Spanner is a fully managed, mission-critical, relational database service that offers transactional consistency at global scale, automatic, synchronous replication for high availability, and support for two SQL dialects: Google Standard SQL (ANSI 2011 with extensions) and PostgreSQL.</p><p><br></p><p>Cloud SQL -&gt; Incorrect. Cloud SQL is a managed relational database service that provides high availability and durability, but it may not scale horizontally as well as Cloud Spanner.</p><p><br></p><p>Cloud Datastore -&gt; Incorrect. Cloud Datastore is a NoSQL document database that can handle large amounts of structured and semi-structured data, but it may not provide strong consistency or transactional semantics.</p><p><br></p><p>Cloud Bigtable -&gt;&nbsp;Incorrect. Cloud Bigtable is a high-performance, NoSQL columnar database that can handle large-scale analytical and operational workloads, but it may not provide strong consistency or transactional semantics either.</p><p><br></p><p>https://cloud.google.com/spanner/docs</p>",
                "answers": [
                    "<p>Cloud SQL</p>",
                    "<p>Cloud Spanner</p>",
                    "<p>Cloud Datastore</p>",
                    "<p>Cloud Bigtable</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "Your customer is planning to run a stateful, multi-tier application in Google Cloud. The customer has the following requirements:the application must store its data in a highly available, highly durable transactional storage systemthe storage system must be able to handle random read/write workloads and scale dynamicallythe storage system must be able to recover from failures quicklythe storage system must be transparent to the application, which should not have to modify its code to use itWhich Google Cloud service should you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430242,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are the lead cloud architect for a global company that is migrating its data processing workloads to Google Cloud Platform (GCP). The company's primary requirements include scalability, cost-effectiveness, and the ability to process and analyze data in real-time. You need to choose the most appropriate GCP service to meet these requirements. Which of the following would you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Dataflow -&gt; Correct. Dataflow is a fully managed service for stream and batch data processing. It is ideal for real-time data analytics and processing as it scales automatically, handles the provisioning of resources, and optimizes for performance and cost. Dataflow integrates seamlessly with other GCP data analytics products, offering a comprehensive solution for complex data processing scenarios.</p><p><br></p><p>Compute Engine -&gt;&nbsp;Incorrect. Compute Engine provides scalable and customizable VMs. However, it requires manual scaling and management, making it less suitable for real-time data processing without extensive automation.</p><p><br></p><p>App Engine -&gt;&nbsp;Incorrect. App Engine automatically scales based on demand and is great for web applications but might not offer the granular control needed for complex real-time data processing and analytics tasks.</p><p><br></p><p>Cloud Functions -&gt;&nbsp;Incorrect. Cloud Functions is a serverless execution environment for building and connecting cloud services. While it is suitable for event-driven applications and can scale automatically, it might not be the best fit for heavy data processing tasks due to execution time limits and potential cost at scale.</p>",
                "answers": [
                    "<p>Dataflow </p>",
                    "<p>Compute Engine</p>",
                    "<p>App Engine</p>",
                    "<p>Cloud Functions</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are the lead cloud architect for a global company that is migrating its data processing workloads to Google Cloud Platform (GCP). The company's primary requirements include scalability, cost-effectiveness, and the ability to process and analyze data in real-time. You need to choose the most appropriate GCP service to meet these requirements. Which of the following would you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430244,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your customer wants to deploy a highly available and scalable web application in Google Cloud. The customer has the following requirements:</p><ul><li><p>the application must be able to handle incoming traffic spikes and scale dynamically</p></li><li><p>the application must be highly available and recover from failures automatically</p></li><li><p>the application must be easy to deploy and manage</p></li><li><p>the application must be cost-effective</p></li></ul><p><br></p><p>Which Google Cloud service should you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>App Engine -&gt;&nbsp;Correct. App Engine is a fully managed, serverless platform for developing and hosting web applications at scale. You can choose from several popular languages, libraries, and frameworks to develop your apps, and then let App Engine take care of provisioning servers and scaling your app instances based on demand.</p><p><br></p><p>Google Kubernetes Engine -&gt; Incorrect. While Google Kubernetes Engine is a highly scalable and reliable service, it may not be the most cost-effective solution to meet all of the customer's requirements.</p><p><br></p><p>Cloud Load Balancer -&gt; Incorrect. While Cloud Load Balancer is a useful service for load balancing incoming traffic, it may not be the best option for meeting all of the customer's requirements.</p><p><br></p><p>Cloud Functions -&gt; Incorrect. While Cloud Functions are a useful service for event-driven computing and small workloads, they may not be the best option for meeting all of the customer's requirements for a highly available and scalable web application.</p><p><br></p><p>https://cloud.google.com/appengine/docs</p>",
                "answers": [
                    "<p>Cloud Load Balancer</p>",
                    "<p>Cloud Functions</p>",
                    "<p>Google Kubernetes Engine</p>",
                    "<p>App Engine</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "Your customer wants to deploy a highly available and scalable web application in Google Cloud. The customer has the following requirements:the application must be able to handle incoming traffic spikes and scale dynamicallythe application must be highly available and recover from failures automaticallythe application must be easy to deploy and managethe application must be cost-effectiveWhich Google Cloud service should you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430246,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your customer is planning to store a large amount of log data in Google Cloud. The customer has the following requirements:</p><ul><li><p>the data must be easily searchable and filterable</p></li><li><p>the data must be stored in a highly durable and scalable storage system</p></li><li><p>the data must be accessible by multiple teams within the customer's organization</p></li><li><p>the data must be cost-effective</p></li></ul><p><br></p><p>Which Google Cloud service should you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Cloud Logging -&gt;&nbsp;Correct. Cloud Logging is a fully managed log management and analysis service that provides centralized storage and analysis capabilities for log data. It offers powerful search, filter, and analysis capabilities, allowing easy querying and visualization of log data. It is highly scalable, durable, and designed specifically for handling log data. Multiple teams within the organization can access and analyze the log data, ensuring data accessibility. Cloud Logging is cost-effective, as you only pay for the storage and analysis usage.</p><p><br></p><p>Cloud Storage -&gt;&nbsp;Incorrect. While Cloud Storage provides highly durable and scalable storage, it is primarily designed for storing objects like files and may not be the most suitable choice for storing and searching large amounts of log data. It may not offer the same level of search and filter capabilities as Cloud Logging.</p><p><br></p><p>Cloud Bigtable -&gt;&nbsp;Incorrect. Cloud Bigtable is a NoSQL database optimized for high-performance, low-latency read and write operations on large datasets. While it can handle large-scale data storage, it may not be the most efficient solution for storing and searching log data with search and filter capabilities.</p><p><br></p><p>Cloud SQL -&gt;&nbsp;Incorrect. Cloud SQL is a fully managed relational database service. While it provides excellent support for structured data and SQL queries, it may not be the most suitable choice for storing and searching log data, especially when dealing with large volumes and specific search requirements.</p><p><br></p><p>https://cloud.google.com/logging/docs</p>",
                "answers": [
                    "<p>Cloud Storage</p>",
                    "<p>Cloud Bigtable</p>",
                    "<p>Cloud SQL</p>",
                    "<p>Cloud Logging</p>"
                ]
            },
            "correct_response": [
                "d"
            ],
            "section": "",
            "question_plain": "Your customer is planning to store a large amount of log data in Google Cloud. The customer has the following requirements:the data must be easily searchable and filterablethe data must be stored in a highly durable and scalable storage systemthe data must be accessible by multiple teams within the customer's organizationthe data must be cost-effectiveWhich Google Cloud service should you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430248,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A global automotive manufacturer is planning to deploy a new car platform that will collect and process data from vehicles in real-time. The platform must be able to handle a large volume of incoming data, support real-time data processing and analytics, and provide secure and reliable access to data for internal and external stakeholders. The platform must also be scalable and flexible to accommodate future growth and changes in business requirements. Which of the following options would be the most effective approach to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Pub/Sub to collect data from vehicles and Cloud Dataflow to process and analyze the data. Store the data in Cloud Bigtable and provide access to stakeholders using Cloud Dataproc and Cloud Data Studio. -&gt;&nbsp;Correct. Cloud Pub/Sub is a messaging service that can handle a large volume of incoming data from vehicles. Cloud Dataflow allows for real-time data processing and analytics. Cloud Bigtable is a scalable NoSQL database that can handle large amounts of data and provide high-performance access. Cloud Dataproc is a managed Spark and Hadoop service that can be used for further data processing and analysis. Cloud Data Studio provides a platform for visualizing and sharing data insights with stakeholders. This approach covers all the requirements stated in the question.</p><p><br></p><p>Use Cloud IoT Core to collect data from vehicles and Cloud Functions to process and analyze the data. Store the data in Cloud Firestore and provide access to stakeholders using Cloud Storage and Cloud BigQuery. -&gt;&nbsp;Incorrect. Cloud Firestore is a document database and may not be the most suitable choice for storing large volumes of incoming data. Cloud Storage and Cloud BigQuery can provide access to stakeholders, but Cloud Dataproc and Cloud Data Studio are better suited for real-time data processing and analysis.</p><p><br></p><p>Use Cloud IoT Core to collect data from vehicles and Cloud Dataproc to process and analyze the data. Store the data in Cloud Bigtable and provide access to stakeholders using Cloud Data Studio and Cloud Pub/Sub. -&gt;&nbsp;Incorrect. Cloud Bigtable can handle large amounts of data, but Cloud Data Studio and Cloud Pub/Sub are not the most suitable choices for providing access to stakeholders. Cloud Data Studio is primarily used for visualizing data, while Cloud Pub/Sub is a messaging service and may not be optimized for direct stakeholder access.</p><p><br></p><p>Use Cloud IoT Core to collect data from vehicles and Cloud Dataflow to process and analyze the data. Store the data in Cloud Datastore and provide access to stakeholders using Cloud BigQuery and Cloud Storage. -&gt;&nbsp;Incorrect. Cloud Datastore is a NoSQL document database and may not be the best choice for storing large volumes of data. Cloud BigQuery and Cloud Storage can provide access to stakeholders, but Cloud Dataproc and Cloud Data Studio are better suited for real-time data processing and analysis.</p>",
                "answers": [
                    "<p>Use Cloud Pub/Sub to collect data from vehicles and Cloud Dataflow to process and analyze the data. Store the data in Cloud Bigtable and provide access to stakeholders using Cloud Dataproc and Cloud Data Studio.</p>",
                    "<p>Use Cloud IoT Core to collect data from vehicles and Cloud Functions to process and analyze the data. Store the data in Cloud Firestore and provide access to stakeholders using Cloud Storage and Cloud BigQuery.</p>",
                    "<p>Use Cloud IoT Core to collect data from vehicles and Cloud Dataproc to process and analyze the data. Store the data in Cloud Bigtable and provide access to stakeholders using Cloud Data Studio and Cloud Pub/Sub.</p>",
                    "<p>Use Cloud IoT Core to collect data from vehicles and Cloud Dataflow to process and analyze the data. Store the data in Cloud Datastore and provide access to stakeholders using Cloud BigQuery and Cloud Storage.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A global automotive manufacturer is planning to deploy a new car platform that will collect and process data from vehicles in real-time. The platform must be able to handle a large volume of incoming data, support real-time data processing and analytics, and provide secure and reliable access to data for internal and external stakeholders. The platform must also be scalable and flexible to accommodate future growth and changes in business requirements. Which of the following options would be the most effective approach to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430250,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A large e-commerce company is planning to migrate its existing monolithic application to a microservices architecture on GCP. The company wants to ensure that the microservices are scalable, highly available, and can handle sudden spikes in traffic. The company also wants to minimize downtime during the migration and ensure that the data remains secure and compliant with industry regulations. Which of the following options would be the most effective approach to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Google Kubernetes Engine to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud SQL to store data and Cloud IAM to control access to the data. -&gt;&nbsp;Correct. Google Kubernetes Engine (GKE) is a managed Kubernetes environment that provides scalable and highly available microservices. Kubernetes has built-in features for scaling, load balancing, and high availability, making it an ideal choice for microservices. Using GKE will also help to minimize downtime during migration as the application can be gradually migrated to the new architecture without disruption to the end-users. Cloud Load Balancer is a global load balancing service that distributes traffic across multiple regions, making it an ideal choice for handling sudden spikes in traffic. Cloud Load Balancer can route traffic to the closest instance of the microservice, ensuring low latency and high performance. Cloud SQL is a managed database service that supports MySQL and PostgreSQL. It provides high availability and automatic replication, making it an ideal choice for storing data for microservices. Cloud SQL also provides features such as automated backups and encryption, ensuring data security and compliance with industry regulations. Cloud IAM is a cloud identity and access management service that enables administrators to manage access to resources in the cloud. Using Cloud IAM will ensure that access to data is controlled and that compliance with industry regulations is maintained.</p><p><br></p><p>Use App Engine to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud Firestore to store data and Cloud IAM to control access to the data. -&gt; Incorrect. It is incorrect because App Engine is a Platform-as-a-Service (PaaS) that does not provide as much control as Kubernetes, making it less suitable for microservices. Cloud Firestore is a NoSQL document database that does not provide the same level of transactional consistency as Cloud SQL, making it less suitable for storing data.</p><p><br></p><p>Use Cloud Functions to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud Bigtable to store data and Cloud IAM to control access to the data. -&gt; Incorrect. It is incorrect because Cloud Functions is a Function-as-a-Service (FaaS) that is designed for small, event-driven functions, and may not be suitable for a monolithic application to microservices migration. Cloud Bigtable is a NoSQL database that is designed for high-performance, low-latency workloads, but it may not provide the same level of consistency as Cloud SQL.</p><p><br></p><p>Use Compute Engine to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud Datastore to store data and Cloud KMS to control access to the data. -&gt; Incorrect. It is incorrect because Compute Engine requires more manual management and does not provide the built-in features for scaling and high availability that Kubernetes provides. Cloud Datastore is a NoSQL document database that does not provide the same level of consistency as Cloud SQL. While Cloud KMS can control access to data, it does not provide the same level of fine-grained access control as Cloud IAM.</p>",
                "answers": [
                    "<p>Use Google Kubernetes Engine to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud SQL to store data and Cloud IAM to control access to the data.</p>",
                    "<p>Use App Engine to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud Firestore to store data and Cloud IAM to control access to the data.</p>",
                    "<p>Use Cloud Functions to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud Bigtable to store data and Cloud IAM to control access to the data.</p>",
                    "<p>Use Compute Engine to deploy the microservices and Cloud Load Balancer to distribute traffic. Use Cloud Datastore to store data and Cloud KMS to control access to the data.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "A large e-commerce company is planning to migrate its existing monolithic application to a microservices architecture on GCP. The company wants to ensure that the microservices are scalable, highly available, and can handle sudden spikes in traffic. The company also wants to minimize downtime during the migration and ensure that the data remains secure and compliant with industry regulations. Which of the following options would be the most effective approach to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430252,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is operating a multi-tier web application on Google Cloud. The frontend servers should be globally available and scale automatically to handle traffic, while the backend servers should be accessed only by the frontend servers and internal systems. The data transfer between backend and frontend servers needs to be encrypted. As a cloud architect, how should you design your network to fulfill these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Deploy the frontend on App Engine and backend on Compute Engine within a shared VPC, and use SSL/TLS for encrypted communication. -&gt; Correct. App Engine provides global availability and auto-scaling capabilities, while Compute Engine can be used for backend servers. Using a shared VPC can isolate backend servers, and SSL/TLS can secure the data transfer between the frontend and backend servers.</p><p><br></p><p>Deploy both frontend and backend servers on Compute Engine instances within the same subnet. -&gt;&nbsp;Incorrect. This option doesn't provide the required isolation between the frontend and backend servers, and doesn't ensure global availability or auto-scaling for the frontend servers.</p><p><br></p><p>Use two separate VPCs for frontend and backend servers, and connect them using VPN. -&gt;&nbsp;Incorrect. While this approach separates the frontend and backend servers, the VPN adds unnecessary complexity for inter-service communication and doesn't provide auto-scaling for the frontend servers.</p><p><br></p><p>Use Kubernetes Engine for both frontend and backend servers, and segregate them using network policies. -&gt;&nbsp;Incorrect. While Kubernetes Engine and network policies can provide a level of segregation, this option doesn't provide global availability without additional configuration and doesn't inherently encrypt communication between frontend and backend servers.</p>",
                "answers": [
                    "<p>Deploy the frontend on App Engine and backend on Compute Engine within a shared VPC, and use SSL/TLS for encrypted communication.</p>",
                    "<p>Deploy both frontend and backend servers on Compute Engine instances within the same subnet.</p>",
                    "<p>Use two separate VPCs for frontend and backend servers, and connect them using VPN.</p>",
                    "<p>Use Kubernetes Engine for both frontend and backend servers, and segregate them using network policies.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company is operating a multi-tier web application on Google Cloud. The frontend servers should be globally available and scale automatically to handle traffic, while the backend servers should be accessed only by the frontend servers and internal systems. The data transfer between backend and frontend servers needs to be encrypted. As a cloud architect, how should you design your network to fulfill these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430254,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your organization has a mission-critical application running on Google Kubernetes Engine (GKE). You have to push frequent updates to this application without causing any downtime or disrupting the user experience. As a cloud architect, which deployment strategy would you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implement a rolling update strategy with readiness and liveness probes. -&gt;&nbsp;Correct. A rolling update strategy will incrementally update the Pods with the new application version, while readiness and liveness probes will ensure that the new Pods are ready to serve traffic and are healthy before they replace the old ones. This leads to no downtime during the update.</p><p><br></p><p>Use the recreate deployment strategy for updating the application. -&gt;&nbsp;Incorrect. This strategy kills all existing pods before creating new ones. This would result in downtime which is not desirable for a mission-critical application.</p><p><br></p><p>Use a blue-green deployment strategy with a manual switch. -&gt;&nbsp;Incorrect. Blue-green deployments involve running two separate environments: Blue (the currently running version) and Green (the new version). Once the Green environment is ready and tested, traffic is switched from Blue to Green. While this strategy does allow for easy rollback and zero-downtime deployments, the manual switch could introduce a chance of human error. Moreover, maintaining two environments might be more resource-intensive.</p><p><br></p><p>Implement a canary deployment strategy. -&gt;&nbsp;Incorrect. Canary deployments release the new version to a small subset of users before rolling it out to everyone. This is useful for testing new features with a smaller audience before a full release. While canary deployments can be useful in many scenarios, they don't guarantee zero-downtime by themselves and can be more complex to set up and manage.</p>",
                "answers": [
                    "<p>Implement a rolling update strategy with readiness and liveness probes.</p>",
                    "<p>Use the recreate deployment strategy for updating the application.</p>",
                    "<p>Use a blue-green deployment strategy with a manual switch.</p>",
                    "<p>Implement a canary deployment strategy.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your organization has a mission-critical application running on Google Kubernetes Engine (GKE). You have to push frequent updates to this application without causing any downtime or disrupting the user experience. As a cloud architect, which deployment strategy would you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430256,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You're designing a real-time analytics solution for a global company. The solution must ingest clickstream data from a worldwide user base in real-time, process it, and make it available for real-time querying. Which solution should you recommend?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Use Cloud Pub/Sub for data ingestion, Cloud Dataflow for processing, and BigQuery for real-time querying. -&gt;&nbsp;Correct. This combination of services will effectively handle high-throughput data ingestion (Cloud Pub/Sub), real-time data processing (Cloud Dataflow), and provide the ability to run real-time SQL-like queries on the processed data (BigQuery).</p><p><br></p><p>Use Cloud Datastore for real-time data ingestion and querying. -&gt;&nbsp;Incorrect. Cloud Datastore is not designed for high-throughput real-time data ingestion or real-time analytics querying. It's more suited for web and mobile applications requiring a NoSQL database.</p><p><br></p><p>Use Cloud SQL for real-time data ingestion and querying. -&gt;&nbsp;Incorrect. Cloud SQL is not designed for high-throughput real-time data ingestion or real-time analytics. It's a fully managed relational database service for MySQL, PostgreSQL, and SQL Server.</p><p><br></p><p>Store the data in Cloud Storage, then process it using Dataproc, and store the results in BigTable for real-time querying. -&gt;&nbsp;Incorrect. This solution doesn't support real-time data ingestion and processing as data needs to be stored first before processing, which doesn't meet the real-time requirement.</p>",
                "answers": [
                    "<p>Use Cloud Pub/Sub for data ingestion, Cloud Dataflow for processing, and BigQuery for real-time querying.</p>",
                    "<p>Use Cloud Datastore for real-time data ingestion and querying.</p>",
                    "<p>Use Cloud SQL for real-time data ingestion and querying.</p>",
                    "<p>Store the data in Cloud Storage, then process it using Dataproc, and store the results in BigTable for real-time querying.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You're designing a real-time analytics solution for a global company. The solution must ingest clickstream data from a worldwide user base in real-time, process it, and make it available for real-time querying. Which solution should you recommend?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430258,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A large financial institution is migrating their trading platform to the cloud to handle increased volume and improve performance. The platform must meet the following requirements:</p><ul><li><p>provide real-time access to market data and trade execution capabilities</p></li><li><p>ensure data privacy and compliance with regulatory requirements</p></li><li><p>have the ability to handle peak loads during trading hours while minimizing costs</p></li><li><p>ensure high availability and disaster recovery</p></li><li><p>enable auditing and compliance reporting</p></li></ul><p><br></p><p>Which solution would you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implementing a managed solution using Cloud Bigtable for real-time market data and trade execution, Cloud Storage for auditing and reporting, and Cloud Pub/Sub for real-time messaging. -&gt; Correct. Cloud Bigtable provides a high-performance, low-latency database solution for real-time market data and trade execution, while Cloud Storage can be used for auditing and compliance reporting, and Cloud Pub/Sub provides real-time messaging capabilities. This solution is designed to handle high loads during trading hours while minimizing costs and ensuring high availability and disaster recovery. It also provides data privacy and compliance with regulatory requirements, making it an appropriate choice for a large financial institution.</p><p><br></p><p>Implementing a custom-built solution using Compute Engine instances, Cloud Storage, and Cloud Pub/Sub. -&gt; Incorrect. It only addresses some of the requirements while missing some critical aspects. Compute Engine instances and Cloud Storage can handle peak loads, but they may not provide real-time access to market data and trade execution capabilities. Additionally, custom-built solutions can be more difficult to manage and may not offer the same level of disaster recovery as managed solutions.</p><p><br></p><p>Implementing a hybrid solution using Cloud Bigtable for real-time market data and trade execution, Cloud SQL for auditing and reporting, and Cloud VPN to connect with on-premises storage. -&gt; Incorrect. It involves a hybrid solution that uses on-premises storage in addition to cloud services. While this solution may be useful in certain cases, it does not necessarily meet all the requirements listed in the question. Using Cloud SQL for auditing and reporting may not be the most cost-effective solution, and using a VPN to connect to on-premises storage may not be the most efficient or scalable solution for handling peak loads during trading hours.</p><p><br></p><p>Implementing a serverless solution using Cloud Functions for real-time market data and trade execution, BigQuery for auditing and reporting, and Cloud Pub/Sub for real-time messaging. -&gt; Incorrect. Cloud Functions have a maximum execution time limit, which might not be suitable for long-running tasks or complex calculations often required in financial trading systems. While serverless solutions like Cloud Functions can be cost-effective for sporadic or event-driven workloads, they may not be the most cost-efficient choice for sustained workloads, such as those required for trading platforms that must handle peak loads during trading hours.</p>",
                "answers": [
                    "<p>Implementing a custom-built solution using Compute Engine instances, Cloud Storage, and Cloud Pub/Sub.</p>",
                    "<p>Implementing a managed solution using Cloud Bigtable for real-time market data and trade execution, Cloud Storage for auditing and reporting, and Cloud Pub/Sub for real-time messaging.</p>",
                    "<p>Implementing a hybrid solution using Cloud Bigtable for real-time market data and trade execution, Cloud SQL for auditing and reporting, and Cloud VPN to connect with on-premises storage.</p>",
                    "<p>Implementing a serverless solution using Cloud Functions for real-time market data and trade execution, BigQuery for auditing and reporting, and Cloud Pub/Sub for real-time messaging.</p>"
                ]
            },
            "correct_response": [
                "b"
            ],
            "section": "",
            "question_plain": "A large financial institution is migrating their trading platform to the cloud to handle increased volume and improve performance. The platform must meet the following requirements:provide real-time access to market data and trade execution capabilitiesensure data privacy and compliance with regulatory requirementshave the ability to handle peak loads during trading hours while minimizing costsensure high availability and disaster recoveryenable auditing and compliance reportingWhich solution would you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430260,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>A large multinational retail company is looking to improve their customer experience by providing real-time, personalized recommendations to customers using a recommendation engine. The solution must meet the following requirements:</p><ul><li><p>handle millions of requests per second with low latency</p></li><li><p>store and process petabytes of customer data in real-time</p></li><li><p>provide the ability to easily update and test recommendation algorithms</p></li><li><p>ensure data privacy and security</p></li><li><p>minimize costs while still providing high performance</p></li></ul><p><br></p><p>Which solution would you recommend to meet these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Implementing a managed solution using Vertex AI Platform for recommendation algorithms, Cloud Dataproc for data processing, and Cloud Bigtable for data storage.-&gt; Correct. Vertex AI Platform provides managed services for building and deploying machine learning models, making it suitable for recommendation algorithms. Cloud Dataproc is a managed Apache Spark and Hadoop service that can handle large-scale data processing with low latency. Cloud Bigtable is a highly scalable NoSQL database that can store and process petabytes of customer data in real-time. This managed solution provides the necessary scalability, real-time processing capabilities, and storage capacity to meet the requirements.</p><p><br></p><p>Implementing a custom-built solution using Cloud Dataflow for data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling. -&gt;&nbsp;Incorrect. While this answer choice includes scalable components like Cloud Dataflow and GKE, it requires custom development and management. The custom-built solution may require additional effort for algorithm development, deployment, and scaling, which could be more complex and time-consuming than other managed solutions.</p><p><br></p><p>Implementing a serverless solution using Cloud Functions for recommendation algorithms, Cloud Pub/Sub for real-time data processing, and BigQuery for data storage. -&gt;&nbsp;Incorrect. This answer choice utilizes serverless components, but it may not be the best fit for handling millions of requests per second with low latency. Cloud Functions and Cloud Pub/Sub have certain limitations in terms of scalability and real-time processing performance. While BigQuery can handle large-scale data storage, it may not be optimized for real-time data processing needs.</p><p><br></p><p>Implementing a hybrid solution using Cloud Dataflow for data processing, Cloud Storage for data storage, and Compute Engine for recommendation algorithms deployment and scaling. -&gt;&nbsp;Incorrect. While this answer choice includes scalable components like Cloud Dataflow and Compute Engine, it requires managing infrastructure with Compute Engine for recommendation algorithms deployment and scaling. This hybrid solution introduces more complexity compared to a fully managed solution and may not be the most cost-effective option while still providing high performance.</p>",
                "answers": [
                    "<p>Implementing a custom-built solution using Cloud Dataflow for data processing, Bigtable for data storage, and Google Kubernetes Engine (GKE) for deployment and scaling.</p>",
                    "<p>Implementing a serverless solution using Cloud Functions for recommendation algorithms, Cloud Pub/Sub for real-time data processing, and BigQuery for data storage.</p>",
                    "<p>Implementing a managed solution using Vertex AI Platform for recommendation algorithms, Cloud Dataproc for data processing, and Cloud Bigtable for data storage.</p>",
                    "<p>Implementing a hybrid solution using Cloud Dataflow for data processing, Cloud Storage for data storage, and Compute Engine for recommendation algorithms deployment and scaling.</p>"
                ]
            },
            "correct_response": [
                "c"
            ],
            "section": "",
            "question_plain": "A large multinational retail company is looking to improve their customer experience by providing real-time, personalized recommendations to customers using a recommendation engine. The solution must meet the following requirements:handle millions of requests per second with low latencystore and process petabytes of customer data in real-timeprovide the ability to easily update and test recommendation algorithmsensure data privacy and securityminimize costs while still providing high performanceWhich solution would you recommend to meet these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430262,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>Your company is deploying a critical application on Google Cloud Platform that requires high availability and a robust disaster recovery strategy. The application will handle large-scale, global user traffic and store sensitive data. What is the best approach to design the application’s infrastructure for these requirements?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Deploy the application across multiple GCP regions, use Multi-regional storage for data, and implement a global load balancer. -&gt;&nbsp;Correct. This design ensures high availability by distributing the application across various regions, reducing the risk of regional outages. Multi-regional storage provides redundancy for data, and a global load balancer efficiently distributes user traffic.</p><p><br></p><p>Host the application in a single region using standard storage and manual scaling to handle traffic spikes. -&gt;&nbsp;Incorrect. This approach does not offer high availability or effective disaster recovery, as it relies on a single region and lacks automated scalability.</p><p><br></p><p>Utilize a single-region, single-zone approach with regular data backups to a different region. -&gt;&nbsp;Incorrect. While backups are good, relying on a single zone and region can lead to significant downtime during outages, which is not ideal for critical applications.</p><p><br></p><p>Use a hybrid cloud approach, hosting the application on GCP and another cloud provider simultaneously. -&gt;&nbsp;Incorrect. While this might offer redundancy, it can lead to increased complexity and potential issues with data consistency and latency.</p>",
                "answers": [
                    "<p>Deploy the application across multiple GCP regions, use Multi-regional storage for data, and implement a global load balancer.</p>",
                    "<p>Host the application in a single region using standard storage and manual scaling to handle traffic spikes.</p>",
                    "<p>Utilize a single-region, single-zone approach with regular data backups to a different region.</p>",
                    "<p>Use a hybrid cloud approach, hosting the application on GCP and another cloud provider simultaneously.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "Your company is deploying a critical application on Google Cloud Platform that requires high availability and a robust disaster recovery strategy. The application will handle large-scale, global user traffic and store sensitive data. What is the best approach to design the application’s infrastructure for these requirements?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430264,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>You are a cloud architect tasked with planning the migration of an enterprise's legacy systems to Google Cloud Platform (GCP). The enterprise has a mix of on-premises databases, applications, and storage systems that need to be moved to the cloud. Your plan needs to minimize downtime, ensure data integrity, and provide scalability. What would be the most effective migration plan for this scenario?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>Conduct a pilot migration with a small, non-critical system before fully migrating. -&gt; Correct. A pilot migration allows for testing the migration process, addressing issues on a smaller scale, and building confidence for the full migration.</p><p><br></p><p>Migrate all systems simultaneously to minimize total migration time. -&gt; Incorrect. Migrating all systems simultaneously can lead to significant risks, including potential downtime and data loss.</p><p><br></p><p>Move only the databases to GCP and keep the applications on-premises. -&gt; Incorrect. Migrating only databases and keeping applications on-premises may lead to performance issues and doesn't fully leverage cloud benefits.</p><p><br></p><p>Start by moving the largest data sets first to reduce complexity. -&gt; Incorrect. Moving the largest data sets first can be more complex and risky. It's better to start small and scale up.</p>",
                "answers": [
                    "<p>Conduct a pilot migration with a small, non-critical system before fully migrating.</p>",
                    "<p>Migrate all systems simultaneously to minimize total migration time.</p>",
                    "<p>Move only the databases to GCP and keep the applications on-premises.</p>",
                    "<p>Start by moving the largest data sets first to reduce complexity.</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "You are a cloud architect tasked with planning the migration of an enterprise's legacy systems to Google Cloud Platform (GCP). The enterprise has a mix of on-premises databases, applications, and storage systems that need to be moved to the cloud. Your plan needs to minimize downtime, ensure data integrity, and provide scalability. What would be the most effective migration plan for this scenario?",
            "related_lectures": []
        },
        {
            "_class": "assessment",
            "id": 78430266,
            "assessment_type": "multiple-choice",
            "prompt": {
                "question": "<p>As a cloud architect, you are designing a solution for a client who wants to leverage Google Cloud SQL for their relational database needs. The client's application requires full transaction support with multi-version concurrency control, along with procedural languages like PL/pgSQL. Which database engine should you recommend for Cloud SQL?</p>",
                "relatedLectureIds": "",
                "feedbacks": [
                    "",
                    "",
                    "",
                    ""
                ],
                "explanation": "<p>PostgreSQL -&gt; Correct. PostgreSQL is a database engine supported by Cloud SQL that offers full transaction support with multi-version concurrency control. It also supports procedural languages like PL/pgSQL, meeting the client's requirements.</p><p><br></p><p>Microsoft SQL Server -&gt; Incorrect. Microsoft SQL Server is a database engine supported by Cloud SQL, but it does not support procedural languages like PL/pgSQL.</p><p><br></p><p>MySQL -&gt;&nbsp;Incorrect. MySQL supports full transaction capabilities but uses a different procedural language, SQL/PSM, which might not meet the client's requirements for PL/pgSQL.</p><p><br></p><p>Oracle Database -&gt; Incorrect. Oracle Database is not a supported database engine on Cloud SQL.</p>",
                "answers": [
                    "<p>PostgreSQL</p>",
                    "<p>Microsoft SQL Server</p>",
                    "<p>MySQL</p>",
                    "<p>Oracle Database</p>"
                ]
            },
            "correct_response": [
                "a"
            ],
            "section": "",
            "question_plain": "As a cloud architect, you are designing a solution for a client who wants to leverage Google Cloud SQL for their relational database needs. The client's application requires full transaction support with multi-version concurrency control, along with procedural languages like PL/pgSQL. Which database engine should you recommend for Cloud SQL?",
            "related_lectures": []
        }
    ]
}